{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eli/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"cooldown\": 25,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 566084.250000\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: 269026.312500\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -83548.476562\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -78159.734375\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -163937.546875\n",
      "    epoch          : 1\n",
      "    loss           : 12142.515453376393\n",
      "    val_loss       : -139657.79001367092\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -204400.906250\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -53952.117188\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -76570.632812\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -378142.875000\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -232445.468750\n",
      "    epoch          : 2\n",
      "    loss           : -168192.0289487933\n",
      "    val_loss       : -211994.4120499611\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -424685.625000\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -195531.046875\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -166918.546875\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -237540.125000\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -247133.187500\n",
      "    epoch          : 3\n",
      "    loss           : -256899.3961362933\n",
      "    val_loss       : -268437.89108092786\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -455294.093750\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -363736.281250\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -209273.703125\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -368469.437500\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -317951.718750\n",
      "    epoch          : 4\n",
      "    loss           : -291745.6597694926\n",
      "    val_loss       : -331628.0428287744\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -535588.687500\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -244078.937500\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -152852.062500\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -487298.281250\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -323371.968750\n",
      "    epoch          : 5\n",
      "    loss           : -351907.0843904703\n",
      "    val_loss       : -342274.12167716026\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -548463.562500\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -108555.210938\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -343113.875000\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -273260.343750\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -381267.437500\n",
      "    epoch          : 6\n",
      "    loss           : -361777.5785891089\n",
      "    val_loss       : -385261.5023917675\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -609124.250000\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -328875.125000\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -124172.234375\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -434270.812500\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -439363.656250\n",
      "    epoch          : 7\n",
      "    loss           : -383658.66127784655\n",
      "    val_loss       : -383331.9753760815\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -597505.937500\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -433747.125000\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -166612.703125\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -584694.250000\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -395955.343750\n",
      "    epoch          : 8\n",
      "    loss           : -393161.4444616337\n",
      "    val_loss       : -407326.11406793597\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -603963.437500\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -349021.562500\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -430666.406250\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -229039.750000\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -479790.312500\n",
      "    epoch          : 9\n",
      "    loss           : -421274.23839727725\n",
      "    val_loss       : -395186.44909596443\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -589673.375000\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -452857.781250\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -419150.250000\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -485592.625000\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -429273.437500\n",
      "    epoch          : 10\n",
      "    loss           : -423396.02862004953\n",
      "    val_loss       : -443347.0441450119\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -628554.875000\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -376105.531250\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -488034.937500\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -463793.968750\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -408944.906250\n",
      "    epoch          : 11\n",
      "    loss           : -441400.3819616337\n",
      "    val_loss       : -426253.902129364\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -596624.687500\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -393838.250000\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -351521.875000\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -391015.250000\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -409306.250000\n",
      "    epoch          : 12\n",
      "    loss           : -423562.72370049503\n",
      "    val_loss       : -411818.4468089581\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -565656.250000\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -395194.156250\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -386950.250000\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -450661.968750\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -439500.000000\n",
      "    epoch          : 13\n",
      "    loss           : -454385.25386757427\n",
      "    val_loss       : -456039.82877440454\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -629963.187500\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -429732.812500\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -416499.687500\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -340832.437500\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -582128.500000\n",
      "    epoch          : 14\n",
      "    loss           : -489300.67110148515\n",
      "    val_loss       : -506130.8763537407\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -668217.187500\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -460096.281250\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -525152.187500\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -551763.250000\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -690979.000000\n",
      "    epoch          : 15\n",
      "    loss           : -564770.4446163366\n",
      "    val_loss       : -635550.2057374954\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -782229.000000\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -640131.437500\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -640251.250000\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -689707.000000\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -707670.187500\n",
      "    epoch          : 16\n",
      "    loss           : -676484.8224009901\n",
      "    val_loss       : -690765.9773288727\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -853749.125000\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -616705.375000\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -691079.750000\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -761290.500000\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -656766.500000\n",
      "    epoch          : 17\n",
      "    loss           : -690119.7599009901\n",
      "    val_loss       : -663638.3593881608\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -794422.375000\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -675055.937500\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -595257.500000\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -597655.250000\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -687469.500000\n",
      "    epoch          : 18\n",
      "    loss           : -680575.0680693069\n",
      "    val_loss       : -694931.9128772735\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -729084.125000\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -672837.750000\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -689522.500000\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -693396.500000\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -696882.187500\n",
      "    epoch          : 19\n",
      "    loss           : -698968.7821782178\n",
      "    val_loss       : -699067.030875969\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -814021.937500\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -709270.375000\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -711655.375000\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -728323.125000\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -726559.625000\n",
      "    epoch          : 20\n",
      "    loss           : -702591.6528465346\n",
      "    val_loss       : -697187.070119381\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -834578.812500\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -688648.687500\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -762541.812500\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -860447.500000\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -684225.125000\n",
      "    epoch          : 21\n",
      "    loss           : -711883.4436881188\n",
      "    val_loss       : -696878.2733773232\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -844645.437500\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -732799.062500\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -743489.875000\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -639065.312500\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -740930.125000\n",
      "    epoch          : 22\n",
      "    loss           : -726611.2698019802\n",
      "    val_loss       : -730502.7230418206\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -882137.937500\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -778631.750000\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -778087.062500\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -663083.000000\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -741899.312500\n",
      "    epoch          : 23\n",
      "    loss           : -730676.5959158416\n",
      "    val_loss       : -728196.8943306922\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -850497.875000\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -708974.500000\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -747462.875000\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -740625.812500\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -721432.750000\n",
      "    epoch          : 24\n",
      "    loss           : -733667.6584158416\n",
      "    val_loss       : -735312.7867604255\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -871989.812500\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -759874.937500\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -714020.875000\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -785048.000000\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -734284.750000\n",
      "    epoch          : 25\n",
      "    loss           : -738095.9857673268\n",
      "    val_loss       : -744010.4257516861\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -893055.687500\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -713279.312500\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -656023.187500\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -897528.687500\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -751893.312500\n",
      "    epoch          : 26\n",
      "    loss           : -741702.9993811881\n",
      "    val_loss       : -739811.2012946128\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -899483.312500\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -714451.250000\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -753988.187500\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -673115.437500\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -733955.187500\n",
      "    epoch          : 27\n",
      "    loss           : -749721.4053217822\n",
      "    val_loss       : -752216.9364778518\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -773724.562500\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -717120.375000\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -678801.187500\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -900018.500000\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -766143.562500\n",
      "    epoch          : 28\n",
      "    loss           : -754716.1398514851\n",
      "    val_loss       : -756038.8463918685\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -893470.562500\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -722052.125000\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -688574.875000\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -910568.125000\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -764216.062500\n",
      "    epoch          : 29\n",
      "    loss           : -757148.0748762377\n",
      "    val_loss       : -761013.7138376236\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -724257.437500\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -786220.250000\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -693318.312500\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -685144.625000\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -759506.875000\n",
      "    epoch          : 30\n",
      "    loss           : -757688.7790841584\n",
      "    val_loss       : -754720.2239624977\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -922933.250000\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -717301.562500\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -690009.187500\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -768187.250000\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -766985.062500\n",
      "    epoch          : 31\n",
      "    loss           : -758725.8465346535\n",
      "    val_loss       : -762335.8006311416\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -807981.125000\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -757005.750000\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -697055.062500\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -814610.562500\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -743334.937500\n",
      "    epoch          : 32\n",
      "    loss           : -762411.2388613861\n",
      "    val_loss       : -765447.6259234429\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -928621.125000\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -701258.875000\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -753083.562500\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -742720.125000\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -693018.687500\n",
      "    epoch          : 33\n",
      "    loss           : -759112.6113861386\n",
      "    val_loss       : -719511.4244503021\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -650505.625000\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -675150.437500\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -675948.250000\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -680630.687500\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -743152.625000\n",
      "    epoch          : 34\n",
      "    loss           : -743783.1243811881\n",
      "    val_loss       : -755765.4002674103\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -912084.750000\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -721552.000000\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -746537.125000\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -735408.375000\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -694172.125000\n",
      "    epoch          : 35\n",
      "    loss           : -760934.8737623763\n",
      "    val_loss       : -753867.7732420921\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -924427.375000\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -733541.625000\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -804430.250000\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -740184.687500\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -691338.875000\n",
      "    epoch          : 36\n",
      "    loss           : -763500.9455445545\n",
      "    val_loss       : -769619.4295446395\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -933002.312500\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -810992.125000\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -704664.437500\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -735930.125000\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -772639.687500\n",
      "    epoch          : 37\n",
      "    loss           : -771477.4678217822\n",
      "    val_loss       : -767076.9362876893\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -929445.500000\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -806283.812500\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -737995.875000\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -770522.375000\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -775682.250000\n",
      "    epoch          : 38\n",
      "    loss           : -772381.020420792\n",
      "    val_loss       : -772119.635677433\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -818902.312500\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -734683.437500\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -700269.312500\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -776618.312500\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -746948.750000\n",
      "    epoch          : 39\n",
      "    loss           : -778305.3805693069\n",
      "    val_loss       : -777252.3638594628\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -947425.875000\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -729243.375000\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -771215.312500\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -772507.937500\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -742147.250000\n",
      "    epoch          : 40\n",
      "    loss           : -773528.780940594\n",
      "    val_loss       : -779372.0835921287\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -943169.500000\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -703236.062500\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -778013.625000\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -750368.250000\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -773631.250000\n",
      "    epoch          : 41\n",
      "    loss           : -782464.0594059406\n",
      "    val_loss       : -782938.7967334747\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -751241.500000\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -709474.187500\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -750988.125000\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -825663.125000\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -778898.500000\n",
      "    epoch          : 42\n",
      "    loss           : -781465.1918316832\n",
      "    val_loss       : -786828.683785057\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -952751.750000\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -648747.937500\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -812930.000000\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -943523.875000\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -776044.062500\n",
      "    epoch          : 43\n",
      "    loss           : -767492.7363861386\n",
      "    val_loss       : -740552.5147669793\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -934621.250000\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -726737.437500\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -687363.750000\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -763632.000000\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -746677.937500\n",
      "    epoch          : 44\n",
      "    loss           : -758179.228960396\n",
      "    val_loss       : -774798.5341804981\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -938477.500000\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -822062.812500\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -692083.562500\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -820164.875000\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -786399.750000\n",
      "    epoch          : 45\n",
      "    loss           : -780947.5365099009\n",
      "    val_loss       : -784032.3165247918\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -953167.125000\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -827626.937500\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -821044.187500\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -824099.250000\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -754811.750000\n",
      "    epoch          : 46\n",
      "    loss           : -782417.385519802\n",
      "    val_loss       : -785927.2695059776\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -834711.250000\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -749420.000000\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -745810.000000\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -714950.250000\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -780275.062500\n",
      "    epoch          : 47\n",
      "    loss           : -777950.0724009901\n",
      "    val_loss       : -782240.0179856301\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -944477.625000\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -778925.375000\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -715101.375000\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -787867.875000\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -832015.687500\n",
      "    epoch          : 48\n",
      "    loss           : -789099.9455445545\n",
      "    val_loss       : -788708.0695489884\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -756489.500000\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -760127.375000\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -752658.000000\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -945294.812500\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -777981.000000\n",
      "    epoch          : 49\n",
      "    loss           : -790109.7246287129\n",
      "    val_loss       : -788595.4664600373\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -948438.562500\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -830304.125000\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -715761.875000\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -727382.500000\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -764929.875000\n",
      "    epoch          : 50\n",
      "    loss           : -792546.7172029703\n",
      "    val_loss       : -790313.8128137589\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0325_145359/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -767359.187500\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -756166.250000\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -782307.875000\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -691619.875000\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -773340.312500\n",
      "    epoch          : 51\n",
      "    loss           : -770253.5655940594\n",
      "    val_loss       : -758679.3586705208\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -788349.250000\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -731557.750000\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -753654.750000\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -702000.750000\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -775551.062500\n",
      "    epoch          : 52\n",
      "    loss           : -773899.9424504951\n",
      "    val_loss       : -774966.5115345955\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -780054.500000\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -741053.687500\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -718742.375000\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -785004.562500\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -831215.125000\n",
      "    epoch          : 53\n",
      "    loss           : -783333.9727722772\n",
      "    val_loss       : -786134.1603955269\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -936049.812500\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -754142.625000\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -725073.062500\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -944226.562500\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -754503.000000\n",
      "    epoch          : 54\n",
      "    loss           : -791773.7790841584\n",
      "    val_loss       : -788673.4335061073\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -947839.687500\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -812706.875000\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -713503.000000\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -748475.312500\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -746343.250000\n",
      "    epoch          : 55\n",
      "    loss           : -779996.4752475248\n",
      "    val_loss       : -787047.793132019\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -840087.625000\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -758760.750000\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -813261.500000\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -787852.437500\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -788853.375000\n",
      "    epoch          : 56\n",
      "    loss           : -788993.9548267326\n",
      "    val_loss       : -763044.5463188172\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -938109.937500\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -828654.937500\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -822237.625000\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -791840.125000\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -791595.562500\n",
      "    epoch          : 57\n",
      "    loss           : -786445.1478960396\n",
      "    val_loss       : -792368.4000696659\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -947614.750000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -840549.000000\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -721661.437500\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -730340.625000\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -795817.625000\n",
      "    epoch          : 58\n",
      "    loss           : -792776.4900990099\n",
      "    val_loss       : -782297.0806248665\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -946598.625000\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -762564.937500\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -700005.125000\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -945622.250000\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -789489.250000\n",
      "    epoch          : 59\n",
      "    loss           : -790930.3842821782\n",
      "    val_loss       : -794914.3574324608\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -830822.000000\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -851433.000000\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -736492.625000\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -756621.500000\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -774854.812500\n",
      "    epoch          : 60\n",
      "    loss           : -789241.7945544554\n",
      "    val_loss       : -784724.6544937134\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -947838.000000\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -733950.187500\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -734579.375000\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -748881.125000\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -693948.500000\n",
      "    epoch          : 61\n",
      "    loss           : -780579.0612623763\n",
      "    val_loss       : -775743.2342329025\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -940415.750000\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -838449.937500\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -762943.250000\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -778545.250000\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -767175.000000\n",
      "    epoch          : 62\n",
      "    loss           : -787312.5037128713\n",
      "    val_loss       : -791948.6821166992\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -955129.125000\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -726652.937500\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -761927.125000\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -794946.812500\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -788366.187500\n",
      "    epoch          : 63\n",
      "    loss           : -797561.1287128713\n",
      "    val_loss       : -797666.5150892257\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -852264.750000\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -764258.062500\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -764701.375000\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -955394.437500\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -798587.312500\n",
      "    epoch          : 64\n",
      "    loss           : -795673.9783415842\n",
      "    val_loss       : -797169.4411188125\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -958325.250000\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -763035.875000\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -724487.687500\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -960915.562500\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -795736.000000\n",
      "    epoch          : 65\n",
      "    loss           : -798191.9752475248\n",
      "    val_loss       : -802220.9512052536\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -856039.375000\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -727582.312500\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -838248.500000\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -743893.875000\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -751451.687500\n",
      "    epoch          : 66\n",
      "    loss           : -802262.7648514851\n",
      "    val_loss       : -794518.9336502075\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -948210.750000\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -843061.562500\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -731665.125000\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -789650.125000\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -802451.125000\n",
      "    epoch          : 67\n",
      "    loss           : -799813.6621287129\n",
      "    val_loss       : -802423.6231367111\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -958221.875000\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -849048.187500\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -791858.187500\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -764129.562500\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -798325.062500\n",
      "    epoch          : 68\n",
      "    loss           : -803159.4214108911\n",
      "    val_loss       : -805326.028981495\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -960796.500000\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -769652.062500\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -757474.187500\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -729412.375000\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -768115.125000\n",
      "    epoch          : 69\n",
      "    loss           : -802616.0909653465\n",
      "    val_loss       : -794949.1642760277\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -959976.562500\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -774725.000000\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -735683.687500\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -760550.750000\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -760134.875000\n",
      "    epoch          : 70\n",
      "    loss           : -795467.1924504951\n",
      "    val_loss       : -793709.6766600609\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -766076.125000\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -769057.687500\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -835593.125000\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -749621.125000\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -771188.875000\n",
      "    epoch          : 71\n",
      "    loss           : -796068.1441831683\n",
      "    val_loss       : -789887.9303915023\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -724546.250000\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -721621.375000\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -841133.500000\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -747201.500000\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -769929.250000\n",
      "    epoch          : 72\n",
      "    loss           : -804363.1206683168\n",
      "    val_loss       : -805409.9247215271\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -956660.062500\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -775362.937500\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -832716.125000\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -843428.750000\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -800981.062500\n",
      "    epoch          : 73\n",
      "    loss           : -807717.510519802\n",
      "    val_loss       : -785879.4662455559\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -954058.000000\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -747695.875000\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -738517.187500\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -776855.500000\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -764717.812500\n",
      "    epoch          : 74\n",
      "    loss           : -793294.7264851485\n",
      "    val_loss       : -797470.7415269852\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -763798.812500\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -724481.625000\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -735931.312500\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -737022.875000\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -779366.875000\n",
      "    epoch          : 75\n",
      "    loss           : -797744.6349009901\n",
      "    val_loss       : -798777.3954101562\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -961050.125000\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -769870.687500\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -844379.000000\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -800892.875000\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -796327.125000\n",
      "    epoch          : 76\n",
      "    loss           : -805821.8948019802\n",
      "    val_loss       : -807450.7734808922\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -965152.562500\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -731800.000000\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -752622.312500\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -844629.000000\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -790539.000000\n",
      "    epoch          : 77\n",
      "    loss           : -808281.4622524752\n",
      "    val_loss       : -795874.2321178436\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -963529.375000\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -849450.875000\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -724223.750000\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -969613.375000\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -799800.562500\n",
      "    epoch          : 78\n",
      "    loss           : -807701.9647277228\n",
      "    val_loss       : -811577.7686626434\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -970995.687500\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -780344.312500\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -800100.000000\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -764812.437500\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -809497.875000\n",
      "    epoch          : 79\n",
      "    loss           : -814840.9108910891\n",
      "    val_loss       : -813933.7359553337\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -970963.500000\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -733365.625000\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -764995.500000\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -807857.687500\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -806592.125000\n",
      "    epoch          : 80\n",
      "    loss           : -815617.9121287129\n",
      "    val_loss       : -812686.1687332153\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -969504.875000\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -765534.500000\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -806015.875000\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -778342.687500\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -805614.000000\n",
      "    epoch          : 81\n",
      "    loss           : -814385.8143564357\n",
      "    val_loss       : -813895.3656416893\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -967239.625000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -782430.375000\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -810662.375000\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -802575.062500\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -782521.250000\n",
      "    epoch          : 82\n",
      "    loss           : -815100.7048267326\n",
      "    val_loss       : -808639.0443256855\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -966080.375000\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -867739.687500\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -736610.312500\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -816012.125000\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -803568.750000\n",
      "    epoch          : 83\n",
      "    loss           : -815848.4300742574\n",
      "    val_loss       : -817024.4248251438\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -970087.625000\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -784771.000000\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -773253.125000\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -766447.000000\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -777273.812500\n",
      "    epoch          : 84\n",
      "    loss           : -815362.405940594\n",
      "    val_loss       : -815919.4476839065\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -971213.875000\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -788465.750000\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -846111.125000\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -745059.187500\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -800042.812500\n",
      "    epoch          : 85\n",
      "    loss           : -812486.1256188119\n",
      "    val_loss       : -813748.0918232917\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -971835.250000\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -787663.250000\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -808044.687500\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -766804.625000\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -809853.250000\n",
      "    epoch          : 86\n",
      "    loss           : -817799.9554455446\n",
      "    val_loss       : -817296.3690406323\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -972141.125000\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -788209.750000\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -768635.250000\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -961124.437500\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -797241.625000\n",
      "    epoch          : 87\n",
      "    loss           : -815897.9845297029\n",
      "    val_loss       : -815030.8822268486\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -969567.625000\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -789140.000000\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -783215.312500\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -774281.937500\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -762239.000000\n",
      "    epoch          : 88\n",
      "    loss           : -801639.9511138614\n",
      "    val_loss       : -778359.8652044773\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -759067.687500\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -749598.687500\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -845806.500000\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -808292.812500\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -814197.750000\n",
      "    epoch          : 89\n",
      "    loss           : -806364.1571782178\n",
      "    val_loss       : -806364.4625696182\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -958008.500000\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -845677.375000\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -855475.250000\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -774119.000000\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -775752.250000\n",
      "    epoch          : 90\n",
      "    loss           : -814203.2530940594\n",
      "    val_loss       : -815552.2489481926\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -970496.875000\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -852729.250000\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -856543.562500\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -778769.750000\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -810500.062500\n",
      "    epoch          : 91\n",
      "    loss           : -820282.7821782178\n",
      "    val_loss       : -809269.2686879158\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -958884.125000\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -829612.500000\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -761926.187500\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -761595.125000\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -782446.312500\n",
      "    epoch          : 92\n",
      "    loss           : -799365.7667079208\n",
      "    val_loss       : -790482.0763860464\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -957358.500000\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -732138.125000\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -836117.375000\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -799126.062500\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -794541.625000\n",
      "    epoch          : 93\n",
      "    loss           : -809182.8663366337\n",
      "    val_loss       : -808782.9992492676\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -775326.000000\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -837775.125000\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -838204.625000\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -765958.750000\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -776449.750000\n",
      "    epoch          : 94\n",
      "    loss           : -813866.2196782178\n",
      "    val_loss       : -812837.6193587303\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -969052.875000\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -789515.125000\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -848757.125000\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -824871.875000\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -781428.125000\n",
      "    epoch          : 95\n",
      "    loss           : -818156.551980198\n",
      "    val_loss       : -820088.9367987395\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -870235.000000\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -823285.000000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -776507.250000\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -787285.062500\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -808238.000000\n",
      "    epoch          : 96\n",
      "    loss           : -821836.5290841584\n",
      "    val_loss       : -820355.5080191136\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -970510.125000\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -784408.875000\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -734625.500000\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -836512.000000\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -802696.125000\n",
      "    epoch          : 97\n",
      "    loss           : -813533.8273514851\n",
      "    val_loss       : -813400.0055308342\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -867129.500000\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -789544.000000\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -849750.125000\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -774399.625000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -820684.000000\n",
      "    epoch          : 98\n",
      "    loss           : -820430.645420792\n",
      "    val_loss       : -821207.9729905963\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -969041.375000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -788534.750000\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -777207.312500\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -781907.750000\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -764280.875000\n",
      "    epoch          : 99\n",
      "    loss           : -818501.707920792\n",
      "    val_loss       : -795570.4312054634\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -837544.250000\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -859413.062500\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -738629.312500\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -962657.625000\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -816514.312500\n",
      "    epoch          : 100\n",
      "    loss           : -813785.0129950495\n",
      "    val_loss       : -818613.6200361252\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0325_145359/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -970268.500000\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -770395.875000\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -818422.000000\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -749144.625000\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -782246.875000\n",
      "    epoch          : 101\n",
      "    loss           : -820732.0928217822\n",
      "    val_loss       : -821343.7025975466\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -968302.062500\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -793132.125000\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -778055.875000\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -817218.437500\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -778573.375000\n",
      "    epoch          : 102\n",
      "    loss           : -822129.2456683168\n",
      "    val_loss       : -817509.6112894773\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -965629.125000\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -791468.875000\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -820803.625000\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -849550.625000\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -797189.562500\n",
      "    epoch          : 103\n",
      "    loss           : -820253.426980198\n",
      "    val_loss       : -819484.6337720274\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -970930.625000\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -870600.812500\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -752752.625000\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -806709.000000\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -815323.500000\n",
      "    epoch          : 104\n",
      "    loss           : -820364.0272277228\n",
      "    val_loss       : -811453.8957107544\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -966791.000000\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -764427.437500\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -757359.750000\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -768382.375000\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -813118.000000\n",
      "    epoch          : 105\n",
      "    loss           : -821338.2475247525\n",
      "    val_loss       : -819908.5113235951\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -970055.000000\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -793585.562500\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -754852.625000\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -844151.187500\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -778617.375000\n",
      "    epoch          : 106\n",
      "    loss           : -821186.5946782178\n",
      "    val_loss       : -811647.0347758413\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -967571.250000\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -870250.187500\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -737594.500000\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -773122.937500\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -806295.562500\n",
      "    epoch          : 107\n",
      "    loss           : -806677.457920792\n",
      "    val_loss       : -810841.6709197998\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -869775.000000\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -788373.500000\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -768986.500000\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -765600.937500\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -794412.000000\n",
      "    epoch          : 108\n",
      "    loss           : -809146.8087871287\n",
      "    val_loss       : -811104.600732398\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -970271.250000\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -862583.937500\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -849969.687500\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -808376.750000\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -809550.750000\n",
      "    epoch          : 109\n",
      "    loss           : -820379.7877475248\n",
      "    val_loss       : -822565.0482311368\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -972811.812500\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -876481.375000\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -785918.437500\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -848747.562500\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -843741.750000\n",
      "    epoch          : 110\n",
      "    loss           : -822885.6646039604\n",
      "    val_loss       : -820546.6485747218\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -973799.125000\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -788942.312500\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -785925.562500\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -782169.500000\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -813274.812500\n",
      "    epoch          : 111\n",
      "    loss           : -822552.9102722772\n",
      "    val_loss       : -818887.3047150135\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -760039.812500\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -759260.812500\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -778558.562500\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -952452.812500\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -770158.312500\n",
      "    epoch          : 112\n",
      "    loss           : -814487.8106435643\n",
      "    val_loss       : -813103.8405532718\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -971588.125000\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -760821.750000\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -764088.687500\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -811747.875000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -813829.500000\n",
      "    epoch          : 113\n",
      "    loss           : -813483.4300742574\n",
      "    val_loss       : -816441.4898824214\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -870729.625000\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -778475.250000\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -767029.375000\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -961898.875000\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -774710.875000\n",
      "    epoch          : 114\n",
      "    loss           : -813601.7629950495\n",
      "    val_loss       : -812708.9270346283\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -967902.437500\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -780794.937500\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -838004.250000\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -779144.687500\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -812382.750000\n",
      "    epoch          : 115\n",
      "    loss           : -818463.9882425743\n",
      "    val_loss       : -814392.8747869373\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -973312.375000\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -871668.625000\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -752685.375000\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -855176.937500\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -816424.750000\n",
      "    epoch          : 116\n",
      "    loss           : -817731.3508663366\n",
      "    val_loss       : -818268.3366919637\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -970718.625000\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -748816.375000\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -771693.875000\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -772069.750000\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -808535.125000\n",
      "    epoch          : 117\n",
      "    loss           : -818955.4764851485\n",
      "    val_loss       : -821943.9220876455\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -873621.750000\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -786883.500000\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -822639.125000\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -775860.562500\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -815781.125000\n",
      "    epoch          : 118\n",
      "    loss           : -824954.0711633663\n",
      "    val_loss       : -824746.2664863825\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -978375.437500\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -851890.687500\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -842033.812500\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -849446.625000\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -823378.875000\n",
      "    epoch          : 119\n",
      "    loss           : -824907.760519802\n",
      "    val_loss       : -809868.6764186143\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -958493.500000\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -785744.937500\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -781996.250000\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -821930.500000\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -815339.312500\n",
      "    epoch          : 120\n",
      "    loss           : -824968.010519802\n",
      "    val_loss       : -824714.0065272332\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -976223.250000\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -876105.375000\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -786670.125000\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -787245.062500\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -824287.250000\n",
      "    epoch          : 121\n",
      "    loss           : -829332.0612623763\n",
      "    val_loss       : -828857.7931936265\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -983495.687500\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -800215.625000\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -860696.562500\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -751556.000000\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -800726.625000\n",
      "    epoch          : 122\n",
      "    loss           : -823059.6311881188\n",
      "    val_loss       : -814445.4531955719\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -964848.375000\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -774032.875000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -757645.062500\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -806405.375000\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -784592.125000\n",
      "    epoch          : 123\n",
      "    loss           : -825053.478960396\n",
      "    val_loss       : -827848.0667383194\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -981546.187500\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -877468.437500\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -858933.687500\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -981413.500000\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -823026.187500\n",
      "    epoch          : 124\n",
      "    loss           : -831952.2679455446\n",
      "    val_loss       : -826458.9672173262\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -974409.812500\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -795237.375000\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -858497.187500\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -851764.500000\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -787267.937500\n",
      "    epoch          : 125\n",
      "    loss           : -829286.3508663366\n",
      "    val_loss       : -828145.4418082715\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -798429.000000\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -881409.812500\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -820347.625000\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -793790.375000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -793172.125000\n",
      "    epoch          : 126\n",
      "    loss           : -830097.6268564357\n",
      "    val_loss       : -827381.2299348831\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -972342.562500\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -733188.250000\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -781690.500000\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -795020.750000\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -795391.250000\n",
      "    epoch          : 127\n",
      "    loss           : -806905.4356435643\n",
      "    val_loss       : -817661.2748213292\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -976592.250000\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -767751.562500\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -806650.625000\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -781315.375000\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -798445.375000\n",
      "    epoch          : 128\n",
      "    loss           : -823868.9362623763\n",
      "    val_loss       : -824204.8624466419\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -980171.625000\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -883844.375000\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -765294.500000\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -806164.500000\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -812879.812500\n",
      "    epoch          : 129\n",
      "    loss           : -827667.3366336634\n",
      "    val_loss       : -829006.5871724129\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -978769.812500\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -751405.125000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -825927.000000\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -854792.125000\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -828402.562500\n",
      "    epoch          : 130\n",
      "    loss           : -826271.2685643565\n",
      "    val_loss       : -822098.0171381235\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -980188.562500\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -878664.375000\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -787513.000000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -817459.125000\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -823201.062500\n",
      "    epoch          : 131\n",
      "    loss           : -826273.9628712871\n",
      "    val_loss       : -827958.5337232113\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -878074.250000\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -788981.625000\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -765806.625000\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -790846.250000\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -784361.750000\n",
      "    epoch          : 132\n",
      "    loss           : -832726.0358910891\n",
      "    val_loss       : -833525.2287893057\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -984842.000000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -763553.750000\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -859480.625000\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -841418.312500\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -784937.312500\n",
      "    epoch          : 133\n",
      "    loss           : -828513.5810643565\n",
      "    val_loss       : -821707.5590083122\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -977623.000000\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -800928.187500\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -855109.625000\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -830044.562500\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -809806.812500\n",
      "    epoch          : 134\n",
      "    loss           : -825680.7376237623\n",
      "    val_loss       : -818653.5378798961\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -974530.687500\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -794967.312500\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -794588.687500\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -827478.437500\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -788632.125000\n",
      "    epoch          : 135\n",
      "    loss           : -824939.5897277228\n",
      "    val_loss       : -819974.5778028012\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -911750.750000\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -801781.812500\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -763279.875000\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -811313.562500\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -811806.187500\n",
      "    epoch          : 136\n",
      "    loss           : -828721.9294554455\n",
      "    val_loss       : -832458.4839693547\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -890333.687500\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -796348.437500\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -845802.125000\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -849398.687500\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -802098.562500\n",
      "    epoch          : 137\n",
      "    loss           : -828953.3601485149\n",
      "    val_loss       : -830547.2027534485\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -983167.625000\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -801169.312500\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -771084.437500\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -808482.750000\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -791989.875000\n",
      "    epoch          : 138\n",
      "    loss           : -816451.1324257426\n",
      "    val_loss       : -811148.9271514893\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -950086.125000\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -770429.750000\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -777098.375000\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -810472.375000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -810884.750000\n",
      "    epoch          : 139\n",
      "    loss           : -811822.4950495049\n",
      "    val_loss       : -821765.3719790459\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -961372.250000\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -862183.000000\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -856305.125000\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -818319.437500\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -825278.312500\n",
      "    epoch          : 140\n",
      "    loss           : -826528.8403465346\n",
      "    val_loss       : -828167.8150184632\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -975366.437500\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -871263.625000\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -822628.500000\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -790772.750000\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -829555.562500\n",
      "    epoch          : 141\n",
      "    loss           : -830689.6850247525\n",
      "    val_loss       : -831182.3336763382\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -980956.500000\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -840011.500000\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -814022.500000\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -798847.250000\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -780615.500000\n",
      "    epoch          : 142\n",
      "    loss           : -823472.2902227723\n",
      "    val_loss       : -826797.3120100021\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -977605.000000\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -800770.125000\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -763391.062500\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -809749.875000\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -782087.250000\n",
      "    epoch          : 143\n",
      "    loss           : -831329.4938118812\n",
      "    val_loss       : -830382.0970165252\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -854642.437500\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -831910.125000\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -818004.125000\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -792422.000000\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -790139.687500\n",
      "    epoch          : 144\n",
      "    loss           : -834903.5228960396\n",
      "    val_loss       : -834010.1575525284\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -982147.375000\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -771346.750000\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -803389.125000\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -983493.750000\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -772615.187500\n",
      "    epoch          : 145\n",
      "    loss           : -833831.0847772277\n",
      "    val_loss       : -823978.3882701874\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -977111.125000\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -796328.187500\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -795062.812500\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -985734.250000\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -828029.312500\n",
      "    epoch          : 146\n",
      "    loss           : -833246.8428217822\n",
      "    val_loss       : -828883.6731769561\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -869635.750000\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -768783.500000\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -793536.250000\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -971022.812500\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -817314.812500\n",
      "    epoch          : 147\n",
      "    loss           : -826844.3910891089\n",
      "    val_loss       : -822685.0327952385\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -975716.250000\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -754629.937500\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -863774.000000\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -982110.250000\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -782172.625000\n",
      "    epoch          : 148\n",
      "    loss           : -831055.8081683168\n",
      "    val_loss       : -828005.8834275246\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -764001.375000\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -768140.812500\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -806416.250000\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -983358.250000\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -856986.625000\n",
      "    epoch          : 149\n",
      "    loss           : -833001.8273514851\n",
      "    val_loss       : -828127.8046164513\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -979925.437500\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -827861.125000\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -806614.937500\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -760283.000000\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -810100.937500\n",
      "    epoch          : 150\n",
      "    loss           : -814832.7636138614\n",
      "    val_loss       : -825125.0332155228\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0325_145359/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -976802.875000\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -793450.000000\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -768801.000000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -853016.125000\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -824380.375000\n",
      "    epoch          : 151\n",
      "    loss           : -832746.2011138614\n",
      "    val_loss       : -834435.6978712082\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -984225.875000\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -802875.437500\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -780131.812500\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -793201.125000\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -821152.750000\n",
      "    epoch          : 152\n",
      "    loss           : -837961.2877475248\n",
      "    val_loss       : -836431.8263277054\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -886601.187500\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -841421.875000\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -808691.375000\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -867660.437500\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -774958.187500\n",
      "    epoch          : 153\n",
      "    loss           : -837185.9418316832\n",
      "    val_loss       : -829983.7041410446\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -984856.500000\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -791184.125000\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -797025.500000\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -766928.312500\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -770699.875000\n",
      "    epoch          : 154\n",
      "    loss           : -827617.6813118812\n",
      "    val_loss       : -808969.7182037353\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -976437.687500\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -769806.312500\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -764175.312500\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -816308.625000\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -816811.687500\n",
      "    epoch          : 155\n",
      "    loss           : -821197.0476485149\n",
      "    val_loss       : -825595.7358175277\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -981854.312500\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -798087.312500\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -785708.875000\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -978113.125000\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -826150.125000\n",
      "    epoch          : 156\n",
      "    loss           : -832428.9430693069\n",
      "    val_loss       : -834057.0526522159\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -982293.562500\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -805655.000000\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -800035.437500\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -863279.375000\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -807066.562500\n",
      "    epoch          : 157\n",
      "    loss           : -838714.135519802\n",
      "    val_loss       : -837772.9257148743\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -984891.500000\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -806357.437500\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -773095.062500\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -786333.812500\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -822770.937500\n",
      "    epoch          : 158\n",
      "    loss           : -838109.0556930694\n",
      "    val_loss       : -837764.071227932\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -990134.625000\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -814027.625000\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -829315.125000\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -790054.312500\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -840532.875000\n",
      "    epoch          : 159\n",
      "    loss           : -836280.4504950495\n",
      "    val_loss       : -817649.2671071052\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -788549.875000\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -801566.500000\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -861468.500000\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -813806.375000\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -795669.062500\n",
      "    epoch          : 160\n",
      "    loss           : -833703.9034653465\n",
      "    val_loss       : -837463.023169899\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -808842.000000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -883500.375000\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -777147.625000\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -805651.500000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -797557.750000\n",
      "    epoch          : 161\n",
      "    loss           : -840634.3273514851\n",
      "    val_loss       : -838992.5298453331\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -988193.062500\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -884043.500000\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -863251.250000\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -804069.500000\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -828194.437500\n",
      "    epoch          : 162\n",
      "    loss           : -841414.4071782178\n",
      "    val_loss       : -842106.3361690522\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -991073.875000\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -812921.500000\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -799599.875000\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -869736.812500\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -795862.750000\n",
      "    epoch          : 163\n",
      "    loss           : -845103.9405940594\n",
      "    val_loss       : -843616.4302419663\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -992148.437500\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -770227.375000\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -818069.562500\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -861980.562500\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -838151.437500\n",
      "    epoch          : 164\n",
      "    loss           : -833386.6608910891\n",
      "    val_loss       : -838109.9881453514\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -979824.000000\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -817226.062500\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -808027.625000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -810241.250000\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -794025.375000\n",
      "    epoch          : 165\n",
      "    loss           : -840528.9597772277\n",
      "    val_loss       : -842413.3347044945\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -985667.812500\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -811935.250000\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -811739.312500\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -793607.000000\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -804332.375000\n",
      "    epoch          : 166\n",
      "    loss           : -837269.0872524752\n",
      "    val_loss       : -835982.1123311042\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -872227.375000\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -810364.375000\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -828368.625000\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -812020.687500\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -800846.750000\n",
      "    epoch          : 167\n",
      "    loss           : -841664.0228960396\n",
      "    val_loss       : -838570.9500395774\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -989074.375000\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -814736.375000\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -812502.250000\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -986461.000000\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -846426.000000\n",
      "    epoch          : 168\n",
      "    loss           : -842665.4306930694\n",
      "    val_loss       : -844122.2513703346\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -990202.250000\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -798200.500000\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -834996.437500\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -809902.562500\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -836973.875000\n",
      "    epoch          : 169\n",
      "    loss           : -844345.7951732674\n",
      "    val_loss       : -843331.7532074929\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -990067.500000\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -817765.875000\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -776671.500000\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -774188.000000\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -792977.062500\n",
      "    epoch          : 170\n",
      "    loss           : -846940.6089108911\n",
      "    val_loss       : -844786.1912412643\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -992438.187500\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -814414.125000\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -815513.875000\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -825229.937500\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -843299.250000\n",
      "    epoch          : 171\n",
      "    loss           : -840326.6695544554\n",
      "    val_loss       : -828212.7661174774\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -920946.000000\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -752918.062500\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -754505.500000\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -792461.562500\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -818144.937500\n",
      "    epoch          : 172\n",
      "    loss           : -814832.2586633663\n",
      "    val_loss       : -819430.1057845115\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -939181.937500\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -783549.125000\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -822578.437500\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -791437.312500\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -831142.625000\n",
      "    epoch          : 173\n",
      "    loss           : -824717.6404702971\n",
      "    val_loss       : -826682.2627780915\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -970842.562500\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -856332.187500\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -849377.250000\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -960141.750000\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -811882.875000\n",
      "    epoch          : 174\n",
      "    loss           : -822822.5860148515\n",
      "    val_loss       : -830988.7022921562\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -973411.875000\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -806237.937500\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -861033.812500\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -831604.000000\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -795405.187500\n",
      "    epoch          : 175\n",
      "    loss           : -835399.0235148515\n",
      "    val_loss       : -837908.6109949112\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -876685.875000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -769152.750000\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -860666.625000\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -793339.500000\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -835638.375000\n",
      "    epoch          : 176\n",
      "    loss           : -841228.4040841584\n",
      "    val_loss       : -838045.917798996\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -982559.125000\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -805220.250000\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -829543.125000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -834767.875000\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -831769.875000\n",
      "    epoch          : 177\n",
      "    loss           : -839844.5235148515\n",
      "    val_loss       : -840777.7545909882\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -983297.000000\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -808362.125000\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -776125.375000\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -802606.437500\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -839334.000000\n",
      "    epoch          : 178\n",
      "    loss           : -842082.093440594\n",
      "    val_loss       : -842803.8853737831\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -985497.000000\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -812377.500000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -838090.437500\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -811539.625000\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -816898.250000\n",
      "    epoch          : 179\n",
      "    loss           : -841954.0544554455\n",
      "    val_loss       : -840518.2883418084\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -990905.250000\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -807702.500000\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -811114.125000\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -806096.500000\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -838049.937500\n",
      "    epoch          : 180\n",
      "    loss           : -836417.7648514851\n",
      "    val_loss       : -835396.3143829346\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -981625.687500\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -809076.250000\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -794955.500000\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -987954.375000\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -829412.375000\n",
      "    epoch          : 181\n",
      "    loss           : -841385.6132425743\n",
      "    val_loss       : -836506.3257356167\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -977477.375000\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -831911.812500\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -831766.687500\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -831905.125000\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -836190.812500\n",
      "    epoch          : 182\n",
      "    loss           : -844593.0761138614\n",
      "    val_loss       : -844980.9580881118\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -993032.125000\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -798423.000000\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -779699.250000\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -863694.875000\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -795295.312500\n",
      "    epoch          : 183\n",
      "    loss           : -842428.041460396\n",
      "    val_loss       : -840856.739628315\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -982466.812500\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -806118.062500\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -865779.250000\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -982055.875000\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -825557.000000\n",
      "    epoch          : 184\n",
      "    loss           : -839910.7370049505\n",
      "    val_loss       : -842235.775432682\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -988988.625000\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -807627.500000\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -829638.937500\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -782056.375000\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -818757.625000\n",
      "    epoch          : 185\n",
      "    loss           : -844377.0501237623\n",
      "    val_loss       : -838481.6447899819\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -986288.750000\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -886445.125000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -841566.562500\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -780129.812500\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -827259.062500\n",
      "    epoch          : 186\n",
      "    loss           : -846063.031559406\n",
      "    val_loss       : -845823.3595062256\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -893277.000000\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -817128.125000\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -839518.625000\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -990955.062500\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -833784.062500\n",
      "    epoch          : 187\n",
      "    loss           : -842220.3155940594\n",
      "    val_loss       : -798843.4266586304\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -880920.000000\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -803351.625000\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -823055.250000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -827674.000000\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -815319.312500\n",
      "    epoch          : 188\n",
      "    loss           : -834180.1188118812\n",
      "    val_loss       : -831892.4074742317\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -888059.625000\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -806965.625000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -776745.500000\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -789523.000000\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -771383.625000\n",
      "    epoch          : 189\n",
      "    loss           : -833208.9183168317\n",
      "    val_loss       : -825430.6094215394\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -979187.250000\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -770730.500000\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -827530.250000\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -981805.187500\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -816537.000000\n",
      "    epoch          : 190\n",
      "    loss           : -837852.4207920792\n",
      "    val_loss       : -840078.4505565644\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -988226.375000\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -776714.000000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -807465.062500\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -837444.375000\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -824785.687500\n",
      "    epoch          : 191\n",
      "    loss           : -838312.0952970297\n",
      "    val_loss       : -833727.1913048744\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -953288.875000\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -810296.125000\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -835536.375000\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -972523.750000\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -826830.250000\n",
      "    epoch          : 192\n",
      "    loss           : -839488.2475247525\n",
      "    val_loss       : -843054.5798626185\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -887767.062500\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -888565.625000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -787624.812500\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -987968.687500\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -837740.500000\n",
      "    epoch          : 193\n",
      "    loss           : -848807.1850247525\n",
      "    val_loss       : -848672.8775907516\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -991569.500000\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -806039.125000\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -789019.625000\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -835351.000000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -798646.875000\n",
      "    epoch          : 194\n",
      "    loss           : -838678.7308168317\n",
      "    val_loss       : -843124.1524115562\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -989303.312500\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -824653.250000\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -829928.062500\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -817393.250000\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -783538.500000\n",
      "    epoch          : 195\n",
      "    loss           : -840435.1429455446\n",
      "    val_loss       : -833234.3894536018\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -979548.375000\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -777939.125000\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -842112.125000\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -992582.937500\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -840101.812500\n",
      "    epoch          : 196\n",
      "    loss           : -843563.0321782178\n",
      "    val_loss       : -844636.2687026977\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -992500.312500\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -824273.437500\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -777175.000000\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -971391.937500\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -845060.750000\n",
      "    epoch          : 197\n",
      "    loss           : -833917.6120049505\n",
      "    val_loss       : -838370.0518129349\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -814017.437500\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -817064.375000\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -819704.000000\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -833946.875000\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -821393.250000\n",
      "    epoch          : 198\n",
      "    loss           : -838334.3818069306\n",
      "    val_loss       : -839869.0801447869\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -955981.625000\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -809499.625000\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -793417.125000\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -978110.875000\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -790365.437500\n",
      "    epoch          : 199\n",
      "    loss           : -841176.1639851485\n",
      "    val_loss       : -834832.7682072639\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -961369.750000\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -865376.687500\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -832515.687500\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -865076.750000\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -830478.375000\n",
      "    epoch          : 200\n",
      "    loss           : -844120.8168316832\n",
      "    val_loss       : -824988.9933330535\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0325_145359/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -977056.625000\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -762181.500000\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -772128.437500\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -818745.750000\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -788757.250000\n",
      "    epoch          : 201\n",
      "    loss           : -810775.395420792\n",
      "    val_loss       : -836197.2931562423\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -980279.250000\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -808926.687500\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -865438.000000\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -832013.625000\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -777896.000000\n",
      "    epoch          : 202\n",
      "    loss           : -841105.5909653465\n",
      "    val_loss       : -834572.7849370956\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -986600.250000\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -810328.000000\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -811877.562500\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -871569.125000\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -839455.000000\n",
      "    epoch          : 203\n",
      "    loss           : -843195.9535891089\n",
      "    val_loss       : -846349.6466415406\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -987721.500000\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -819603.250000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -763522.500000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -988146.000000\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -788718.250000\n",
      "    epoch          : 204\n",
      "    loss           : -842553.582920792\n",
      "    val_loss       : -839387.7167352677\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -985546.437500\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -818882.375000\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -835913.250000\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -990087.937500\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -802987.375000\n",
      "    epoch          : 205\n",
      "    loss           : -845845.9628712871\n",
      "    val_loss       : -847051.4454827309\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -993718.625000\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -873929.500000\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -780577.062500\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -993241.562500\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -808086.250000\n",
      "    epoch          : 206\n",
      "    loss           : -847341.0724009901\n",
      "    val_loss       : -838830.3426444053\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -873741.250000\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -821269.250000\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -848156.000000\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -794955.625000\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -835911.687500\n",
      "    epoch          : 207\n",
      "    loss           : -840674.1051980198\n",
      "    val_loss       : -833048.4749423027\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -982174.687500\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -865355.625000\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -783502.187500\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -993813.562500\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -850025.687500\n",
      "    epoch          : 208\n",
      "    loss           : -846051.9622524752\n",
      "    val_loss       : -849252.550007248\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -996039.125000\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -821363.812500\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -800637.500000\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -819119.125000\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -850900.250000\n",
      "    epoch          : 209\n",
      "    loss           : -850994.3465346535\n",
      "    val_loss       : -848465.838158989\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -896181.125000\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -811101.812500\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -804975.375000\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -836503.250000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -849770.750000\n",
      "    epoch          : 210\n",
      "    loss           : -847796.426980198\n",
      "    val_loss       : -844136.5864815712\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -988343.875000\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -868633.375000\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -850545.375000\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -805579.750000\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -851084.875000\n",
      "    epoch          : 211\n",
      "    loss           : -850986.770420792\n",
      "    val_loss       : -850104.5261443139\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -991241.562500\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -814420.250000\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -786318.125000\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -841531.500000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -837995.812500\n",
      "    epoch          : 212\n",
      "    loss           : -847812.2858910891\n",
      "    val_loss       : -850484.2212631225\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -991509.500000\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -875317.625000\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -871852.875000\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -841283.812500\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -876607.562500\n",
      "    epoch          : 213\n",
      "    loss           : -853655.9845297029\n",
      "    val_loss       : -852944.4037926674\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -995274.812500\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -794298.000000\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -829825.250000\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -990966.375000\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -843268.750000\n",
      "    epoch          : 214\n",
      "    loss           : -854828.5241336634\n",
      "    val_loss       : -852922.4725869179\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -824488.937500\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -799650.750000\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -790522.687500\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -823932.500000\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -854053.625000\n",
      "    epoch          : 215\n",
      "    loss           : -854626.031559406\n",
      "    val_loss       : -854527.1073473931\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -997086.562500\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -826494.500000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -835019.687500\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -829144.750000\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -877798.937500\n",
      "    epoch          : 216\n",
      "    loss           : -848235.0643564357\n",
      "    val_loss       : -806333.9064656257\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -980272.437500\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -793089.375000\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -815711.937500\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -862128.875000\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -853292.750000\n",
      "    epoch          : 217\n",
      "    loss           : -838923.0693069306\n",
      "    val_loss       : -848679.6820057869\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -994788.000000\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -852876.250000\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -804587.812500\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -835192.750000\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -798629.750000\n",
      "    epoch          : 218\n",
      "    loss           : -845374.853960396\n",
      "    val_loss       : -845875.5660218239\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -992380.625000\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -826017.000000\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -871800.375000\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -990693.750000\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -807205.500000\n",
      "    epoch          : 219\n",
      "    loss           : -850356.6844059406\n",
      "    val_loss       : -850673.7691123008\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -996412.750000\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -806898.250000\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -774617.125000\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -834818.187500\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -841415.687500\n",
      "    epoch          : 220\n",
      "    loss           : -847585.5167079208\n",
      "    val_loss       : -845502.3428183555\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -988354.437500\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -881526.875000\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -840778.250000\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -875423.937500\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -846773.125000\n",
      "    epoch          : 221\n",
      "    loss           : -851428.2178217822\n",
      "    val_loss       : -850165.7420639992\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -888466.062500\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -867392.375000\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -854814.875000\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -838506.250000\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -847638.562500\n",
      "    epoch          : 222\n",
      "    loss           : -848561.4139851485\n",
      "    val_loss       : -850069.0935780525\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -993458.687500\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -824994.687500\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -825871.187500\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -873953.437500\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -874753.937500\n",
      "    epoch          : 223\n",
      "    loss           : -852666.1076732674\n",
      "    val_loss       : -847839.9672106743\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -898506.437500\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -786005.250000\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -792946.500000\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -837401.125000\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -804868.375000\n",
      "    epoch          : 224\n",
      "    loss           : -851854.9226485149\n",
      "    val_loss       : -849444.6738534927\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -993950.125000\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -857632.937500\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -767026.625000\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -858618.687500\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -837176.500000\n",
      "    epoch          : 225\n",
      "    loss           : -841322.3849009901\n",
      "    val_loss       : -839263.0831377984\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -800603.000000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -894321.812500\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -838867.625000\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -846804.500000\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -794850.250000\n",
      "    epoch          : 226\n",
      "    loss           : -849551.1305693069\n",
      "    val_loss       : -836540.0093891143\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -987790.500000\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -811092.312500\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -839994.125000\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -845641.375000\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -841987.125000\n",
      "    epoch          : 227\n",
      "    loss           : -850070.2308168317\n",
      "    val_loss       : -852551.7888871193\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -996743.687500\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -820963.875000\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -879156.062500\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -846614.375000\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -799807.312500\n",
      "    epoch          : 228\n",
      "    loss           : -854982.9845297029\n",
      "    val_loss       : -848145.2554093361\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -819412.062500\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -820233.312500\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -872576.500000\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -788791.062500\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -839800.687500\n",
      "    epoch          : 229\n",
      "    loss           : -852368.4801980198\n",
      "    val_loss       : -855379.2755108833\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -998181.562500\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -826521.687500\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -832845.125000\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -854707.812500\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -808781.562500\n",
      "    epoch          : 230\n",
      "    loss           : -856729.021039604\n",
      "    val_loss       : -853828.784040451\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -791256.500000\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -830599.687500\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -877257.687500\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -832383.125000\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -844621.937500\n",
      "    epoch          : 231\n",
      "    loss           : -856646.2716584158\n",
      "    val_loss       : -848935.5477126122\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -997223.250000\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -822634.625000\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -851411.875000\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -866477.187500\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -833000.625000\n",
      "    epoch          : 232\n",
      "    loss           : -848251.1751237623\n",
      "    val_loss       : -852115.9824655533\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -994189.500000\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -822896.625000\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -809159.562500\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -876895.062500\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -831638.125000\n",
      "    epoch          : 233\n",
      "    loss           : -850689.2561881188\n",
      "    val_loss       : -838269.7130208969\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -966230.125000\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -818682.187500\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -837375.437500\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -813585.000000\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -856185.250000\n",
      "    epoch          : 234\n",
      "    loss           : -847965.4232673268\n",
      "    val_loss       : -852777.1969669343\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -990707.937500\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -832751.000000\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -853742.000000\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -803605.750000\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -849490.312500\n",
      "    epoch          : 235\n",
      "    loss           : -851456.1924504951\n",
      "    val_loss       : -835706.0629434586\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -992527.187500\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -892973.937500\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -752469.062500\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -834034.750000\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -835731.625000\n",
      "    epoch          : 236\n",
      "    loss           : -841146.3929455446\n",
      "    val_loss       : -842397.2408631325\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -815224.375000\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -805078.750000\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -869187.625000\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -851246.187500\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -844303.250000\n",
      "    epoch          : 237\n",
      "    loss           : -848657.7227722772\n",
      "    val_loss       : -850718.6369249343\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -995740.875000\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -853449.375000\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -771661.062500\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -781026.312500\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -820916.750000\n",
      "    epoch          : 238\n",
      "    loss           : -838399.2654702971\n",
      "    val_loss       : -847568.2269244194\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -994163.437500\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -865683.375000\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -781617.937500\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -986019.875000\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -853445.625000\n",
      "    epoch          : 239\n",
      "    loss           : -848225.781559406\n",
      "    val_loss       : -850448.7296984673\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -993425.625000\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -824588.312500\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -814243.812500\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -807796.312500\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -833237.875000\n",
      "    epoch          : 240\n",
      "    loss           : -850711.8254950495\n",
      "    val_loss       : -851230.166463089\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -989851.500000\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -785729.687500\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -878070.750000\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -831046.687500\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -807411.500000\n",
      "    epoch          : 241\n",
      "    loss           : -855892.1336633663\n",
      "    val_loss       : -850297.3863600731\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -993440.000000\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -894940.687500\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -832078.375000\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -802194.937500\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -835733.125000\n",
      "    epoch          : 242\n",
      "    loss           : -850162.9603960396\n",
      "    val_loss       : -846312.7777366638\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -991055.625000\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -810507.875000\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -802733.187500\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -816911.000000\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -802523.875000\n",
      "    epoch          : 243\n",
      "    loss           : -847540.9028465346\n",
      "    val_loss       : -834716.335835743\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -971289.000000\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -850919.937500\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -783051.687500\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -966143.375000\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -820641.000000\n",
      "    epoch          : 244\n",
      "    loss           : -833297.4832920792\n",
      "    val_loss       : -838314.7306000709\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -985193.000000\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -799128.250000\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -769760.625000\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -803135.687500\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -850210.625000\n",
      "    epoch          : 245\n",
      "    loss           : -845499.0439356435\n",
      "    val_loss       : -849258.1940206528\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -993118.312500\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -821631.500000\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -783967.125000\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -829462.500000\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -800552.750000\n",
      "    epoch          : 246\n",
      "    loss           : -849164.1256188119\n",
      "    val_loss       : -840917.7844628334\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -988202.937500\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -819074.750000\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -820870.625000\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -837875.250000\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -838449.000000\n",
      "    epoch          : 247\n",
      "    loss           : -850435.926980198\n",
      "    val_loss       : -849431.3920161247\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -986632.562500\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -819076.250000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -801835.375000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -849263.812500\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -879254.687500\n",
      "    epoch          : 248\n",
      "    loss           : -848060.8360148515\n",
      "    val_loss       : -851838.2819488526\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -991454.125000\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -823056.500000\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -836811.750000\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -870965.687500\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -839504.312500\n",
      "    epoch          : 249\n",
      "    loss           : -847855.7277227723\n",
      "    val_loss       : -850099.0746301651\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -993955.562500\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -906229.750000\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -838358.375000\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -855564.562500\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -845976.687500\n",
      "    epoch          : 250\n",
      "    loss           : -854446.7128712871\n",
      "    val_loss       : -854519.0442997932\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0325_145359/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -992549.437500\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -841112.625000\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -796244.687500\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -797941.250000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -807693.625000\n",
      "    epoch          : 251\n",
      "    loss           : -852681.9634900991\n",
      "    val_loss       : -852613.8183979035\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -989724.125000\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -818115.250000\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -884378.500000\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -844811.500000\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -803026.750000\n",
      "    epoch          : 252\n",
      "    loss           : -856529.7382425743\n",
      "    val_loss       : -858341.5440943718\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -999144.625000\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -825439.062500\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -826824.687500\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -993281.375000\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -841833.062500\n",
      "    epoch          : 253\n",
      "    loss           : -851301.2753712871\n",
      "    val_loss       : -851555.7430298806\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -995928.062500\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -820155.125000\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -859914.750000\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -840756.187500\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -833887.187500\n",
      "    epoch          : 254\n",
      "    loss           : -855227.4826732674\n",
      "    val_loss       : -856226.8071826935\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -996468.500000\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -826065.437500\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -847262.812500\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -869467.375000\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -807400.812500\n",
      "    epoch          : 255\n",
      "    loss           : -854614.2871287129\n",
      "    val_loss       : -839081.8352332115\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -897087.312500\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -806151.312500\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -814330.250000\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -994320.562500\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -849895.562500\n",
      "    epoch          : 256\n",
      "    loss           : -851656.145420792\n",
      "    val_loss       : -854912.2340655327\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -998142.312500\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -826245.375000\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -801169.250000\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -848980.750000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -808674.062500\n",
      "    epoch          : 257\n",
      "    loss           : -857513.9016089109\n",
      "    val_loss       : -853132.3207164764\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -997156.500000\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -806660.250000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -806450.625000\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -810186.062500\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -805511.000000\n",
      "    epoch          : 258\n",
      "    loss           : -857878.3873762377\n",
      "    val_loss       : -857593.7655779838\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -996239.375000\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -817688.125000\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -853797.187500\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -885642.125000\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -845565.625000\n",
      "    epoch          : 259\n",
      "    loss           : -858822.6825495049\n",
      "    val_loss       : -858073.6481865883\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -901972.125000\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -813889.125000\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -783831.437500\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -877145.500000\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -848184.875000\n",
      "    epoch          : 260\n",
      "    loss           : -854749.8849009901\n",
      "    val_loss       : -857114.2489775658\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -998600.000000\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -855778.312500\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -856195.437500\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -848649.375000\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -853342.750000\n",
      "    epoch          : 261\n",
      "    loss           : -859994.1305693069\n",
      "    val_loss       : -860344.3807907104\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -1002000.375000\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -800696.062500\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -826734.437500\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -873735.687500\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -807529.437500\n",
      "    epoch          : 262\n",
      "    loss           : -855805.5884900991\n",
      "    val_loss       : -856230.7845797539\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -985200.187500\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -832259.000000\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -841238.187500\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -862458.000000\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -811545.625000\n",
      "    epoch          : 263\n",
      "    loss           : -860427.8180693069\n",
      "    val_loss       : -859632.8666620255\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -999499.562500\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -809608.750000\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -826618.750000\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -886610.375000\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -852234.500000\n",
      "    epoch          : 264\n",
      "    loss           : -861545.1547029703\n",
      "    val_loss       : -859251.0530775071\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -998713.562500\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -825997.062500\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -876512.625000\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -878358.250000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -862771.000000\n",
      "    epoch          : 265\n",
      "    loss           : -856178.3892326732\n",
      "    val_loss       : -851677.80343647\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -905767.062500\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -868451.500000\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -859606.062500\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -838632.375000\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -856981.187500\n",
      "    epoch          : 266\n",
      "    loss           : -855591.729579208\n",
      "    val_loss       : -853941.2025893212\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -958515.625000\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -907524.250000\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -877799.250000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -825262.625000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -802765.812500\n",
      "    epoch          : 267\n",
      "    loss           : -853569.5142326732\n",
      "    val_loss       : -854727.855318737\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -876911.250000\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -877124.937500\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -787922.437500\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -845774.875000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -851438.000000\n",
      "    epoch          : 268\n",
      "    loss           : -851260.7215346535\n",
      "    val_loss       : -849746.1180308342\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -991874.437500\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -841648.500000\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -836614.750000\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -840578.625000\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -808692.250000\n",
      "    epoch          : 269\n",
      "    loss           : -854926.9845297029\n",
      "    val_loss       : -850956.4215751648\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -992242.750000\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -879188.875000\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -859812.250000\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -794738.500000\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -880558.125000\n",
      "    epoch          : 270\n",
      "    loss           : -858705.2778465346\n",
      "    val_loss       : -857440.8389640808\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -999819.812500\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -822670.125000\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -885177.250000\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -809594.125000\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -813378.937500\n",
      "    epoch          : 271\n",
      "    loss           : -861380.6825495049\n",
      "    val_loss       : -861395.5417325974\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -840785.125000\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -844653.000000\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -833363.750000\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -882978.000000\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -856838.312500\n",
      "    epoch          : 272\n",
      "    loss           : -859979.7902227723\n",
      "    val_loss       : -856429.121212101\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -911819.250000\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -817212.625000\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -839870.062500\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -1000429.375000\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -864258.812500\n",
      "    epoch          : 273\n",
      "    loss           : -861111.1905940594\n",
      "    val_loss       : -860735.9077222825\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -1003060.125000\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -807941.750000\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -770199.625000\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -774931.250000\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -851669.312500\n",
      "    epoch          : 274\n",
      "    loss           : -853637.1497524752\n",
      "    val_loss       : -855719.9239608764\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -994515.250000\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -910277.687500\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -842634.062500\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -876593.625000\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -845175.812500\n",
      "    epoch          : 275\n",
      "    loss           : -853788.7128712871\n",
      "    val_loss       : -840693.4947368621\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -965987.875000\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -791561.187500\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -786108.750000\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -857667.312500\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -827133.625000\n",
      "    epoch          : 276\n",
      "    loss           : -832122.1875\n",
      "    val_loss       : -811352.3891803741\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -942032.625000\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -827469.187500\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -775201.000000\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -837268.375000\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -801053.750000\n",
      "    epoch          : 277\n",
      "    loss           : -837983.916460396\n",
      "    val_loss       : -846999.2243592262\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -974134.187500\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -818080.250000\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -883916.250000\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -830828.500000\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -842441.250000\n",
      "    epoch          : 278\n",
      "    loss           : -853298.9826732674\n",
      "    val_loss       : -850578.2438815117\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -982869.687500\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -823239.875000\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -802324.375000\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -879692.750000\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -819972.125000\n",
      "    epoch          : 279\n",
      "    loss           : -855567.2438118812\n",
      "    val_loss       : -856343.9334843636\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -987522.437500\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -804668.375000\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -839851.625000\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -801203.250000\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -846466.062500\n",
      "    epoch          : 280\n",
      "    loss           : -854997.8032178218\n",
      "    val_loss       : -852977.2190843582\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -988236.250000\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -833256.750000\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -796785.125000\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -852161.125000\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -804303.062500\n",
      "    epoch          : 281\n",
      "    loss           : -857850.6200495049\n",
      "    val_loss       : -858108.7192643166\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -994946.000000\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -828221.750000\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -847555.625000\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -792246.812500\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -803215.187500\n",
      "    epoch          : 282\n",
      "    loss           : -858868.6274752475\n",
      "    val_loss       : -856145.1401607513\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -996767.812500\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -831238.000000\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -849010.500000\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -883067.875000\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -811999.125000\n",
      "    epoch          : 283\n",
      "    loss           : -859684.1448019802\n",
      "    val_loss       : -857901.1336585998\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -996822.750000\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -906012.000000\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -841207.625000\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -859843.750000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -813519.625000\n",
      "    epoch          : 284\n",
      "    loss           : -851489.1113861386\n",
      "    val_loss       : -808649.7689393997\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -825401.875000\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -823175.250000\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -859119.125000\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -992224.625000\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -858063.125000\n",
      "    epoch          : 285\n",
      "    loss           : -848518.9938118812\n",
      "    val_loss       : -856317.437937212\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -996421.625000\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -795776.000000\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -813735.750000\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -847074.312500\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -861286.250000\n",
      "    epoch          : 286\n",
      "    loss           : -861205.2215346535\n",
      "    val_loss       : -861507.0703397751\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -996710.187500\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -830690.937500\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -801098.125000\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -862633.375000\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -849277.750000\n",
      "    epoch          : 287\n",
      "    loss           : -863795.7246287129\n",
      "    val_loss       : -862760.6153275489\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -999849.812500\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -832639.062500\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -829445.875000\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -820367.875000\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -838920.500000\n",
      "    epoch          : 288\n",
      "    loss           : -853138.9201732674\n",
      "    val_loss       : -858610.8026524544\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -998993.875000\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -910547.875000\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -836877.875000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -844287.250000\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -804640.875000\n",
      "    epoch          : 289\n",
      "    loss           : -857054.8836633663\n",
      "    val_loss       : -854165.2443292618\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -901374.500000\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -823910.875000\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -837022.437500\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -809024.125000\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -856652.812500\n",
      "    epoch          : 290\n",
      "    loss           : -860972.2537128713\n",
      "    val_loss       : -862296.7185337066\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -997941.937500\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -803079.062500\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -882207.687500\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -885504.750000\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -859434.500000\n",
      "    epoch          : 291\n",
      "    loss           : -864960.8298267326\n",
      "    val_loss       : -863584.1216047287\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -1001336.875000\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -908249.187500\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -826015.437500\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -988813.062500\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -863523.062500\n",
      "    epoch          : 292\n",
      "    loss           : -858742.4393564357\n",
      "    val_loss       : -858185.1150027275\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -821634.187500\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -839344.125000\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -852000.875000\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -803465.375000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -788778.500000\n",
      "    epoch          : 293\n",
      "    loss           : -844094.8112623763\n",
      "    val_loss       : -847980.349977684\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -989275.000000\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -825149.562500\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -836606.250000\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -839274.312500\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -883557.937500\n",
      "    epoch          : 294\n",
      "    loss           : -858197.7803217822\n",
      "    val_loss       : -860581.8424887657\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -1001636.187500\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -898870.187500\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -796933.625000\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -1001976.625000\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -883957.437500\n",
      "    epoch          : 295\n",
      "    loss           : -863487.2308168317\n",
      "    val_loss       : -863542.782874298\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -1003961.000000\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -826615.500000\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -848344.687500\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -810033.062500\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -812534.125000\n",
      "    epoch          : 296\n",
      "    loss           : -861283.146039604\n",
      "    val_loss       : -860447.4602126122\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -993075.062500\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -826569.375000\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -879551.125000\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -855457.625000\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -818256.125000\n",
      "    epoch          : 297\n",
      "    loss           : -861905.781559406\n",
      "    val_loss       : -860324.4093665123\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -1001791.375000\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -860645.062500\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -885458.375000\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -854442.625000\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -863227.937500\n",
      "    epoch          : 298\n",
      "    loss           : -864711.2011138614\n",
      "    val_loss       : -864272.4000679016\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -1002979.125000\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -832082.687500\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -880690.375000\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -1002468.687500\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -810220.562500\n",
      "    epoch          : 299\n",
      "    loss           : -864367.0464108911\n",
      "    val_loss       : -857066.450983715\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -1000617.375000\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -866228.937500\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -841588.125000\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -795795.437500\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -857362.625000\n",
      "    epoch          : 300\n",
      "    loss           : -857433.7772277228\n",
      "    val_loss       : -860898.0940446854\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0325_145359/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -997787.125000\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -903294.187500\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -860761.875000\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -843826.562500\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -846142.375000\n",
      "    epoch          : 301\n",
      "    loss           : -862046.5470297029\n",
      "    val_loss       : -860482.7460736275\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -990475.375000\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -831560.875000\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -806773.125000\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -860312.250000\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -853815.875000\n",
      "    epoch          : 302\n",
      "    loss           : -863502.8966584158\n",
      "    val_loss       : -861909.3936626434\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -998449.375000\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -831734.000000\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -859063.687500\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -1000874.750000\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -855869.750000\n",
      "    epoch          : 303\n",
      "    loss           : -863451.0352722772\n",
      "    val_loss       : -863146.6064852715\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -1002393.000000\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -852797.500000\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -861444.687500\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -888310.625000\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -854110.750000\n",
      "    epoch          : 304\n",
      "    loss           : -858663.5878712871\n",
      "    val_loss       : -862901.0679950714\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -799690.562500\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -830982.187500\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -856614.750000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -864908.812500\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -809453.750000\n",
      "    epoch          : 305\n",
      "    loss           : -864792.5006188119\n",
      "    val_loss       : -864782.8578480721\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -838514.375000\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -906619.062500\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -861675.000000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -811326.875000\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -812268.687500\n",
      "    epoch          : 306\n",
      "    loss           : -866467.7636138614\n",
      "    val_loss       : -864458.8056384086\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -1002552.500000\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -831126.562500\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -845310.000000\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -838034.000000\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -853236.625000\n",
      "    epoch          : 307\n",
      "    loss           : -863249.5816831683\n",
      "    val_loss       : -863597.3851568222\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -828711.500000\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -902778.937500\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -851471.625000\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -787493.500000\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -845982.375000\n",
      "    epoch          : 308\n",
      "    loss           : -850389.0476485149\n",
      "    val_loss       : -855206.1487590789\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -996233.875000\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -827904.250000\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -796578.625000\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -1000436.437500\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -814542.812500\n",
      "    epoch          : 309\n",
      "    loss           : -860227.1924504951\n",
      "    val_loss       : -860984.3681404113\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -828212.750000\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -835465.875000\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -807085.625000\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -801964.875000\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -844331.062500\n",
      "    epoch          : 310\n",
      "    loss           : -846440.7277227723\n",
      "    val_loss       : -850937.3499885559\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -997566.875000\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -826945.562500\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -884852.500000\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -1002890.312500\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -852345.250000\n",
      "    epoch          : 311\n",
      "    loss           : -857833.354579208\n",
      "    val_loss       : -851918.348674202\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -994224.750000\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -797399.250000\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -831855.437500\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -837365.437500\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -841419.875000\n",
      "    epoch          : 312\n",
      "    loss           : -859532.7004950495\n",
      "    val_loss       : -861707.2519873619\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -889750.812500\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -809723.062500\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -878832.375000\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -809017.437500\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -814712.000000\n",
      "    epoch          : 313\n",
      "    loss           : -856190.0643564357\n",
      "    val_loss       : -856754.3107019424\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -999720.812500\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -907287.062500\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -888627.187500\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -886099.000000\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -854884.687500\n",
      "    epoch          : 314\n",
      "    loss           : -858654.5525990099\n",
      "    val_loss       : -842254.799642086\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -1001308.500000\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -762102.062500\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -795841.875000\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -851534.812500\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -808330.750000\n",
      "    epoch          : 315\n",
      "    loss           : -851271.6243811881\n",
      "    val_loss       : -856675.5749585151\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -998193.937500\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -823932.500000\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -883279.812500\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -852177.125000\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -856125.562500\n",
      "    epoch          : 316\n",
      "    loss           : -859543.4090346535\n",
      "    val_loss       : -861414.7864180565\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -1004573.062500\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -830348.375000\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -832400.625000\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -815890.250000\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -882421.250000\n",
      "    epoch          : 317\n",
      "    loss           : -863565.3341584158\n",
      "    val_loss       : -844466.6467923165\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -982118.875000\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -743552.125000\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -838604.375000\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -844692.375000\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -802303.125000\n",
      "    epoch          : 318\n",
      "    loss           : -839222.5006188119\n",
      "    val_loss       : -852062.874337101\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -997945.750000\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -897716.750000\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -798736.375000\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -833864.000000\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -849446.625000\n",
      "    epoch          : 319\n",
      "    loss           : -854815.9740099009\n",
      "    val_loss       : -856753.2923843383\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -1001416.750000\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -846484.875000\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -855793.312500\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -812500.250000\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -849158.500000\n",
      "    epoch          : 320\n",
      "    loss           : -860961.8378712871\n",
      "    val_loss       : -860034.7518426895\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -998521.750000\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -860933.250000\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -889542.187500\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -843928.000000\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -814182.500000\n",
      "    epoch          : 321\n",
      "    loss           : -863610.2240099009\n",
      "    val_loss       : -861985.8597118377\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -1002303.875000\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -837485.375000\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -810163.750000\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -798676.375000\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -855432.125000\n",
      "    epoch          : 322\n",
      "    loss           : -863788.7642326732\n",
      "    val_loss       : -863589.2841622352\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -909246.375000\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -830640.375000\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -868820.875000\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -854094.375000\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -836772.000000\n",
      "    epoch          : 323\n",
      "    loss           : -855347.7957920792\n",
      "    val_loss       : -860413.0756565094\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -994287.375000\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -905929.687500\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -857030.812500\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -856359.562500\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -854932.375000\n",
      "    epoch          : 324\n",
      "    loss           : -863427.2698019802\n",
      "    val_loss       : -861784.7147425652\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -997497.187500\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -894817.375000\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -853065.062500\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -812587.187500\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -849502.375000\n",
      "    epoch          : 325\n",
      "    loss           : -858074.4127475248\n",
      "    val_loss       : -858299.9783295632\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -826697.625000\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -906735.625000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -853424.812500\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -840027.250000\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -813870.125000\n",
      "    epoch          : 326\n",
      "    loss           : -864111.667079208\n",
      "    val_loss       : -863267.1427321434\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -1004238.937500\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -882173.250000\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -808312.750000\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -853529.000000\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -865159.000000\n",
      "    epoch          : 327\n",
      "    loss           : -866102.1441831683\n",
      "    val_loss       : -861094.592859745\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -986235.750000\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -835811.000000\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -835934.875000\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -844731.875000\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -862110.250000\n",
      "    epoch          : 328\n",
      "    loss           : -863827.9096534654\n",
      "    val_loss       : -858969.2189224244\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -892200.937500\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -801526.937500\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -885102.875000\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -857920.500000\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -844582.687500\n",
      "    epoch          : 329\n",
      "    loss           : -864586.6875\n",
      "    val_loss       : -865007.0490703583\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -835948.062500\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -807374.250000\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -887576.375000\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -998498.312500\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -859328.562500\n",
      "    epoch          : 330\n",
      "    loss           : -862091.9127475248\n",
      "    val_loss       : -847690.4207507133\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -997129.437500\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -827292.562500\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -846574.250000\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -820988.062500\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -834165.625000\n",
      "    epoch          : 331\n",
      "    loss           : -855751.0160891089\n",
      "    val_loss       : -859510.1234390258\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -1001412.875000\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -826999.437500\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -798240.250000\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -1002538.812500\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -813337.250000\n",
      "    epoch          : 332\n",
      "    loss           : -858519.5810643565\n",
      "    val_loss       : -857989.6116490364\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -999999.500000\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -832057.687500\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -875340.437500\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -845632.187500\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -818959.812500\n",
      "    epoch          : 333\n",
      "    loss           : -861880.5222772277\n",
      "    val_loss       : -861466.7317293168\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -998075.375000\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -833214.500000\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -843606.687500\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -886926.375000\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -827981.375000\n",
      "    epoch          : 334\n",
      "    loss           : -860602.0507425743\n",
      "    val_loss       : -832311.6831655502\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -798426.250000\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -812342.812500\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -804256.000000\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -837769.562500\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -839023.250000\n",
      "    epoch          : 335\n",
      "    loss           : -844330.1448019802\n",
      "    val_loss       : -853022.7967364311\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -937709.937500\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -832044.250000\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -878964.812500\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -845904.000000\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -840767.437500\n",
      "    epoch          : 336\n",
      "    loss           : -856771.781559406\n",
      "    val_loss       : -852406.3135078431\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -956929.000000\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -907646.500000\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -804656.375000\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -969730.250000\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -824872.812500\n",
      "    epoch          : 337\n",
      "    loss           : -853062.3805693069\n",
      "    val_loss       : -852517.5987235069\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -831857.812500\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -903994.937500\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -803967.250000\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -857480.437500\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -794868.062500\n",
      "    epoch          : 338\n",
      "    loss           : -856454.7029702971\n",
      "    val_loss       : -826375.8146084786\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -884653.187500\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -820622.750000\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -878716.750000\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -809370.562500\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -809889.125000\n",
      "    epoch          : 339\n",
      "    loss           : -850608.7574257426\n",
      "    val_loss       : -858387.4093942642\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -986327.375000\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -829808.750000\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -884192.250000\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -979186.375000\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -763860.687500\n",
      "    epoch          : 340\n",
      "    loss           : -847694.0300123763\n",
      "    val_loss       : -788550.4387615204\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -984375.687500\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -810707.875000\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -828945.375000\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -870438.937500\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -811605.375000\n",
      "    epoch          : 341\n",
      "    loss           : -835754.4758663366\n",
      "    val_loss       : -849926.820004177\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -991771.500000\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -851830.312500\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -796354.187500\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -852404.500000\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -854905.062500\n",
      "    epoch          : 342\n",
      "    loss           : -857507.9294554455\n",
      "    val_loss       : -858784.7515939713\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -995443.937500\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -835997.000000\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -882946.625000\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -817045.000000\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -856559.687500\n",
      "    epoch          : 343\n",
      "    loss           : -861984.3168316832\n",
      "    val_loss       : -862087.476214981\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -994776.000000\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -833390.250000\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -884016.187500\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -832378.687500\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -853730.500000\n",
      "    epoch          : 344\n",
      "    loss           : -863359.0922029703\n",
      "    val_loss       : -863558.1291052818\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -1002051.000000\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -833806.937500\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -856397.312500\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -830720.750000\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -803782.125000\n",
      "    epoch          : 345\n",
      "    loss           : -857403.5228960396\n",
      "    val_loss       : -845466.6621421814\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -981518.937500\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -814876.062500\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -820714.125000\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -793526.875000\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -854630.562500\n",
      "    epoch          : 346\n",
      "    loss           : -856747.5841584158\n",
      "    val_loss       : -859101.6214780807\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -825563.687500\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -818878.562500\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -836135.562500\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -815788.875000\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -881990.375000\n",
      "    epoch          : 347\n",
      "    loss           : -859866.8168316832\n",
      "    val_loss       : -856539.4379455566\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -1000280.312500\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -910784.750000\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -887093.187500\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -853746.937500\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -841344.000000\n",
      "    epoch          : 348\n",
      "    loss           : -862031.5358910891\n",
      "    val_loss       : -862449.6951285362\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -827053.875000\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -887922.625000\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -886512.187500\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -890555.750000\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -812454.000000\n",
      "    epoch          : 349\n",
      "    loss           : -864863.8205445545\n",
      "    val_loss       : -863628.3153292655\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -1002351.750000\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -837612.375000\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -887067.000000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -891288.562500\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -858777.875000\n",
      "    epoch          : 350\n",
      "    loss           : -867301.8446782178\n",
      "    val_loss       : -863925.2206195832\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0325_145359/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -1003280.250000\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -800953.125000\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -886485.875000\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -812750.875000\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -845478.312500\n",
      "    epoch          : 351\n",
      "    loss           : -866107.0891089109\n",
      "    val_loss       : -865518.9366147041\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -1003626.875000\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -835956.312500\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -844849.625000\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -812529.000000\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -816932.437500\n",
      "    epoch          : 352\n",
      "    loss           : -866784.0389851485\n",
      "    val_loss       : -856651.1804066658\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -987813.562500\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -905463.875000\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -860122.625000\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -845383.375000\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -813822.875000\n",
      "    epoch          : 353\n",
      "    loss           : -863218.8551980198\n",
      "    val_loss       : -859953.6710554123\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -998575.500000\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -900086.750000\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -890189.812500\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -857040.750000\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -822687.500000\n",
      "    epoch          : 354\n",
      "    loss           : -865226.2029702971\n",
      "    val_loss       : -865258.037059021\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -905195.375000\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -831748.000000\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -890133.125000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -816579.250000\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -846888.375000\n",
      "    epoch          : 355\n",
      "    loss           : -866250.478960396\n",
      "    val_loss       : -866384.8224460601\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -1003188.250000\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -914843.000000\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -851818.875000\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -890155.187500\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -863606.437500\n",
      "    epoch          : 356\n",
      "    loss           : -869953.1732673268\n",
      "    val_loss       : -865773.3078968048\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -1000365.312500\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -830196.125000\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -850586.125000\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -865684.937500\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -858473.250000\n",
      "    epoch          : 357\n",
      "    loss           : -868457.6547029703\n",
      "    val_loss       : -866582.561252594\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -997591.250000\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -833733.500000\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -800363.875000\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -860579.500000\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -859380.812500\n",
      "    epoch          : 358\n",
      "    loss           : -865340.3032178218\n",
      "    val_loss       : -855871.2618662834\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -975039.750000\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -901483.375000\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -865974.250000\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -805610.375000\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -856441.375000\n",
      "    epoch          : 359\n",
      "    loss           : -863123.9814356435\n",
      "    val_loss       : -860576.9432339668\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -905681.250000\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -831683.687500\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -847484.000000\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -823676.937500\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -815879.500000\n",
      "    epoch          : 360\n",
      "    loss           : -866313.9028465346\n",
      "    val_loss       : -866227.5484099388\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -1003213.875000\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -806807.250000\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -889452.375000\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -990889.687500\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -842427.062500\n",
      "    epoch          : 361\n",
      "    loss           : -865540.5810643565\n",
      "    val_loss       : -864592.9094044685\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -905842.875000\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -835249.375000\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -809908.875000\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -863861.187500\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -819390.375000\n",
      "    epoch          : 362\n",
      "    loss           : -864217.0705445545\n",
      "    val_loss       : -864508.1207085609\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -992106.750000\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -794998.562500\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -852444.062500\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -847716.937500\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -861823.875000\n",
      "    epoch          : 363\n",
      "    loss           : -861027.1534653465\n",
      "    val_loss       : -856476.5903266907\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -972105.125000\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -831467.062500\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -863922.562500\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -802587.625000\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -809181.312500\n",
      "    epoch          : 364\n",
      "    loss           : -862322.6188118812\n",
      "    val_loss       : -859486.5842172622\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -985739.500000\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -907746.625000\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -816712.687500\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -994742.062500\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -816884.187500\n",
      "    epoch          : 365\n",
      "    loss           : -861821.6540841584\n",
      "    val_loss       : -863660.1225550652\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -995278.000000\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -913521.125000\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -819802.937500\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -885503.625000\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -855136.500000\n",
      "    epoch          : 366\n",
      "    loss           : -865908.5346534654\n",
      "    val_loss       : -864180.3639910698\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -997157.250000\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -840156.750000\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -817634.625000\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -998756.250000\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -863276.875000\n",
      "    epoch          : 367\n",
      "    loss           : -868394.7858910891\n",
      "    val_loss       : -867933.3422236443\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -1000817.812500\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -850692.062500\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -804893.187500\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -813168.875000\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -870321.375000\n",
      "    epoch          : 368\n",
      "    loss           : -869288.4727722772\n",
      "    val_loss       : -868609.4893428802\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -1005375.187500\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -833765.062500\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -848574.625000\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -999861.687500\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -812468.687500\n",
      "    epoch          : 369\n",
      "    loss           : -866718.3607673268\n",
      "    val_loss       : -865167.1058898926\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -1000869.875000\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -834827.250000\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -805830.375000\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -860605.500000\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -853292.500000\n",
      "    epoch          : 370\n",
      "    loss           : -867374.5049504951\n",
      "    val_loss       : -866730.1651156426\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -904855.875000\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -820510.937500\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -883114.000000\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -814701.375000\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -861376.875000\n",
      "    epoch          : 371\n",
      "    loss           : -865054.9139851485\n",
      "    val_loss       : -855842.7190685272\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -994494.625000\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -822610.812500\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -795915.437500\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -998354.687500\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -811152.062500\n",
      "    epoch          : 372\n",
      "    loss           : -859463.1503712871\n",
      "    val_loss       : -863872.7256621361\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -914879.125000\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -831967.187500\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -880434.312500\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -815364.750000\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -864566.937500\n",
      "    epoch          : 373\n",
      "    loss           : -866422.6448019802\n",
      "    val_loss       : -866172.0376648903\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -997428.250000\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -810328.250000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -848999.125000\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -868648.437500\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -862934.062500\n",
      "    epoch          : 374\n",
      "    loss           : -868965.5878712871\n",
      "    val_loss       : -858253.5842878341\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -1000913.625000\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -876884.750000\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -888882.250000\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -888805.875000\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -864020.000000\n",
      "    epoch          : 375\n",
      "    loss           : -865323.3570544554\n",
      "    val_loss       : -863135.586128521\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -990968.875000\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -912028.437500\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -809798.562500\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -847882.500000\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -813477.375000\n",
      "    epoch          : 376\n",
      "    loss           : -866853.9232673268\n",
      "    val_loss       : -867398.3770988465\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -1003454.000000\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -833271.500000\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -811751.437500\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -883276.937500\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -861147.062500\n",
      "    epoch          : 377\n",
      "    loss           : -865528.146039604\n",
      "    val_loss       : -868121.2324714661\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -1006016.250000\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -911230.625000\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -868910.250000\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -1004391.000000\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -890874.250000\n",
      "    epoch          : 378\n",
      "    loss           : -871108.7648514851\n",
      "    val_loss       : -865550.4803228378\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -1002981.875000\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -911803.312500\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -877891.250000\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -1002637.312500\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -810725.062500\n",
      "    epoch          : 379\n",
      "    loss           : -864335.7685643565\n",
      "    val_loss       : -860044.648543644\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -997833.375000\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -831032.000000\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -812781.062500\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -889675.500000\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -862734.562500\n",
      "    epoch          : 380\n",
      "    loss           : -867243.3477722772\n",
      "    val_loss       : -866756.3968212127\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -1003137.625000\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -833130.375000\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -850721.625000\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -868281.750000\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -867080.062500\n",
      "    epoch          : 381\n",
      "    loss           : -869137.0872524752\n",
      "    val_loss       : -868408.2226516723\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -1003335.750000\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -835645.312500\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -807848.875000\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -863065.625000\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -866764.750000\n",
      "    epoch          : 382\n",
      "    loss           : -857994.2908415842\n",
      "    val_loss       : -865294.8383847236\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -997063.000000\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -818193.250000\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -792502.187500\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -859728.687500\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -808686.437500\n",
      "    epoch          : 383\n",
      "    loss           : -864256.4498762377\n",
      "    val_loss       : -858016.778442955\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -999400.625000\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -791019.125000\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -882785.500000\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -864593.250000\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -847313.625000\n",
      "    epoch          : 384\n",
      "    loss           : -857771.3087871287\n",
      "    val_loss       : -858233.6219044685\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -1002942.625000\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -894840.250000\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -886853.250000\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -799931.875000\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -842699.562500\n",
      "    epoch          : 385\n",
      "    loss           : -861025.2636138614\n",
      "    val_loss       : -859169.9063755035\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -987485.500000\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -840585.687500\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -824476.125000\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -815943.875000\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -843329.187500\n",
      "    epoch          : 386\n",
      "    loss           : -855980.9975247525\n",
      "    val_loss       : -853321.2559916496\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -1000409.062500\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -835722.500000\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -890772.500000\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -849664.375000\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -860254.125000\n",
      "    epoch          : 387\n",
      "    loss           : -862040.6386138614\n",
      "    val_loss       : -859176.5310854912\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -984294.625000\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -908238.500000\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -847598.875000\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -863120.562500\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -860626.625000\n",
      "    epoch          : 388\n",
      "    loss           : -864806.6491336634\n",
      "    val_loss       : -866436.3825374603\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -1000940.000000\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -865999.125000\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -867115.062500\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -853851.750000\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -862473.062500\n",
      "    epoch          : 389\n",
      "    loss           : -869792.7518564357\n",
      "    val_loss       : -868261.331363678\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -1002231.000000\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -889510.125000\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -799924.625000\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -958107.562500\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -815568.500000\n",
      "    epoch          : 390\n",
      "    loss           : -855873.4659653465\n",
      "    val_loss       : -859423.110023117\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -995266.187500\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -829300.000000\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -770473.500000\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -857931.625000\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -807628.062500\n",
      "    epoch          : 391\n",
      "    loss           : -859887.9090346535\n",
      "    val_loss       : -861980.7076500893\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -999122.125000\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -832588.500000\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -893519.562500\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -864093.062500\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -863721.875000\n",
      "    epoch          : 392\n",
      "    loss           : -866782.2042079208\n",
      "    val_loss       : -867458.8382109642\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -1003331.312500\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -805921.500000\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -864647.250000\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -849879.250000\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -860087.125000\n",
      "    epoch          : 393\n",
      "    loss           : -867863.0099009901\n",
      "    val_loss       : -868538.6905792237\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -1000515.937500\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -809655.687500\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -893791.875000\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -860086.875000\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -821453.500000\n",
      "    epoch          : 394\n",
      "    loss           : -870994.0724009901\n",
      "    val_loss       : -870152.2143661499\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -1004016.437500\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -838078.437500\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -867566.000000\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -993726.062500\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -812186.250000\n",
      "    epoch          : 395\n",
      "    loss           : -858540.7017326732\n",
      "    val_loss       : -853615.6461800575\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -1001106.937500\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -901349.312500\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -771799.750000\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -887387.500000\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -851779.250000\n",
      "    epoch          : 396\n",
      "    loss           : -854442.7301980198\n",
      "    val_loss       : -859680.6752114296\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -904741.562500\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -800112.625000\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -857514.687500\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -865950.500000\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -863998.937500\n",
      "    epoch          : 397\n",
      "    loss           : -865021.8842821782\n",
      "    val_loss       : -865790.6990635872\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -997399.000000\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -842146.625000\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -894305.312500\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -810786.625000\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -849751.875000\n",
      "    epoch          : 398\n",
      "    loss           : -866842.4016089109\n",
      "    val_loss       : -862405.6667920112\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -1004527.937500\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -907959.812500\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -880718.750000\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -879995.250000\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -855242.312500\n",
      "    epoch          : 399\n",
      "    loss           : -862880.9696782178\n",
      "    val_loss       : -856159.5090301514\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -994617.250000\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -854318.062500\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -840947.437500\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -814147.625000\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -859052.125000\n",
      "    epoch          : 400\n",
      "    loss           : -863900.7667079208\n",
      "    val_loss       : -865985.365225029\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0325_145359/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -805798.625000\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -912148.750000\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -858329.000000\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -892721.937500\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -848261.625000\n",
      "    epoch          : 401\n",
      "    loss           : -867942.7611386139\n",
      "    val_loss       : -868723.3547344208\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -1002774.000000\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -912702.125000\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -890037.625000\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -861069.500000\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -855912.875000\n",
      "    epoch          : 402\n",
      "    loss           : -865460.0977722772\n",
      "    val_loss       : -860392.8230275154\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -993753.625000\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -834323.375000\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -819255.750000\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -859610.687500\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -893097.750000\n",
      "    epoch          : 403\n",
      "    loss           : -867223.8422029703\n",
      "    val_loss       : -868543.7741680145\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -1005017.000000\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -851189.062500\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -850085.625000\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -803862.375000\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -867413.750000\n",
      "    epoch          : 404\n",
      "    loss           : -870176.2376237623\n",
      "    val_loss       : -868291.3930540085\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -1003896.625000\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -839353.375000\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -809967.250000\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -861774.750000\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -858404.937500\n",
      "    epoch          : 405\n",
      "    loss           : -870837.7388613861\n",
      "    val_loss       : -867501.8149147987\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -998521.750000\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -840954.000000\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -860738.625000\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -1000916.562500\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -814184.500000\n",
      "    epoch          : 406\n",
      "    loss           : -868284.6652227723\n",
      "    val_loss       : -867036.8887583732\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -835536.812500\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -912204.562500\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -838425.937500\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -998258.875000\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -868122.437500\n",
      "    epoch          : 407\n",
      "    loss           : -866623.3675742574\n",
      "    val_loss       : -865575.8401780128\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -1000022.500000\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -912903.562500\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -809650.875000\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -811563.937500\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -856891.187500\n",
      "    epoch          : 408\n",
      "    loss           : -869909.2642326732\n",
      "    val_loss       : -867026.2145953178\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -998613.750000\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -839487.187500\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -876875.250000\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -840065.687500\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -855282.312500\n",
      "    epoch          : 409\n",
      "    loss           : -863704.6683168317\n",
      "    val_loss       : -863954.8174975396\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -986832.875000\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -838985.625000\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -805136.000000\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -983469.875000\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -811721.562500\n",
      "    epoch          : 410\n",
      "    loss           : -861651.8762376237\n",
      "    val_loss       : -857764.3489564896\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -993676.000000\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -834945.125000\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -843691.000000\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -982409.500000\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -834149.937500\n",
      "    epoch          : 411\n",
      "    loss           : -859299.3323019802\n",
      "    val_loss       : -863393.7534222603\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -889353.000000\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -834188.000000\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -807377.500000\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -865153.187500\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -852476.312500\n",
      "    epoch          : 412\n",
      "    loss           : -866198.5711633663\n",
      "    val_loss       : -860288.0034318924\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -1000373.250000\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -902324.687500\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -804320.750000\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -857200.375000\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -885356.875000\n",
      "    epoch          : 413\n",
      "    loss           : -863818.0241336634\n",
      "    val_loss       : -863197.3286960602\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -1003215.937500\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -835153.875000\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -894018.625000\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -892009.625000\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -823696.937500\n",
      "    epoch          : 414\n",
      "    loss           : -869624.1751237623\n",
      "    val_loss       : -869500.2110765458\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -1005005.062500\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -839606.375000\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -868447.500000\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -813285.187500\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -867536.375000\n",
      "    epoch          : 415\n",
      "    loss           : -871275.5148514851\n",
      "    val_loss       : -867894.3492601395\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -999360.687500\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -914532.187500\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -808642.250000\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -893950.687500\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -819929.687500\n",
      "    epoch          : 416\n",
      "    loss           : -872392.8551980198\n",
      "    val_loss       : -869125.2508831024\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -1001048.000000\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -841220.500000\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -865155.625000\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -867295.250000\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -867032.312500\n",
      "    epoch          : 417\n",
      "    loss           : -873070.6639851485\n",
      "    val_loss       : -857823.9922297478\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -985068.625000\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -837537.562500\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -885643.375000\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -865125.875000\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -815506.125000\n",
      "    epoch          : 418\n",
      "    loss           : -864588.6478960396\n",
      "    val_loss       : -862806.7164175033\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -996644.312500\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -909165.375000\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -858848.875000\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -1005359.500000\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -801504.562500\n",
      "    epoch          : 419\n",
      "    loss           : -867025.8378712871\n",
      "    val_loss       : -861190.8739385605\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -1001733.625000\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -832776.625000\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -851839.250000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -848995.812500\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -847680.062500\n",
      "    epoch          : 420\n",
      "    loss           : -867872.0154702971\n",
      "    val_loss       : -869079.3782693862\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -1005127.125000\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -816937.250000\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -867322.375000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -858765.187500\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -864926.500000\n",
      "    epoch          : 421\n",
      "    loss           : -873390.968440594\n",
      "    val_loss       : -872274.3152882576\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -1007362.312500\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -918787.875000\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -815560.000000\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -817660.750000\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -866562.875000\n",
      "    epoch          : 422\n",
      "    loss           : -872276.0915841584\n",
      "    val_loss       : -869887.935423851\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -1000754.000000\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -914224.375000\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -866697.937500\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -852295.187500\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -865836.062500\n",
      "    epoch          : 423\n",
      "    loss           : -870999.2351485149\n",
      "    val_loss       : -869096.803955555\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -810562.375000\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -836406.625000\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -814892.125000\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -848157.500000\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -814254.812500\n",
      "    epoch          : 424\n",
      "    loss           : -870689.9313118812\n",
      "    val_loss       : -858603.3121699333\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -829957.375000\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -835224.437500\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -776468.562500\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -800992.875000\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -813230.875000\n",
      "    epoch          : 425\n",
      "    loss           : -859396.0346534654\n",
      "    val_loss       : -865109.7865073204\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -1000616.937500\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -829066.937500\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -820448.687500\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -881432.187500\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -864198.812500\n",
      "    epoch          : 426\n",
      "    loss           : -869394.7896039604\n",
      "    val_loss       : -868833.4145391465\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -1005684.000000\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -837116.875000\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -810366.000000\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -813819.500000\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -823048.375000\n",
      "    epoch          : 427\n",
      "    loss           : -870976.9412128713\n",
      "    val_loss       : -868991.2441883087\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -1006411.062500\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -909112.562500\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -865696.250000\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -893054.687500\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -847004.500000\n",
      "    epoch          : 428\n",
      "    loss           : -870706.7011138614\n",
      "    val_loss       : -867114.765701294\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -912890.437500\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -918526.125000\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -883696.562500\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -1006561.937500\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -863435.000000\n",
      "    epoch          : 429\n",
      "    loss           : -868248.1472772277\n",
      "    val_loss       : -870370.8847156524\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -916232.562500\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -811053.250000\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -855530.625000\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -1002261.375000\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -826264.937500\n",
      "    epoch          : 430\n",
      "    loss           : -871639.0123762377\n",
      "    val_loss       : -833288.0459223747\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -970461.687500\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -827394.625000\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -771597.000000\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -832878.250000\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -832797.875000\n",
      "    epoch          : 431\n",
      "    loss           : -849399.2370049505\n",
      "    val_loss       : -855294.0203791618\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -997308.875000\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -883121.000000\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -852469.375000\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -874062.687500\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -853884.187500\n",
      "    epoch          : 432\n",
      "    loss           : -858441.7512376237\n",
      "    val_loss       : -854947.2549229622\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -1000262.562500\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -886286.625000\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -901655.375000\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -853263.875000\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -850010.375000\n",
      "    epoch          : 433\n",
      "    loss           : -862678.5779702971\n",
      "    val_loss       : -854800.4546291351\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -999637.750000\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -878114.750000\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -884580.000000\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -856614.000000\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -824744.125000\n",
      "    epoch          : 434\n",
      "    loss           : -863638.885519802\n",
      "    val_loss       : -863985.7129925728\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -1001765.062500\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -906742.687500\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -854220.625000\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -1004399.875000\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -819395.312500\n",
      "    epoch          : 435\n",
      "    loss           : -868851.0655940594\n",
      "    val_loss       : -868353.8870332718\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -1003716.125000\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -844448.062500\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -891790.750000\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -1009081.125000\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -815796.375000\n",
      "    epoch          : 436\n",
      "    loss           : -872715.0680693069\n",
      "    val_loss       : -871041.762369442\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -1006789.062500\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -857638.250000\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -810671.500000\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -813687.750000\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -862030.562500\n",
      "    epoch          : 437\n",
      "    loss           : -871528.6485148515\n",
      "    val_loss       : -867138.726088047\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -1002083.625000\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -919323.500000\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -892393.187500\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -864638.500000\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -868127.500000\n",
      "    epoch          : 438\n",
      "    loss           : -870780.5525990099\n",
      "    val_loss       : -848831.5599703789\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -998054.625000\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -912671.312500\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -786189.187500\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -849083.500000\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -873335.625000\n",
      "    epoch          : 439\n",
      "    loss           : -864745.0649752475\n",
      "    val_loss       : -868609.5622397423\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -1006822.625000\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -804355.437500\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -844045.187500\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -857005.687500\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -871332.812500\n",
      "    epoch          : 440\n",
      "    loss           : -873094.6714108911\n",
      "    val_loss       : -871400.5487902642\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -1011240.000000\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -839125.375000\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -894028.500000\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -887547.750000\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -815933.000000\n",
      "    epoch          : 441\n",
      "    loss           : -870535.3094059406\n",
      "    val_loss       : -868403.048105812\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -1008670.125000\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -892334.375000\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -892787.375000\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -869254.312500\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -865437.937500\n",
      "    epoch          : 442\n",
      "    loss           : -871858.2648514851\n",
      "    val_loss       : -872525.8248182297\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -1008272.062500\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -856706.750000\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -819893.625000\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -819710.625000\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -819144.750000\n",
      "    epoch          : 443\n",
      "    loss           : -872442.7413366337\n",
      "    val_loss       : -861597.9607690811\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -911118.500000\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -829215.375000\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -809120.375000\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -865719.750000\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -865414.000000\n",
      "    epoch          : 444\n",
      "    loss           : -868547.1349009901\n",
      "    val_loss       : -867002.9239160537\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -913739.375000\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -852370.125000\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -894248.500000\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -856805.875000\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -752527.875000\n",
      "    epoch          : 445\n",
      "    loss           : -862140.2772277228\n",
      "    val_loss       : -822222.4168494225\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -989709.750000\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -821599.500000\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -856048.250000\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -829170.250000\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -845989.375000\n",
      "    epoch          : 446\n",
      "    loss           : -855091.4777227723\n",
      "    val_loss       : -846718.2501566887\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -887185.000000\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -832707.687500\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -882323.250000\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -1005256.625000\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -849260.187500\n",
      "    epoch          : 447\n",
      "    loss           : -861944.2425742574\n",
      "    val_loss       : -864593.077529335\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -1003138.375000\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -917020.625000\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -847065.500000\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -883311.750000\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -863784.562500\n",
      "    epoch          : 448\n",
      "    loss           : -867077.1138613861\n",
      "    val_loss       : -865367.2653809547\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -1001364.875000\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -918635.187500\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -853255.375000\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -801498.562500\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -856547.875000\n",
      "    epoch          : 449\n",
      "    loss           : -869030.7790841584\n",
      "    val_loss       : -867032.6058261872\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -996226.625000\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -917937.750000\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -896117.375000\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -1001465.187500\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -820239.375000\n",
      "    epoch          : 450\n",
      "    loss           : -870836.8224009901\n",
      "    val_loss       : -871218.3401535988\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0325_145359/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -1005659.000000\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -837909.500000\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -849025.500000\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -1000395.625000\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -821334.625000\n",
      "    epoch          : 451\n",
      "    loss           : -870338.8892326732\n",
      "    val_loss       : -869645.6813210488\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -911105.000000\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -838814.187500\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -816922.312500\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -860545.125000\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -805331.187500\n",
      "    epoch          : 452\n",
      "    loss           : -868604.3452970297\n",
      "    val_loss       : -862571.4455931664\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -998474.687500\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -916084.375000\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -863769.250000\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -899746.375000\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -865314.875000\n",
      "    epoch          : 453\n",
      "    loss           : -869436.9603960396\n",
      "    val_loss       : -868510.0008505821\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -1001463.500000\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -838462.625000\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -857741.750000\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -1003652.062500\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -865535.000000\n",
      "    epoch          : 454\n",
      "    loss           : -873485.2425742574\n",
      "    val_loss       : -865078.2267893791\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -827706.187500\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -916665.250000\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -805824.125000\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -889211.562500\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -889677.625000\n",
      "    epoch          : 455\n",
      "    loss           : -866672.8490099009\n",
      "    val_loss       : -870215.5206183434\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -1003290.875000\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -916395.312500\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -820272.000000\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -871897.875000\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -875038.750000\n",
      "    epoch          : 456\n",
      "    loss           : -872595.1497524752\n",
      "    val_loss       : -871062.1081985474\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -995756.750000\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -813533.437500\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -856047.062500\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -866480.375000\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -870682.125000\n",
      "    epoch          : 457\n",
      "    loss           : -869879.791460396\n",
      "    val_loss       : -851593.1539873123\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -955241.937500\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -913093.000000\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -869505.750000\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -865994.812500\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -867652.250000\n",
      "    epoch          : 458\n",
      "    loss           : -867529.9863861386\n",
      "    val_loss       : -868808.1178833961\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -1003125.750000\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -837018.250000\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -855562.312500\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -1005394.750000\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -817464.250000\n",
      "    epoch          : 459\n",
      "    loss           : -872519.2252475248\n",
      "    val_loss       : -872160.0140033722\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -1005671.812500\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -871823.125000\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -812245.562500\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -813209.187500\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -826831.750000\n",
      "    epoch          : 460\n",
      "    loss           : -875964.292079208\n",
      "    val_loss       : -875096.3678853989\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -1005652.625000\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -845531.062500\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -856696.187500\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -821037.062500\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -869608.250000\n",
      "    epoch          : 461\n",
      "    loss           : -874840.8025990099\n",
      "    val_loss       : -864430.6288750649\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -994839.437500\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -837449.000000\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -859100.750000\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -1003898.875000\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -876727.062500\n",
      "    epoch          : 462\n",
      "    loss           : -870787.6051980198\n",
      "    val_loss       : -871685.4831015586\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -1002623.437500\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -912140.500000\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -869030.375000\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -870009.937500\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -870327.125000\n",
      "    epoch          : 463\n",
      "    loss           : -874997.8799504951\n",
      "    val_loss       : -873559.2545693398\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -1002738.687500\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -921158.312500\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -822693.375000\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -808490.125000\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -825617.500000\n",
      "    epoch          : 464\n",
      "    loss           : -874785.0185643565\n",
      "    val_loss       : -874684.8473375321\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -1007240.812500\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -863267.500000\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -876729.125000\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -815630.750000\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -870235.375000\n",
      "    epoch          : 465\n",
      "    loss           : -877263.6633663366\n",
      "    val_loss       : -874585.2085318565\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -1008890.312500\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -848383.062500\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -872370.500000\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -896035.812500\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -827605.125000\n",
      "    epoch          : 466\n",
      "    loss           : -877540.0222772277\n",
      "    val_loss       : -874876.8966621399\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -1005713.125000\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -920740.812500\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -877072.500000\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -958258.125000\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -810662.000000\n",
      "    epoch          : 467\n",
      "    loss           : -861437.6899752475\n",
      "    val_loss       : -859497.2094231605\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -1000859.625000\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -811189.750000\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -878522.000000\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -847202.625000\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -864794.375000\n",
      "    epoch          : 468\n",
      "    loss           : -863528.5711633663\n",
      "    val_loss       : -863408.5528102875\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -995936.625000\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -846230.000000\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -815277.250000\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -867431.375000\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -846132.500000\n",
      "    epoch          : 469\n",
      "    loss           : -867692.010519802\n",
      "    val_loss       : -868884.8226214409\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -1005023.000000\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -838986.000000\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -891465.125000\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -1001669.250000\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -824587.562500\n",
      "    epoch          : 470\n",
      "    loss           : -870352.4535891089\n",
      "    val_loss       : -868909.1070259095\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -1000575.437500\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -914074.562500\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -853418.875000\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -822958.000000\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -858927.125000\n",
      "    epoch          : 471\n",
      "    loss           : -868943.0377475248\n",
      "    val_loss       : -870224.4223987579\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -1004249.812500\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -834978.375000\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -887666.125000\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -855576.437500\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -817766.875000\n",
      "    epoch          : 472\n",
      "    loss           : -870771.2512376237\n",
      "    val_loss       : -867661.4958456993\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -1005100.375000\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -893487.250000\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -814543.000000\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -843074.750000\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -865452.687500\n",
      "    epoch          : 473\n",
      "    loss           : -868996.8131188119\n",
      "    val_loss       : -872017.8318789483\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -815390.687500\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -837501.000000\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -856662.125000\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -853770.187500\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -865502.875000\n",
      "    epoch          : 474\n",
      "    loss           : -871592.0501237623\n",
      "    val_loss       : -869352.2596408844\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -1009674.125000\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -861052.000000\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -808134.500000\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -998062.187500\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -869904.625000\n",
      "    epoch          : 475\n",
      "    loss           : -863708.6850247525\n",
      "    val_loss       : -867585.9004984855\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -1003485.437500\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -914793.812500\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -830645.250000\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -992224.687500\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -862550.812500\n",
      "    epoch          : 476\n",
      "    loss           : -869610.0823019802\n",
      "    val_loss       : -867420.4342336655\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -1003379.062500\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -839245.500000\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -849827.812500\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -996890.500000\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -864405.250000\n",
      "    epoch          : 477\n",
      "    loss           : -863427.8162128713\n",
      "    val_loss       : -868449.7528869628\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -1002690.625000\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -832025.750000\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -803335.562500\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -1000195.625000\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -866153.500000\n",
      "    epoch          : 478\n",
      "    loss           : -871579.9498762377\n",
      "    val_loss       : -870647.0838601112\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -838640.000000\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -897121.750000\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -807442.000000\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -863131.187500\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -871613.562500\n",
      "    epoch          : 479\n",
      "    loss           : -872999.3378712871\n",
      "    val_loss       : -872361.5810060501\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -1004063.375000\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -813228.625000\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -865042.875000\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -869226.750000\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -866464.625000\n",
      "    epoch          : 480\n",
      "    loss           : -872185.8075495049\n",
      "    val_loss       : -862104.7131432533\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -940174.500000\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -833625.625000\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -834849.812500\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -862196.875000\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -816006.875000\n",
      "    epoch          : 481\n",
      "    loss           : -861186.0439356435\n",
      "    val_loss       : -866764.3552283287\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -977486.750000\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -823886.625000\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -821973.437500\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -992206.437500\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -817814.875000\n",
      "    epoch          : 482\n",
      "    loss           : -869950.5878712871\n",
      "    val_loss       : -864538.8842819214\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -916783.937500\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -845142.625000\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -864023.125000\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -858502.000000\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -864524.000000\n",
      "    epoch          : 483\n",
      "    loss           : -871763.3310643565\n",
      "    val_loss       : -871741.6406466484\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -918878.875000\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -914362.750000\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -895082.875000\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -825204.187500\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -870404.687500\n",
      "    epoch          : 484\n",
      "    loss           : -874120.5990099009\n",
      "    val_loss       : -870797.9342505455\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -842474.000000\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -918791.250000\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -812106.000000\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -841997.125000\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -857223.125000\n",
      "    epoch          : 485\n",
      "    loss           : -870816.3873762377\n",
      "    val_loss       : -870984.4197527885\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -1001230.500000\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -896824.125000\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -861051.125000\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -860288.062500\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -857782.312500\n",
      "    epoch          : 486\n",
      "    loss           : -865913.6992574257\n",
      "    val_loss       : -870733.4781438827\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -1003782.000000\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -838327.375000\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -816554.500000\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -1001195.125000\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -815681.500000\n",
      "    epoch          : 487\n",
      "    loss           : -867177.1120049505\n",
      "    val_loss       : -868922.6465605736\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -1006767.937500\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -840843.500000\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -865143.687500\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -848914.187500\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -824884.000000\n",
      "    epoch          : 488\n",
      "    loss           : -869256.9975247525\n",
      "    val_loss       : -818262.91542902\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -990011.500000\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -818702.625000\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -858891.000000\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -846655.500000\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -838701.937500\n",
      "    epoch          : 489\n",
      "    loss           : -841459.6138613861\n",
      "    val_loss       : -836335.4384738922\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -981827.000000\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -838050.937500\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -845954.000000\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -864970.812500\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -853558.937500\n",
      "    epoch          : 490\n",
      "    loss           : -855976.7926980198\n",
      "    val_loss       : -861175.5307679176\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -1000995.062500\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -897237.062500\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -862321.937500\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -995311.875000\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -840527.625000\n",
      "    epoch          : 491\n",
      "    loss           : -860468.989480198\n",
      "    val_loss       : -853985.4630948066\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -826224.125000\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -895965.125000\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -835561.687500\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -816880.625000\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -861934.000000\n",
      "    epoch          : 492\n",
      "    loss           : -861970.6101485149\n",
      "    val_loss       : -864046.8798563003\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -900791.125000\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -868150.750000\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -849297.625000\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -885166.000000\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -869438.375000\n",
      "    epoch          : 493\n",
      "    loss           : -869014.2487623763\n",
      "    val_loss       : -869307.3791455269\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -1004440.250000\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -812739.562500\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -839134.062500\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -848398.625000\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -820433.250000\n",
      "    epoch          : 494\n",
      "    loss           : -871098.2827970297\n",
      "    val_loss       : -870722.3294091225\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -1004928.375000\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -843520.125000\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -887772.500000\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -814811.187500\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -821845.625000\n",
      "    epoch          : 495\n",
      "    loss           : -873920.2834158416\n",
      "    val_loss       : -872295.2079298019\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -917670.937500\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -843960.250000\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -856729.500000\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -857841.312500\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -869864.125000\n",
      "    epoch          : 496\n",
      "    loss           : -874618.3044554455\n",
      "    val_loss       : -874589.2615917206\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -1007609.250000\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -844827.750000\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -891789.125000\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -857637.500000\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -868548.625000\n",
      "    epoch          : 497\n",
      "    loss           : -875766.2400990099\n",
      "    val_loss       : -872482.4431000709\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -1003707.187500\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -914610.125000\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -865431.062500\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -805870.375000\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -825066.750000\n",
      "    epoch          : 498\n",
      "    loss           : -856099.6850247525\n",
      "    val_loss       : -849227.3063397408\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -980456.812500\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -896839.375000\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -841531.250000\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -880118.625000\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -851897.500000\n",
      "    epoch          : 499\n",
      "    loss           : -862405.3242574257\n",
      "    val_loss       : -866498.0160099029\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -858313.750000\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -916777.250000\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -836855.250000\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -856529.812500\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -869864.750000\n",
      "    epoch          : 500\n",
      "    loss           : -869875.9727722772\n",
      "    val_loss       : -869069.838305378\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0325_145359/checkpoint-epoch500.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VaeCategoryModel(\n",
       "  (_category): FreeCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=12, bias=True)\n",
       "        (1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (4): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=12, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=20, bias=True)\n",
       "        (1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=20, out_features=20, bias=True)\n",
       "        (4): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=20, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=36, bias=True)\n",
       "        (1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (4): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=36, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=68, bias=True)\n",
       "        (1): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=68, out_features=68, bias=True)\n",
       "        (4): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=68, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=102, bias=True)\n",
       "        (1): LayerNorm((102,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=102, out_features=102, bias=True)\n",
       "        (4): LayerNorm((102,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=102, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=106, bias=True)\n",
       "        (1): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=106, out_features=106, bias=True)\n",
       "        (4): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=106, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_10): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_11): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_12): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_13): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=114, bias=True)\n",
       "        (1): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=114, out_features=114, bias=True)\n",
       "        (4): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=114, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_14): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_15): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_16): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=130, bias=True)\n",
       "        (1): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=130, out_features=130, bias=True)\n",
       "        (4): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=130, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_17): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_18): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=162, bias=True)\n",
       "        (1): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=162, out_features=162, bias=True)\n",
       "        (4): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=162, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_19): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_20): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=54, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (encoders): ModuleDict(\n",
       "    ($p(Z^{16} | Z^{8})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{32} | Z^{8})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=32, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{64} | Z^{8})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=64, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{128} | Z^{8})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | Z^{8})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (3): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=196, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | Z^{8})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{32} | Z^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{64} | Z^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=64, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{128} | Z^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=128, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | Z^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (3): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=196, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | Z^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{64} | Z^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{128} | Z^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=128, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | Z^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (3): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=196, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | Z^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{128} | Z^{64})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | Z^{64})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (3): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=196, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | Z^{64})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{196} | Z^{128})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (3): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=196, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | Z^{128})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(X^{784} | Z^{196})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{32})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{128})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{8})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{64})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{16})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAALLklEQVR4nO3cMVIbW9qA4e9MOcCZiuROLHYgMysYKbyZGK/AaAemvAIX7ADtwNA7QEuQtQM6HgemepLfDv6qM4GFLthg3/EFCfl7nioVqCX1OURvne4jSq01ACCLv216AgCwTsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0Aqz9Y52PPnz//9+fPn39Y5JgDbYWdn58OnT5/+/tjjlFrrY4/xx2Cl1HWOB8D2KKVErbU89jgudQKQivABkIrwAZCK8AGQivABkIrwAZCK8AGQivABkIrwAZCK8AGQivABkIrwAZCK8AGQivABkIrwAZCK8AGQivABkIrwAZCK8AGQivABkIrwAZCK8AGQivABkIrwAZCK8AGQivABkIrwwRPSdV0sFotomia6rtv0dOCXJHzwhMzn85jP59Hv96Nt201PB35Jwgdr0jTNreeTySSOjo6iaZqYTqext7cX+/v78f79+3j16lX0+/2I+LIK/PqzwM97tukJQAaz2SwGg8GtYy9evIjDw8OIiBiNRnFxcRFnZ2dxenoaXdfFdDqN169fR6/Xi4iItm1XMQR+Xqm1rm+wUuo6x4N1WywWMZvNot/vry5XjsfjODo6iuPj4zs/c3BwEG/evInBYBCLxSKurq5id3c3er3erdB97xzwKyilRK21PPY4LnXCA/v48WP0+/0YDAbx7t27iIh7N6pMJpN4+fLlajU4GAxiOBzGYDD4ZnXnnh88DOGDBzQYDKJt21XIvrcz8+TkJPb29mI8HkfXdT/cxbm7u/uAM4W8hA8eSdM0MZlM7n3t8vIyXr9+HRERZ2dnq3t5wOMSPnhAbdtG13Uxm83i6uoqxuNxRMStqC0Wizg6OorRaLSK4+Xl5Q/PLYzwMGxugQc0nU6j3+/HcDi8dfzmhpef8Vc/D9vA5hbYMl3Xxfn5+Z2vDYfDWCwWP33eiBA9eCBWfAA8CVZ8APAIhA+AVIQPgFSED4BUhA+AVIQPgFSED4BUhA+AVIQPgFSED4BUhA+AVIQPgFSED4BUhA+AVIQPgFSED4BUhA+AVIQPgFSED4BUhA+AVIQPgFSED4BUhA+AVIQPgFSED4BUhA+AVIQPgFSED4BUnq1zsJ2dnQ+llN/WOSYA22FnZ+fDOsYptdZ1jAPcUEr5PSIOa62/b3oukI1LnQCkInwApCJ8AKQifACkInwApCJ8AKQifACkInwApCJ8AKQifACkstb/1Qnw2EopvYjoLx+zWmu30Qnx5FjxAb+a/eWjjS/xg1us+ICtU0oZRsTB8ullRPwjIk5rrbOImC9fm0TEPzczQ54y4QO2Tq11trykGbXWppQyjoir5bF/1Vony98PI+JkYxPlSRI+4FeyGxHz5YrwKiKaDc+HJ0j4gG3WL6UcRsQoIt7VWoWOH7K5Bdhm7fLnhejxZwkfsO3O4sv9vcNNT4TtIHzA1lnewxstH7sR0UVEr5RyvMl5sR3c4wO2zvJrC7Mbh9qvnsO9rPgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4AEhF+ABIRfgASEX4YDP+PyL+b9OTgIxKrXVtgz1//vzfnz9//m1tAwKwNXZ2dj58+vTp7489zlrDV0qp6xwPgO1RSolaa3nscVzqBCAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeGDNWiaJg4ODmIymcR0Oo2u62KxWETTNNF13ep9R0dH//O57zsXcLdnm54AZNDv9+P8/Dzato3d3d2Yz+fRtm3s7+9H27YxGAxisVj8VLjuOhdwPys++ElN03zzvJQSi8UiIiImk0kcHR1F13WrGLVtG71eL/b39+P9+/fx6tWr6Pf7q3P0er3V79Pp9Nb5r8/XNE1Mp9PY29uLiPjmXF3XfTM34A9WfPATZrPZNyur8Xgc4/E4rq6uouu6ODg4iOFwuHq9aZrVZ87OzuL09DS6rovpdBr9fj/6/X60bRtt296K4bUXL17E4eFhRESMRqO4uLi481yvX7+OiLj3PJCdFR98x2KxiJOTk2iaZnUfLSLi4uLizqi8efMmjo+PYz6f34peRKwuc0Z8WaXNZrNo23YVzOvV2n2uo3dwcBDHx8er8b8+V8SXCJ+env7lvx9+RVZ88AMfP36M4XAYg8Eg3r59G+Px+N5ADQaDmM/nsb+//81r1yux6/d9rdfrrVZx95lMJvHy5ctbn7/vnl7btt89F2RlxQffMRgMbm0Y+dHmk6Zp4s2bN/H27dsHn8vJyUns7e2twvujuVyvLoHbhA/+pKZpYjKZfPf18Xgch4eHD765pGmauLy8XK0az87Obm2EAf484YPvaNs2uq6L2WwWV1dXq3toN6OzWCxiNBp9E6Kf+U7eXRaLRRwdHcVoNFrF9/Ly8oefE0a4W6m1rm+wUuo6x4O/6nrH5dcbVWaz2Won5mOOfb2h5X+1jvnBQyulRK21PPY4Vnxwj67r4vz8/M7XhsPh6vt6T831vT/Rg7tZ8QHwJFjxAcAjED4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUhE+AFIRPgBSET4AUnm2zsF2dnY+lFJ+W+eYAGyHnZ2dD+sYp9Ra1zEOcEMp5feIOKy1/r7puUA2LnUCkIrwAZCK8AGQivABkIrwAZCK8AGQivABkIrwAZCK8AGQivABkIrwAb+UUsq4lHJeSjktpRyWUnqllMHyeO/G+443OE02aK3/pBpgDdpa60EppR8RVxGxHxH9iJgvfy5KKYOI6G1uimyS8AFbp5QyjIiD5dPLiPhHRJzWWme11sXyeL/W2pZS5sv3TiLinzdO061rvjwtwgdsnVrr7PqyZa21KaWMI+KqlNKrtXbL59cB/FetdbJ8/2EppY2INiL6pZR+rbXdxN/A5ggf8CvZjS8ruX5EzJbH5ssV4lVENMtVYC9c6kxL+IBt1i+lHEbEKCLe1VqbiIha68n1G25c+owbx7rlZ0jIrk5gm11fpry4jh78iPAB2+4svtzfO9z0RNgOwgdsneU9u9HycX1fr+e7efwZ7vEBW6fWOos/Nq9EfLnkObvn7XCLFR8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIHwCpCB8AqQgfAKkIH2zGfyLictOTgIxKrXXTcwCAtbHiAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIJX/Apqy1+vo5DjTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEFCAYAAADOqip4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAPkUlEQVR4nO3dW4wb93XH8XNkr3cdWTK1vsXXWJSvdZzIa6oJjAZxo3XSp/aFthv09lBk9dRXCS4QJO1DARkFWvRNQvtS9JJI2z7UaWF4NyiK2nVSUaoDIzGi2lvbiWUH0a7oi2zLa+n0Yf+0KIpzSA53yNmj7wcgRM7hcM6O9rcznOHwr2YmAGLaNO4GABSHgAOBEXAgMAIOBEbAgcAuH3cDkalqXUSmRWRFRJoiUjWzgwUvc1ZEDpjZjj6fPyMiNRF50Mz2FNkbRo8teEFUtSoiu8zsoJnNy1rIK0Uv18wWRWRpgFmeEJFDZQ23qr4y7h42MgJenKqILLcemNkxGSx4o1Ixs+a4m3A8OO4GNjICXpyGiDyhqnvT1lzSllxE1nal022/qlbapp1S1Zl0/4CqVtPjA63XaXveRa/RSVXn0nP2dj4n7Z5Pp+dUVbWuqq+k5x9u66ueptXTW4C+e+1YXmbf3Zad+jvaNn+3Prr2jMTMuBV0E5EZEVkQEZO1X9RKW+1A+ndWRPa3TV8QkZl0f7+I7M143ievl5ZzuP012qbvT/crrWV29LjQ+TjNV217jb3tfbctt69eO17f7bt92V1+FreP9vm4rd3YghfIzI6Z2SNmpiKyKGshaNXa3/NWOmZt7covt91f6fL6zdZyZC1UnR4XkeW0JaymWy/Tqe/WcveIyLG2+isdy+qr1z777lx2O68Pb75LGgEvSGsXssXM9klbwNLu6aw4wU2anfUBVETkWPrlP2Zmj/QxjxvOZLp1Zx177XfZ3foYdL5LBgEvTiWdJhMRkfTecCndnxORZVs74t2qzwy6gLb3r1VZ20PodFhEHml7/sDLSK/RPt+ujGX1rY++R9LHpYCAFywdBKqLyJyI7EuTF0VkR8dWfrq1K912YO4REXk0BWKPiMx2HLyaTa+xR0S+kZbXeo259AekdQDqol34juVV0nNq6Q+QiHxy2q3ZOrgla+/jl3L02q5b3xctu8vP0q2Pi+bDeZoOUmCDUdWjZrbhTiFt1L43KrbgQGAEfANKu6XVjbZbulH73sjYRQcCYwsOBEbAgcAKv1z0Cp20Kdlc9GKAS9q7cuqkmV3XOT1XwNN5yKb0cX3zlGyWL+juPIsB0KdFm3+t2/SBd9Fbn85qfQqr2wcoAJRDnvfgu+T8RQVLcuHHB0Xkk0sUG6raWJUzw/QHYAh5Al7peHxN5xNs7VtMamZWm5DJXI0BGF6egDel7WoiAOWVJ+BH5PxWvCprF9sDKKGBA25rXztUTQfXKu2XPAIol1ynyczsyXSXcAMlxifZgMAIOBAYAQcCI+BAYAQcCIyAA4ERcCAwAg4ERsCBwAg4EBgBBwIj4EBgBBwIjIADgRFwIDACDgRGwIHACDgQGAEHAiPgQGAEHAis8NFFsfFcdu1Fg9Vc4OM7b3HrJ76cPZrs+zefdee9/Sm/PvFMw63jQmzBgcAIOBAYAQcCI+BAYAQcCIyAA4ERcCAwzoMHdPn2z7j1l751rVv/gweed+vfuu77A/fUsmr+ee4/+dJOt/69e77k1m/4q/8atKXQ2IIDgeUKuKqeUtUFVd273g0BWD95d9EfNbPFde0EwLrLu4teUdVqVlFV51S1oaqNVTmTcxEAhpU34NMisqKqB7oVzeygmdXMrDYhk/m7AzCUXAFPAW6KSFNV6+vbEoD1MnDA0+73TBHNAFhfeQ6yHRKRamvLbWbz69sS+rFpaiqz9pNvXufO+59f+Uu3fsvlV+VpqS/n5Jxbf6xyxK0ff+x6t/7i9EOZtdu+femdIx844GnX/Fi6EW6gxPigCxAYAQcCI+BAYAQcCIyAA4FxuegGtfLYA5m1fV/8F3feXqfBTp19360/dfo2t/7vzXszazNbXnfnrW/5sVs/VPUvVX36d5/NrP3Ft7P7iootOBAYAQcCI+BAYAQcCIyAA4ERcCAwAg4ExnnwDWr5ax9m1h7f8r/uvP99ZsKt73v5627950dvcuue4zv9S1n/6HOv5X5tEZGFt+9zqv6lqhGxBQcCI+BAYAQcCIyAA4ERcCAwAg4ERsCBwDgPvkGdW83+23z4vTvceQ+feNCtv/td/zz3nc+vuPU3H74ms/blr73szjusH799o1N9o9BllxFbcCAwAg4ERsCBwAg4EBgBBwIj4EBgBBwIjPPgG9TVjcnM2j/evMud97WXPu3W7/7BKX/hJ/36mens8+DfmO41hO9wQxcff/HWzNqdnAcHEEnPgKtqXVUXukybVdW54loDMKyeATez+fbHqlpP0xfT49liWgMwrDy76LtEZCndXxKRmc4nqOqcqjZUtbEqZ4bpD8AQ8gS80vH4oiMqZnbQzGpmVpuQ7INBAIqVJ+BNEZle5z4AFCBPwI/I+a14VUQWsp8KYJx6ngdPB9Fqqlo3s3kzm1fVvWl6pXWwDaM1tZL9Hd+vLl3vznvTs+rWN516x61/cH/2uWYRkasf+kVmbfvEcOe5e7nrb9/NrFmhSy6nngFPAd7WMe3JdJdwAyXGB12AwAg4EBgBBwIj4EBgBBwIjMtFS2rT5+916807s/82T02fduf9aLN/quq9B25x6yc/5//a/P09/+BUh/tk473P/Z5bv+3oi0O9fjRswYHACDgQGAEHAiPgQGAEHAiMgAOBEXAgMM6Dl9Qbu7e5db0/+5LOq670vybr5MMTbn25eYVb/+zOJbe+czL/ue73zn3o1jc/U+zlptGwBQcCI+BAYAQcCIyAA4ERcCAwAg4ERsCBwDgPPibvPv5Ftz79Gyfc+vaty5m126/MromIvFTxhw++euIDt/6b0//j1ofx12/f49ZvWPCHAP54PZsJgC04EBgBBwIj4EBgBBwIjIADgRFwIDACDgTGefCCbNr5K2799Nffdut/eOsP3PrmTdnXfJ/t8Xe7ttm/nnvV/F+LX5/yhxcW8a8n9xz+2Yxbv+pVv3dcqOcWXFXrqrrQMe2Uqi6o6t7iWgMwrH7GB59X1T0dkx9N44YDKLG878Erqlpd104ArLu8AZ8WkRVVPdCtqKpzqtpQ1caq+N8PBqA4uQJuZgfNrCkiTVWtZ9RrZlabGHKwOQD5DRzwtHX2D3UCKIV+jqLPikitbUt9KE2vi6wdhCuuPQDD6Oco+qKIbGt73BSRY+lGuDMc//2tbv1P7z3s1ndO/tytv3AmewzvCfWvin71o+vc+q4r/XPNZ8x//U8558FX7aw774nXr3HrdwnnwQfBJ9mAwAg4EBgBBwIj4EBgBBwIjIADgXG5aE6X3X2HW7//wf9z67+zxf9q47PmfwLwuQ+y60//0v8c0mtv+0MT//OVD7j1P97+r25995XZp8Im9DJ3Xjmnfh0DYQsOBEbAgcAIOBAYAQcCI+BAYAQcCIyAA4FxHjyn5V/1L7l8eOvLQ73+O+c+dOuHTzyYWVv60c3uvJ960/+7vrTdvxz0O1v8oY933/pcZu2FM/5XeE29ya/kemILDgRGwIHACDgQGAEHAiPgQGAEHAiMgAOBcdIxp23fOerWn6nf49b33/CCW1/62P+veeVn12fWbnze3Hm3Hj/l1t/Y7V8vfm4m/zXbk+p/bfLkSu6XRhdswYHACDgQGAEHAiPgQGAEHAiMgAOBEXAgMM6D52SrH7n1M41pt/70ff73nv/ogzv9Bt7J/q9T/zS4fLx1yq2/f/M5t16ZeN9fgOPP3/qqW7/58Ctu3b9SHZ3cgKtqRUSq6bbLzPal6XURaYpI1cwOFtwjgJx67aI/JiI1M5sXEVHVuRRuMbPFNG222BYB5OUG3MwOtm2hqyKyJCK70r+S/vXHyQEwNn0dZFPVqoispK12paN8TZfnz6lqQ1Ubq+J/BxeA4vR7FL1uZnvS/aaIuEeQ0pa/Zma1CfEPJgEoTs+Aq2rdzJ5M92dE5Iic34pXRWShsO4ADKXXUfRZEdmvqk+kSfvMbF5V96ZapXWwDRe66nX/XNU/Ldfc+u7KT9z6jvtOZNZenrjRnVcm/SF8f+vzDbf+Z5/+of/6MpFZ+Wkz+zJXEZGrL/O/LhqDcQOewrujy/Qn013CDZQYn2QDAiPgQGAEHAiMgAOBEXAgMAIOBMblogW59ns/devf3323W9++86Rb/+b2pzJrb916tTtvZZN/ueevTZ1265N6hVt/6aPs13/rxRvceTe/8bxbx2DYggOBEXAgMAIOBEbAgcAIOBAYAQcCI+BAYJwHL8jZZX8c3On/8M+D/82mh9z6TbXsIYBfOH2bO+9vb/Ov5/67d293688273DrjX/7bGbtru/+wp3XH1wYg2ILDgRGwIHACDgQGAEHAiPgQGAEHAiMgAOBqVmPsWaHtFWn7Qu6u9BlAJe6RZs/amYXfdk+W3AgMAIOBEbAgcAIOBAYAQcCI+BAYAQcCIyAA4G5AVfViqrOqGpdVfe3TT+lqguqurf4FgHk1WsL/piI1MxsXkREVefS9EfN7BEze7LQ7gAMxf3KJjM72PawKiIL6X5FVatmtlRYZwCG1td7cFWtisiKmS2mSdMisqKqBzKeP6eqDVVtrMqZdWoVwKD6PchWN7M9rQdmdtDMmiLSVNV655NTvWZmtQmZXKdWAQyq57eqqmq99V5bVWdEpCYiDTM7VnRzAIbT6yj6rIjsV9WjqnpU1nbND6VaXUSkdQAOQPn0Osi2KCI7upSOpRvhBkqMD7oAgRFwIDACDgRGwIHACDgQGAEHAiPgQGAEHAiMgAOBEXAgMAIOBEbAgcAIOBAYAQcCK3z4YFX9pYi81jbpWhE5WehC86O3fMraW1n7Eln/3j5jZtd1Tiw84BctULXRbRzjMqC3fMraW1n7Ehldb+yiA4ERcCCwcQT8YO+njA295VPW3sral8iIehv5e3AAo8MuOhAYAQcCG2nA0yils22DGJZCGUdLTetqocu0sa+/jN7Gug6dkXDHvs7GOUrvyALeNlDCYno8O6pl96F0o6V2DihRpvWXMdjFuNfhRSPhlmidjW2U3lFuwXeJSGs00iURmRnhsnuppAEWy6zM609kzOswjYfXOjJdlbV1VIp1ltGbyAjW2SgDXul4fM0Il92LO1pqSVQ6Hpdp/YmUZB12jIRb6SiPdZ0NOkrvehhlwJuy9gOVTq/RUkuiKSVdfyKlWoftI+E2pVzrbKBRetfDKAN+RM7/Ra2KyEL2U0cnvVcr2+5uN6VcfyLlWYddRsItzTrr7G1U62xkAU8HGKrpQEelbTdl3Eo5WmpaT7WOvkqx/jp7kxKsw24j4ZZlnY1zlF4+yQYExgddgMAIOBAYAQcCI+BAYAQcCIyAA4ERcCCw/wfPhM0UZmKBeQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    path.draw()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
