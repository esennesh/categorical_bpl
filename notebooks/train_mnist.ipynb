{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.5,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 135866.406250\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -53212.695312\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -63197.398438\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -68127.234375\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -84725.640625\n",
      "    epoch          : 1\n",
      "    loss           : -54001.43854424505\n",
      "    val_loss       : -73676.9875\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -59257.640625\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -82477.757812\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -84791.625000\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -91264.429688\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -105683.796875\n",
      "    epoch          : 2\n",
      "    loss           : -83190.13985148515\n",
      "    val_loss       : -97218.15390625\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -84716.835938\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -97273.539062\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -108843.843750\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -113741.750000\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -119296.281250\n",
      "    epoch          : 3\n",
      "    loss           : -108197.85852413367\n",
      "    val_loss       : -120079.26953125\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -110247.343750\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -122762.703125\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -130375.953125\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -125955.750000\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -147012.187500\n",
      "    epoch          : 4\n",
      "    loss           : -130108.55081992575\n",
      "    val_loss       : -142019.59921875\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -135998.218750\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -142926.078125\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -160439.031250\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -156116.250000\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -160212.296875\n",
      "    epoch          : 5\n",
      "    loss           : -152402.92125618813\n",
      "    val_loss       : -163243.604296875\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -161303.562500\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -168603.171875\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -166256.171875\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -170474.937500\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -186774.468750\n",
      "    epoch          : 6\n",
      "    loss           : -173065.54393564357\n",
      "    val_loss       : -183394.487109375\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -184929.171875\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -183861.703125\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -197591.265625\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -200966.171875\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -204626.234375\n",
      "    epoch          : 7\n",
      "    loss           : -192810.27846534652\n",
      "    val_loss       : -202404.98828125\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -200986.093750\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -205148.765625\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -210183.468750\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -212365.562500\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -218477.531250\n",
      "    epoch          : 8\n",
      "    loss           : -210980.69832920792\n",
      "    val_loss       : -219960.627734375\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -218101.546875\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -216998.625000\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -226283.593750\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -241684.203125\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -219697.515625\n",
      "    epoch          : 9\n",
      "    loss           : -228034.83261138614\n",
      "    val_loss       : -236452.26875\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -248528.718750\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -237575.421875\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -249796.640625\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -253840.156250\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -251511.343750\n",
      "    epoch          : 10\n",
      "    loss           : -244689.74504950494\n",
      "    val_loss       : -252590.46875\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -267677.093750\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -258634.078125\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -262115.562500\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -243236.156250\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -263578.593750\n",
      "    epoch          : 11\n",
      "    loss           : -259880.29811262377\n",
      "    val_loss       : -267358.799609375\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -284883.437500\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -266680.250000\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -263167.062500\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -277681.750000\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -276987.812500\n",
      "    epoch          : 12\n",
      "    loss           : -274514.64990717825\n",
      "    val_loss       : -281424.98515625\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -301724.906250\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -280357.000000\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -291029.562500\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -311752.281250\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -294418.312500\n",
      "    epoch          : 13\n",
      "    loss           : -287977.08570544556\n",
      "    val_loss       : -294642.4859375\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -297943.000000\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -293905.468750\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -303232.218750\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -279264.593750\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -306358.781250\n",
      "    epoch          : 14\n",
      "    loss           : -300842.31373762374\n",
      "    val_loss       : -306999.653125\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -332652.062500\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -305946.531250\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -287042.312500\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -341251.625000\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -314504.562500\n",
      "    epoch          : 15\n",
      "    loss           : -312705.854269802\n",
      "    val_loss       : -318647.28359375\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -346131.437500\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -313774.687500\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -333425.562500\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -298446.437500\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -331190.625000\n",
      "    epoch          : 16\n",
      "    loss           : -324342.2119430693\n",
      "    val_loss       : -329257.21640625\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -360320.437500\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -324822.312500\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -330499.406250\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -336513.156250\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -311336.593750\n",
      "    epoch          : 17\n",
      "    loss           : -335226.71751237626\n",
      "    val_loss       : -340312.18359375\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -372073.000000\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -334279.437500\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -343036.781250\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -344056.625000\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -351451.500000\n",
      "    epoch          : 18\n",
      "    loss           : -345495.353960396\n",
      "    val_loss       : -350044.9796875\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -384437.031250\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -337141.156250\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -339706.125000\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -370999.187500\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -325674.906250\n",
      "    epoch          : 19\n",
      "    loss           : -355396.40903465345\n",
      "    val_loss       : -359946.9921875\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -395877.906250\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -355921.187500\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -375888.437500\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -370000.343750\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -362858.937500\n",
      "    epoch          : 20\n",
      "    loss           : -364271.60550742573\n",
      "    val_loss       : -368713.4234375\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -406894.843750\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -381256.812500\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -356846.656250\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -373341.437500\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -371540.875000\n",
      "    epoch          : 21\n",
      "    loss           : -373421.8941831683\n",
      "    val_loss       : -377365.84609375\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -417327.281250\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -371436.562500\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -363865.937500\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -377983.437500\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -348685.187500\n",
      "    epoch          : 22\n",
      "    loss           : -381883.6729579208\n",
      "    val_loss       : -386615.296875\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -428101.343750\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -386316.687500\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -382998.031250\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -384466.281250\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -389275.500000\n",
      "    epoch          : 23\n",
      "    loss           : -390339.4832920792\n",
      "    val_loss       : -392495.4921875\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -436136.375000\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -386755.625000\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -377031.281250\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -392577.093750\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -407262.062500\n",
      "    epoch          : 24\n",
      "    loss           : -397761.6395420792\n",
      "    val_loss       : -402055.45390625\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -445535.750000\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -400697.562500\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -402569.937500\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -400912.031250\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -405901.312500\n",
      "    epoch          : 25\n",
      "    loss           : -406050.59962871287\n",
      "    val_loss       : -409148.54921875\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -454250.656250\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -422272.875000\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -371103.156250\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -407908.625000\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -379310.062500\n",
      "    epoch          : 26\n",
      "    loss           : -413014.39449257427\n",
      "    val_loss       : -416403.82421875\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -462770.812500\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -435153.531250\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -436809.000000\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -436481.875000\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -417841.937500\n",
      "    epoch          : 27\n",
      "    loss           : -420216.297029703\n",
      "    val_loss       : -423499.5671875\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -468708.312500\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -417518.406250\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -421379.625000\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -474620.781250\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -383809.156250\n",
      "    epoch          : 28\n",
      "    loss           : -427100.5222772277\n",
      "    val_loss       : -429822.075\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -476843.593750\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -425321.687500\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -436517.531250\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -396000.437500\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -440606.843750\n",
      "    epoch          : 29\n",
      "    loss           : -433998.3363242574\n",
      "    val_loss       : -436492.32421875\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -484823.812500\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -449446.062500\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -417549.125000\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -444825.312500\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -401030.000000\n",
      "    epoch          : 30\n",
      "    loss           : -439897.88273514854\n",
      "    val_loss       : -442342.428125\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -492868.156250\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -402767.312500\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -465256.500000\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -423148.187500\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -452930.531250\n",
      "    epoch          : 31\n",
      "    loss           : -446257.6126237624\n",
      "    val_loss       : -449268.60390625\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -499329.187500\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -440486.312500\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -430309.437500\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -457991.875000\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -454969.375000\n",
      "    epoch          : 32\n",
      "    loss           : -451652.52351485146\n",
      "    val_loss       : -454215.50390625\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -504993.000000\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -447880.812500\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -453714.031250\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -416124.218750\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -456403.437500\n",
      "    epoch          : 33\n",
      "    loss           : -458097.2713490099\n",
      "    val_loss       : -460518.8671875\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -511163.781250\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -457009.000000\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -456904.593750\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -457328.031250\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -458973.312500\n",
      "    epoch          : 34\n",
      "    loss           : -463525.4254331683\n",
      "    val_loss       : -464813.07734375\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -517660.687500\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -456749.312500\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -471342.250000\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -522315.312500\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -474050.625000\n",
      "    epoch          : 35\n",
      "    loss           : -468400.5699257426\n",
      "    val_loss       : -469562.41015625\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -522829.500000\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -459131.281250\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -428232.500000\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -470723.968750\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -469737.343750\n",
      "    epoch          : 36\n",
      "    loss           : -473331.00030940596\n",
      "    val_loss       : -475058.80625\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -528953.937500\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -466949.406250\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -466529.562500\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -430468.843750\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -434455.750000\n",
      "    epoch          : 37\n",
      "    loss           : -477172.7116336634\n",
      "    val_loss       : -479564.2546875\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -534120.687500\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -469248.750000\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -479499.937500\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -434897.750000\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -476927.437500\n",
      "    epoch          : 38\n",
      "    loss           : -483305.72648514854\n",
      "    val_loss       : -485154.79375\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -440098.375000\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -439971.687500\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -440025.562500\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -510568.093750\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -482766.562500\n",
      "    epoch          : 39\n",
      "    loss           : -488013.90068069304\n",
      "    val_loss       : -489689.83984375\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -545960.750000\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -447014.968750\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -515531.343750\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -499644.843750\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -488915.500000\n",
      "    epoch          : 40\n",
      "    loss           : -492982.40191831684\n",
      "    val_loss       : -494124.65078125\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -512144.375000\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -514923.687500\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -519926.281250\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -518435.500000\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -449005.093750\n",
      "    epoch          : 41\n",
      "    loss           : -496248.0278465347\n",
      "    val_loss       : -495608.81640625\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -554620.375000\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -487893.437500\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -450627.843750\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -496437.718750\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -504432.312500\n",
      "    epoch          : 42\n",
      "    loss           : -500846.8604579208\n",
      "    val_loss       : -502810.69140625\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -561173.875000\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -492966.812500\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -477829.750000\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -563987.000000\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -500195.593750\n",
      "    epoch          : 43\n",
      "    loss           : -505676.3917079208\n",
      "    val_loss       : -506879.9234375\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -478306.500000\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -533747.750000\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -502956.156250\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -499719.875000\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -503209.312500\n",
      "    epoch          : 44\n",
      "    loss           : -509489.63459158415\n",
      "    val_loss       : -510662.35625\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -499059.312500\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -499763.625000\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -536523.750000\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -572512.187500\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -503975.187500\n",
      "    epoch          : 45\n",
      "    loss           : -512204.2224628713\n",
      "    val_loss       : -513994.75\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -574201.687500\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -504923.312500\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -539660.312500\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -466879.312500\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -523410.343750\n",
      "    epoch          : 46\n",
      "    loss           : -517535.9603960396\n",
      "    val_loss       : -519119.16875\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -579409.562500\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -545246.375000\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -488888.125000\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -581170.500000\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -513443.468750\n",
      "    epoch          : 47\n",
      "    loss           : -520745.2431930693\n",
      "    val_loss       : -522210.1625\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -583394.500000\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -546165.500000\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -529694.000000\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -550453.250000\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -475200.250000\n",
      "    epoch          : 48\n",
      "    loss           : -524298.7416460396\n",
      "    val_loss       : -525588.4875\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -587415.625000\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -512538.187500\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -495839.750000\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -522300.562500\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -476818.500000\n",
      "    epoch          : 49\n",
      "    loss           : -527792.0495049505\n",
      "    val_loss       : -527934.7765625\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -589674.750000\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -513765.718750\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -558017.562500\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -483788.187500\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -525958.250000\n",
      "    epoch          : 50\n",
      "    loss           : -531816.3728341584\n",
      "    val_loss       : -533224.875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0710_172137/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -594956.375000\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -521729.750000\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -561160.437500\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -598286.625000\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -537162.500000\n",
      "    epoch          : 51\n",
      "    loss           : -535007.7626856435\n",
      "    val_loss       : -535078.346875\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -600798.125000\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -529393.250000\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -487723.250000\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -601805.875000\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -528415.687500\n",
      "    epoch          : 52\n",
      "    loss           : -538436.6030321782\n",
      "    val_loss       : -540075.809375\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -562794.375000\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -532481.125000\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -532401.437500\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -536560.625000\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -537484.000000\n",
      "    epoch          : 53\n",
      "    loss           : -540336.9935024752\n",
      "    val_loss       : -542008.7390625\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -606447.812500\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -506596.593750\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -510536.968750\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -570945.875000\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -541946.750000\n",
      "    epoch          : 54\n",
      "    loss           : -545010.0507425743\n",
      "    val_loss       : -546471.96015625\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -611303.187500\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -567613.125000\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -517490.843750\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -613100.062500\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -538247.062500\n",
      "    epoch          : 55\n",
      "    loss           : -548448.3573638614\n",
      "    val_loss       : -549162.05703125\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -615138.312500\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -499182.750000\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -555516.062500\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -576989.062500\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -552438.312500\n",
      "    epoch          : 56\n",
      "    loss           : -550980.1237623763\n",
      "    val_loss       : -551867.271875\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -615423.812500\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -538810.062500\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -547526.062500\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -619916.937500\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -544752.000000\n",
      "    epoch          : 57\n",
      "    loss           : -554127.8938737623\n",
      "    val_loss       : -555293.57578125\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -620996.250000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -576986.250000\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -501256.968750\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -624643.562500\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -555857.437500\n",
      "    epoch          : 58\n",
      "    loss           : -557162.4102722772\n",
      "    val_loss       : -559002.8359375\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -624568.500000\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -548273.125000\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -555745.437500\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -587957.500000\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -551617.812500\n",
      "    epoch          : 59\n",
      "    loss           : -561323.8131188119\n",
      "    val_loss       : -561934.828125\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -629540.250000\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -562544.000000\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -555876.125000\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -592623.750000\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -557272.187500\n",
      "    epoch          : 60\n",
      "    loss           : -564411.6642945545\n",
      "    val_loss       : -563559.04140625\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -631570.125000\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -553610.625000\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -529110.062500\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -565393.125000\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -561310.500000\n",
      "    epoch          : 61\n",
      "    loss           : -566140.6181930694\n",
      "    val_loss       : -567144.23671875\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -635262.437500\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -556692.875000\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -575606.250000\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -518179.000000\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -560334.500000\n",
      "    epoch          : 62\n",
      "    loss           : -570691.0306311881\n",
      "    val_loss       : -572251.3578125\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -639625.125000\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -519825.250000\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -515336.468750\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -603117.125000\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -561978.375000\n",
      "    epoch          : 63\n",
      "    loss           : -573565.1905940594\n",
      "    val_loss       : -573644.7484375\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -641453.125000\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -560447.125000\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -580551.875000\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -582336.875000\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -568455.812500\n",
      "    epoch          : 64\n",
      "    loss           : -576467.6243811881\n",
      "    val_loss       : -578227.7140625\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -644456.125000\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -602791.625000\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -574244.750000\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -605905.875000\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -575824.062500\n",
      "    epoch          : 65\n",
      "    loss           : -580538.2456683168\n",
      "    val_loss       : -580653.80703125\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -607309.125000\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -549544.375000\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -525551.000000\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -574856.625000\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -575961.000000\n",
      "    epoch          : 66\n",
      "    loss           : -582683.823019802\n",
      "    val_loss       : -582800.6265625\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -650324.062500\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -565732.250000\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -531229.500000\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -546132.625000\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -591355.500000\n",
      "    epoch          : 67\n",
      "    loss           : -585056.2790841584\n",
      "    val_loss       : -585562.24375\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -653054.750000\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -611645.375000\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -551260.000000\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -574487.500000\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -581730.375000\n",
      "    epoch          : 68\n",
      "    loss           : -587556.1113861386\n",
      "    val_loss       : -587523.5875\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -657150.875000\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -613437.750000\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -555736.000000\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -587851.250000\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -576464.500000\n",
      "    epoch          : 69\n",
      "    loss           : -590337.4121287129\n",
      "    val_loss       : -591314.50078125\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -659486.250000\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -574478.375000\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -559428.812500\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -660856.000000\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -588714.750000\n",
      "    epoch          : 70\n",
      "    loss           : -593332.2004950495\n",
      "    val_loss       : -594094.3734375\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -662487.500000\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -623261.562500\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -541632.500000\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -665073.187500\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -603356.312500\n",
      "    epoch          : 71\n",
      "    loss           : -596095.1986386139\n",
      "    val_loss       : -597700.640625\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -668231.500000\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -584532.875000\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -581929.937500\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -584411.562500\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -548352.312500\n",
      "    epoch          : 72\n",
      "    loss           : -599800.0662128713\n",
      "    val_loss       : -600429.725\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -588228.750000\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -626964.875000\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -596674.500000\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -630802.562500\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -585097.562500\n",
      "    epoch          : 73\n",
      "    loss           : -601607.0674504951\n",
      "    val_loss       : -601214.3140625\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -671411.875000\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -594953.312500\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -550039.125000\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -629833.250000\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -608725.625000\n",
      "    epoch          : 74\n",
      "    loss           : -602748.5018564357\n",
      "    val_loss       : -605144.0734375\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -675562.125000\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -592135.625000\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -638114.187500\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -572631.812500\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -606992.125000\n",
      "    epoch          : 75\n",
      "    loss           : -607765.0946782178\n",
      "    val_loss       : -608057.62578125\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -635406.187500\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -592622.062500\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -607229.375000\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -556594.000000\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -592359.687500\n",
      "    epoch          : 76\n",
      "    loss           : -609093.5742574257\n",
      "    val_loss       : -608253.4859375\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -677174.562500\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -639704.812500\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -616866.062500\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -614688.375000\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -617753.062500\n",
      "    epoch          : 77\n",
      "    loss           : -611935.1596534654\n",
      "    val_loss       : -612942.13828125\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -561146.750000\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -561945.500000\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -558759.000000\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -560934.750000\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -597571.375000\n",
      "    epoch          : 78\n",
      "    loss           : -613717.6751237623\n",
      "    val_loss       : -614129.575\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -687150.500000\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -597003.500000\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -581454.500000\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -561604.750000\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -614250.937500\n",
      "    epoch          : 79\n",
      "    loss           : -616436.7753712871\n",
      "    val_loss       : -615777.23515625\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -687919.500000\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -600024.125000\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -602795.000000\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -686313.437500\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -619812.750000\n",
      "    epoch          : 80\n",
      "    loss           : -617348.4882425743\n",
      "    val_loss       : -616850.265625\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -690032.875000\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -582668.125000\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -563704.000000\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -600541.250000\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -629716.375000\n",
      "    epoch          : 81\n",
      "    loss           : -620964.0649752475\n",
      "    val_loss       : -622420.6265625\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -695561.000000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -609181.125000\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -605620.250000\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -572271.625000\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -619670.750000\n",
      "    epoch          : 82\n",
      "    loss           : -624255.6497524752\n",
      "    val_loss       : -624023.60390625\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -654128.125000\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -603469.125000\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -581927.562500\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -605117.625000\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -653927.062500\n",
      "    epoch          : 83\n",
      "    loss           : -624161.6615099009\n",
      "    val_loss       : -625623.884375\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -699402.625000\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -632852.000000\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -635400.250000\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -623651.437500\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -630832.937500\n",
      "    epoch          : 84\n",
      "    loss           : -629347.6683168317\n",
      "    val_loss       : -629443.84296875\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -702741.750000\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -615772.687500\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -579708.437500\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -704080.812500\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -614991.000000\n",
      "    epoch          : 85\n",
      "    loss           : -631303.3774752475\n",
      "    val_loss       : -631408.6015625\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -704983.125000\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -596488.375000\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -580088.000000\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -662517.500000\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -579750.125000\n",
      "    epoch          : 86\n",
      "    loss           : -633498.5625\n",
      "    val_loss       : -632828.13671875\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -708541.687500\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -637873.750000\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -596713.312500\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -663496.812500\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -618545.500000\n",
      "    epoch          : 87\n",
      "    loss           : -636347.0136138614\n",
      "    val_loss       : -636406.909375\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -710701.312500\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -664710.500000\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -636146.625000\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -587734.500000\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -615790.500000\n",
      "    epoch          : 88\n",
      "    loss           : -637931.5426980198\n",
      "    val_loss       : -636979.87421875\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -713803.375000\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -624471.375000\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -634755.375000\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -639672.812500\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -640348.312500\n",
      "    epoch          : 89\n",
      "    loss           : -639933.6584158416\n",
      "    val_loss       : -639800.0609375\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -714636.312500\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -623213.562500\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -617415.437500\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -588745.750000\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -624258.687500\n",
      "    epoch          : 90\n",
      "    loss           : -641649.6404702971\n",
      "    val_loss       : -639817.15\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -716880.187500\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -625312.687500\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -674905.187500\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -628014.562500\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -644944.937500\n",
      "    epoch          : 91\n",
      "    loss           : -644124.7159653465\n",
      "    val_loss       : -645471.91953125\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -722410.062500\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -607518.812500\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -608124.437500\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -645657.000000\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -595760.312500\n",
      "    epoch          : 92\n",
      "    loss           : -646987.6423267326\n",
      "    val_loss       : -642845.46171875\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -719852.437500\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -629060.125000\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -678326.062500\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -724356.375000\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -649193.562500\n",
      "    epoch          : 93\n",
      "    loss           : -648416.4993811881\n",
      "    val_loss       : -649893.43046875\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -727053.562500\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -634601.500000\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -651103.937500\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -634348.062500\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -650233.437500\n",
      "    epoch          : 94\n",
      "    loss           : -651752.8997524752\n",
      "    val_loss       : -651047.54375\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -729807.500000\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -634205.250000\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -657829.500000\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -731696.125000\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -658239.625000\n",
      "    epoch          : 95\n",
      "    loss           : -654974.198019802\n",
      "    val_loss       : -652657.2828125\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -730920.875000\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -685898.312500\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -616917.625000\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -658234.500000\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -654118.750000\n",
      "    epoch          : 96\n",
      "    loss           : -656271.9721534654\n",
      "    val_loss       : -656312.93828125\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -640376.500000\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -687611.125000\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -660647.125000\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -687691.375000\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -659397.312500\n",
      "    epoch          : 97\n",
      "    loss           : -658162.8780940594\n",
      "    val_loss       : -658227.2828125\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -735382.437500\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -687274.687500\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -660401.500000\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -609363.000000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -640351.937500\n",
      "    epoch          : 98\n",
      "    loss           : -660556.0600247525\n",
      "    val_loss       : -660649.846875\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -692832.000000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -622595.812500\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -642223.125000\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -660429.812500\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -667307.125000\n",
      "    epoch          : 99\n",
      "    loss           : -662313.3904702971\n",
      "    val_loss       : -662340.48203125\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -740949.812500\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -647644.375000\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -664254.562500\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -625103.625000\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -665355.750000\n",
      "    epoch          : 100\n",
      "    loss           : -664451.5897277228\n",
      "    val_loss       : -663650.3046875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0710_172137/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -742700.750000\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -647407.875000\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -647560.000000\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -650434.062500\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -667844.187500\n",
      "    epoch          : 101\n",
      "    loss           : -666350.5049504951\n",
      "    val_loss       : -665810.70859375\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -700201.000000\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -644171.000000\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -650531.062500\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -651756.750000\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -671406.937500\n",
      "    epoch          : 102\n",
      "    loss           : -668853.3991336634\n",
      "    val_loss       : -669953.28046875\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -748788.500000\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -655944.312500\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -633178.250000\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -622657.312500\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -644974.062500\n",
      "    epoch          : 103\n",
      "    loss           : -672702.3879950495\n",
      "    val_loss       : -671586.74375\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -750794.625000\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -701208.500000\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -674239.125000\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -671675.187500\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -705970.875000\n",
      "    epoch          : 104\n",
      "    loss           : -674979.2215346535\n",
      "    val_loss       : -673837.509375\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -754871.375000\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -707173.875000\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -633730.625000\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -656204.750000\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -655054.250000\n",
      "    epoch          : 105\n",
      "    loss           : -675882.8112623763\n",
      "    val_loss       : -676626.53671875\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -756053.125000\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -638972.437500\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -659886.125000\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -708253.937500\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -669281.500000\n",
      "    epoch          : 106\n",
      "    loss           : -677082.8514851485\n",
      "    val_loss       : -675178.15625\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -625700.250000\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -703980.500000\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -631245.687500\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -760519.375000\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -682635.625000\n",
      "    epoch          : 107\n",
      "    loss           : -680068.0662128713\n",
      "    val_loss       : -678304.3078125\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -758240.625000\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -656640.875000\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -641875.375000\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -704150.375000\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -677847.625000\n",
      "    epoch          : 108\n",
      "    loss           : -680130.4350247525\n",
      "    val_loss       : -680939.22890625\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -761711.000000\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -633423.875000\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -627668.000000\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -662531.062500\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -661979.750000\n",
      "    epoch          : 109\n",
      "    loss           : -684029.8069306931\n",
      "    val_loss       : -684335.35234375\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -765709.750000\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -666543.625000\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -716763.312500\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -662674.625000\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -630216.000000\n",
      "    epoch          : 110\n",
      "    loss           : -686385.2512376237\n",
      "    val_loss       : -681248.46640625\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -714759.250000\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -665323.375000\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -714785.062500\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -666972.125000\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -688867.125000\n",
      "    epoch          : 111\n",
      "    loss           : -688156.3236386139\n",
      "    val_loss       : -688569.85234375\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -721096.500000\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -669244.562500\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -667867.875000\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -639479.750000\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -668165.875000\n",
      "    epoch          : 112\n",
      "    loss           : -690690.1856435643\n",
      "    val_loss       : -690718.18203125\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -719557.562500\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -649249.000000\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -643668.750000\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -649525.750000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -689901.437500\n",
      "    epoch          : 113\n",
      "    loss           : -691798.5185643565\n",
      "    val_loss       : -691742.1359375\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -772637.500000\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -723369.875000\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -624237.375000\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -773632.687500\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -693079.687500\n",
      "    epoch          : 114\n",
      "    loss           : -691914.6200495049\n",
      "    val_loss       : -693211.55859375\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -775504.125000\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -674808.500000\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -698825.375000\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -696800.250000\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -673359.937500\n",
      "    epoch          : 115\n",
      "    loss           : -696312.2759900991\n",
      "    val_loss       : -696430.653125\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -656580.375000\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -675636.375000\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -696007.875000\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -695184.500000\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -695986.625000\n",
      "    epoch          : 116\n",
      "    loss           : -696946.4740099009\n",
      "    val_loss       : -697146.3734375\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -780868.500000\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -678831.062500\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -729380.687500\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -782083.125000\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -697795.437500\n",
      "    epoch          : 117\n",
      "    loss           : -698648.6237623763\n",
      "    val_loss       : -699098.42578125\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -731479.625000\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -683354.000000\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -733806.750000\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -784175.687500\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -703568.437500\n",
      "    epoch          : 118\n",
      "    loss           : -702612.2462871287\n",
      "    val_loss       : -701241.58515625\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -784987.437500\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -731765.125000\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -729972.250000\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -703188.062500\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -705458.375000\n",
      "    epoch          : 119\n",
      "    loss           : -703169.2029702971\n",
      "    val_loss       : -703613.28984375\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -789145.500000\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -738227.625000\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -736237.312500\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -703408.125000\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -677593.500000\n",
      "    epoch          : 120\n",
      "    loss           : -705517.770420792\n",
      "    val_loss       : -703263.034375\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -788766.375000\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -689215.875000\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -737831.375000\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -711386.437500\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -682664.750000\n",
      "    epoch          : 121\n",
      "    loss           : -707808.4628712871\n",
      "    val_loss       : -706636.93828125\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -792832.750000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -666256.750000\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -739632.875000\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -711798.000000\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -709671.750000\n",
      "    epoch          : 122\n",
      "    loss           : -709889.6652227723\n",
      "    val_loss       : -708104.10625\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -792872.000000\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -689676.125000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -710421.000000\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -662662.750000\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -734933.875000\n",
      "    epoch          : 123\n",
      "    loss           : -711299.7549504951\n",
      "    val_loss       : -708886.0734375\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -793702.875000\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -693111.687500\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -667408.625000\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -657833.937500\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -709695.625000\n",
      "    epoch          : 124\n",
      "    loss           : -711151.6268564357\n",
      "    val_loss       : -710897.3546875\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -798037.375000\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -748071.562500\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -689402.812500\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -665435.437500\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -686891.750000\n",
      "    epoch          : 125\n",
      "    loss           : -713908.6806930694\n",
      "    val_loss       : -712594.87265625\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -798245.625000\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -693260.000000\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -716749.250000\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -802601.000000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -717304.125000\n",
      "    epoch          : 126\n",
      "    loss           : -716123.6528465346\n",
      "    val_loss       : -714639.65390625\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -802713.937500\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -748712.312500\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -708463.250000\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -745140.750000\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -712230.937500\n",
      "    epoch          : 127\n",
      "    loss           : -714830.3465346535\n",
      "    val_loss       : -713276.10859375\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -662490.562500\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -668750.875000\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -672638.875000\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -669408.937500\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -720143.750000\n",
      "    epoch          : 128\n",
      "    loss           : -719161.2017326732\n",
      "    val_loss       : -717701.36015625\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -805069.500000\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -698315.750000\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -722229.375000\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -749573.562500\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -725219.625000\n",
      "    epoch          : 129\n",
      "    loss           : -721981.4127475248\n",
      "    val_loss       : -720799.2421875\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -809036.125000\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -753203.625000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -678003.250000\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -746700.937500\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -726379.875000\n",
      "    epoch          : 130\n",
      "    loss           : -723823.6788366337\n",
      "    val_loss       : -722069.40390625\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -811898.750000\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -701759.250000\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -695948.125000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -727083.562500\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -698270.312500\n",
      "    epoch          : 131\n",
      "    loss           : -724933.8415841584\n",
      "    val_loss       : -722274.77890625\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -811073.687500\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -702553.562500\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -678291.812500\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -696948.125000\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -674318.750000\n",
      "    epoch          : 132\n",
      "    loss           : -726110.541460396\n",
      "    val_loss       : -725194.7796875\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -813022.375000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -731247.187500\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -733175.750000\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -728681.000000\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -673213.875000\n",
      "    epoch          : 133\n",
      "    loss           : -726027.1986386139\n",
      "    val_loss       : -720176.56484375\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -808831.250000\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -754441.687500\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -697986.312500\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -731602.000000\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -705682.625000\n",
      "    epoch          : 134\n",
      "    loss           : -727179.2716584158\n",
      "    val_loss       : -727682.565625\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -816849.187500\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -762162.500000\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -688265.500000\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -761451.125000\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -733720.000000\n",
      "    epoch          : 135\n",
      "    loss           : -731575.0396039604\n",
      "    val_loss       : -730132.8984375\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -819191.625000\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -708108.562500\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -732755.125000\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -676917.875000\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -731291.125000\n",
      "    epoch          : 136\n",
      "    loss           : -732096.0693069306\n",
      "    val_loss       : -731773.85234375\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -821412.250000\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -767724.375000\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -693840.812500\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -735485.250000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -700013.062500\n",
      "    epoch          : 137\n",
      "    loss           : -734067.1027227723\n",
      "    val_loss       : -730554.890625\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -823738.750000\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -716254.687500\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -734149.500000\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -685427.125000\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -685970.062500\n",
      "    epoch          : 138\n",
      "    loss           : -735430.6813118812\n",
      "    val_loss       : -734613.703125\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -740722.312500\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -717434.125000\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -709479.500000\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -740960.875000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -689460.750000\n",
      "    epoch          : 139\n",
      "    loss           : -737608.2716584158\n",
      "    val_loss       : -736790.04453125\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -774199.875000\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -717108.937500\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -695319.375000\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -710004.625000\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -743260.250000\n",
      "    epoch          : 140\n",
      "    loss           : -739361.6893564357\n",
      "    val_loss       : -737346.03125\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -829905.000000\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -723624.250000\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -740596.937500\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -830473.312500\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -740810.750000\n",
      "    epoch          : 141\n",
      "    loss           : -741840.4486386139\n",
      "    val_loss       : -738716.9296875\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -830557.000000\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -718586.125000\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -695557.875000\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -768928.812500\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -713068.812500\n",
      "    epoch          : 142\n",
      "    loss           : -740867.3502475248\n",
      "    val_loss       : -736534.771875\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -830700.000000\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -678399.937500\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -679180.125000\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -769028.250000\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -713722.812500\n",
      "    epoch          : 143\n",
      "    loss           : -737576.3824257426\n",
      "    val_loss       : -739299.6765625\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -832290.500000\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -722383.187500\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -772194.375000\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -747436.875000\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -690988.312500\n",
      "    epoch          : 144\n",
      "    loss           : -743966.2957920792\n",
      "    val_loss       : -741142.4046875\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -834184.187500\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -774840.500000\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -695680.875000\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -751748.250000\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -746677.937500\n",
      "    epoch          : 145\n",
      "    loss           : -746560.9962871287\n",
      "    val_loss       : -745391.55546875\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -781236.875000\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -725903.375000\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -721166.250000\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -721938.062500\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -719867.562500\n",
      "    epoch          : 146\n",
      "    loss           : -749000.1051980198\n",
      "    val_loss       : -746602.28828125\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -841024.750000\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -724652.750000\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -750790.750000\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -781980.750000\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -754414.500000\n",
      "    epoch          : 147\n",
      "    loss           : -749217.5061881188\n",
      "    val_loss       : -746236.12265625\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -841348.500000\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -724751.500000\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -706085.500000\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -742866.875000\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -715289.437500\n",
      "    epoch          : 148\n",
      "    loss           : -747723.4944306931\n",
      "    val_loss       : -747095.21640625\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -707429.750000\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -727975.000000\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -702119.562500\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -751316.812500\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -725148.875000\n",
      "    epoch          : 149\n",
      "    loss           : -751376.1107673268\n",
      "    val_loss       : -750255.01484375\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -844994.000000\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -732338.250000\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -704120.437500\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -724322.562500\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -756821.875000\n",
      "    epoch          : 150\n",
      "    loss           : -754135.9387376237\n",
      "    val_loss       : -751252.221875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0710_172137/checkpoint-epoch150.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -845762.500000\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -709950.375000\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -709505.375000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -846447.625000\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -730176.750000\n",
      "    epoch          : 151\n",
      "    loss           : -755830.7004950495\n",
      "    val_loss       : -751377.16875\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -847193.687500\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -786736.625000\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -710946.250000\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -702688.750000\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -753262.875000\n",
      "    epoch          : 152\n",
      "    loss           : -754732.7487623763\n",
      "    val_loss       : -750837.12890625\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -849084.562500\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -724810.187500\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -784683.250000\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -756395.125000\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -708800.875000\n",
      "    epoch          : 153\n",
      "    loss           : -753783.6169554455\n",
      "    val_loss       : -755046.87890625\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -848810.875000\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -793122.812500\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -757294.937500\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -851799.625000\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -726329.312500\n",
      "    epoch          : 154\n",
      "    loss           : -759188.2283415842\n",
      "    val_loss       : -756931.03984375\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -851238.875000\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -791682.000000\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -701622.000000\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -762041.875000\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -723469.187500\n",
      "    epoch          : 155\n",
      "    loss           : -757437.0136138614\n",
      "    val_loss       : -757217.91328125\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -853024.875000\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -735875.437500\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -721265.250000\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -764878.500000\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -762885.875000\n",
      "    epoch          : 156\n",
      "    loss           : -762665.2902227723\n",
      "    val_loss       : -759034.86015625\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -795411.625000\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -714771.562500\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -763510.562500\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -856274.250000\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -762765.250000\n",
      "    epoch          : 157\n",
      "    loss           : -762759.8125\n",
      "    val_loss       : -759899.17890625\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -797236.875000\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -735790.437500\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -790931.000000\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -852370.250000\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -757209.437500\n",
      "    epoch          : 158\n",
      "    loss           : -757703.2586633663\n",
      "    val_loss       : -755735.63515625\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -852084.500000\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -737231.375000\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -737958.500000\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -760307.562500\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -770954.250000\n",
      "    epoch          : 159\n",
      "    loss           : -763347.6726485149\n",
      "    val_loss       : -762987.10703125\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -859794.812500\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -748276.562500\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -718323.187500\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -792864.250000\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -736129.375000\n",
      "    epoch          : 160\n",
      "    loss           : -767783.5748762377\n",
      "    val_loss       : -763056.41484375\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -858239.875000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -772103.875000\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -768073.562500\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -798568.375000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -720408.562500\n",
      "    epoch          : 161\n",
      "    loss           : -768344.9319306931\n",
      "    val_loss       : -764511.97578125\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -861540.500000\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -726250.187500\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -794063.625000\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -723011.625000\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -733459.875000\n",
      "    epoch          : 162\n",
      "    loss           : -767498.7722772277\n",
      "    val_loss       : -761781.24375\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -863070.125000\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -762026.625000\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -764951.375000\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -735321.375000\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -771322.625000\n",
      "    epoch          : 163\n",
      "    loss           : -768218.3805693069\n",
      "    val_loss       : -767509.23203125\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -864920.500000\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -726712.812500\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -725106.937500\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -724559.000000\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -775321.312500\n",
      "    epoch          : 164\n",
      "    loss           : -773068.3081683168\n",
      "    val_loss       : -769246.990625\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -867259.250000\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -808808.062500\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -800898.562500\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -867337.875000\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -738752.875000\n",
      "    epoch          : 165\n",
      "    loss           : -773436.7196782178\n",
      "    val_loss       : -768048.66015625\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -866368.437500\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -803710.937500\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -802862.375000\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -870211.500000\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -744184.875000\n",
      "    epoch          : 166\n",
      "    loss           : -773209.4622524752\n",
      "    val_loss       : -770326.75546875\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -870005.875000\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -755345.375000\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -802445.562500\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -770657.812500\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -772969.750000\n",
      "    epoch          : 167\n",
      "    loss           : -774749.7147277228\n",
      "    val_loss       : -770004.21015625\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -870717.937500\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -751580.687500\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -732766.875000\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -724148.125000\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -774500.875000\n",
      "    epoch          : 168\n",
      "    loss           : -775136.7896039604\n",
      "    val_loss       : -771293.3171875\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -872592.687500\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -810324.687500\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -806280.937500\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -726877.125000\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -780926.500000\n",
      "    epoch          : 169\n",
      "    loss           : -776628.4659653465\n",
      "    val_loss       : -774063.1484375\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -812591.875000\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -810294.375000\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -732266.312500\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -872399.000000\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -778816.000000\n",
      "    epoch          : 170\n",
      "    loss           : -775353.4418316832\n",
      "    val_loss       : -771627.68046875\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -872812.000000\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -751479.687500\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -781646.062500\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -876624.812500\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -733013.125000\n",
      "    epoch          : 171\n",
      "    loss           : -779500.3997524752\n",
      "    val_loss       : -776954.19140625\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -877809.625000\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -815873.875000\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -811054.500000\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -736140.937500\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -746632.500000\n",
      "    epoch          : 172\n",
      "    loss           : -782242.708539604\n",
      "    val_loss       : -777405.15859375\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -879541.187500\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -750278.562500\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -781354.375000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -725530.062500\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -734269.250000\n",
      "    epoch          : 173\n",
      "    loss           : -780366.0049504951\n",
      "    val_loss       : -778323.72734375\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -814684.375000\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -756449.125000\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -810340.687500\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -880187.625000\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -750129.562500\n",
      "    epoch          : 174\n",
      "    loss           : -783762.8849009901\n",
      "    val_loss       : -780063.7515625\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -879962.750000\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -755945.562500\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -813452.000000\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -786629.750000\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -758134.125000\n",
      "    epoch          : 175\n",
      "    loss           : -785502.6751237623\n",
      "    val_loss       : -780926.30859375\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -883105.437500\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -817359.625000\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -785004.500000\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -790334.312500\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -732410.500000\n",
      "    epoch          : 176\n",
      "    loss           : -785872.0290841584\n",
      "    val_loss       : -780498.8015625\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -885274.375000\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -754285.375000\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -741768.000000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -743708.812500\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -739815.625000\n",
      "    epoch          : 177\n",
      "    loss           : -786002.9387376237\n",
      "    val_loss       : -783066.353125\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -884978.500000\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -759909.937500\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -740133.750000\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -791185.750000\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -754805.500000\n",
      "    epoch          : 178\n",
      "    loss           : -789009.9053217822\n",
      "    val_loss       : -783498.85\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -886326.500000\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -737938.500000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -787408.437500\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -815026.375000\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -742598.500000\n",
      "    epoch          : 179\n",
      "    loss           : -789021.0154702971\n",
      "    val_loss       : -784523.9109375\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -888443.500000\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -740951.000000\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -743012.062500\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -794120.437500\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -755297.500000\n",
      "    epoch          : 180\n",
      "    loss           : -788648.7988861386\n",
      "    val_loss       : -784501.98671875\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -887717.875000\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -764346.000000\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -736299.875000\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -737527.375000\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -792554.062500\n",
      "    epoch          : 181\n",
      "    loss           : -787424.5457920792\n",
      "    val_loss       : -785031.79921875\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -887319.562500\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -764863.875000\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -743800.875000\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -740243.125000\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -760464.312500\n",
      "    epoch          : 182\n",
      "    loss           : -790017.1311881188\n",
      "    val_loss       : -786922.509375\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -891117.062500\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -742284.250000\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -820888.625000\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -893588.000000\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -747180.437500\n",
      "    epoch          : 183\n",
      "    loss           : -793355.2172029703\n",
      "    val_loss       : -789329.22265625\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -895327.562500\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -769415.750000\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -821136.750000\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -750534.875000\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -801397.562500\n",
      "    epoch          : 184\n",
      "    loss           : -793307.3669554455\n",
      "    val_loss       : -788815.54375\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -893191.500000\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -762809.125000\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -789666.562500\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -759721.375000\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -763042.875000\n",
      "    epoch          : 185\n",
      "    loss           : -792945.1237623763\n",
      "    val_loss       : -790853.90703125\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -894300.000000\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -829860.687500\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -745400.500000\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -764870.187500\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -795994.812500\n",
      "    epoch          : 186\n",
      "    loss           : -796208.2110148515\n",
      "    val_loss       : -790600.434375\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -895557.125000\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -770799.000000\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -765714.875000\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -824999.750000\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -767882.437500\n",
      "    epoch          : 187\n",
      "    loss           : -797748.2209158416\n",
      "    val_loss       : -793650.7328125\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -896168.562500\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -828935.250000\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -731278.750000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -896698.375000\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -804927.375000\n",
      "    epoch          : 188\n",
      "    loss           : -797033.4882425743\n",
      "    val_loss       : -791786.465625\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -831298.562500\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -741242.250000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -754069.125000\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -746851.187500\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -798122.750000\n",
      "    epoch          : 189\n",
      "    loss           : -797090.1503712871\n",
      "    val_loss       : -793051.09609375\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -901362.687500\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -745747.375000\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -799610.625000\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -801449.187500\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -764732.250000\n",
      "    epoch          : 190\n",
      "    loss           : -798540.3162128713\n",
      "    val_loss       : -794673.06875\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -903621.875000\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -831873.500000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -803969.562500\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -795041.937500\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -803201.562500\n",
      "    epoch          : 191\n",
      "    loss           : -800195.5266089109\n",
      "    val_loss       : -795340.484375\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -903404.000000\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -776248.875000\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -749669.375000\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -805509.687500\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -765633.687500\n",
      "    epoch          : 192\n",
      "    loss           : -802858.8490099009\n",
      "    val_loss       : -798365.59765625\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -903851.000000\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -832083.625000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -751191.875000\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -904941.250000\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -808809.250000\n",
      "    epoch          : 193\n",
      "    loss           : -803259.6899752475\n",
      "    val_loss       : -799107.99765625\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -842375.625000\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -823724.375000\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -799454.125000\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -895534.250000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -803931.625000\n",
      "    epoch          : 194\n",
      "    loss           : -799978.8948019802\n",
      "    val_loss       : -799281.94296875\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -905565.875000\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -759829.500000\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -806750.000000\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -809308.625000\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -805863.437500\n",
      "    epoch          : 195\n",
      "    loss           : -805601.3589108911\n",
      "    val_loss       : -800576.69609375\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -906950.062500\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -774626.750000\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -755283.750000\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -835984.687500\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -803172.750000\n",
      "    epoch          : 196\n",
      "    loss           : -803299.8663366337\n",
      "    val_loss       : -800147.33828125\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -908393.125000\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -755945.562500\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -761333.250000\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -802957.875000\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -768857.250000\n",
      "    epoch          : 197\n",
      "    loss           : -805975.1330445545\n",
      "    val_loss       : -799653.53203125\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -770285.687500\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -837985.250000\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -833508.000000\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -811116.125000\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -810882.625000\n",
      "    epoch          : 198\n",
      "    loss           : -806458.3867574257\n",
      "    val_loss       : -801809.94140625\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -908305.687500\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -833290.250000\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -752491.062500\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -776504.375000\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -771920.000000\n",
      "    epoch          : 199\n",
      "    loss           : -807698.1813118812\n",
      "    val_loss       : -804071.8453125\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -909545.312500\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -779509.375000\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -762410.312500\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -807782.937500\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -805907.750000\n",
      "    epoch          : 200\n",
      "    loss           : -808677.8533415842\n",
      "    val_loss       : -804022.63984375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0710_172137/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -782525.750000\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -835547.125000\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -834445.625000\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -762498.062500\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -814397.625000\n",
      "    epoch          : 201\n",
      "    loss           : -811494.1794554455\n",
      "    val_loss       : -807313.4140625\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -913711.250000\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -783564.875000\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -817442.437500\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -916182.625000\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -815867.750000\n",
      "    epoch          : 202\n",
      "    loss           : -813314.0346534654\n",
      "    val_loss       : -806621.6890625\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -916166.500000\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -787722.687500\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -774769.875000\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -767639.687500\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -778133.625000\n",
      "    epoch          : 203\n",
      "    loss           : -811951.5928217822\n",
      "    val_loss       : -808043.5859375\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -784667.437500\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -845302.312500\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -813345.375000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -816897.125000\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -777725.500000\n",
      "    epoch          : 204\n",
      "    loss           : -811421.5693069306\n",
      "    val_loss       : -807553.32421875\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -765592.187500\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -770729.000000\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -769994.187500\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -919040.562500\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -821317.875000\n",
      "    epoch          : 205\n",
      "    loss           : -815441.1293316832\n",
      "    val_loss       : -809174.77734375\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -919624.500000\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -788008.000000\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -842531.125000\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -773509.875000\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -787208.937500\n",
      "    epoch          : 206\n",
      "    loss           : -816226.7134900991\n",
      "    val_loss       : -811682.965625\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -917622.437500\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -791486.375000\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -818597.000000\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -917183.812500\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -820453.187500\n",
      "    epoch          : 207\n",
      "    loss           : -817293.3694306931\n",
      "    val_loss       : -810411.02578125\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -920129.625000\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -787864.625000\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -783234.437500\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -786333.875000\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -775017.875000\n",
      "    epoch          : 208\n",
      "    loss           : -817097.3966584158\n",
      "    val_loss       : -813196.5296875\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -918985.750000\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -791546.500000\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -771408.375000\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -823092.437500\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -824765.562500\n",
      "    epoch          : 209\n",
      "    loss           : -819546.0575495049\n",
      "    val_loss       : -814487.0328125\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -923759.562500\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -794608.250000\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -822300.062500\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -781414.500000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -822406.750000\n",
      "    epoch          : 210\n",
      "    loss           : -817753.9040841584\n",
      "    val_loss       : -811782.3875\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -922718.187500\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -790406.937500\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -817995.625000\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -818060.625000\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -847812.500000\n",
      "    epoch          : 211\n",
      "    loss           : -819160.8972772277\n",
      "    val_loss       : -815450.81796875\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -924175.250000\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -795683.125000\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -824089.062500\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -819791.312500\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -837744.250000\n",
      "    epoch          : 212\n",
      "    loss           : -820167.3205445545\n",
      "    val_loss       : -811311.8421875\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -921452.562500\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -820841.687500\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -771527.500000\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -780839.750000\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -788403.375000\n",
      "    epoch          : 213\n",
      "    loss           : -818856.0111386139\n",
      "    val_loss       : -817378.53203125\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -926495.000000\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -794261.375000\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -829895.375000\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -851778.937500\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -825983.875000\n",
      "    epoch          : 214\n",
      "    loss           : -823038.3217821782\n",
      "    val_loss       : -819291.49921875\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -928829.812500\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -791596.750000\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -826284.500000\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -927976.375000\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -827271.000000\n",
      "    epoch          : 215\n",
      "    loss           : -824774.4474009901\n",
      "    val_loss       : -819263.40390625\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -927589.312500\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -833298.875000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -828744.125000\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -929752.250000\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -816553.875000\n",
      "    epoch          : 216\n",
      "    loss           : -826656.7988861386\n",
      "    val_loss       : -819980.50703125\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -927939.750000\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -803957.812500\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -785339.187500\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -793771.125000\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -786277.625000\n",
      "    epoch          : 217\n",
      "    loss           : -828521.6534653465\n",
      "    val_loss       : -824426.53828125\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -803220.250000\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -803654.937500\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -831273.875000\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -828878.000000\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -823650.375000\n",
      "    epoch          : 218\n",
      "    loss           : -827494.2339108911\n",
      "    val_loss       : -820091.54296875\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -929449.625000\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -864198.875000\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -797919.500000\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -800711.750000\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -795070.375000\n",
      "    epoch          : 219\n",
      "    loss           : -830302.2636138614\n",
      "    val_loss       : -823311.959375\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -931555.000000\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -772483.125000\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -832354.062500\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -786363.750000\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -800400.312500\n",
      "    epoch          : 220\n",
      "    loss           : -829392.5327970297\n",
      "    val_loss       : -825772.159375\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -932487.875000\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -782918.125000\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -785991.937500\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -786743.500000\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -805887.812500\n",
      "    epoch          : 221\n",
      "    loss           : -832250.6707920792\n",
      "    val_loss       : -826427.090625\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -934573.125000\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -806719.000000\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -834524.875000\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -797588.937500\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -834367.875000\n",
      "    epoch          : 222\n",
      "    loss           : -831959.2827970297\n",
      "    val_loss       : -827881.865625\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -935222.812500\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -832609.125000\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -778961.687500\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -780404.625000\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -835081.312500\n",
      "    epoch          : 223\n",
      "    loss           : -831490.2326732674\n",
      "    val_loss       : -828711.53984375\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -868063.500000\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -778875.250000\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -780096.250000\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -788012.125000\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -801281.562500\n",
      "    epoch          : 224\n",
      "    loss           : -834114.7475247525\n",
      "    val_loss       : -829047.25546875\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -936372.125000\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -807702.125000\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -838646.875000\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -834973.062500\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -804474.562500\n",
      "    epoch          : 225\n",
      "    loss           : -834886.406559406\n",
      "    val_loss       : -830211.021875\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -793868.250000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -795105.375000\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -795391.000000\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -839383.500000\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -800989.437500\n",
      "    epoch          : 226\n",
      "    loss           : -837871.3205445545\n",
      "    val_loss       : -831531.653125\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -937787.875000\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -796422.750000\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -790960.875000\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -834687.687500\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -832246.062500\n",
      "    epoch          : 227\n",
      "    loss           : -836230.5160891089\n",
      "    val_loss       : -823719.8671875\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -836693.500000\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -844155.937500\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -838503.500000\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -831221.750000\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -799369.125000\n",
      "    epoch          : 228\n",
      "    loss           : -833623.4832920792\n",
      "    val_loss       : -831376.88515625\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -939173.750000\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -810320.812500\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -806731.437500\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -807840.375000\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -801249.500000\n",
      "    epoch          : 229\n",
      "    loss           : -839390.3174504951\n",
      "    val_loss       : -833698.96875\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -814568.812500\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -872735.625000\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -784876.375000\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -941640.625000\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -841727.000000\n",
      "    epoch          : 230\n",
      "    loss           : -838394.9591584158\n",
      "    val_loss       : -834054.5609375\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -943487.375000\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -871932.000000\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -866469.562500\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -842618.250000\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -800305.125000\n",
      "    epoch          : 231\n",
      "    loss           : -840651.6305693069\n",
      "    val_loss       : -835158.1203125\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -943566.250000\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -873956.875000\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -867415.750000\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -843981.875000\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -796538.375000\n",
      "    epoch          : 232\n",
      "    loss           : -840008.9956683168\n",
      "    val_loss       : -834490.86953125\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -943485.312500\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -874775.062500\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -844521.625000\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -840456.625000\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -847336.312500\n",
      "    epoch          : 233\n",
      "    loss           : -839833.458539604\n",
      "    val_loss       : -836530.79921875\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -945233.500000\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -814774.000000\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -870168.250000\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -798760.875000\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -851716.750000\n",
      "    epoch          : 234\n",
      "    loss           : -843754.8700495049\n",
      "    val_loss       : -835927.14765625\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -947839.875000\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -870967.625000\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -790898.437500\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -946643.750000\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -807197.125000\n",
      "    epoch          : 235\n",
      "    loss           : -843110.7908415842\n",
      "    val_loss       : -836135.76171875\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -945627.375000\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -813889.187500\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -795875.750000\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -846428.125000\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -804264.500000\n",
      "    epoch          : 236\n",
      "    loss           : -843809.3490099009\n",
      "    val_loss       : -838844.1671875\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -804610.875000\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -881239.500000\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -796787.437500\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -849301.000000\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -808113.000000\n",
      "    epoch          : 237\n",
      "    loss           : -844467.3892326732\n",
      "    val_loss       : -837600.94296875\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -946818.250000\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -792376.062500\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -790140.687500\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -950646.875000\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -848490.562500\n",
      "    epoch          : 238\n",
      "    loss           : -844752.0637376237\n",
      "    val_loss       : -839901.80546875\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -951081.375000\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -819000.937500\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -803614.062500\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -849000.875000\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -850157.375000\n",
      "    epoch          : 239\n",
      "    loss           : -847484.4257425743\n",
      "    val_loss       : -840450.67734375\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -949074.500000\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -820951.625000\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -876119.750000\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -851310.875000\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -851287.187500\n",
      "    epoch          : 240\n",
      "    loss           : -848194.8211633663\n",
      "    val_loss       : -841067.859375\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -950330.875000\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -821313.750000\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -802695.312500\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -849146.750000\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -811263.000000\n",
      "    epoch          : 241\n",
      "    loss           : -848531.4368811881\n",
      "    val_loss       : -842672.3203125\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -876556.125000\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -802514.625000\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -875163.250000\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -849576.500000\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -807295.062500\n",
      "    epoch          : 242\n",
      "    loss           : -848804.5222772277\n",
      "    val_loss       : -840968.98125\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -950767.312500\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -816726.750000\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -851058.062500\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -952499.625000\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -850190.750000\n",
      "    epoch          : 243\n",
      "    loss           : -848255.6448019802\n",
      "    val_loss       : -843423.2734375\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -808307.125000\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -814753.250000\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -855468.625000\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -857530.750000\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -854833.000000\n",
      "    epoch          : 244\n",
      "    loss           : -851957.1646039604\n",
      "    val_loss       : -844187.784375\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -885573.500000\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -795068.750000\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -800119.000000\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -809759.187500\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -811316.500000\n",
      "    epoch          : 245\n",
      "    loss           : -848665.875\n",
      "    val_loss       : -842322.32578125\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -952582.750000\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -815426.625000\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -875300.687500\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -801332.875000\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -846835.250000\n",
      "    epoch          : 246\n",
      "    loss           : -848427.1101485149\n",
      "    val_loss       : -841607.46875\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -948481.312500\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -817411.875000\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -793907.687500\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -808686.750000\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -849377.000000\n",
      "    epoch          : 247\n",
      "    loss           : -847346.6237623763\n",
      "    val_loss       : -845645.81875\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -956697.062500\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -822127.250000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -802467.937500\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -856793.625000\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -856045.062500\n",
      "    epoch          : 248\n",
      "    loss           : -853170.3799504951\n",
      "    val_loss       : -846509.7390625\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -957542.187500\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -888637.625000\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -804696.750000\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -860382.375000\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -853662.250000\n",
      "    epoch          : 249\n",
      "    loss           : -852542.0284653465\n",
      "    val_loss       : -844607.6359375\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -958207.437500\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -876223.375000\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -880332.812500\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -809214.187500\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -854761.500000\n",
      "    epoch          : 250\n",
      "    loss           : -852137.3595297029\n",
      "    val_loss       : -846895.4859375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0710_172137/checkpoint-epoch250.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -812149.937500\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -823332.562500\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -821911.125000\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -804443.250000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -859022.125000\n",
      "    epoch          : 251\n",
      "    loss           : -855042.6905940594\n",
      "    val_loss       : -847558.4765625\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -891065.500000\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -794608.375000\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -812362.625000\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -855662.000000\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -857066.375000\n",
      "    epoch          : 252\n",
      "    loss           : -854080.4412128713\n",
      "    val_loss       : -847569.01875\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -960626.125000\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -804431.625000\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -858356.500000\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -856606.437500\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -849998.375000\n",
      "    epoch          : 253\n",
      "    loss           : -852806.7345297029\n",
      "    val_loss       : -843940.86875\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -959400.250000\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -877000.500000\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -872843.312500\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -809077.500000\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -860838.375000\n",
      "    epoch          : 254\n",
      "    loss           : -849342.8422029703\n",
      "    val_loss       : -849234.48359375\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -962458.500000\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -823729.687500\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -825945.250000\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -881572.500000\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -823259.500000\n",
      "    epoch          : 255\n",
      "    loss           : -857743.6974009901\n",
      "    val_loss       : -850667.72578125\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -963678.750000\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -885553.812500\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -825506.687500\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -963413.750000\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -813728.812500\n",
      "    epoch          : 256\n",
      "    loss           : -856200.2419554455\n",
      "    val_loss       : -850910.8015625\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -965551.000000\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -859871.812500\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -812317.812500\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -858258.250000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -860491.125000\n",
      "    epoch          : 257\n",
      "    loss           : -857796.0352722772\n",
      "    val_loss       : -850833.0359375\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -892283.625000\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -806982.625000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -821387.625000\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -817264.750000\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -861263.250000\n",
      "    epoch          : 258\n",
      "    loss           : -857496.4362623763\n",
      "    val_loss       : -850088.65625\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -962963.687500\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -831447.625000\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -818161.875000\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -856674.000000\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -861478.687500\n",
      "    epoch          : 259\n",
      "    loss           : -857584.8650990099\n",
      "    val_loss       : -850060.38203125\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -965279.000000\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -827026.312500\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -820547.125000\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -819820.750000\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -823260.875000\n",
      "    epoch          : 260\n",
      "    loss           : -859825.8069306931\n",
      "    val_loss       : -853175.98515625\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -965770.687500\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -833532.625000\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -818923.500000\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -819674.625000\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -821960.062500\n",
      "    epoch          : 261\n",
      "    loss           : -861395.5841584158\n",
      "    val_loss       : -852819.54453125\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -832516.062500\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -822562.750000\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -820800.000000\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -820380.000000\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -857695.125000\n",
      "    epoch          : 262\n",
      "    loss           : -859781.926980198\n",
      "    val_loss       : -852665.40390625\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -896447.812500\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -828508.000000\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -819867.875000\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -862883.500000\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -866893.250000\n",
      "    epoch          : 263\n",
      "    loss           : -860079.7883663366\n",
      "    val_loss       : -852658.35390625\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -969271.875000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -892242.625000\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -830730.062500\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -822030.375000\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -818591.812500\n",
      "    epoch          : 264\n",
      "    loss           : -859755.1441831683\n",
      "    val_loss       : -852491.63984375\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -968270.437500\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -887452.812500\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -810137.250000\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -814337.625000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -813992.875000\n",
      "    epoch          : 265\n",
      "    loss           : -858435.4344059406\n",
      "    val_loss       : -853769.83359375\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -968921.000000\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -831857.437500\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -814764.625000\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -968526.250000\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -828564.500000\n",
      "    epoch          : 266\n",
      "    loss           : -862067.0402227723\n",
      "    val_loss       : -855399.0796875\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -832790.875000\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -893676.125000\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -888549.625000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -866742.875000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -868456.375000\n",
      "    epoch          : 267\n",
      "    loss           : -862365.7530940594\n",
      "    val_loss       : -856318.715625\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -896846.875000\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -834692.687500\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -814735.062500\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -890105.125000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -820511.875000\n",
      "    epoch          : 268\n",
      "    loss           : -864717.1893564357\n",
      "    val_loss       : -856027.39453125\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -972337.437500\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -830706.062500\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -890635.562500\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -972429.500000\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -869625.375000\n",
      "    epoch          : 269\n",
      "    loss           : -865205.1262376237\n",
      "    val_loss       : -855371.18046875\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -972608.500000\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -823047.000000\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -814416.812500\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -872211.500000\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -865354.937500\n",
      "    epoch          : 270\n",
      "    loss           : -864354.5358910891\n",
      "    val_loss       : -855311.1953125\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -971141.562500\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -823920.062500\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -864119.000000\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -821515.750000\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -863547.562500\n",
      "    epoch          : 271\n",
      "    loss           : -861533.1330445545\n",
      "    val_loss       : -854548.2875\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -972465.687500\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -871563.812500\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -826151.250000\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -820938.812500\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -870628.750000\n",
      "    epoch          : 272\n",
      "    loss           : -865988.2933168317\n",
      "    val_loss       : -857691.7171875\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -973971.375000\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -899059.312500\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -815806.812500\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -971693.250000\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -874144.875000\n",
      "    epoch          : 273\n",
      "    loss           : -866234.0575495049\n",
      "    val_loss       : -857614.75703125\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -974229.375000\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -887517.000000\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -813599.187500\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -814683.125000\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -821813.937500\n",
      "    epoch          : 274\n",
      "    loss           : -863736.1683168317\n",
      "    val_loss       : -855906.09140625\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -975578.562500\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -831030.875000\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -868553.750000\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -973882.375000\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -813249.812500\n",
      "    epoch          : 275\n",
      "    loss           : -862337.6528465346\n",
      "    val_loss       : -856121.7\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -972852.625000\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -834156.250000\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -891394.375000\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -818789.125000\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -869646.687500\n",
      "    epoch          : 276\n",
      "    loss           : -863580.5587871287\n",
      "    val_loss       : -857920.04609375\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -974407.500000\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -899994.125000\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -870560.625000\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -827115.437500\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -872796.750000\n",
      "    epoch          : 277\n",
      "    loss           : -866580.7097772277\n",
      "    val_loss       : -859009.5671875\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -977513.125000\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -890414.062500\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -891008.375000\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -809855.000000\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -869943.625000\n",
      "    epoch          : 278\n",
      "    loss           : -865089.6429455446\n",
      "    val_loss       : -858506.25703125\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -977621.625000\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -832328.000000\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -822032.562500\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -864375.187500\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -871674.875000\n",
      "    epoch          : 279\n",
      "    loss           : -867152.5445544554\n",
      "    val_loss       : -858581.86015625\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -977677.562500\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -871457.125000\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -813166.562500\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -886782.250000\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -833240.500000\n",
      "    epoch          : 280\n",
      "    loss           : -865418.2784653465\n",
      "    val_loss       : -857809.37734375\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -976335.625000\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -811500.437500\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -873862.312500\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -977400.937500\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -873962.437500\n",
      "    epoch          : 281\n",
      "    loss           : -866017.5550742574\n",
      "    val_loss       : -859530.18203125\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -974644.687500\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -815345.250000\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -864990.250000\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -977467.937500\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -873902.312500\n",
      "    epoch          : 282\n",
      "    loss           : -868180.7048267326\n",
      "    val_loss       : -861039.5671875\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -842704.250000\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -904153.000000\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -891952.125000\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -826645.875000\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -829238.250000\n",
      "    epoch          : 283\n",
      "    loss           : -868640.2759900991\n",
      "    val_loss       : -860465.8703125\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -978361.312500\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -835617.187500\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -893602.250000\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -873146.125000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -868909.812500\n",
      "    epoch          : 284\n",
      "    loss           : -868201.4925742574\n",
      "    val_loss       : -858982.9234375\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -979654.687500\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -899272.687500\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -874462.875000\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -867429.312500\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -872262.687500\n",
      "    epoch          : 285\n",
      "    loss           : -867709.2475247525\n",
      "    val_loss       : -858201.0953125\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -977779.312500\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -893695.625000\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -897379.625000\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -980256.687500\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -874839.125000\n",
      "    epoch          : 286\n",
      "    loss           : -869874.9962871287\n",
      "    val_loss       : -861761.19921875\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -978060.750000\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -830868.000000\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -819125.250000\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -981294.375000\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -871821.000000\n",
      "    epoch          : 287\n",
      "    loss           : -871608.739480198\n",
      "    val_loss       : -862319.0921875\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -982984.625000\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -842563.375000\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -818417.250000\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -829156.500000\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -833172.000000\n",
      "    epoch          : 288\n",
      "    loss           : -870371.3391089109\n",
      "    val_loss       : -861184.584375\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -980313.187500\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -901705.625000\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -812424.000000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -834010.125000\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -826982.687500\n",
      "    epoch          : 289\n",
      "    loss           : -868523.6367574257\n",
      "    val_loss       : -857287.23046875\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -977087.312500\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -832536.500000\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -897029.062500\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -830156.750000\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -832730.875000\n",
      "    epoch          : 290\n",
      "    loss           : -868197.7648514851\n",
      "    val_loss       : -862128.7015625\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -982334.187500\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -837416.812500\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -830999.375000\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -817851.125000\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -824375.312500\n",
      "    epoch          : 291\n",
      "    loss           : -871292.5841584158\n",
      "    val_loss       : -862367.7515625\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -896176.000000\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -902608.062500\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -833781.312500\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -874366.125000\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -824475.875000\n",
      "    epoch          : 292\n",
      "    loss           : -870857.5637376237\n",
      "    val_loss       : -862801.3515625\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -840764.500000\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -903412.500000\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -816249.000000\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -884334.875000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -829173.437500\n",
      "    epoch          : 293\n",
      "    loss           : -869962.875\n",
      "    val_loss       : -862866.08359375\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -982463.562500\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -840537.062500\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -887989.500000\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -831401.687500\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -867601.500000\n",
      "    epoch          : 294\n",
      "    loss           : -870043.1621287129\n",
      "    val_loss       : -859582.90234375\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -978525.500000\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -873466.687500\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -875950.375000\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -980284.500000\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -873774.375000\n",
      "    epoch          : 295\n",
      "    loss           : -870237.2128712871\n",
      "    val_loss       : -861039.9015625\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -983313.250000\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -834227.187500\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -832438.750000\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -825624.937500\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -883874.312500\n",
      "    epoch          : 296\n",
      "    loss           : -872501.4647277228\n",
      "    val_loss       : -864812.28671875\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -986250.250000\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -909126.812500\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -897319.500000\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -879586.750000\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -824934.437500\n",
      "    epoch          : 297\n",
      "    loss           : -873354.6683168317\n",
      "    val_loss       : -863867.33046875\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -984083.750000\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -834871.500000\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -810212.250000\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -982512.375000\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -829572.687500\n",
      "    epoch          : 298\n",
      "    loss           : -871502.6516089109\n",
      "    val_loss       : -864425.115625\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -985088.625000\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -899043.500000\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -827155.875000\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -831881.000000\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -877808.062500\n",
      "    epoch          : 299\n",
      "    loss           : -872813.6175742574\n",
      "    val_loss       : -865297.05\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -985977.437500\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -831042.875000\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -901160.375000\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -833291.062500\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -833730.625000\n",
      "    epoch          : 300\n",
      "    loss           : -875269.8626237623\n",
      "    val_loss       : -865175.659375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0710_172137/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -986189.625000\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -880871.625000\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -879391.250000\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -881283.312500\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -834347.125000\n",
      "    epoch          : 301\n",
      "    loss           : -875484.4950495049\n",
      "    val_loss       : -864504.121875\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -988072.687500\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -842283.437500\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -877657.375000\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -898770.687500\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -874757.625000\n",
      "    epoch          : 302\n",
      "    loss           : -874455.020420792\n",
      "    val_loss       : -865165.6921875\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -985739.812500\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -882231.125000\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -825868.000000\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -832150.250000\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -880071.000000\n",
      "    epoch          : 303\n",
      "    loss           : -874624.4733910891\n",
      "    val_loss       : -863911.715625\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -986811.187500\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -842131.875000\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -838589.125000\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -825791.125000\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -881715.375000\n",
      "    epoch          : 304\n",
      "    loss           : -874992.2976485149\n",
      "    val_loss       : -865369.809375\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -886059.250000\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -911134.875000\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -876435.000000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -838895.125000\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -838073.375000\n",
      "    epoch          : 305\n",
      "    loss           : -873403.271039604\n",
      "    val_loss       : -864091.88046875\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -986369.812500\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -842440.687500\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -822994.375000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -988396.125000\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -877038.125000\n",
      "    epoch          : 306\n",
      "    loss           : -874543.8001237623\n",
      "    val_loss       : -865427.92265625\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -846420.375000\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -842431.750000\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -899381.125000\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -837573.000000\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -827501.250000\n",
      "    epoch          : 307\n",
      "    loss           : -876262.7537128713\n",
      "    val_loss       : -866479.7953125\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -987197.625000\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -847414.375000\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -827140.125000\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -887237.125000\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -881002.500000\n",
      "    epoch          : 308\n",
      "    loss           : -875771.4573019802\n",
      "    val_loss       : -863251.328125\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -985630.875000\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -842815.812500\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -822711.062500\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -835339.437500\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -837225.500000\n",
      "    epoch          : 309\n",
      "    loss           : -873958.8452970297\n",
      "    val_loss       : -865781.940625\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -820434.375000\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -905529.000000\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -893848.000000\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -871532.937500\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -876602.312500\n",
      "    epoch          : 310\n",
      "    loss           : -873709.3607673268\n",
      "    val_loss       : -866333.9015625\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -989322.625000\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -846787.125000\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -840729.000000\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -990371.937500\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -832792.125000\n",
      "    epoch          : 311\n",
      "    loss           : -876777.3712871287\n",
      "    val_loss       : -864927.22734375\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -989888.562500\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -828577.375000\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -900820.000000\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -830445.125000\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -832144.625000\n",
      "    epoch          : 312\n",
      "    loss           : -873880.4653465346\n",
      "    val_loss       : -867656.18671875\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -990755.125000\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -909897.375000\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -824949.062500\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -988437.375000\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -874547.062500\n",
      "    epoch          : 313\n",
      "    loss           : -876389.4758663366\n",
      "    val_loss       : -864333.896875\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -985106.250000\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -894802.250000\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -821742.875000\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -877939.062500\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -886193.125000\n",
      "    epoch          : 314\n",
      "    loss           : -874021.3756188119\n",
      "    val_loss       : -866904.484375\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -989300.750000\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -849371.000000\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -900185.625000\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -899479.875000\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -896169.125000\n",
      "    epoch          : 315\n",
      "    loss           : -878474.0575495049\n",
      "    val_loss       : -866532.3203125\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -992239.187500\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -848246.562500\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -884853.125000\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -990078.000000\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -878015.187500\n",
      "    epoch          : 316\n",
      "    loss           : -876647.4356435643\n",
      "    val_loss       : -866713.26015625\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -990576.500000\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -850676.375000\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -825025.125000\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -838607.062500\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -882742.750000\n",
      "    epoch          : 317\n",
      "    loss           : -878317.146039604\n",
      "    val_loss       : -868069.91171875\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -992962.875000\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -829518.625000\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -897117.312500\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -884953.437500\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -887107.562500\n",
      "    epoch          : 318\n",
      "    loss           : -878779.1652227723\n",
      "    val_loss       : -867785.6171875\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -991669.875000\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -913752.875000\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -882491.000000\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -883211.375000\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -832804.750000\n",
      "    epoch          : 319\n",
      "    loss           : -879478.1868811881\n",
      "    val_loss       : -867809.1\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -992003.062500\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -915422.375000\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -839692.437500\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -829969.812500\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -827944.500000\n",
      "    epoch          : 320\n",
      "    loss           : -877905.770420792\n",
      "    val_loss       : -866477.4890625\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -991484.125000\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -837450.750000\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -879748.750000\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -992203.125000\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -834109.000000\n",
      "    epoch          : 321\n",
      "    loss           : -876311.9783415842\n",
      "    val_loss       : -868752.5921875\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -993671.062500\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -848196.625000\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -881057.625000\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -883659.687500\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -839113.500000\n",
      "    epoch          : 322\n",
      "    loss           : -877986.6497524752\n",
      "    val_loss       : -868107.03671875\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -992797.625000\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -909982.937500\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -841948.500000\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -836125.937500\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -836131.062500\n",
      "    epoch          : 323\n",
      "    loss           : -880149.5513613861\n",
      "    val_loss       : -869778.834375\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -995423.000000\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -847649.625000\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -901881.125000\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -836271.250000\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -833042.187500\n",
      "    epoch          : 324\n",
      "    loss           : -879817.3737623763\n",
      "    val_loss       : -868199.8328125\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -993141.250000\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -912400.062500\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -829653.500000\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -829530.875000\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -882659.937500\n",
      "    epoch          : 325\n",
      "    loss           : -879245.6373762377\n",
      "    val_loss       : -868881.51484375\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -993714.750000\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -909535.562500\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -843182.500000\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -837754.375000\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -888570.250000\n",
      "    epoch          : 326\n",
      "    loss           : -880626.3650990099\n",
      "    val_loss       : -869717.99609375\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -995729.000000\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -827526.562500\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -822932.687500\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -886505.187500\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -890533.250000\n",
      "    epoch          : 327\n",
      "    loss           : -878988.9573019802\n",
      "    val_loss       : -869010.2109375\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -994936.625000\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -886393.875000\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -843129.125000\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -903384.562500\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -836936.625000\n",
      "    epoch          : 328\n",
      "    loss           : -879622.781559406\n",
      "    val_loss       : -868830.7859375\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -995507.000000\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -907853.750000\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -903636.000000\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -877185.125000\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -884092.625000\n",
      "    epoch          : 329\n",
      "    loss           : -879298.4313118812\n",
      "    val_loss       : -868882.3671875\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -845717.312500\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -848827.750000\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -903556.937500\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -905484.000000\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -883904.687500\n",
      "    epoch          : 330\n",
      "    loss           : -881325.771039604\n",
      "    val_loss       : -870258.853125\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -997913.250000\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -847337.000000\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -899351.875000\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -993851.937500\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -844838.125000\n",
      "    epoch          : 331\n",
      "    loss           : -881000.2537128713\n",
      "    val_loss       : -868707.80078125\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -994802.187500\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -838877.125000\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -836502.250000\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -883124.375000\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -841445.625000\n",
      "    epoch          : 332\n",
      "    loss           : -880663.510519802\n",
      "    val_loss       : -871285.79140625\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -913760.437500\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -890915.750000\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -842959.000000\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -830727.000000\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -889322.937500\n",
      "    epoch          : 333\n",
      "    loss           : -882831.5457920792\n",
      "    val_loss       : -870214.43125\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -994616.625000\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -909739.625000\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -827314.937500\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -896009.625000\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -885920.625000\n",
      "    epoch          : 334\n",
      "    loss           : -880947.521039604\n",
      "    val_loss       : -870249.3578125\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -848841.437500\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -849893.250000\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -832248.312500\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -836556.312500\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -892807.500000\n",
      "    epoch          : 335\n",
      "    loss           : -882730.1305693069\n",
      "    val_loss       : -871665.7953125\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -999100.937500\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -892690.250000\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -844897.750000\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -995330.625000\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -888945.875000\n",
      "    epoch          : 336\n",
      "    loss           : -882453.0086633663\n",
      "    val_loss       : -870345.8125\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -994917.250000\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -847702.375000\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -846358.625000\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -873263.250000\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -875293.812500\n",
      "    epoch          : 337\n",
      "    loss           : -877086.4956683168\n",
      "    val_loss       : -860505.43359375\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -990715.250000\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -839442.187500\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -883083.375000\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -997932.750000\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -884128.625000\n",
      "    epoch          : 338\n",
      "    loss           : -876951.4560643565\n",
      "    val_loss       : -870727.6421875\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -995174.625000\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -911350.312500\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -884999.875000\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -889552.375000\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -839931.875000\n",
      "    epoch          : 339\n",
      "    loss           : -882146.3681930694\n",
      "    val_loss       : -870361.0796875\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -995638.687500\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -843699.625000\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -850306.000000\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -830146.750000\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -885099.000000\n",
      "    epoch          : 340\n",
      "    loss           : -883415.8917079208\n",
      "    val_loss       : -871739.3953125\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -998384.125000\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -915367.250000\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -831862.375000\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -834705.125000\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -843715.312500\n",
      "    epoch          : 341\n",
      "    loss           : -884935.1293316832\n",
      "    val_loss       : -872221.3234375\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -996784.937500\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -834420.687500\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -833750.375000\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -905033.375000\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -889667.000000\n",
      "    epoch          : 342\n",
      "    loss           : -883870.0625\n",
      "    val_loss       : -871603.34765625\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -996404.000000\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -914957.875000\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -891500.437500\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -908248.375000\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -903205.437500\n",
      "    epoch          : 343\n",
      "    loss           : -883837.6652227723\n",
      "    val_loss       : -869658.5921875\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -997782.375000\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -852169.062500\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -885824.500000\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -850158.312500\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -892158.312500\n",
      "    epoch          : 344\n",
      "    loss           : -882403.4764851485\n",
      "    val_loss       : -872323.6671875\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -999142.750000\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -825461.812500\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -847008.500000\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -893365.875000\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -886133.687500\n",
      "    epoch          : 345\n",
      "    loss           : -881054.0\n",
      "    val_loss       : -869131.44375\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -994954.250000\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -852504.750000\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -823085.375000\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -878955.625000\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -884214.687500\n",
      "    epoch          : 346\n",
      "    loss           : -878055.5464108911\n",
      "    val_loss       : -866909.0859375\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -992358.875000\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -848742.750000\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -851843.750000\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -1000612.062500\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -894684.250000\n",
      "    epoch          : 347\n",
      "    loss           : -883280.0365099009\n",
      "    val_loss       : -872867.0796875\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -996798.062500\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -852496.125000\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -846735.125000\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -999207.125000\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -888707.000000\n",
      "    epoch          : 348\n",
      "    loss           : -884716.4857673268\n",
      "    val_loss       : -872647.38046875\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -1000932.312500\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -850454.250000\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -844771.250000\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -849104.375000\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -889555.500000\n",
      "    epoch          : 349\n",
      "    loss           : -885428.6082920792\n",
      "    val_loss       : -873565.265625\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -999822.562500\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -917739.625000\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -906325.625000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -905527.250000\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -888450.875000\n",
      "    epoch          : 350\n",
      "    loss           : -885775.9022277228\n",
      "    val_loss       : -873280.85703125\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0710_172137/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -1000603.562500\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -916278.125000\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -907483.562500\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -848557.250000\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -842614.375000\n",
      "    epoch          : 351\n",
      "    loss           : -884802.6280940594\n",
      "    val_loss       : -871791.24375\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -1000856.875000\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -850778.625000\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -907944.312500\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -849500.187500\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -888644.250000\n",
      "    epoch          : 352\n",
      "    loss           : -884959.9535891089\n",
      "    val_loss       : -872070.04609375\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -996956.812500\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -853507.625000\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -830319.875000\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -901185.875000\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -842279.750000\n",
      "    epoch          : 353\n",
      "    loss           : -882942.9950495049\n",
      "    val_loss       : -872969.3609375\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -851374.312500\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -908253.875000\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -850065.625000\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -1001041.187500\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -836202.062500\n",
      "    epoch          : 354\n",
      "    loss           : -885189.0\n",
      "    val_loss       : -872773.33125\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -999569.437500\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -854208.125000\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -906391.375000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -852574.312500\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -884161.937500\n",
      "    epoch          : 355\n",
      "    loss           : -884670.0024752475\n",
      "    val_loss       : -870819.44609375\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -997801.500000\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -920144.625000\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -908236.875000\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -829471.812500\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -891739.375000\n",
      "    epoch          : 356\n",
      "    loss           : -884146.0563118812\n",
      "    val_loss       : -872890.98984375\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -1001456.562500\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -915444.125000\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -909494.625000\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -899974.500000\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -888795.250000\n",
      "    epoch          : 357\n",
      "    loss           : -885374.8898514851\n",
      "    val_loss       : -872304.609375\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -1000887.687500\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -843252.625000\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -849936.812500\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -894364.000000\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -835037.937500\n",
      "    epoch          : 358\n",
      "    loss           : -882570.2858910891\n",
      "    val_loss       : -869791.70546875\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -998725.125000\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -891747.312500\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -827986.375000\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -1002666.375000\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -890936.437500\n",
      "    epoch          : 359\n",
      "    loss           : -884215.5525990099\n",
      "    val_loss       : -873008.88828125\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -853619.062500\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -921004.250000\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -854358.562500\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -893628.125000\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -890095.937500\n",
      "    epoch          : 360\n",
      "    loss           : -887807.5724009901\n",
      "    val_loss       : -874050.315625\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -1004573.000000\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -844303.875000\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -832000.812500\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -851989.875000\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -896732.562500\n",
      "    epoch          : 361\n",
      "    loss           : -886387.1330445545\n",
      "    val_loss       : -873229.15625\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -1000835.187500\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -907458.250000\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -906072.250000\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -997942.750000\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -842386.750000\n",
      "    epoch          : 362\n",
      "    loss           : -883730.968440594\n",
      "    val_loss       : -872646.12890625\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -1003283.500000\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -844116.125000\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -891322.750000\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -895626.000000\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -844500.187500\n",
      "    epoch          : 363\n",
      "    loss           : -886053.823019802\n",
      "    val_loss       : -874500.2796875\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -999535.062500\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -858005.312500\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -855629.250000\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -888583.750000\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -894337.937500\n",
      "    epoch          : 364\n",
      "    loss           : -886852.4220297029\n",
      "    val_loss       : -873442.884375\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -999763.750000\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -855297.000000\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -832902.625000\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -836021.875000\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -894473.000000\n",
      "    epoch          : 365\n",
      "    loss           : -886633.5173267326\n",
      "    val_loss       : -871389.871875\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -1000360.875000\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -853841.875000\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -886551.125000\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -855523.875000\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -892323.187500\n",
      "    epoch          : 366\n",
      "    loss           : -886098.8007425743\n",
      "    val_loss       : -874169.4390625\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -999111.500000\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -852925.000000\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -854982.125000\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -909010.125000\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -898711.375000\n",
      "    epoch          : 367\n",
      "    loss           : -887193.7846534654\n",
      "    val_loss       : -874264.82421875\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -1003077.312500\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -858448.125000\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -895435.312500\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -889986.750000\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -893362.125000\n",
      "    epoch          : 368\n",
      "    loss           : -887431.4146039604\n",
      "    val_loss       : -873036.73203125\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -1004400.125000\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -920157.625000\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -887721.375000\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -889602.687500\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -892380.750000\n",
      "    epoch          : 369\n",
      "    loss           : -886044.5804455446\n",
      "    val_loss       : -873384.52734375\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -1005003.875000\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -829406.562500\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -903494.750000\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -835948.000000\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -838029.250000\n",
      "    epoch          : 370\n",
      "    loss           : -885549.4251237623\n",
      "    val_loss       : -873418.4078125\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -1002133.187500\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -853827.375000\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -839932.000000\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -840534.500000\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -843347.250000\n",
      "    epoch          : 371\n",
      "    loss           : -887967.6256188119\n",
      "    val_loss       : -873571.925\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -1004078.312500\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -855992.187500\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -850162.562500\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -1002994.562500\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -888080.812500\n",
      "    epoch          : 372\n",
      "    loss           : -887694.1181930694\n",
      "    val_loss       : -874403.25390625\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -1003824.062500\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -852012.125000\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -854897.687500\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -837135.687500\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -895507.937500\n",
      "    epoch          : 373\n",
      "    loss           : -887086.6367574257\n",
      "    val_loss       : -873885.1875\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -1003929.437500\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -899657.125000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -907165.375000\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -893628.000000\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -833812.750000\n",
      "    epoch          : 374\n",
      "    loss           : -884418.4634900991\n",
      "    val_loss       : -874013.22265625\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -1003149.812500\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -907776.625000\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -835966.750000\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -905037.625000\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -884524.375000\n",
      "    epoch          : 375\n",
      "    loss           : -885457.1126237623\n",
      "    val_loss       : -871192.1546875\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -1000740.000000\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -914965.250000\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -843359.937500\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -891238.000000\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -891800.500000\n",
      "    epoch          : 376\n",
      "    loss           : -884961.2629950495\n",
      "    val_loss       : -875085.040625\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -1001827.625000\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -853867.375000\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -911023.500000\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -909032.250000\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -838411.312500\n",
      "    epoch          : 377\n",
      "    loss           : -888425.3762376237\n",
      "    val_loss       : -874822.00625\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -1003614.500000\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -897361.750000\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -856242.187500\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -840606.687500\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -893495.625000\n",
      "    epoch          : 378\n",
      "    loss           : -889424.3211633663\n",
      "    val_loss       : -874464.40234375\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -1002241.125000\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -852403.125000\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -905574.562500\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -892982.937500\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -893832.125000\n",
      "    epoch          : 379\n",
      "    loss           : -887507.6175742574\n",
      "    val_loss       : -874099.3140625\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -922512.375000\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -852680.500000\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -853327.000000\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -885768.375000\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -842791.500000\n",
      "    epoch          : 380\n",
      "    loss           : -887220.2419554455\n",
      "    val_loss       : -874288.52421875\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -1004187.625000\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -830476.062500\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -850815.562500\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -1004596.000000\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -887976.812500\n",
      "    epoch          : 381\n",
      "    loss           : -885873.4962871287\n",
      "    val_loss       : -873870.89453125\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -1004571.625000\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -921724.250000\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -891587.500000\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -999699.875000\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -839070.437500\n",
      "    epoch          : 382\n",
      "    loss           : -884774.6590346535\n",
      "    val_loss       : -871423.66015625\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -1001958.625000\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -836500.562500\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -888235.625000\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -905069.687500\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -856687.500000\n",
      "    epoch          : 383\n",
      "    loss           : -886613.0866336634\n",
      "    val_loss       : -875890.6140625\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -1004417.625000\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -856443.500000\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -841273.875000\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -894849.375000\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -858402.500000\n",
      "    epoch          : 384\n",
      "    loss           : -890090.3168316832\n",
      "    val_loss       : -875327.63671875\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -1006173.625000\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -925561.062500\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -909754.125000\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -847170.187500\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -891721.312500\n",
      "    epoch          : 385\n",
      "    loss           : -889912.9381188119\n",
      "    val_loss       : -875579.23984375\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -1006963.875000\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -924359.500000\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -857357.250000\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -1004660.187500\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -900326.375000\n",
      "    epoch          : 386\n",
      "    loss           : -890812.7128712871\n",
      "    val_loss       : -875565.81953125\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -857172.250000\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -920220.750000\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -856843.500000\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -892927.437500\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -897559.812500\n",
      "    epoch          : 387\n",
      "    loss           : -890222.9003712871\n",
      "    val_loss       : -875438.3390625\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -1005053.000000\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -859091.375000\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -846545.625000\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -895491.250000\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -845011.000000\n",
      "    epoch          : 388\n",
      "    loss           : -889064.7450495049\n",
      "    val_loss       : -874286.8859375\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -1004914.937500\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -849517.687500\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -832609.000000\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -1004118.125000\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -841827.625000\n",
      "    epoch          : 389\n",
      "    loss           : -887988.0501237623\n",
      "    val_loss       : -875837.9390625\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -1006053.687500\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -921846.312500\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -858239.437500\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -1006861.625000\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -843169.000000\n",
      "    epoch          : 390\n",
      "    loss           : -891822.7735148515\n",
      "    val_loss       : -876321.7578125\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -1008439.375000\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -921912.375000\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -841019.250000\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -902337.437500\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -890267.625000\n",
      "    epoch          : 391\n",
      "    loss           : -889262.7419554455\n",
      "    val_loss       : -871356.309375\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -1003202.125000\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -849938.500000\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -897178.875000\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -893122.437500\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -893717.250000\n",
      "    epoch          : 392\n",
      "    loss           : -883656.208539604\n",
      "    val_loss       : -872143.21484375\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -924378.250000\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -907895.000000\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -895856.000000\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -898801.125000\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -841929.812500\n",
      "    epoch          : 393\n",
      "    loss           : -886288.646039604\n",
      "    val_loss       : -875485.2078125\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -1008135.187500\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -925592.750000\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -836269.500000\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -1006773.375000\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -905977.500000\n",
      "    epoch          : 394\n",
      "    loss           : -890237.2877475248\n",
      "    val_loss       : -874469.50390625\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -1008465.000000\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -862717.375000\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -900693.750000\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -842089.625000\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -898855.375000\n",
      "    epoch          : 395\n",
      "    loss           : -891411.4183168317\n",
      "    val_loss       : -876974.91796875\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -1008424.062500\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -843562.437500\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -840590.125000\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -900848.500000\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -843858.875000\n",
      "    epoch          : 396\n",
      "    loss           : -892190.771039604\n",
      "    val_loss       : -875523.078125\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -1007486.187500\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -856915.562500\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -860312.687500\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -901789.875000\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -902290.437500\n",
      "    epoch          : 397\n",
      "    loss           : -891789.2147277228\n",
      "    val_loss       : -875699.0484375\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -1007463.500000\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -860602.250000\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -912648.187500\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -857039.875000\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -847711.500000\n",
      "    epoch          : 398\n",
      "    loss           : -891858.5847772277\n",
      "    val_loss       : -875567.86953125\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -1005835.500000\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -859548.750000\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -842595.875000\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -859922.312500\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -842666.250000\n",
      "    epoch          : 399\n",
      "    loss           : -892506.0965346535\n",
      "    val_loss       : -875879.87421875\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -1009434.062500\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -858403.875000\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -842850.750000\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -856409.750000\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -841276.875000\n",
      "    epoch          : 400\n",
      "    loss           : -890042.3836633663\n",
      "    val_loss       : -873071.11484375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0710_172137/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -1006004.000000\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -921097.250000\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -834846.312500\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -1009128.437500\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -840555.187500\n",
      "    epoch          : 401\n",
      "    loss           : -889584.3842821782\n",
      "    val_loss       : -877058.76640625\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -1007684.062500\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -911676.750000\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -840729.062500\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -841770.125000\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -912818.687500\n",
      "    epoch          : 402\n",
      "    loss           : -892483.7939356435\n",
      "    val_loss       : -876262.74765625\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -1006875.562500\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -856170.437500\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -837838.750000\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -910439.250000\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -896264.375000\n",
      "    epoch          : 403\n",
      "    loss           : -892922.7506188119\n",
      "    val_loss       : -876929.87265625\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -1008638.625000\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -901154.000000\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -842246.000000\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -861472.562500\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -904600.875000\n",
      "    epoch          : 404\n",
      "    loss           : -892017.8780940594\n",
      "    val_loss       : -875429.9625\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -1006552.437500\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -890124.500000\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -831692.500000\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -895806.250000\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -896726.250000\n",
      "    epoch          : 405\n",
      "    loss           : -888320.8564356435\n",
      "    val_loss       : -874943.01484375\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -1003930.312500\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -916940.750000\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -892255.312500\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -904788.625000\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -900828.250000\n",
      "    epoch          : 406\n",
      "    loss           : -888653.5043316832\n",
      "    val_loss       : -877600.4921875\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -1006863.625000\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -850972.000000\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -838529.125000\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -898229.187500\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -896881.000000\n",
      "    epoch          : 407\n",
      "    loss           : -892065.770420792\n",
      "    val_loss       : -877131.3859375\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -1007688.000000\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -860298.000000\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -853944.750000\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -855686.000000\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -837915.375000\n",
      "    epoch          : 408\n",
      "    loss           : -889945.4195544554\n",
      "    val_loss       : -874718.26796875\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -1007475.250000\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -851962.625000\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -854450.937500\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -1009290.937500\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -841373.250000\n",
      "    epoch          : 409\n",
      "    loss           : -890490.2685643565\n",
      "    val_loss       : -876270.64609375\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -1007563.000000\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -924394.250000\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -858284.187500\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -901188.125000\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -901038.625000\n",
      "    epoch          : 410\n",
      "    loss           : -891969.9207920792\n",
      "    val_loss       : -875362.26171875\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -843900.250000\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -837628.062500\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -893448.875000\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -886879.375000\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -845764.250000\n",
      "    epoch          : 411\n",
      "    loss           : -890789.8013613861\n",
      "    val_loss       : -874866.07109375\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -1010791.000000\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -860252.000000\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -908192.125000\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -861776.125000\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -894799.875000\n",
      "    epoch          : 412\n",
      "    loss           : -892161.8626237623\n",
      "    val_loss       : -876282.22578125\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -1009686.187500\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -852913.187500\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -855036.875000\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -1008388.625000\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -896859.125000\n",
      "    epoch          : 413\n",
      "    loss           : -891533.780940594\n",
      "    val_loss       : -875937.39375\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -1010206.625000\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -925004.937500\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -898045.125000\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -1007613.500000\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -829547.562500\n",
      "    epoch          : 414\n",
      "    loss           : -887328.2537128713\n",
      "    val_loss       : -872662.53125\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -1004358.000000\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -853580.437500\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -860568.250000\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -897090.000000\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -901520.500000\n",
      "    epoch          : 415\n",
      "    loss           : -891829.0612623763\n",
      "    val_loss       : -877688.16640625\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -1008875.500000\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -857125.875000\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -843926.687500\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -1007668.250000\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -846980.000000\n",
      "    epoch          : 416\n",
      "    loss           : -893299.3805693069\n",
      "    val_loss       : -876147.53203125\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -1009634.687500\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -896055.500000\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -843199.000000\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -910269.937500\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -899663.312500\n",
      "    epoch          : 417\n",
      "    loss           : -893212.5544554455\n",
      "    val_loss       : -877158.41328125\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -1012457.750000\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -862156.437500\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -860859.062500\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -897654.625000\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -897320.250000\n",
      "    epoch          : 418\n",
      "    loss           : -893366.9560643565\n",
      "    val_loss       : -877410.2484375\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -1010127.500000\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -858544.250000\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -836325.937500\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -890682.500000\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -899956.937500\n",
      "    epoch          : 419\n",
      "    loss           : -891841.2004950495\n",
      "    val_loss       : -876999.9375\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -1011791.875000\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -864753.250000\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -837396.625000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -899329.562500\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -904609.875000\n",
      "    epoch          : 420\n",
      "    loss           : -893842.5228960396\n",
      "    val_loss       : -877832.91484375\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -928965.875000\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -853165.375000\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -859732.125000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -896459.875000\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -900018.750000\n",
      "    epoch          : 421\n",
      "    loss           : -892292.6058168317\n",
      "    val_loss       : -877570.1390625\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -1008709.125000\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -924079.687500\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -843468.625000\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -841471.125000\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -848507.250000\n",
      "    epoch          : 422\n",
      "    loss           : -893285.4814356435\n",
      "    val_loss       : -876760.7453125\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -1010582.750000\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -923273.812500\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -903335.312500\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -847676.187500\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -862749.625000\n",
      "    epoch          : 423\n",
      "    loss           : -894068.1404702971\n",
      "    val_loss       : -876907.19453125\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -1010164.375000\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -923511.562500\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -863921.000000\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -901517.937500\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -903595.125000\n",
      "    epoch          : 424\n",
      "    loss           : -894415.5915841584\n",
      "    val_loss       : -876614.48359375\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -858863.187500\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -925672.437500\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -840203.250000\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -905074.625000\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -835852.500000\n",
      "    epoch          : 425\n",
      "    loss           : -891175.417079208\n",
      "    val_loss       : -872711.19765625\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -1007366.000000\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -833988.375000\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -847782.500000\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -863483.812500\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -896209.125000\n",
      "    epoch          : 426\n",
      "    loss           : -891036.8360148515\n",
      "    val_loss       : -877048.49609375\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -1010425.875000\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -926775.562500\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -901641.687500\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -1009432.937500\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -857007.062500\n",
      "    epoch          : 427\n",
      "    loss           : -893558.9461633663\n",
      "    val_loss       : -878541.55625\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -1010880.125000\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -859273.625000\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -911271.250000\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -1012209.375000\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -845477.500000\n",
      "    epoch          : 428\n",
      "    loss           : -894516.0043316832\n",
      "    val_loss       : -876963.32265625\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -1010645.937500\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -861071.937500\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -840838.875000\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -1008952.937500\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -865997.062500\n",
      "    epoch          : 429\n",
      "    loss           : -893479.8910891089\n",
      "    val_loss       : -877424.34453125\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -1011017.875000\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -911779.437500\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -896639.937500\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -899300.250000\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -897217.500000\n",
      "    epoch          : 430\n",
      "    loss           : -891259.0167079208\n",
      "    val_loss       : -877410.32734375\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -1012781.250000\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -899241.562500\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -899056.125000\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -913014.250000\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -901753.312500\n",
      "    epoch          : 431\n",
      "    loss           : -895175.0266089109\n",
      "    val_loss       : -877876.57890625\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -1010093.562500\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -864009.375000\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -864259.000000\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -866100.000000\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -840452.187500\n",
      "    epoch          : 432\n",
      "    loss           : -893803.6188118812\n",
      "    val_loss       : -875946.6546875\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -860430.687500\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -849266.625000\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -890630.750000\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -901958.062500\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -839995.187500\n",
      "    epoch          : 433\n",
      "    loss           : -887087.4282178218\n",
      "    val_loss       : -876056.36640625\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -1009090.500000\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -922568.500000\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -912229.187500\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -844257.562500\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -904159.000000\n",
      "    epoch          : 434\n",
      "    loss           : -894071.531559406\n",
      "    val_loss       : -877669.034375\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -928162.625000\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -928397.125000\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -841156.062500\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -901216.875000\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -900587.312500\n",
      "    epoch          : 435\n",
      "    loss           : -894977.655940594\n",
      "    val_loss       : -877872.93203125\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -1011462.750000\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -928464.812500\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -842785.750000\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -909825.687500\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -900919.750000\n",
      "    epoch          : 436\n",
      "    loss           : -896022.7456683168\n",
      "    val_loss       : -878392.71640625\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -1009967.750000\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -857860.750000\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -914456.000000\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -903361.937500\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -901234.125000\n",
      "    epoch          : 437\n",
      "    loss           : -895889.6955445545\n",
      "    val_loss       : -877158.21171875\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -928791.750000\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -866587.375000\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -843976.625000\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -901467.375000\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -869101.750000\n",
      "    epoch          : 438\n",
      "    loss           : -895338.8768564357\n",
      "    val_loss       : -877355.85390625\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -1011576.250000\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -926795.125000\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -864644.375000\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -897705.812500\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -849452.875000\n",
      "    epoch          : 439\n",
      "    loss           : -894431.5389851485\n",
      "    val_loss       : -878628.8578125\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -1012743.875000\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -860492.687500\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -842728.750000\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -1011901.500000\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -900655.937500\n",
      "    epoch          : 440\n",
      "    loss           : -894825.9814356435\n",
      "    val_loss       : -877360.70234375\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -1012106.625000\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -925562.125000\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -911731.687500\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -914417.562500\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -850066.125000\n",
      "    epoch          : 441\n",
      "    loss           : -895640.0235148515\n",
      "    val_loss       : -877705.8453125\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -1013300.625000\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -861963.125000\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -848489.125000\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -905469.125000\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -848311.062500\n",
      "    epoch          : 442\n",
      "    loss           : -896656.9034653465\n",
      "    val_loss       : -878839.25703125\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -1013037.437500\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -925990.812500\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -839543.875000\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -848571.687500\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -846531.875000\n",
      "    epoch          : 443\n",
      "    loss           : -895461.1386138614\n",
      "    val_loss       : -877772.084375\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -1012780.500000\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -841301.687500\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -860351.250000\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -910900.500000\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -899497.937500\n",
      "    epoch          : 444\n",
      "    loss           : -892304.9275990099\n",
      "    val_loss       : -877623.47421875\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -900721.812500\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -904266.437500\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -902150.250000\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -844313.000000\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -842060.875000\n",
      "    epoch          : 445\n",
      "    loss           : -895148.3904702971\n",
      "    val_loss       : -877767.00703125\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -1011355.000000\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -863653.500000\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -901675.500000\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -1007160.500000\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -894434.625000\n",
      "    epoch          : 446\n",
      "    loss           : -892747.2543316832\n",
      "    val_loss       : -873722.10390625\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -1011015.937500\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -858417.687500\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -858520.187500\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -1010923.375000\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -903700.687500\n",
      "    epoch          : 447\n",
      "    loss           : -891993.8428217822\n",
      "    val_loss       : -878461.64609375\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -902186.000000\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -857391.500000\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -844910.000000\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -866546.625000\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -900990.875000\n",
      "    epoch          : 448\n",
      "    loss           : -896338.2301980198\n",
      "    val_loss       : -877438.47421875\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -1013143.500000\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -861703.750000\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -864889.375000\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -862189.125000\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -906388.687500\n",
      "    epoch          : 449\n",
      "    loss           : -896234.0340346535\n",
      "    val_loss       : -876651.09921875\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -1012221.312500\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -917133.625000\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -840740.750000\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -902730.687500\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -899630.312500\n",
      "    epoch          : 450\n",
      "    loss           : -892698.4257425743\n",
      "    val_loss       : -877684.13671875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0710_172137/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -1011970.000000\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -868804.500000\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -913885.750000\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -1012940.500000\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -903692.062500\n",
      "    epoch          : 451\n",
      "    loss           : -895558.5340346535\n",
      "    val_loss       : -878715.96484375\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -1013830.875000\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -842454.812500\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -901496.937500\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -912602.375000\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -868740.937500\n",
      "    epoch          : 452\n",
      "    loss           : -894352.6658415842\n",
      "    val_loss       : -877806.3703125\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -1013045.625000\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -860832.000000\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -914529.250000\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -905155.625000\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -900057.375000\n",
      "    epoch          : 453\n",
      "    loss           : -896796.1868811881\n",
      "    val_loss       : -877570.82890625\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -1012378.000000\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -898822.125000\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -849467.375000\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -1012833.250000\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -897886.375000\n",
      "    epoch          : 454\n",
      "    loss           : -895753.229579208\n",
      "    val_loss       : -878136.6046875\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -1013650.062500\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -862578.000000\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -866722.562500\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -1013741.000000\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -907337.375000\n",
      "    epoch          : 455\n",
      "    loss           : -896631.4263613861\n",
      "    val_loss       : -877901.94296875\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -1011871.500000\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -923684.875000\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -864602.750000\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -906567.750000\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -901695.187500\n",
      "    epoch          : 456\n",
      "    loss           : -895316.4195544554\n",
      "    val_loss       : -876981.34375\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -1012732.625000\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -913339.500000\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -910608.625000\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -902477.125000\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -903084.750000\n",
      "    epoch          : 457\n",
      "    loss           : -894622.5625\n",
      "    val_loss       : -877373.453125\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -1015948.375000\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -925328.187500\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -847859.375000\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -913575.312500\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -852568.000000\n",
      "    epoch          : 458\n",
      "    loss           : -895400.1633663366\n",
      "    val_loss       : -878730.884375\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -1014223.125000\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -851265.250000\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -910175.937500\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -902926.125000\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -842550.062500\n",
      "    epoch          : 459\n",
      "    loss           : -894698.6200495049\n",
      "    val_loss       : -878083.30859375\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -1011631.375000\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -908626.062500\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -857510.187500\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -1014758.250000\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -849647.500000\n",
      "    epoch          : 460\n",
      "    loss           : -895225.468440594\n",
      "    val_loss       : -878977.96953125\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -1015623.250000\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -846379.750000\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -912611.125000\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -1012555.000000\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -903112.875000\n",
      "    epoch          : 461\n",
      "    loss           : -897616.0136138614\n",
      "    val_loss       : -879292.703125\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -1015668.000000\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -927268.375000\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -864650.750000\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -901139.500000\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -904898.500000\n",
      "    epoch          : 462\n",
      "    loss           : -897294.6596534654\n",
      "    val_loss       : -877689.1046875\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -1013115.625000\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -866360.500000\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -837975.375000\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -911651.812500\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -904818.625000\n",
      "    epoch          : 463\n",
      "    loss           : -895207.9622524752\n",
      "    val_loss       : -878368.0140625\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -927789.750000\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -865932.500000\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -841581.500000\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -902300.875000\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -840958.375000\n",
      "    epoch          : 464\n",
      "    loss           : -895846.5\n",
      "    val_loss       : -875269.61484375\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -1008561.750000\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -855602.437500\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -859953.312500\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -899090.687500\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -897641.750000\n",
      "    epoch          : 465\n",
      "    loss           : -893430.3180693069\n",
      "    val_loss       : -877139.9890625\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -1015094.000000\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -932193.000000\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -913225.000000\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -915744.500000\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -908627.375000\n",
      "    epoch          : 466\n",
      "    loss           : -897509.4641089109\n",
      "    val_loss       : -878523.675\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -1014659.937500\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -857281.312500\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -902189.000000\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -847532.687500\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -905156.875000\n",
      "    epoch          : 467\n",
      "    loss           : -895226.6027227723\n",
      "    val_loss       : -876294.12421875\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -1012739.750000\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -862136.250000\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -902570.125000\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -900530.875000\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -902659.375000\n",
      "    epoch          : 468\n",
      "    loss           : -896687.4424504951\n",
      "    val_loss       : -878359.62578125\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -1012395.500000\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -865039.812500\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -841696.500000\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -901690.750000\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -905206.875000\n",
      "    epoch          : 469\n",
      "    loss           : -895870.1435643565\n",
      "    val_loss       : -878101.01953125\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -1013624.750000\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -867908.250000\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -848497.250000\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -910924.250000\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -894324.187500\n",
      "    epoch          : 470\n",
      "    loss           : -897044.6398514851\n",
      "    val_loss       : -877495.3671875\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -1015045.500000\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -860589.500000\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -846235.000000\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -1015408.000000\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -904859.187500\n",
      "    epoch          : 471\n",
      "    loss           : -897153.0785891089\n",
      "    val_loss       : -877899.38046875\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -927869.375000\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -923850.750000\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -862986.125000\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -1012090.500000\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -840396.437500\n",
      "    epoch          : 472\n",
      "    loss           : -893140.8452970297\n",
      "    val_loss       : -877423.68828125\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -1013831.500000\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -861292.625000\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -862511.000000\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -899174.312500\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -832541.625000\n",
      "    epoch          : 473\n",
      "    loss           : -891624.9337871287\n",
      "    val_loss       : -872988.00859375\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -1006389.750000\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -856049.875000\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -897684.000000\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -910449.875000\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -851501.375000\n",
      "    epoch          : 474\n",
      "    loss           : -891773.7097772277\n",
      "    val_loss       : -879076.78046875\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -1015504.125000\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -932729.437500\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -907004.625000\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -904633.500000\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -906391.937500\n",
      "    epoch          : 475\n",
      "    loss           : -898601.4108910891\n",
      "    val_loss       : -879965.90625\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -1016664.375000\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -867613.687500\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -901721.250000\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -917482.500000\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -906256.687500\n",
      "    epoch          : 476\n",
      "    loss           : -899399.6633663366\n",
      "    val_loss       : -878933.678125\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -934278.125000\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -915671.437500\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -848285.437500\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -905397.562500\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -903600.875000\n",
      "    epoch          : 477\n",
      "    loss           : -898329.2753712871\n",
      "    val_loss       : -878316.621875\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -1015790.875000\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -858241.250000\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -837471.312500\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -912450.750000\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -843067.125000\n",
      "    epoch          : 478\n",
      "    loss           : -893063.8743811881\n",
      "    val_loss       : -876124.834375\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -927170.312500\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -901822.375000\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -864998.375000\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -848132.750000\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -851113.312500\n",
      "    epoch          : 479\n",
      "    loss           : -896397.614480198\n",
      "    val_loss       : -879705.81171875\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -1015315.062500\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -865060.562500\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -917260.250000\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -849823.812500\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -916335.250000\n",
      "    epoch          : 480\n",
      "    loss           : -899038.2518564357\n",
      "    val_loss       : -879600.01796875\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -1015139.312500\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -908206.500000\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -867329.062500\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -911468.500000\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -899928.625000\n",
      "    epoch          : 481\n",
      "    loss           : -896362.6986386139\n",
      "    val_loss       : -876909.58828125\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -1011953.250000\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -864056.500000\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -913702.062500\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -911544.375000\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -903834.250000\n",
      "    epoch          : 482\n",
      "    loss           : -897241.8261138614\n",
      "    val_loss       : -878105.06875\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -1015341.875000\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -868215.750000\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -903008.687500\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -910294.875000\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -905608.687500\n",
      "    epoch          : 483\n",
      "    loss           : -899407.593440594\n",
      "    val_loss       : -878990.9140625\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -1014913.312500\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -862648.875000\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -850131.500000\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -1014778.625000\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -852808.375000\n",
      "    epoch          : 484\n",
      "    loss           : -899934.8972772277\n",
      "    val_loss       : -879557.25625\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -1016238.437500\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -900481.937500\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -847719.812500\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -1016709.250000\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -847976.750000\n",
      "    epoch          : 485\n",
      "    loss           : -899377.4133663366\n",
      "    val_loss       : -878265.85390625\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -1014727.437500\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -916578.500000\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -900825.562500\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -847647.062500\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -850053.750000\n",
      "    epoch          : 486\n",
      "    loss           : -896260.8886138614\n",
      "    val_loss       : -876630.778125\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -1015350.375000\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -865445.125000\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -858058.125000\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -866180.000000\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -905405.500000\n",
      "    epoch          : 487\n",
      "    loss           : -896934.271039604\n",
      "    val_loss       : -877583.45546875\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -859712.375000\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -860232.375000\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -865550.000000\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -846791.250000\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -848921.312500\n",
      "    epoch          : 488\n",
      "    loss           : -895746.1441831683\n",
      "    val_loss       : -878387.215625\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -1016600.000000\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -927091.687500\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -902772.250000\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -907896.750000\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -902431.625000\n",
      "    epoch          : 489\n",
      "    loss           : -898558.3415841584\n",
      "    val_loss       : -878700.64140625\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -932580.750000\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -863661.875000\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -902907.250000\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -867274.750000\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -847244.500000\n",
      "    epoch          : 490\n",
      "    loss           : -898157.1639851485\n",
      "    val_loss       : -879506.08359375\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -1015699.000000\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -913366.000000\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -900698.562500\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -903976.625000\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -900725.875000\n",
      "    epoch          : 491\n",
      "    loss           : -895993.281559406\n",
      "    val_loss       : -877886.5671875\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -1015267.062500\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -921425.875000\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -871784.687500\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -852317.500000\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -909169.000000\n",
      "    epoch          : 492\n",
      "    loss           : -898210.2926980198\n",
      "    val_loss       : -879654.87109375\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -1016525.750000\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -868095.625000\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -847573.125000\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -866216.500000\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -904509.750000\n",
      "    epoch          : 493\n",
      "    loss           : -899244.5878712871\n",
      "    val_loss       : -878819.93359375\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -927568.812500\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -859633.312500\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -868957.500000\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -911626.125000\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -848428.562500\n",
      "    epoch          : 494\n",
      "    loss           : -896435.3576732674\n",
      "    val_loss       : -877729.9984375\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -1013553.812500\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -863097.500000\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -849910.562500\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -915829.562500\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -904204.250000\n",
      "    epoch          : 495\n",
      "    loss           : -896873.905940594\n",
      "    val_loss       : -877681.5640625\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -1015224.687500\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -870855.625000\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -870364.625000\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -1012673.750000\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -915900.750000\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   496: reducing learning rate of group 0 to 5.0000e-04.\n",
      "    epoch          : 496\n",
      "    loss           : -898619.2332920792\n",
      "    val_loss       : -878903.6453125\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -1015239.875000\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -864628.875000\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -917377.375000\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -917578.375000\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -906721.687500\n",
      "    epoch          : 497\n",
      "    loss           : -901936.1349009901\n",
      "    val_loss       : -880460.553125\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -1018784.000000\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -866094.812500\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -872587.937500\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -918910.750000\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -907319.000000\n",
      "    epoch          : 498\n",
      "    loss           : -902552.426980198\n",
      "    val_loss       : -880602.409375\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -1016629.812500\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -908781.125000\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -922465.812500\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -907306.437500\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -855735.875000\n",
      "    epoch          : 499\n",
      "    loss           : -902765.9492574257\n",
      "    val_loss       : -879967.83125\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -1018929.375000\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -853825.437500\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -850971.375000\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -911959.375000\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -854874.062500\n",
      "    epoch          : 500\n",
      "    loss           : -902837.9764851485\n",
      "    val_loss       : -881076.98203125\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0710_172137/checkpoint-epoch500.pth ...\n",
      "Saving current best: model_best.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAECategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_3_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_9_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "  )\n",
       "  (guide_embedding): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=49, bias=True)\n",
       "    (1): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=49, out_features=49, bias=True)\n",
       "    (4): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (guide_confidences): Sequential(\n",
       "    (0): Linear(in_features=49, out_features=2, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJwAAAEuCAYAAAB7zXuHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAZV0lEQVR4nO3de3AVZZrH8e9DYu4DEQWCARIF4gqCLDKj3MFxhhFFZnRHqcVBa7DEXQLjekcRcVUUCi+LF3TcURgdLRUtUVdUYABTouPIsoMQvAEiiqDcERJIyLN/dOd4Ln0ugfAeEp9PVRfn9Pv222/3+aXf7iY5LaqKMa60SHcHzI+LBc44ZYEzTlngjFMWOOOUBc44ZYEzTlngjFMWOOOUBc44ZYEzTlngjFMWOOOUBc44ZYEzTlngjFMWOOOUBc44ZYEzTmWmuwNHS25u7pbq6up26e7H4crJydlaVVVVlO5+NDZprn9EIyLalLdNRFBVSXc/GpsNqcYpC5xxygJnnLLAGacscMYpC5xxygJnnLLAGacscMYpC5xxygJnnLLAGacscMYpC5xxygJnnLLAGaeaTeBEJFtEKkWkwb8lu2LFCu644w42b94ct46qcvnll9O6dWvKy8uPqK+pEpGHRWSHiPxFRFpElfUUkeVOOtKYVLVZTMAE4DH/dSdA8/PzI6aMjAwdOnSohlu7dq2eeOKJOmDAAO3Ro4fu3LlTg6xatUqzs7N1y5YtEfOvu+467dKlixYUFOipp56qc+fOjSgHNC8vL9SHsWPHRpSvWLFCBw4cqPn5+dq2bVt98MEHQ8v529IG2A/0CtjmN4AR6d73Dfqc0t2BRtsQWA30D3sf8cGuWrVKCwoKdOHChaF5mzZt0tLSUn3iiSe0rq5OJ06cqAMHDtSqqiqNtnTpUi0uLo6ZP2XKFF27dq0eOnRI33//fS0sLNR33303VA7oZ599FrOcqup3332nbdq00WeeeUarq6t1z549WllZGVoubFu+AH4esM2jgdfTve8bMqW9Aw3qrLfjJwGVwE7gKSDHP6JVAZlhdUMf7O7du7Vr16565513huZt375de/bsqU8//XRECG666SYdOXKk1tbWRsxftGiRduzYUZMZMWKEzpw5M/Q+UeAmTZqkl112WWBZVODWA8MC9kexv93ZifbbsTSlvQMN6qwXuNVAR6A18C5wF3A+sCaqbujDu+iii/S8887Turq6wA83mUOHDumkSZO0f//+Cevt379fi4qKdMGCBaF5gLZv317btWunv/nNb3TDhg2hsqFDh+rEiRO1b9++2qZNG73gggt048aNoeXCtmUZMAPICNgne4CeR7JfXU5p70CDOusF7uqw98OBdf7Q8n5UXVVVnTlzppaUlOj27dv1cGzfvl2zsrK0oKBAly9fnrDumDFjdNiwYRHBXrZsmR44cEB37typ48eP1+7du2tNTY2qqnbt2lVbtWqlH3zwgVZVVemECRO0X79+qhoTuL7AXuAA0DZqO78GBh3N/d6YU9o70KDOeoE7P+x9d39IGR50hKuoqND8/Hz9+9//HhiQVNXU1Oj48eP1/PPPj1vn+uuv1969e+vu3bvj1qmtrdW8vDxdtWqVqqr27NlTr7jiilD5tm3bFNBdu3ZFB+5V4LHwU4awsiZ1hGuKt0U6hr3uBGwGVgGniEjEH3ZfeumlzJw5kz59+hzRCjMzMxkxYgSVlZWB5bfffjsLFizg7bffpmXLlgnb8v/eFICePXsiIhFlQKg8zGnAa6paG9XWSUAW8ElDtiet0p34hkx4R7iPgA5453AVwDS/bBXQz3+dAcQ9IT8cS5YsCbxKnTZtmnbp0kU3b94cU7Z69WpduXKl1tbW6t69e/UPf/iDlpWV6cGDB1VVdfHixVpYWKgrV67UgwcP6jXXXKMDBgxQ9TZCo7b73ID98a/AGw3dj+mc0t6BBnU28ip1FzAXyPPLxgOz/deDAM3NzY25F9etW7eYYKRi2bJl2r59+5j5gGZlZUWs4+6771ZVL1BlZWWal5enbdq00ZEjR+qnn34asfyjjz6qJ510khYWFuoFF1ygX375ZajdsO3eBJwTsD/+B7gw3Z9LQ6a0d6BBnY3zk+6XZftBbO+/18b0ySefaGZmpq5fv75R242HH278dsS7WOimkdvbA3hPj8J+PppTUzyHC6SqB1S1m6p+czTaLysrY/z48QwcOJCJEycejVXEEJFZwPvAH1U14gRSVT9S1b5OOtKImtSX2YjIF8CVqroohbralLYtWnP9MpsmFbiGsMAdm5rNkGqaBgucccoCZ5yywBmnLHDGKQucccoCZ5yywBmnLHDGKQucccoCZ5yywBmnLHDGKQucccoCZ5xqto+vzMnJ2SoiTfrxlenuw9HQbH8B82gSkduALFW9Ld19aWpsSDVOWeCMUxY445QFzjhlgTNOWeCMUxY445QFzjhlgTNOWeAcEJFyEflQRA6IyJx09yedmu3/pR5jNuN9+fUwIDfNfUkrC5wDqvoygIj0wfv2zh8tG1KNUxY445QFzjhlgTNO2UWDA/7zIzLxvs4/Q0RygFqNeu7Cj4Ed4dyYjPfEnJuBy/zXk9PaozSxI5wDqjoVmJrmbhwT7AhnnLLAGacscMYpC5xxygJnnLLAGacscMYpC5xxygJnnLLAGacscMYpC5xxygJnnLLAGacscMYpC5xxygJnnLLAGacscMYpC5xxygJnnLLAGacscMYpC5xxygJnnLLAGacscMapZvv4ytzc3C3V1dVN+nmpVVVVRenuR2NrtoETEW3K2yYiqKqkux+NzYZU45QFzjhlgTNOWeCMUxY445QFzjhlgTNOWeCMUxY445QFzjhlgTNOWeCMUxY445QFzjhlgTNOWeCMU0ccOBHJFpFKEWnwb6eKyJkicruInJSgjojIXBHZISIPH1lvU1NeXk7r1q0ZPXo0dXV1SeuLNM7vSa5bt46pU6dSWVmZbH33iMi3IvJao6w4CRG5UUR2ishbIpIXVdZORNaKSHZKjanqEU3ABOAx/3Un4PuAqRb4a9Ry/wR8B1QAq4DCOO33AKqBdlHz5wAHo9aT4ZdlAVpSUqKALlmyRMPNmDFDu3fvrgUFBVpaWqozZszQaN9++63m5ubqypUrY8qiebsxWH5+fsyUmZmpJ598ckS9b775Rk855RQdNGiQFhcX17cZtD9+AijQK2r+DcBqYC+wAbghqvwLvOe01u+rt6PKTwFe95ffBsyIKs8HNgK/DujTo8CEoP7G1E2lUsIGvI3sn6C8h78R54bN6+DvlCsBAf4LeAfICVh+MPBVwPw5wF1x1pkFaEVFhRYVFcUEbvr06bpixQqtqanRjz/+WDt16qTPPfdcTFhKSkp00aJF8bIUkihw0b7++mtt27at/ulPfwrN2717t/bq1UsnT56sqqozZ85UP1QnBGxbiV+WGTX/RqA33jNwT/XDMSqs/IvwzyBgf60DrvWDlQP0DKi3FBgbML8/sDqo7Zi6SSt4HZ0EVAI7gafqg+Ef0aqiNz5s2ZbAp8DksHmtgX8Al0XVvRd4pf4oFTb/58CXAW3HDZxfrqqqxcXFMYGLNmHCBC0vL4+Zf/LJJ+ubb76ZcFnV1ANXU1Oj/fv317Fjx4bmVVdX65AhQ3TatGkxbQLLgXyN3K7OflkLjbPtfr1ZwENh7xMF7iqgIlF7fr2/AuMC5mcC+4GSZG2keg43Ghjmb2wZPzw+uwewXuM/u/0p4HPg7voZqrpDVc9Q1WfCK6rqzar6a1U9VD9PRFrUBy5O+//un9utEJGLU9yWCKpKRUUF3bt3jynr2LEjixcv5tChQwFLNtyNN97Ivn37ePjhH05Fs7OzWbJkCZMmTQrqWz9V3Vf/XryTxV8AX6tq3JNLv95AYE1U0V9E5DsReVtEzgibfzbwhYgsEJFtIrJURHoENL0JGCwiWVH9rMX7nM8IWCZmo1I5wl0d9n44sM5/PRp4P85y1/nLtk62jjjLtwYO4A3HfQPKewMn4P10Dffr9Q8rV9XkR7gpU6Zoz549tbq6OqZs+fLlWlBQoFlZWbp169a4bZDCEW7evHlaWFio69atS1o3rM3obd4G1AAXRZdF1bsDbxTJDpvXH8gF8vBGrC34583A23675+ENrzcA64GsqHY7+8vVAH2iyt4FxiTql2rqQ+r5Ye+7A1X6Q/jWBCwzAO/EtE+y9pOsOxN4GHg9hbqPAfeFvVfVxIF76KGHtLS0VDdt2hRYPmLECB03bpzW1NQEltdLFrhPP/1UW7Vqpa+88krCegFtRm9jC7xztbjnS0A53vlxh3h1/HofAyP81/OBJWFlAuwGzohaZhbwGpAb0N4q4MJE61RNfUjtGPa6E7DZf70KOEVEMusLRaQd8Dxwvap+mGL7gdQ7VL8GdEulOt6OSsmTTz7Jvffey+LFi+nQoUNgnbVr1zJixAgyMzMDy1Oxf/9+Lr74Yq6++mpGjhx52O0AqDeMzgdOk4B7MSLye+Bm4Oeq+lWy5vhhf63y3ydzGvCmqlZFrTcT6IJ3VE2y1tSOcB/hXVm2xruNMS0q2f381xl4J5ZPJ2s31QkYQvBV6r8ABXg/9b/EG1KHhJVrVVWVFhcX61tvvaVVVVVaV1enqqrPPPOMtmvXTisrKxMeZUpKSnThwoUJ66gmPsKNGTNGBw8erLW1tUnbCWgzaH+UEnyVOhpvuDstYJlOeENqFt4V6A14t6RO8MtPxTvpP9f/DP8D76o1ekhdClwZ0H4/oDKovzF1k1aIvErdBcwF8sLKxwOz/deD/J2xn9h7cTFDb0od9NrcHDC/Au+wv8f/yRoVVa7R04YNG1RVtbS0VDMzMyPujY0bNy7mQ+/QoYMuXrw41XDE2LhxowKanZ0deD8uhTaD9kcnf3uiw7AB79wqfJ/X3x/t7h8Y9gHbgcXEnoNdhHfiv8cPVvc4+/z3AfMfASYG9TembtIKCS6n/fJsP4ztU1lhQye8q+Ia4OQGLpfwA03myy+/1KysLF2zZk3Suke6rgRtBm1XHt6N9AFB5UdrAgrxTqWGR81vC6wl4B5qYDsprChh4Bxt7IPAV8CsBiyTyucaaMKECXrSSScF3psL4jJwXhE3+p/LK/HqNOYEXI83XD9H2JXvYbWVwsrSHrjD3ElJPtLGc/vttzd6m4kC15Qn+/akY5R9e5IxjcACZ5yywBmnLHDGKQucccoCZ5yywBmnLHDGKQucccoCZ5yywBmnLHDGKQucccoCZ5yywBmnLHDGqcP/+7djXE5Ozlb/TxabpJycnK3p7sPR0Gx/4/doEpHb8P5q6rZ096WpsSHVOGWBM05Z4IxTFjjjlAXOOGWBM05Z4IxTFjjjlAXOOGWBc0BEykXkQxE5ICJz0t2fdGq2/5d6jNkM3IX3TfC5ae5LWlngHFDVlwFEpA/eV9f+aNmQapyywBmnLHDGKQucccouGhzwH5yRifcMhAwRyQFqNf4zypotO8K5MRnvqYs3A5f5rycnXKKZsiOcA6o6FZia5m4cE+wIZ5yywBmnLHDGKQucccoCZ5yywBmnLHDGKQucccoCZ5yywBmnLHDGKQucccoCZ5yywBmnLHDGKQucccoCZ5yywBmnLHDGKQucccoCZ5yywBmnLHDGKQucccoCZ5yywBmnmu3TBHNzc7dUV1c36cdXVlVVFaW7H42t2QZORLQpb5uIoKqS7n40NhtSjVMWOOOUBc44ZYEzTlngjFMWOOOUBc44ZYEzTlngjFMWOOOUBc44ZYEzTlngjFMWOOOUBc44ZYEzTh1x4EQkW0QqRaTBv50qIp1FZKqIdEtS7x4R+VZEXjv8nqbu+eefp1WrVvTr14/NmzcnrT9kyBCWLl16xOutrq5m2rRpvPHGGwnrichvRWSDiOw84pWmQET6isgeEfk/EekRUP6BiHRPqTFVPaIJmAA8Fvb+JeCPUXVeAR6OmlcErAOWAV8BneK0/xNAgV5R8xcA34dNB4GPwsp1wIAB2rJlSy0uLtY77rhDg0ydOlUBXbhwYcT8gwcPat++ffWBBx4IXC7c4MGDdcmSJYFlv/rVrzQ/Pz9iys7OVkA3btwYqldbW6sXXnih9unTR1u2bKneRxN3n68AromaNwSoi9onl4eVlwMfAgeAOVHLng0sBHYA3wEvAu2j6rQAXgAeDOjPJcBL8fobPjXGkDoOeDrs/XjgYhEZCiAilwL/jPesUPx5Lf3APKuqg4EHgDdF5ISA9lv7/64On6mq56lqQf0ELMfbUSGDBg1ix44dLFu2jNmzZ/Pqq69GNLxu3TrmzZtH+/btY1Z63HHHUVZWxvbt21PYBfEtWLCA77//PjTt3r2bs88+mzFjxtCpU6dQvauuuooDBw7wzjvvhPopIj+L02xrovaHb3P4PlHVueFlwF3AkwHLHQ/8ESgFSoC9wFPhFVS1DqgEgj6jV4GhIhK7I6MkDZyIfCEik/xhc6eIPOU/0RgR6QR0Bv4W1rEtwHXAE375LGCcqn7vL5MNzAdeUNXb/GXuAx4GXhOR/Kgu1D/TtS5BH0uBgUQGn9GjR5ORkUHnzp0ZMGAAa9asiViuvLyc6dOnk5WVFdhuixYtqK1t3Ic233LLLezYsYPZs2eH5k2aNIlt27Yxf/58cnNzGTx4cH3RsyJyakAzmSTYH0FU9WVVfQWI+QlS1QWq+qKq7lHV/XifRf+AZuoIeMauqlbjHXV/mawfqR7hRgPD8MJVxg9PM+4BrNeoR2mr6hy84fJ/gTdV9c2wsgOqOlRV74la5lFV7aeq++rniYgAvwC+9n/C4hkDVKjqhvCZf/7zn6mpqeGTTz7hvffe49xzzw2Vvfjii2RlZTF8+PC4jXbs2JHly5ezb9++uHUaYv78+Tz++OO89NJL5OXlhebfc889zJ8/n+zs7Ij6qtpFVT8Jn+efQxUBXwasoq2IbPXP7x4I+OFN1SBgTcD8TcCZcUaitcAZSVtONuYCXwBXh70fDqzzX48G3o+z3GS8c69fpDK2x2ljG1ADXJSk3ufAFVHztHPnzpqRkaGATpkyJXS+tHfvXu3SpYuuX79eVVVLSkpizuFUVXfs2KFdu3bVFi1a6EsvvRRTXi/ROVy9zz//XAsLC3XevHkJ69Uj4BwOmOfv0/sCyoqAbngHkZOBd4DHA+rdRdQ5XFR5T7xzuYEBZcfhnbooseeQdwNPxms3VC9pBS9w54e97w5U6Q/hWxOwTFdgF/AIsAo4Ltl64qy7BXAjsDpBnQF4J8gFYfNaAzp37lytqanRTZs26VlnnaWPPPKIqqpee+21ERcR8QJ333336Zlnnqm7du1KGI5kgauqqtJevXrptddem7CdcEGB87etH94FUlFQeVi9s4HtAfPjBg7oAnwN/C5O+UV4R7n2AWWzgPsT9Uk19YuGjmGvO+GdgOKH6RQRCY3r/jD438CDeFew+4CbUlxPBPWG0fnAaX67QS4HXlb/HNF3CsCYMWPIzMykQ4cOjBo1KnS7YfHixcyaNYuioiKKiorYtGkTl1xyCdOnT49oeO3atQwdOpRWrVodTvdDxo8fT35+fkz7h0NVl+Odh5Ulqwqk/HetIlICLALuVNWn41Q7DW9E+yZO2T+SrSfmBDCO8SLyOrAfuAV4HkBVvxKRz4Cf4R1qAf4NOBGYpqp1IjIWeF9E5qnqxymuL9wBvCNdBhBxrigiucBv8X7ywn0K8OyzzzJq1Ci+/fZbnn/+ec455xzAC1xNTU2o8k9/+lPuv/9+zjvvvIhGampqYs6rGurJJ5/k9ddfZ+XKlWRmprq7kzoARFzpiMgQYD3eEagDcC/eD2t9eSbe550BZPgXfrWqWisixcBfgUdU9bEE6z3OX3cE/0LwTLwf/oRSPcI9C7ztb9B6vMNyvceB3/kr7ghMA8aq6kEAVa0E7sO7aj2cvySvv1gI6uuvgd3AkvCZqroH4IEHHuD444+nV69enH766dx6660AnHDCCaGjW1FRERkZGRx//PEUFBRENH7o0CFatDiyO0d33XUXO3bsoKysjIKCgoipoqLicJutI3Z/9AbewxtRluPdNpkYVj4ZqMK7PXWZ/7r+4u9KvFHhdhH5vn4KWG8GwVfHFwJLVTX5XfJkYy7eOdy5Ccqz8e7PxIzrjTEBeXhHtgENXC7JWVJi+/fv1969e4fO+xJJ5aKhoUh843c5cCv+V3W4mPCOjq8CMwLK/gacnko7R3zjV73bHN00eFw/YurdF7oFeEZEXjka64j2wgsvUFJSQrt27bjkkktcrLKh/hMYBWx1sTIRORvYAhTi3SCOoKpnqWrQjejYtvyEJlrZF8CVqrqo4V1NH5dfZjNnzhyGDBlCaWlpo7XZXL/Mxr496RjVXANnv55knLLAGacscMYpC5xxygJnnLLAGacscMYpC5xxygJnnLLAGacscMYpC5xxygJnnLLAGacscMYpC5xxqtH+jOhYk5OTs1VEmvTzUtPdh6Oh2f7G79EkIrcBWep/N4pJnQ2pxikLnHHKAmecssAZpyxwxikLnHHKAmecssAZpyxwxikLnAMiUi4iH4rIARGZk+7+pFOz/b/UY0z9MxKGAblp7ktaWeAcUNWXAUSkD97Xof5o2ZBqnLLAGacscMYpC5xxyi4aHEj0jIT09sw9O8K5kegZCT8qdoRzQFWnAlPT3I1jgh3hjFMWOOOUDamH53O8B52ZBrI/EzRO2ZBqnLLAGacscMYpC5xxygJnnLLAGacscMYpC5xxygJnnLLAGacscMYpC5xxygJnnLLAGacscMYpC5xxygJnnLLAGacscMap/wdCm6zTeBLFqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAARp0lEQVR4nO3de5BU9ZnG8e8DjKhcVIIiAoJBIhJ10ZqQVHRTWq6uoWrV/GEqZJPCLXbxD002FVdjudmSbO2Wluslyca1ahJZQYyXihfYrBt1MYR1raijEsRLFAkKgqKyysWAM8O7f/Qh1U6mT890n55u5vd8qrqm+7ynz3m7Z545p/v06Z8iAjMb+oY1uwEzGxwOu1kiHHazRDjsZolw2M0S4bCbJcJhH2IkLZK0rMb7niDpOUk7JX2z6N6KJulYSbskDW92LwcCh70gks6Q9ISkDyRtl/S/kj7T7L4G6EpgVUSMiYgfNruZaiLijYgYHRE9ze7lQOCwF0DSWODnwL8C44BJwPeAvc3sqwZTgRcqFVtpCyppRDPvfyBy2IvxKYCIuCsieiLi9xHxSESsBZA0XdJjkt6T9K6kOyUdvv/OkjZKukLSWkm7Jd0maYKk/8p2qf9b0hHZvNMkhaSFkrZI2irp8kqNSfpctsfxvqTfSDqzwnyPAWcBP8p2jT8l6XZJt0p6SNJu4CxJh0laKukdSa9L+q6kYdkyLs72aG7O1rdB0uez6ZskbZM0P6fXVZKulfRUtoe0XNK4Xo97gaQ3gMfKpo3I5jlG0opsz2q9pL8pW/YiST+TtEzSDuDifv1mh5KI8KXOCzAWeA9YAnwROKJX/XjgHGAkcCSwGvh+WX0j8GtgAqW9gm3As8Cp2X0eA67J5p0GBHAXMAo4GXgH+LOsvghYll2flPU1l9I/9nOy20dWeByrgL8uu3078AFwenb/g4GlwHJgTNbLK8CCbP6LgW7gr4DhwD8BbwC3ZI/jXGAnMDpn/W8CJ2WP7b6yx7L/cS/NaoeUTRuRzfMr4N+yPmdnz8vZZc9LF3Bh9lgOafbfzaD/nTa7gaFyAU7MwrE5+4NfAUyoMO+FwHNltzcCf1l2+z7g1rLb3wAezK7v/wOfWVa/Hrgtu14e9u8Ad/Ra98PA/Ap99RX2pWW3h1N6aTKrbNollF7n7w/7q2W1k7NeJ5RNew+YnbP+68puzwI+yta7/3F/sqz+h7ADU4AeYExZ/Vrg9rLnZXWz/06aefFufEEi4qWIuDgiJlPaMh0DfB9A0lGS7pb0ZrYLuQwY32sRb5dd/30ft0f3mn9T2fXXs/X1NhW4KNulfl/S+8AZwMQBPLTy9YwHDsrWV77uSWW3e/dNRFR7LJXW9zrQxsefq0307Rhge0TszOmt0n2T4LA3QES8TGmreFI26VpKW6BTImIs8DVAda5mStn1Y4EtfcyzidKW/fCyy6iIuG4A6yk/LfJdSrvCU3ut+80BLK+a3o+rK1tvX/2U2wKMkzQmp7ekT/F02AsgaaakyyVNzm5PAeZReh0Opde3u4D3JU0Crihgtf8g6VBJn6b0GvmePuZZBvyFpD+XNFzSwZLO3N/nQEXpENe9wD9LGiNpKvDtbD1F+ZqkWZIOBf4R+Fn049BaRGwCngCuzR7nKcAC4M4CezugOezF2Al8Fngye9f618A6YP+75N8DTqP0Ztd/AvcXsM5fAeuBlcANEfFI7xmyAFwAXE3pzapNlP7R1PN7/wawG9gAPA78FFhcx/J6u4PSXtFblN5oG8iHe+ZReh2/BXiA0puajxbY2wFN2ZsXdoCQNA34HdAWEd3N7aZYklZRenPxJ83uZSjylt0sEQ67WSK8G2+WCG/ZzRIxqCcDHKSRcTCjBnOVZknZw24+ir19foaj3jOHzgN+QOnjjD+p9mGNgxnFZ3V2Pas0sxxPxsqKtZp347PTHW+hdOLHLGCepFm1Ls/MGque1+xzgPURsSEiPgLupvQBDjNrQfWEfRIfP7FgMx8/6QCA7LzrTkmdXQfcdzmYDR31hL2vNwH+6DheRHRERHtEtLcxso7VmVk96gn7Zj5+htJk+j7zysxaQD1hfxqYIek4SQcBX6H0hQ1m1oJqPvQWEd2SLqP0zSfDgcURUfHLCs2sueo6zh4RDwEPFdSLmTWQPy5rlgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJGNSvkrb0qO2girVhow7JvW/P+x8U3U7SvGU3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLh4+xWF43MH+Xnllcqjyq6sfuw3Pve8OnP5Nb37dmTW7eP85bdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEj7NbXW787arc+vS20TnV/PPVfRy9WHWFXdJGYCfQA3RHRHsRTZlZ8YrYsp8VEe8WsBwzayC/ZjdLRL1hD+ARSc9IWtjXDJIWSuqU1NnF3jpXZ2a1qnc3/vSI2CLpKOBRSS9HxOryGSKiA+gAGKtxUef6zKxGdW3ZI2JL9nMb8AAwp4imzKx4NYdd0ihJY/ZfB84F1hXVmJkVq57d+AnAA5L2L+enEfGLQrqy1lH6/VY0sy3/fPbN3bsq1i478dwqK/+wSt0GouawR8QG4E8K7MXMGsiH3swS4bCbJcJhN0uEw26WCIfdLBE+xdVyHbLqqNz6cOVvL776zW9XXvaHT9XUk9XGW3azRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBE+zp66YcNzyw/OeLiuxR/682cr1vy1RYPLW3azRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBE+zp6415adXGWOZ3Krr3Ttzq1Hd/cAO7JG8ZbdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEj7Mnbu0XOqrMcVBude7jl+XWp/PcADuyRqm6ZZe0WNI2SevKpo2T9KikV7OfRzS2TTOrV392428Hzus17SpgZUTMAFZmt82shVUNe0SsBrb3mnwBsCS7vgS4sOC+zKxgtb5BNyEitgJkPysOCCZpoaROSZ1d7K1xdWZWr4a/Gx8RHRHRHhHtbYxs9OrMrIJaw/62pIkA2c9txbVkZo1Qa9hXAPOz6/OB5cW0Y2aNUvU4u6S7gDOB8ZI2A9cA1wH3SloAvAFc1MgmrQ5SbvnQYfnH0asZO+bD3PrwsWMr1np27Khr3TYwVcMeEfMqlM4uuBczayB/XNYsEQ67WSIcdrNEOOxmiXDYzRLhU1yHuOGHVT70BdAVPbn1YeQfunv8tDty61eu/NOKtfWfzz/sF10f5dZtYLxlN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4ePsQ9x758/Krb/S9Yvc+sy2/G8XGqn8P6FLj/xlxdrfTflq7n27N2zMrdvAeMtulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCx9mHuCu/e2du/X8+PD63Pnns+rrWv6VnTMVaz6YtdS3bBsZbdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sET7OPgQMO2Vmxdo73e/k3nfx9efn1jsOzl/3kRdtyq2/9taRFWvTu9fkL9wKVXXLLmmxpG2S1pVNWyTpTUlrssvcxrZpZvXqz2787cB5fUy/OSJmZ5eHim3LzIpWNewRsRrYPgi9mFkD1fMG3WWS1ma7+UdUmknSQkmdkjq72FvH6sysHrWG/VZgOjAb2ArcWGnGiOiIiPaIaG8j/8sLzaxxagp7RLwdET0RsQ/4MTCn2LbMrGg1hV3SxLKbXwLWVZrXzFpD1ePsku4CzgTGS9oMXAOcKWk2EMBG4JIG9mhVzLnj+Yq1mx/IP45+3J3P5NaHf6Li2zEAbJ17eG6dLVUO1NugqRr2iJjXx+TbGtCLmTWQPy5rlgiH3SwRDrtZIhx2s0Q47GaJ8CmuB4DhJ+R/3fMT71b+GPL0f3kx9749XR/l1vft2JlbXzH7vtz6wzMq937vFUfn3teK5S27WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIH2c/AIzo2JVbf3nNtIq1GR9srmvdL990Um598ogncut7oq2u9VtxvGU3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLh4+ytQMotTzzkg9z68237cpad//98xITxufXfnd+RW6/mP06ZkFPtrmvZNjDespslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiejPkM1TgKXA0cA+oCMifiBpHHAPMI3SsM1fjoj/a1yrQ9eIScfk1hcdfU/+AtorlzqXT8+961On3Z2/7Crbg7t35g/pHN0+lt4q+rNl7wYuj4gTgc8Bl0qaBVwFrIyIGcDK7LaZtaiqYY+IrRHxbHZ9J/ASMAm4AFiSzbYEuLBRTZpZ/Qb0ml3SNOBU4ElgQkRshdI/BOCoopszs+L0O+ySRgP3Ad+KiB0DuN9CSZ2SOruoPCaZmTVWv8IuqY1S0O+MiPuzyW9LmpjVJwLb+rpvRHRERHtEtLcxsoiezawGVcMuScBtwEsRcVNZaQUwP7s+H1hefHtmVpT+nOJ6OvB14HlJa7JpVwPXAfdKWgC8AVzUmBaHvn3jD8utv9iVX//hpNUVayMnV/sq5/z/9x/uyx/S+d9PmFpl+dYqqoY9Ih4HKp1wfXax7ZhZo/gTdGaJcNjNEuGwmyXCYTdLhMNulgiH3SwR/irpVvDyhtzyW92H59Z7Sqco9C3/W6p5cPfo3PqtM47PX4AdMLxlN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4ePsLWDfnj259aUzj82vk1OPqKUlG4K8ZTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuHj7AcCHyu3AnjLbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslomrYJU2R9EtJL0l6QdLfZtMXSXpT0prsMrfx7ZpZrfrzoZpu4PKIeFbSGOAZSY9mtZsj4obGtWdmRaka9ojYCmzNru+U9BIwqdGNmVmxBvSaXdI04FTgyWzSZZLWSlos6YgK91koqVNSZxd762rWzGrX77BLGg3cB3wrInYAtwLTgdmUtvw39nW/iOiIiPaIaG9jZAEtm1kt+hV2SW2Ugn5nRNwPEBFvR0RPROwDfgzMaVybZlav/rwbL+A24KWIuKls+sSy2b4ErCu+PTMrSn/ejT8d+DrwvKQ12bSrgXmSZgMBbAQuaUiHZlaI/rwb/zh9j/L9UPHtmFmj+BN0Zolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBGKQRwOWNI7wOtlk8YD7w5aAwPTqr21al/g3mpVZG9TI+LIvgqDGvY/WrnUGRHtTWsgR6v21qp9gXur1WD15t14s0Q47GaJaHbYO5q8/jyt2lur9gXurVaD0ltTX7Ob2eBp9pbdzAaJw26WiKaEXdJ5kn4rab2kq5rRQyWSNkp6PhuGurPJvSyWtE3SurJp4yQ9KunV7GefY+w1qbeWGMY7Z5jxpj53zR7+fNBfs0saDrwCnANsBp4G5kXEi4PaSAWSNgLtEdH0D2BI+gKwC1gaESdl064HtkfEddk/yiMi4jst0tsiYFezh/HORiuaWD7MOHAhcDFNfO5y+voyg/C8NWPLPgdYHxEbIuIj4G7ggib00fIiYjWwvdfkC4Al2fUllP5YBl2F3lpCRGyNiGez6zuB/cOMN/W5y+lrUDQj7JOATWW3N9Na470H8IikZyQtbHYzfZgQEVuh9McDHNXkfnqrOoz3YOo1zHjLPHe1DH9er2aEva+hpFrp+N/pEXEa8EXg0mx31fqnX8N4D5Y+hhlvCbUOf16vZoR9MzCl7PZkYEsT+uhTRGzJfm4DHqD1hqJ+e/8IutnPbU3u5w9aaRjvvoYZpwWeu2YOf96MsD8NzJB0nKSDgK8AK5rQxx+RNCp74wRJo4Bzab2hqFcA87Pr84HlTezlY1plGO9Kw4zT5Oeu6cOfR8SgX4C5lN6Rfw34+2b0UKGvTwK/yS4vNLs34C5Ku3VdlPaIFgCfAFYCr2Y/x7VQb3cAzwNrKQVrYpN6O4PSS8O1wJrsMrfZz11OX4PyvPnjsmaJ8CfozBLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNE/D8CdVCZJsxc+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAARx0lEQVR4nO3dfbBU9X3H8feH6wWUh4YrShCJ+FgfU7Q36lTbYq3G0Gk1f5gJk3QwY4vTRlsba+KYdCSZNjppokmaxBmMVhTjw/hQndYmWtRYa6ReH6JYNBqDgiCoVEFUhHu//WMP6XLdPXvv7tk9C7/Pa2bn7p7fOfv77rIfztnzsD9FBGa26xtTdgFm1hkOu1kiHHazRDjsZolw2M0S4bCbJcJh38VIWihpSZPL/qakJyRtkvRXRddWNEkfkfS2pJ6ya9kZOOwFkXSipIclvSVpg6T/kvSxsusapS8CD0TEpIj4btnFNBIRL0fExIgYLLuWnYHDXgBJk4F/Bf4J6ANmAF8FtpRZVxP2A56p19hNa1BJu5W5/M7IYS/GIQARcWNEDEbEuxFxT0Q8BSDpQEn3SXpD0uuSbpD0oe0LS1op6UJJT0naLOlqSdMk/Xu2Sf0fkqZk886SFJIWSFojaa2kC+oVJun4bIvjTUk/lzSnznz3AScB38s2jQ+RdK2kKyXdLWkzcJKk35B0naTXJL0k6SuSxmTPcVa2RXNF1t+Lkn4nm75K0npJ83NqfUDSpZL+O9tCulNS37DXfbakl4H7qqbtls2zj6S7si2rFyT9edVzL5R0q6QlkjYCZ43oX3ZXEhG+tXgDJgNvAIuBTwBThrUfBJwCjAP2Ah4Evl3VvhJ4BJhGZatgPfA4cHS2zH3AJdm8s4AAbgQmAEcBrwF/mLUvBJZk92dkdc2l8h/7Kdnjveq8jgeAP6t6fC3wFnBCtvx44DrgTmBSVssvgLOz+c8CtgGfA3qAvwdeBr6fvY5TgU3AxJz+XwGOzF7bbVWvZfvrvi5r271q2m7ZPD8FfpDVOTt7X06uel+2Amdkr2X3sj83Hf+cll3ArnIDDsvCsTr7wN8FTKsz7xnAE1WPVwKfqXp8G3Bl1ePzgH/J7m//gB9a1f4N4OrsfnXYvwRcP6zvnwDz69RVK+zXVT3uofLV5PCqaedQ+Z6/PezPV7UdldU6rWraG8DsnP4vq3p8OPB+1u/2131AVfuvww7MBAaBSVXtlwLXVr0vD5b9OSnz5s34gkTEiog4KyL2pbJm2gf4NoCkvSXdJOmVbBNyCTB12FOsq7r/bo3HE4fNv6rq/ktZf8PtB5yZbVK/KelN4ERg+iheWnU/U4GxWX/Vfc+oejy8biKi0Wup199LQC87vlerqG0fYENEbMqprd6ySXDY2yAinqWyVjwym3QplTXQRyNiMvBZQC12M7Pq/keANTXmWUVlzf6hqtuEiLhsFP1UXxb5OpVN4f2G9f3KKJ6vkeGva2vWb616qq0B+iRNyqkt6Us8HfYCSDpU0gWS9s0ezwTmUfkeDpXvt28Db0qaAVxYQLd/J2kPSUdQ+Y58c415lgB/LOnjknokjZc0Z3udoxWVQ1y3AP8gaZKk/YAvZP0U5bOSDpe0B/A14NYYwaG1iFgFPAxcmr3OjwJnAzcUWNtOzWEvxibgOGBZttf6EWA5sH0v+VeBY6js7Po34PYC+vwp8AKwFPhmRNwzfIYsAKcDF1PZWbWKyn80rfy7nwdsBl4EHgJ+BFzTwvMNdz2VraJXqexoG83JPfOofI9fA9xBZafmvQXWtlNTtvPCdhKSZgG/AnojYlu51RRL0gNUdi7+sOxadkVes5slwmE3S4Q3480S4TW7WSI6ejHAWI2L8UzoZJdmSXmPzbwfW2qew9HqlUOnAd+hcjrjDxudrDGeCRynk1vp0sxyLIulddua3ozPLnf8PpULPw4H5kk6vNnnM7P2auU7+7HACxHxYkS8D9xE5QQOM+tCrYR9BjteWLCaHS86ACC77npA0sDWne63HMx2Ha2EvdZOgA8cx4uIRRHRHxH9vYxroTsza0UrYV/Njlco7UvtK6/MrAu0EvZHgYMl7S9pLPBpKj/YYGZdqOlDbxGxTdK5VH75pAe4JiLq/lihmZWrpePsEXE3cHdBtZhZG/l0WbNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S0RHh2y29ug5+IC6bbfef1PusnuMGZvbPhhDue1/0j83t33b2ldz261zvGY3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLh4+y7gKGXX6nb9kcrzsxd9v4j7sxt71H++uDM+x/Pbb/xsBn1GyNyl7VitRR2SSuBTcAgsC0i+osoysyKV8Sa/aSIeL2A5zGzNvJ3drNEtBr2AO6R9JikBbVmkLRA0oCkga1sabE7M2tWq5vxJ0TEGkl7A/dKejYiHqyeISIWAYsAJqvPe2TMStLSmj0i1mR/1wN3AMcWUZSZFa/psEuaIGnS9vvAqcDyogozs2K1shk/DbhD0vbn+VFE/LiQqmxUYkv9fSFjT3kpd9kfvzAut/20PfL3s3x8jxdz22/e/aC6bUPvvJO7rBWr6bBHxIvAbxVYi5m1kQ+9mSXCYTdLhMNulgiH3SwRDrtZIhQdvMxwsvriOJ3csf6ssZ49+3Lb7376vpae/5H3Buu2LTzkuNxlY9u2lvpO0bJYysbYoFptXrObJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZonwT0knbvCNDbnta7e9nds+fbeJue3Hj++p2/bcVfkXTR7yucdy2210vGY3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLh4+yWa2rP7m177ln7ejzQTvKa3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhI+zJ07j8ods7lX969FbddSUNbntz7Wt5zQ1XLNLukbSeknLq6b1SbpX0vPZ3yntLdPMWjWSzfhrgdOGTbsIWBoRBwNLs8dm1sUahj0iHgSG/3bR6cDi7P5i4IyC6zKzgjW7g25aRKwFyP7uXW9GSQskDUga2MqWJrszs1a1fW98RCyKiP6I6O8lf2eQmbVPs2FfJ2k6QPZ3fXElmVk7NBv2u4D52f35wJ3FlGNm7dLwOLukG4E5wFRJq4FLgMuAWySdDbwMnNnOIq19jnh4a2l9P/cxj7/eSQ3DHhHz6jSdXHAtZtZGPl3WLBEOu1kiHHazRDjsZolw2M0S4Utcd3FDv390bvvXP7yowTP0ttT/QTf8Rd22A+NnLT23jY7X7GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZInycfWcg5Tb3TJ1at23VefmXsO5Gaz8VvTUGc9sP+fqzddvyl7Siec1ulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCx9l3AmMmTsxtj+l71m3b4yeTcpe9/NCDc9u/MOX53PYx5J8DsHbeYXXb9v7Bw7nLWrG8ZjdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEqGI6Fhnk9UXx8mDv47WmAkTctuH3nmnbpvGjs1d9qUlh+S2rzjh+tz2RvKudz/9sJNylx3cuLGlvlO0LJayMTbUPPmh4Zpd0jWS1ktaXjVtoaRXJD2Z3eYWWbCZFW8km/HXAqfVmH5FRMzObncXW5aZFa1h2CPiQWBDB2oxszZqZQfduZKeyjbzp9SbSdICSQOSBraypYXuzKwVzYb9SuBAYDawFvhWvRkjYlFE9EdEfy/jmuzOzFrVVNgjYl1EDEbEEHAVcGyxZZlZ0ZoKu6TpVQ8/CSyvN6+ZdYeG17NLuhGYA0yVtBq4BJgjaTYQwErgnDbWmLyhzZubXja25O8n2f8r7+Y/wf1Ndw1Ar+r/Lv3QgTPzF37imdY6tx00DHtEzKsx+eo21GJmbeTTZc0S4bCbJcJhN0uEw26WCIfdLBH+KenU9ZT3//2YX63ObfeQzsXymt0sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4SPsyduwzF9bX3+t4feq9s2+OZbbe3bduQ1u1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCB9n39WNqf9TzgDrfjf/qvEtsTW3fZx6c9v/4Mt/U7dtCj/LXdaK5TW7WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpaIkQzZPBO4DvgwMAQsiojvSOoDbgZmURm2+VMR8b/tK3XX9e4Zx+a2/+0/Lsltv/z8z9RtG//qO7nL3nHq93Lbx2lcbnsjez5R/yMx1NIz22iNZM2+DbggIg4Djgc+L+lw4CJgaUQcDCzNHptZl2oY9ohYGxGPZ/c3ASuAGcDpwOJstsXAGe0q0sxaN6rv7JJmAUcDy4BpEbEWKv8hAHsXXZyZFWfEYZc0EbgNOD8iNo5iuQWSBiQNbGVLMzWaWQFGFHZJvVSCfkNE3J5NXidpetY+HVhfa9mIWBQR/RHR30trO3vMrHkNwy5JwNXAioi4vKrpLmB+dn8+cGfx5ZlZURQR+TNIJwL/CTzN/x8tuZjK9/ZbgI8ALwNnRsSGvOearL44Tie3WvMu54u/fDq3fc74/MtMt+UMbrwb+Ze49qi1Uy0GI/8A2tx9f7t+Y4PPno3esljKxtigWm0Nj7NHxENAzYUBJ9dsJ+Ez6MwS4bCbJcJhN0uEw26WCIfdLBEOu1ki/FPSXeBr55+d2z54xT/ntp+8e/3TkFs9jt7IiRf+ZW775Hikrf3byHnNbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslouH17EXy9ezdZ8z48bntQ++916FKrAh517N7zW6WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcLXsyfOx9HT4TW7WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpaIhmGXNFPS/ZJWSHpG0l9n0xdKekXSk9ltbvvLNbNmjeSkmm3ABRHxuKRJwGOS7s3aroiIb7avPDMrSsOwR8RaYG12f5OkFcCMdhdmZsUa1Xd2SbOAo4Fl2aRzJT0l6RpJU+oss0DSgKSBrdQfpsjM2mvEYZc0EbgNOD8iNgJXAgcCs6ms+b9Va7mIWBQR/RHR38u4Ako2s2aMKOySeqkE/YaIuB0gItZFxGBEDAFXAce2r0wza9VI9sYLuBpYERGXV02fXjXbJ4HlxZdnZkUZyd74E4A/BZ6W9GQ27WJgnqTZQAArgXPaUqGZFWIke+MfAmr9DvXdxZdjZu3iM+jMEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhQRnetMeg14qWrSVOD1jhUwOt1aW7fWBa6tWUXWtl9E7FWroaNh/0Dn0kBE9JdWQI5ura1b6wLX1qxO1ebNeLNEOOxmiSg77ItK7j9Pt9bWrXWBa2tWR2or9Tu7mXVO2Wt2M+sQh90sEaWEXdJpkp6T9IKki8qooR5JKyU9nQ1DPVByLddIWi9pedW0Pkn3Sno++1tzjL2SauuKYbxzhhkv9b0re/jzjn9nl9QD/AI4BVgNPArMi4j/6WghdUhaCfRHROknYEj6PeBt4LqIODKb9g1gQ0Rclv1HOSUivtQltS0E3i57GO9stKLp1cOMA2cAZ1Hie5dT16fowPtWxpr9WOCFiHgxIt4HbgJOL6GOrhcRDwIbhk0+HVic3V9M5cPScXVq6woRsTYiHs/ubwK2DzNe6nuXU1dHlBH2GcCqqser6a7x3gO4R9JjkhaUXUwN0yJiLVQ+PMDeJdczXMNhvDtp2DDjXfPeNTP8eavKCHutoaS66fjfCRFxDPAJ4PPZ5qqNzIiG8e6UGsOMd4Vmhz9vVRlhXw3MrHq8L7CmhDpqiog12d/1wB1031DU67aPoJv9XV9yPb/WTcN41xpmnC5478oc/ryMsD8KHCxpf0ljgU8Dd5VQxwdImpDtOEHSBOBUum8o6ruA+dn9+cCdJdayg24ZxrveMOOU/N6VPvx5RHT8Bsylskf+l8CXy6ihTl0HAD/Pbs+UXRtwI5XNuq1UtojOBvYElgLPZ3/7uqi264GngaeoBGt6SbWdSOWr4VPAk9ltbtnvXU5dHXnffLqsWSJ8Bp1ZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNuloj/A/9beF1Ped4fAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASUklEQVR4nO3dfbBcdX3H8fcnNzc3kgckjcQQEqKICqJG55pQoR0cRB76ALbVmqITHGxwFKsjozK2Fey0A2Ml+EwnSJrwIEJFCm3REkMxPpFyeRCCUUljIE8mYIoEkOTm5ts/9sQul92z9+45+3Dv7/Oa2bm753sevrvZT87ZPXvOUURgZuPfhE43YGbt4bCbJcJhN0uEw26WCIfdLBEOu1kiHPZxRtIlkq5rctpXSbpf0h5Jf1V2b2WTNE/S05J6Ot3LWOCwl0TSSZJ+KOnXknZL+oGkN3W6r1H6OHBXREyLiC90uplGIuKxiJgaEUOd7mUscNhLIGk68O/AF4EZwBzg08DeTvbVhKOAh+sVu2kNKmliJ6cfixz2crwSICJuiIihiPhNRNwREQ8CSDpa0p2SfiXpCUnXS3rxwYklbZb0MUkPSnpG0tWSZkn6VrZJ/R1Jh2XjzpcUkpZK2i5ph6QL6zUm6YRsi+NJST+WdHKd8e4E3gJ8Kds0fqWklZKulHS7pGeAt0g6VNI1kh6X9Kikv5E0IZvHudkWzRXZ8jZJenM2fIukXZKW5PR6l6RLJf13toV0q6QZw573eZIeA+6sGjYxG+cISbdlW1YbJf1l1bwvkfQNSddJego4d0T/suNJRPhW8AZMB34FrALOAA4bVn8FcCrQB7wEWAt8rqq+GbgbmEVlq2AXcB/whmyaO4GLs3HnAwHcAEwBXgs8Drw1q18CXJfdn5P1dSaV/9hPzR6/pM7zuAt4X9XjlcCvgROz6ScD1wC3AtOyXn4OnJeNfy6wH3gv0AP8PfAY8OXsebwN2ANMzVn+NuD47LndXPVcDj7va7Lai6qGTczG+S7wlazPBdnrckrV6zIInJ09lxd1+n3T9vdppxsYLzfg2CwcW7M3/G3ArDrjng3cX/V4M3BO1eObgSurHn8I+Nfs/sE3+Kur6p8Brs7uV4f9E8C1w5b9n8CSOn3VCvs1VY97qHw0Oa5q2PlUPucfDPsjVbXXZr3Oqhr2K2BBzvIvq3p8HLAvW+7B5/3yqvpvww7MBYaAaVX1S4GVVa/L2k6/Tzp582Z8SSJiQ0ScGxFHUlkzHQF8DkDS4ZK+Lmlbtgl5HTBz2Cx2Vt3/TY3HU4eNv6Xq/qPZ8oY7CnhHtkn9pKQngZOA2aN4atXLmQlMypZXvew5VY+H901ENHou9Zb3KNDL81+rLdR2BLA7Ivbk9FZv2iQ47C0QET+lslY8Pht0KZU10OsiYjrwbkAFFzO36v48YHuNcbZQWbO/uOo2JSIuG8Vyqg+LfILKpvBRw5a9bRTza2T48xrMllurn2rbgRmSpuX0lvQhng57CSS9WtKFko7MHs8FFlP5HA6Vz7dPA09KmgN8rITF/q2kQyS9hspn5BtrjHMd8EeSTpPUI2mypJMP9jlaUdnFdRPwD5KmSToK+Gi2nLK8W9Jxkg4B/g74Roxg11pEbAF+CFyaPc/XAecB15fY25jmsJdjD7AIWJd9a303sB44+C35p4E3Uvmy6z+Ab5awzO8CG4E1wGcj4o7hI2QBOAv4JJUvq7ZQ+Y+myL/7h4BngE3A94GvASsKzG+4a6lsFf2Syhdto/lxz2Iqn+O3A7dQ+VJzdYm9jWnKvrywMULSfOAXQG9E7O9sN+WSdBeVLxe/2ulexiOv2c0S4bCbJcKb8WaJ8JrdLBFtPRhgkvpiMlPauUizpDzHM+yLvTV/w1H0yKHTgc9T+TnjVxv9WGMyU1ikU4os0sxyrIs1dWtNb8Znhzt+mcqBH8cBiyUd1+z8zKy1inxmXwhsjIhNEbEP+DqVH3CYWRcqEvY5PP/Agq08/6ADALLjrgckDQyOuXM5mI0fRcJe60uAF+zHi4jlEdEfEf299BVYnJkVUSTsW3n+EUpHUvvIKzPrAkXCfg9wjKSXSZoEvIvKCRvMrAs1vestIvZLuoDKmU96gBURUfdkhWbWWYX2s0fE7cDtJfViZi3kn8uaJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1ki2nrJZmvOhMmTc+svvau3bm353Ltyp3029uXWr37yNbn1lf98em599rIf1S/GCy4gZC3kNbtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulghFG/d1TteMWKRT2ra8sUJ9fbn1P3lgS2596aHby2ynVLuGnqlbe8/cE9vYSRrWxRqeit2qVSv0oxpJm4E9wBCwPyL6i8zPzFqnjF/QvSUinihhPmbWQv7MbpaIomEP4A5J90paWmsESUslDUgaGGRvwcWZWbOKbsafGBHbJR0OrJb004hYWz1CRCwHlkPlC7qCyzOzJhVas0fE9uzvLuAWYGEZTZlZ+ZoOu6QpkqYdvA+8DVhfVmNmVq4im/GzgFskHZzP1yLi26V0lZgY3J9bv/wbZ+XWX3fOl+rWXt77XO60zxzI/2Q1b+IhufUe5a8vDu+ZUn/aY4/JnXZowyO5dRudpsMeEZuA15fYi5m1kHe9mSXCYTdLhMNulgiH3SwRDrtZInwq6W5wYCi3PP9Td+fWL/5UzsGGDXaNTXhR/mmqlz28Ord+7KT8XXN59s6enlufuKHpWVsNXrObJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZonwfvaxoMjpviN/H/6B3+QfAjt3YuvWB73feyi37tMalctrdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEd7PnrhtH1+UW5864d5C88+7ZHMM7is0bxsdr9nNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0R4P/s4F2/Ov9Du6g98psEcphZa/jvf/5G6tT7uKTRvG52Ga3ZJKyTtkrS+atgMSaslPZL9Pay1bZpZUSPZjF8JnD5s2EXAmog4BliTPTazLtYw7BGxFtg9bPBZwKrs/irg7JL7MrOSNfsF3ayI2AGQ/T283oiSlkoakDQwyN4mF2dmRbX82/iIWB4R/RHR30tfqxdnZnU0G/adkmYDZH93ldeSmbVCs2G/DViS3V8C3FpOO2bWKg33s0u6ATgZmClpK3AxcBlwk6TzgMeAd7SyydRpYv4/044LFtatrbnwH3OnndlTbD/6UBzIre+d3lO35g917dUw7BGxuE7plJJ7MbMW8s9lzRLhsJslwmE3S4TDbpYIh90sET7EdQzQq1+RW3924bN1a4dOmFx2O8/To/z1xY8u/6e6tdN/ek7utHH/w031ZLV5zW6WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJUIR0baFTdeMWCQfLDdaE6ZMya0PLTimbm3TB/PnveKElbn1RX2DufU+9eYvoIA/eONpufX9v9zZsmWPVetiDU/FbtWqec1ulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXC+9nHO9Xc5fr/5Yn5+8l7jpydW1+19mu59Zk9+b8RKOK0Ixa0bN5jlfezm5nDbpYKh90sEQ67WSIcdrNEOOxmiXDYzRLh88aPdw1+RxGD+3Lr+3/xaG79nLkn5tbnrau/n/2quT/InbaRjctOyK2/4qN3F5r/eNNwzS5phaRdktZXDbtE0jZJD2S3M1vbppkVNZLN+JXA6TWGXxERC7Lb7eW2ZWZlaxj2iFgL7G5DL2bWQkW+oLtA0oPZZv5h9UaStFTSgKSBQfYWWJyZFdFs2K8EjgYWADuAy+uNGBHLI6I/Ivp76WtycWZWVFNhj4idETEUEQeAq4CF5bZlZmVrKuySqo97fDuwvt64ZtYdGu5nl3QDcDIwU9JW4GLgZEkLgAA2A+e3sEcbwx47of6149lWbN4/+fMv5tb/+KNvKraAcaZh2CNicY3BV7egFzNrIf9c1iwRDrtZIhx2s0Q47GaJcNjNEuFDXK21Wniq8on0tGze45HX7GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIryfvQ3Uf3xufcLGLbn1oSd/XWY7baWJrXuL/XzwuZbNezzymt0sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4T3s7fBz953SG79xtO+k1t/7/IP59bnXXFf3dqB5zq7L/rfHl2XUy12PPoH3p//ukzinkLzH2+8ZjdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEjGSSzbPBa4BXgocAJZHxOclzQBuBOZTuWzzOyPif1vX6tj14ofyX+bj/zD/3OoPf+grufXBC4bq1n7wXG/utCt2nZRbX3bkt3LrM3um5NaL7kvPM+nb3o8+GiNZs+8HLoyIY4ETgA9KOg64CFgTEccAa7LHZtalGoY9InZExH3Z/T3ABmAOcBawKhttFXB2q5o0s+JG9Zld0nzgDcA6YFZE7IDKfwjA4WU3Z2blGXHYJU0FbgY+EhFPjWK6pZIGJA0MsreZHs2sBCMKu6ReKkG/PiK+mQ3eKWl2Vp8N7Ko1bUQsj4j+iOjvpa+Mns2sCQ3DLknA1cCGiFhWVboNWJLdXwLcWn57ZlYWRYNL6ko6Cfge8BCVXW8An6Tyuf0mYB7wGPCOiNidN6/pmhGLdErRnseeCfm7n67a/N3c+ryJU8vsZsw441W/l1s/sGdPmzoZO9bFGp6K3apVa7ifPSK+D9ScGEgwuWZjk39BZ5YIh90sEQ67WSIcdrNEOOxmiXDYzRLhU0m3w4H6h6ACnP/Kt+bW/+z+zbn18w795Wg7apunD9Q/lfWfzv3d/InD+9HL5DW7WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYI72fvAo0uq3zTsS/Nrf9L31F1a8+e8frcaW/8wrLc+uMH8t8iF536F7n1oUc25VTzz6Vg5fKa3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLRMPzxpcp2fPGm7VJ3nnjvWY3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLRMOyS5kr6L0kbJD0s6cPZ8EskbZP0QHY7s/XtmlmzRnLyiv3AhRFxn6RpwL2SVme1KyLis61rz8zK0jDsEbED2JHd3yNpAzCn1Y2ZWblG9Zld0nzgDcC6bNAFkh6UtELSYXWmWSppQNLAIHsLNWtmzRtx2CVNBW4GPhIRTwFXAkcDC6is+S+vNV1ELI+I/ojo76WvhJbNrBkjCrukXipBvz4ivgkQETsjYigiDgBXAQtb16aZFTWSb+MFXA1siIhlVcNnV432dmB9+e2ZWVlG8m38icB7gIckPZAN+ySwWNICKucD3gyc35IOzawUI/k2/vtAreNjby+/HTNrFf+CziwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyWirZdslvQ48GjVoJnAE21rYHS6tbdu7QvcW7PK7O2oiHhJrUJbw/6ChUsDEdHfsQZydGtv3doXuLdmtas3b8abJcJhN0tEp8O+vMPLz9OtvXVrX+DemtWW3jr6md3M2qfTa3YzaxOH3SwRHQm7pNMl/UzSRkkXdaKHeiRtlvRQdhnqgQ73skLSLknrq4bNkLRa0iPZ35rX2OtQb11xGe+cy4x39LXr9OXP2/6ZXVIP8HPgVGArcA+wOCJ+0tZG6pC0GeiPiI7/AEPS7wNPA9dExPHZsM8AuyPisuw/ysMi4hNd0tslwNOdvox3drWi2dWXGQfOBs6lg69dTl/vpA2vWyfW7AuBjRGxKSL2AV8HzupAH10vItYCu4cNPgtYld1fReXN0nZ1eusKEbEjIu7L7u8BDl5mvKOvXU5fbdGJsM8BtlQ93kp3Xe89gDsk3StpaaebqWFWROyAypsHOLzD/QzX8DLe7TTsMuNd89o1c/nzojoR9lqXkuqm/X8nRsQbgTOAD2abqzYyI7qMd7vUuMx4V2j28udFdSLsW4G5VY+PBLZ3oI+aImJ79ncXcAvddynqnQevoJv93dXhfn6rmy7jXesy43TBa9fJy593Iuz3AMdIepmkScC7gNs60McLSJqSfXGCpCnA2+i+S1HfBizJ7i8Bbu1gL8/TLZfxrneZcTr82nX88ucR0fYbcCaVb+T/B/jrTvRQp6+XAz/Obg93ujfgBiqbdYNUtojOA34HWAM8kv2d0UW9XQs8BDxIJVizO9TbSVQ+Gj4IPJDdzuz0a5fTV1teN/9c1iwR/gWdWSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpaI/wNQBZwRLpVR5AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    path.draw()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
