{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"factor\": 0.5,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 176687.687500\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -46170.945312\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -74599.382812\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -79931.828125\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -68139.453125\n",
      "    epoch          : 1\n",
      "    loss           : -55466.50569500309\n",
      "    val_loss       : -74384.64921875\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -68251.875000\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -82103.351562\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -90188.359375\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -89065.906250\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -93276.578125\n",
      "    epoch          : 2\n",
      "    loss           : -86604.69248917079\n",
      "    val_loss       : -99904.8671875\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -101313.281250\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -106347.921875\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -110256.015625\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -112814.125000\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -116157.421875\n",
      "    epoch          : 3\n",
      "    loss           : -110938.95714727722\n",
      "    val_loss       : -123319.3283203125\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -115903.953125\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -127823.000000\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -138352.406250\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -143276.031250\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -148945.843750\n",
      "    epoch          : 4\n",
      "    loss           : -133408.79904084158\n",
      "    val_loss       : -145135.5248046875\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -142195.500000\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -148002.000000\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -159241.500000\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -157086.406250\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -161275.046875\n",
      "    epoch          : 5\n",
      "    loss           : -155074.8844368812\n",
      "    val_loss       : -166153.821484375\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -165044.578125\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -169266.078125\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -176070.843750\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -179417.562500\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -187233.156250\n",
      "    epoch          : 6\n",
      "    loss           : -175387.56033415842\n",
      "    val_loss       : -185891.123046875\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -187629.234375\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -189139.812500\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -198115.687500\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -197915.015625\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -205394.062500\n",
      "    epoch          : 7\n",
      "    loss           : -191674.28314511137\n",
      "    val_loss       : -204152.740625\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -202535.609375\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -206432.484375\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -209751.937500\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -222811.734375\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -217513.187500\n",
      "    epoch          : 8\n",
      "    loss           : -212816.44353341585\n",
      "    val_loss       : -221326.266015625\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -218919.156250\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -223055.593750\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -220418.875000\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -230554.890625\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -238281.015625\n",
      "    epoch          : 9\n",
      "    loss           : -229966.0228960396\n",
      "    val_loss       : -238471.5455078125\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -249468.500000\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -240054.687500\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -252739.312500\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -256118.500000\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -254546.953125\n",
      "    epoch          : 10\n",
      "    loss           : -246581.5869430693\n",
      "    val_loss       : -254711.4623046875\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -268733.406250\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -261923.609375\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -264025.125000\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -272774.687500\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -264896.625000\n",
      "    epoch          : 11\n",
      "    loss           : -262233.9668935643\n",
      "    val_loss       : -269290.8697265625\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -285544.562500\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -262260.062500\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -273127.812500\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -297217.593750\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -284281.031250\n",
      "    epoch          : 12\n",
      "    loss           : -276496.16893564357\n",
      "    val_loss       : -283592.905078125\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -302706.312500\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -282082.593750\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -276636.406250\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -313055.218750\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -277557.562500\n",
      "    epoch          : 13\n",
      "    loss           : -290458.6670792079\n",
      "    val_loss       : -297111.0662109375\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -318492.937500\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -299725.718750\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -284440.500000\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -307957.375000\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -311811.687500\n",
      "    epoch          : 14\n",
      "    loss           : -303711.09931930696\n",
      "    val_loss       : -310207.5427734375\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -317932.000000\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -301301.500000\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -326007.937500\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -322119.281250\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -320063.656250\n",
      "    epoch          : 15\n",
      "    loss           : -316605.171720297\n",
      "    val_loss       : -322153.7126953125\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -330444.843750\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -317302.000000\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -295509.750000\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -356831.875000\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -331112.625000\n",
      "    epoch          : 16\n",
      "    loss           : -327878.734220297\n",
      "    val_loss       : -333832.840625\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -361005.000000\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -340094.093750\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -351181.593750\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -338626.937500\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -339625.843750\n",
      "    epoch          : 17\n",
      "    loss           : -339487.671720297\n",
      "    val_loss       : -345033.2544921875\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -354053.218750\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -340964.375000\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -362394.687500\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -347477.687500\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -357034.843750\n",
      "    epoch          : 18\n",
      "    loss           : -350359.08570544556\n",
      "    val_loss       : -354754.278125\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -386923.875000\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -367620.312500\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -330486.625000\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -359924.187500\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -359064.312500\n",
      "    epoch          : 19\n",
      "    loss           : -360519.27165841585\n",
      "    val_loss       : -365421.028125\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -399032.437500\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -365003.500000\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -384369.875000\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -376056.187500\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -342561.906250\n",
      "    epoch          : 20\n",
      "    loss           : -370581.4204826733\n",
      "    val_loss       : -374548.071875\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -409464.687500\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -388523.187500\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -382348.343750\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -376846.187500\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -352266.343750\n",
      "    epoch          : 21\n",
      "    loss           : -379946.0402227723\n",
      "    val_loss       : -383985.36796875\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -421269.031250\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -396608.750000\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -371139.468750\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -384067.125000\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -385894.281250\n",
      "    epoch          : 22\n",
      "    loss           : -388086.13613861386\n",
      "    val_loss       : -390914.6765625\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -428294.875000\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -408007.656250\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -400696.156250\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -362308.375000\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -393247.843750\n",
      "    epoch          : 23\n",
      "    loss           : -396989.6441831683\n",
      "    val_loss       : -401657.5447265625\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -415308.062500\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -396526.312500\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -408332.781250\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -403748.843750\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -399908.250000\n",
      "    epoch          : 24\n",
      "    loss           : -406010.12221534655\n",
      "    val_loss       : -409561.1630859375\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -425426.250000\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -391775.562500\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -392099.250000\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -375249.875000\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -421186.250000\n",
      "    epoch          : 25\n",
      "    loss           : -413443.1163366337\n",
      "    val_loss       : -417319.763671875\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -458759.375000\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -433493.437500\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -427937.187500\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -425652.000000\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -418326.156250\n",
      "    epoch          : 26\n",
      "    loss           : -421920.25278465345\n",
      "    val_loss       : -424742.836328125\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -467198.562500\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -404356.125000\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -420973.000000\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -424968.312500\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -425465.437500\n",
      "    epoch          : 27\n",
      "    loss           : -428269.2883663366\n",
      "    val_loss       : -431946.970703125\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -475406.000000\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -449043.093750\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -398904.343750\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -454509.312500\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -442713.375000\n",
      "    epoch          : 28\n",
      "    loss           : -436283.1036509901\n",
      "    val_loss       : -439365.1865234375\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -454899.531250\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -432837.468750\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -419357.062500\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -435884.343750\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -439057.468750\n",
      "    epoch          : 29\n",
      "    loss           : -442675.40872524754\n",
      "    val_loss       : -446251.9224609375\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -437500.093750\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -462720.375000\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -425668.687500\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -466197.781250\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -454966.406250\n",
      "    epoch          : 30\n",
      "    loss           : -449477.4603960396\n",
      "    val_loss       : -451973.2271484375\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -500212.468750\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -431256.500000\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -475269.187500\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -503633.500000\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -463144.062500\n",
      "    epoch          : 31\n",
      "    loss           : -455646.8836633663\n",
      "    val_loss       : -457568.10546875\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -506556.125000\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -475593.812500\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -419890.562500\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -454123.375000\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -456475.312500\n",
      "    epoch          : 32\n",
      "    loss           : -461690.1126237624\n",
      "    val_loss       : -463682.70625\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -514829.312500\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -443157.375000\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -464124.437500\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -463020.281250\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -462645.875000\n",
      "    epoch          : 33\n",
      "    loss           : -467525.9161509901\n",
      "    val_loss       : -470042.990625\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -427202.937500\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -463032.875000\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -431570.187500\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -523720.562500\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -468034.937500\n",
      "    epoch          : 34\n",
      "    loss           : -474056.8313737624\n",
      "    val_loss       : -476066.655078125\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -527140.375000\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -496822.656250\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -500825.031250\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -475018.750000\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -488298.062500\n",
      "    epoch          : 35\n",
      "    loss           : -479744.09344059404\n",
      "    val_loss       : -481367.7208984375\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -534647.000000\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -474983.750000\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -477416.531250\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -480280.937500\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -490091.593750\n",
      "    epoch          : 36\n",
      "    loss           : -486004.67976485146\n",
      "    val_loss       : -487947.6298828125\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -540683.875000\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -465375.625000\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -480160.562500\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -490289.937500\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -478582.250000\n",
      "    epoch          : 37\n",
      "    loss           : -489499.50247524754\n",
      "    val_loss       : -492835.2591796875\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -546733.687500\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -513508.812500\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -467676.750000\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -550104.812500\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -489922.437500\n",
      "    epoch          : 38\n",
      "    loss           : -496343.7988861386\n",
      "    val_loss       : -499178.2955078125\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -552401.187500\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -474284.000000\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -505610.062500\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -453997.468750\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -455321.437500\n",
      "    epoch          : 39\n",
      "    loss           : -501551.708230198\n",
      "    val_loss       : -502431.5345703125\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -559345.937500\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -493460.843750\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -478165.875000\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -465839.218750\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -502740.500000\n",
      "    epoch          : 40\n",
      "    loss           : -505938.27629950497\n",
      "    val_loss       : -508405.934765625\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -562850.000000\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -497225.750000\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -535166.250000\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -504832.062500\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -505410.062500\n",
      "    epoch          : 41\n",
      "    loss           : -511276.6547029703\n",
      "    val_loss       : -512443.6513671875\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -566879.125000\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -534137.375000\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -485962.625000\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -539525.875000\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -475045.468750\n",
      "    epoch          : 42\n",
      "    loss           : -515883.708539604\n",
      "    val_loss       : -516991.9203125\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -573919.062500\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -535813.500000\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -491604.937500\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -525263.250000\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -511150.500000\n",
      "    epoch          : 43\n",
      "    loss           : -520554.36912128713\n",
      "    val_loss       : -523429.180078125\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -580532.500000\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -512272.000000\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -529531.000000\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -476272.718750\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -480878.375000\n",
      "    epoch          : 44\n",
      "    loss           : -524786.4331683168\n",
      "    val_loss       : -524166.59765625\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -582192.062500\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -514274.500000\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -517806.406250\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -516599.937500\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -537682.375000\n",
      "    epoch          : 45\n",
      "    loss           : -529570.8372524752\n",
      "    val_loss       : -531371.3294921875\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -589359.625000\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -551923.875000\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -537829.000000\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -525929.375000\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -486815.937500\n",
      "    epoch          : 46\n",
      "    loss           : -533619.5946782178\n",
      "    val_loss       : -534363.869140625\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -594428.375000\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -558704.875000\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -490716.500000\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -531889.187500\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -544123.937500\n",
      "    epoch          : 47\n",
      "    loss           : -538335.1336633663\n",
      "    val_loss       : -540087.197265625\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -564363.500000\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -565771.562500\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -509806.187500\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -536785.187500\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -504568.750000\n",
      "    epoch          : 48\n",
      "    loss           : -543279.1219059406\n",
      "    val_loss       : -544808.8015625\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -605231.562500\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -536634.875000\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -570986.625000\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -552180.062500\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -553409.562500\n",
      "    epoch          : 49\n",
      "    loss           : -547574.7422648515\n",
      "    val_loss       : -548205.6193359375\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -609659.625000\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -539171.000000\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -572521.125000\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -504168.937500\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -536829.250000\n",
      "    epoch          : 50\n",
      "    loss           : -550416.9421410891\n",
      "    val_loss       : -551697.0958984375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0803_215641/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -507047.062500\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -575356.000000\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -510865.437500\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -525987.875000\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -542663.125000\n",
      "    epoch          : 51\n",
      "    loss           : -556135.3456064357\n",
      "    val_loss       : -557001.920703125\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -619425.187500\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -526722.437500\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -528294.375000\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -546768.500000\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -565250.750000\n",
      "    epoch          : 52\n",
      "    loss           : -559785.2846534654\n",
      "    val_loss       : -560938.737109375\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -624945.875000\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -565516.125000\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -527770.375000\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -623617.875000\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -557057.250000\n",
      "    epoch          : 53\n",
      "    loss           : -562023.6079826732\n",
      "    val_loss       : -563787.779296875\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -628356.687500\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -552044.625000\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -516417.968750\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -520334.500000\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -562137.687500\n",
      "    epoch          : 54\n",
      "    loss           : -566095.9832920792\n",
      "    val_loss       : -567620.1033203125\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -631494.750000\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -593669.000000\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -571551.250000\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -634364.000000\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -564914.000000\n",
      "    epoch          : 55\n",
      "    loss           : -570212.4842202971\n",
      "    val_loss       : -570378.1388671875\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -635866.875000\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -556408.562500\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -599138.125000\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -555883.375000\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -580999.500000\n",
      "    epoch          : 56\n",
      "    loss           : -574509.1639851485\n",
      "    val_loss       : -576644.08515625\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -601341.187500\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -565814.250000\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -543039.875000\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -534298.687500\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -583649.562500\n",
      "    epoch          : 57\n",
      "    loss           : -578066.8217821782\n",
      "    val_loss       : -578744.041015625\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -605636.125000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -604426.375000\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -566608.375000\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -647484.562500\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -584541.312500\n",
      "    epoch          : 58\n",
      "    loss           : -580488.1943069306\n",
      "    val_loss       : -580931.535546875\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -647734.375000\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -610065.750000\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -552357.375000\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -580645.750000\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -580392.875000\n",
      "    epoch          : 59\n",
      "    loss           : -584928.8632425743\n",
      "    val_loss       : -584603.3681640625\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -652671.375000\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -572687.375000\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -616260.125000\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -545255.062500\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -596634.125000\n",
      "    epoch          : 60\n",
      "    loss           : -589222.8428217822\n",
      "    val_loss       : -589867.4193359375\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -656328.750000\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -580701.500000\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -544176.250000\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -547201.062500\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -546196.250000\n",
      "    epoch          : 61\n",
      "    loss           : -591449.1707920792\n",
      "    val_loss       : -591131.778125\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -659560.062500\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -597458.625000\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -583699.437500\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -603307.437500\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -580803.625000\n",
      "    epoch          : 62\n",
      "    loss           : -595914.6101485149\n",
      "    val_loss       : -596857.698828125\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -663640.125000\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -621230.500000\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -548477.125000\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -581519.500000\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -582159.562500\n",
      "    epoch          : 63\n",
      "    loss           : -598427.1553217822\n",
      "    val_loss       : -598947.4845703125\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -668695.687500\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -586381.187500\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -553860.500000\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -610142.187500\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -605635.687500\n",
      "    epoch          : 64\n",
      "    loss           : -601368.8360148515\n",
      "    val_loss       : -600273.24296875\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -671075.125000\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -584130.750000\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -586878.062500\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -588649.062500\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -585532.937500\n",
      "    epoch          : 65\n",
      "    loss           : -604702.3118811881\n",
      "    val_loss       : -606037.7025390625\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -676091.875000\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -593743.187500\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -568945.625000\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -562015.812500\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -591313.375000\n",
      "    epoch          : 66\n",
      "    loss           : -607637.7116336634\n",
      "    val_loss       : -608459.8083984375\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -634437.062500\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -594558.250000\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -574452.187500\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -603856.562500\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -602181.812500\n",
      "    epoch          : 67\n",
      "    loss           : -610285.7827970297\n",
      "    val_loss       : -610357.3650390625\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -680347.437500\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -640160.625000\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -595874.250000\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -598102.250000\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -611204.125000\n",
      "    epoch          : 68\n",
      "    loss           : -614841.8873762377\n",
      "    val_loss       : -616459.833203125\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -686239.687500\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -645222.062500\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -581644.187500\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -608631.687500\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -597876.625000\n",
      "    epoch          : 69\n",
      "    loss           : -616680.5946782178\n",
      "    val_loss       : -614236.9466796875\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -600592.375000\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -644321.062500\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -573728.000000\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -601932.625000\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -601723.750000\n",
      "    epoch          : 70\n",
      "    loss           : -618555.7908415842\n",
      "    val_loss       : -621665.0478515625\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -692964.687500\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -650273.750000\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -579961.750000\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -695484.125000\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -605020.937500\n",
      "    epoch          : 71\n",
      "    loss           : -624343.1262376237\n",
      "    val_loss       : -622940.794921875\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -697174.125000\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -604172.000000\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -577354.375000\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -620056.000000\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -624212.750000\n",
      "    epoch          : 72\n",
      "    loss           : -624276.9449257426\n",
      "    val_loss       : -625884.4537109375\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -700033.875000\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -624781.062500\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -623646.937500\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -699839.625000\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -628893.750000\n",
      "    epoch          : 73\n",
      "    loss           : -628582.9993811881\n",
      "    val_loss       : -626869.5115234375\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -701925.437500\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -657235.062500\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -589467.000000\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -626240.250000\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -639552.875000\n",
      "    epoch          : 74\n",
      "    loss           : -632216.2097772277\n",
      "    val_loss       : -631798.979296875\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -660425.250000\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -595072.750000\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -590238.187500\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -663118.875000\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -632799.062500\n",
      "    epoch          : 75\n",
      "    loss           : -634871.1225247525\n",
      "    val_loss       : -636270.8064453125\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -619841.375000\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -620947.375000\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -598146.500000\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -633962.500000\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -642290.250000\n",
      "    epoch          : 76\n",
      "    loss           : -637575.3595297029\n",
      "    val_loss       : -637612.3060546875\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -712069.750000\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -633385.000000\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -627950.875000\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -594393.062500\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -638636.687500\n",
      "    epoch          : 77\n",
      "    loss           : -639488.6200495049\n",
      "    val_loss       : -639229.425390625\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -714529.812500\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -628034.500000\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -596463.125000\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -667739.250000\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -642340.812500\n",
      "    epoch          : 78\n",
      "    loss           : -643523.7042079208\n",
      "    val_loss       : -641990.96796875\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -719376.125000\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -673835.562500\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -637071.375000\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -641726.875000\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -624313.500000\n",
      "    epoch          : 79\n",
      "    loss           : -644903.5253712871\n",
      "    val_loss       : -645185.50625\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -722133.562500\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -628689.062500\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -642795.000000\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -673309.750000\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -625484.125000\n",
      "    epoch          : 80\n",
      "    loss           : -646938.9412128713\n",
      "    val_loss       : -647160.6771484375\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -676922.937500\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -675877.500000\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -631367.000000\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -605725.375000\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -630881.812500\n",
      "    epoch          : 81\n",
      "    loss           : -651168.3242574257\n",
      "    val_loss       : -652365.4294921875\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -728331.125000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -636017.000000\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -680215.812500\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -658832.937500\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -651939.375000\n",
      "    epoch          : 82\n",
      "    loss           : -654369.0792079208\n",
      "    val_loss       : -652984.984765625\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -732090.500000\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -638504.500000\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -617404.000000\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -659927.812500\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -635503.500000\n",
      "    epoch          : 83\n",
      "    loss           : -655538.4535891089\n",
      "    val_loss       : -655315.071484375\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -638357.687500\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -675528.625000\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -615750.375000\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -655578.687500\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -653846.000000\n",
      "    epoch          : 84\n",
      "    loss           : -658582.4603960396\n",
      "    val_loss       : -658868.5005859375\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -736716.312500\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -685966.562500\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -617881.312500\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -738075.750000\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -617409.750000\n",
      "    epoch          : 85\n",
      "    loss           : -661239.0482673268\n",
      "    val_loss       : -660853.7755859375\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -739552.375000\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -642706.500000\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -622660.125000\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -682250.937500\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -664690.500000\n",
      "    epoch          : 86\n",
      "    loss           : -660783.5191831683\n",
      "    val_loss       : -661215.3478515625\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -741322.500000\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -620681.625000\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -616323.500000\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -670743.750000\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -646194.750000\n",
      "    epoch          : 87\n",
      "    loss           : -664631.2264851485\n",
      "    val_loss       : -666419.3078125\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -665066.250000\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -651941.625000\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -673124.500000\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -670638.125000\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -667433.750000\n",
      "    epoch          : 88\n",
      "    loss           : -668415.7431930694\n",
      "    val_loss       : -668417.501953125\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -746781.000000\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -630001.562500\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -624556.187500\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -675000.500000\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -670983.937500\n",
      "    epoch          : 89\n",
      "    loss           : -670916.6089108911\n",
      "    val_loss       : -669548.7181640625\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -751815.500000\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -663140.375000\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -625584.750000\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -633512.187500\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -672286.750000\n",
      "    epoch          : 90\n",
      "    loss           : -672529.8589108911\n",
      "    val_loss       : -672305.3712890625\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -701925.500000\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -648787.562500\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -647778.937500\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -676188.187500\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -676499.125000\n",
      "    epoch          : 91\n",
      "    loss           : -672762.3787128713\n",
      "    val_loss       : -671576.832421875\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -754450.000000\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -674476.125000\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -634876.750000\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -699375.687500\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -683181.750000\n",
      "    epoch          : 92\n",
      "    loss           : -675291.1491336634\n",
      "    val_loss       : -676695.70234375\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -758946.875000\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -657755.875000\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -706575.750000\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -684158.000000\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -656249.000000\n",
      "    epoch          : 93\n",
      "    loss           : -679955.8211633663\n",
      "    val_loss       : -679653.791015625\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -761966.187500\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -660601.250000\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -706631.312500\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -658652.812500\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -656936.625000\n",
      "    epoch          : 94\n",
      "    loss           : -681377.4529702971\n",
      "    val_loss       : -681691.3728515625\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -763611.375000\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -662594.000000\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -644589.375000\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -681169.687500\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -639862.375000\n",
      "    epoch          : 95\n",
      "    loss           : -684739.4969059406\n",
      "    val_loss       : -683307.7470703125\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -766175.125000\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -635872.250000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -678699.687500\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -690226.375000\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -662387.375000\n",
      "    epoch          : 96\n",
      "    loss           : -685672.5408415842\n",
      "    val_loss       : -685252.850390625\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -768072.250000\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -639754.875000\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -712065.875000\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -688629.375000\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -694225.625000\n",
      "    epoch          : 97\n",
      "    loss           : -688139.2456683168\n",
      "    val_loss       : -684189.709375\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -769349.250000\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -719802.000000\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -644503.500000\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -688607.250000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -657837.250000\n",
      "    epoch          : 98\n",
      "    loss           : -689156.4634900991\n",
      "    val_loss       : -687821.8068359375\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -669306.375000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -666437.250000\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -715728.750000\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -644385.250000\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -698560.750000\n",
      "    epoch          : 99\n",
      "    loss           : -692163.5903465346\n",
      "    val_loss       : -692375.809375\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -773105.500000\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -720575.062500\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -702135.375000\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -657792.562500\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -702209.437500\n",
      "    epoch          : 100\n",
      "    loss           : -695801.8818069306\n",
      "    val_loss       : -693642.3216796875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0803_215641/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -777809.250000\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -724919.312500\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -720614.375000\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -778507.625000\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -694693.625000\n",
      "    epoch          : 101\n",
      "    loss           : -695390.1639851485\n",
      "    val_loss       : -691457.9322265625\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -779494.500000\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -670528.250000\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -653820.812500\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -723446.562500\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -675128.500000\n",
      "    epoch          : 102\n",
      "    loss           : -695623.6163366337\n",
      "    val_loss       : -698500.6732421875\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -782677.062500\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -728663.500000\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -726925.125000\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -729261.750000\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -676775.125000\n",
      "    epoch          : 103\n",
      "    loss           : -701372.3694306931\n",
      "    val_loss       : -699212.513671875\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -785377.562500\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -733449.562500\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -732228.500000\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -673579.375000\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -707708.687500\n",
      "    epoch          : 104\n",
      "    loss           : -702756.0154702971\n",
      "    val_loss       : -701515.388671875\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -787911.687500\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -682379.437500\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -731527.312500\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -659679.875000\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -712535.500000\n",
      "    epoch          : 105\n",
      "    loss           : -704102.0544554455\n",
      "    val_loss       : -703445.0908203125\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -789427.375000\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -685063.812500\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -664165.250000\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -790513.437500\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -673616.562500\n",
      "    epoch          : 106\n",
      "    loss           : -704105.4133663366\n",
      "    val_loss       : -701362.1732421875\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -785822.812500\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -737475.687500\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -664251.875000\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -662889.375000\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -714379.062500\n",
      "    epoch          : 107\n",
      "    loss           : -707364.9362623763\n",
      "    val_loss       : -707632.3853515625\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -793407.687500\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -736262.625000\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -716070.437500\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -696373.062500\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -678045.500000\n",
      "    epoch          : 108\n",
      "    loss           : -709237.2580445545\n",
      "    val_loss       : -708650.3892578125\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -794681.062500\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -689299.062500\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -734864.687500\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -709352.250000\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -718427.375000\n",
      "    epoch          : 109\n",
      "    loss           : -711464.4084158416\n",
      "    val_loss       : -709848.6896484375\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -799472.375000\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -669028.875000\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -690481.062500\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -801053.687500\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -687757.750000\n",
      "    epoch          : 110\n",
      "    loss           : -713797.8353960396\n",
      "    val_loss       : -712643.3287109375\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -801255.375000\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -665628.812500\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -668493.000000\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -673914.750000\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -685517.312500\n",
      "    epoch          : 111\n",
      "    loss           : -714469.9535891089\n",
      "    val_loss       : -712513.711328125\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -801748.500000\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -695458.375000\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -694808.187500\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -693513.250000\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -689248.000000\n",
      "    epoch          : 112\n",
      "    loss           : -717892.5191831683\n",
      "    val_loss       : -716866.6359375\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -806462.500000\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -750882.125000\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -743315.000000\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -807221.000000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -674842.187500\n",
      "    epoch          : 113\n",
      "    loss           : -719211.3793316832\n",
      "    val_loss       : -718950.5390625\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -808066.375000\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -696439.000000\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -696614.437500\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -721125.625000\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -677745.312500\n",
      "    epoch          : 114\n",
      "    loss           : -721368.4857673268\n",
      "    val_loss       : -719959.362890625\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -750104.000000\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -698898.000000\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -721765.312500\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -693046.437500\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -741791.125000\n",
      "    epoch          : 115\n",
      "    loss           : -720566.9344059406\n",
      "    val_loss       : -719192.15078125\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -809088.125000\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -754558.625000\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -749558.187500\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -676574.437500\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -697255.125000\n",
      "    epoch          : 116\n",
      "    loss           : -723268.9907178218\n",
      "    val_loss       : -723465.9369140625\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -755831.250000\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -682973.062500\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -748291.500000\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -682109.500000\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -731524.500000\n",
      "    epoch          : 117\n",
      "    loss           : -726414.9381188119\n",
      "    val_loss       : -725642.7544921875\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -815904.375000\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -758687.000000\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -685708.750000\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -727870.500000\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -731274.437500\n",
      "    epoch          : 118\n",
      "    loss           : -729244.3793316832\n",
      "    val_loss       : -727511.686328125\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -819267.500000\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -705001.062500\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -683442.812500\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -723801.812500\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -701459.250000\n",
      "    epoch          : 119\n",
      "    loss           : -727457.9275990099\n",
      "    val_loss       : -726370.320703125\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -815850.125000\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -705721.937500\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -687397.875000\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -754430.125000\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -700743.125000\n",
      "    epoch          : 120\n",
      "    loss           : -730755.8384900991\n",
      "    val_loss       : -730295.23203125\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -763577.375000\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -689019.750000\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -692068.812500\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -823032.875000\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -736381.937500\n",
      "    epoch          : 121\n",
      "    loss           : -732759.3669554455\n",
      "    val_loss       : -728959.4837890625\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -822108.500000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -705084.125000\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -758870.375000\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -741233.812500\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -741739.562500\n",
      "    epoch          : 122\n",
      "    loss           : -733318.8978960396\n",
      "    val_loss       : -733151.7462890625\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -824916.000000\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -712173.750000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -758557.500000\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -683392.250000\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -739916.250000\n",
      "    epoch          : 123\n",
      "    loss           : -734768.7487623763\n",
      "    val_loss       : -733418.0740234375\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -824026.500000\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -764710.625000\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -691870.250000\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -734505.312500\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -747503.875000\n",
      "    epoch          : 124\n",
      "    loss           : -737619.896039604\n",
      "    val_loss       : -736402.680078125\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -827708.625000\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -693192.750000\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -696994.625000\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -828040.750000\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -707418.250000\n",
      "    epoch          : 125\n",
      "    loss           : -737701.6633663366\n",
      "    val_loss       : -737167.199609375\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -830001.437500\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -711835.937500\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -695654.562500\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -746797.437500\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -712307.562500\n",
      "    epoch          : 126\n",
      "    loss           : -740220.9863861386\n",
      "    val_loss       : -739361.6107421875\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -773627.625000\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -719499.250000\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -700296.625000\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -834488.562500\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -715817.375000\n",
      "    epoch          : 127\n",
      "    loss           : -743653.8824257426\n",
      "    val_loss       : -741771.4501953125\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -834603.750000\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -719871.125000\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -716278.875000\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -745305.875000\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -747893.500000\n",
      "    epoch          : 128\n",
      "    loss           : -743280.0191831683\n",
      "    val_loss       : -739109.4404296875\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -711263.062500\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -772729.187500\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -699446.875000\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -699921.062500\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -750123.000000\n",
      "    epoch          : 129\n",
      "    loss           : -745111.2097772277\n",
      "    val_loss       : -743974.3015625\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -837338.000000\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -703091.375000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -775367.375000\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -754269.562500\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -749414.250000\n",
      "    epoch          : 130\n",
      "    loss           : -746647.0996287129\n",
      "    val_loss       : -743153.5033203125\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -775982.625000\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -765545.125000\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -721560.125000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -747758.375000\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -716257.500000\n",
      "    epoch          : 131\n",
      "    loss           : -746055.3533415842\n",
      "    val_loss       : -746333.3087890625\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -840758.312500\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -777239.500000\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -751981.625000\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -780407.375000\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -754813.750000\n",
      "    epoch          : 132\n",
      "    loss           : -750335.9696782178\n",
      "    val_loss       : -748374.066015625\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -842533.000000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -774297.875000\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -706488.375000\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -845363.625000\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -758288.875000\n",
      "    epoch          : 133\n",
      "    loss           : -752522.4771039604\n",
      "    val_loss       : -749460.767578125\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -844366.000000\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -768829.562500\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -701484.375000\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -700435.375000\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -758907.062500\n",
      "    epoch          : 134\n",
      "    loss           : -750391.4523514851\n",
      "    val_loss       : -750324.9416015625\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -845903.562500\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -728184.687500\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -758192.687500\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -755854.500000\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -723513.687500\n",
      "    epoch          : 135\n",
      "    loss           : -754217.6027227723\n",
      "    val_loss       : -752175.803515625\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -848075.125000\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -785417.000000\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -705055.000000\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -707888.500000\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -727768.437500\n",
      "    epoch          : 136\n",
      "    loss           : -755710.1745049505\n",
      "    val_loss       : -754158.237890625\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -786453.625000\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -776313.125000\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -763347.500000\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -707000.750000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -759995.000000\n",
      "    epoch          : 137\n",
      "    loss           : -756406.0860148515\n",
      "    val_loss       : -754183.7021484375\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -849966.437500\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -731944.187500\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -708686.312500\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -731069.000000\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -762922.500000\n",
      "    epoch          : 138\n",
      "    loss           : -756857.2246287129\n",
      "    val_loss       : -755326.206640625\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -850169.500000\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -732167.500000\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -784906.437500\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -786990.250000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -763613.875000\n",
      "    epoch          : 139\n",
      "    loss           : -759326.5717821782\n",
      "    val_loss       : -758410.4486328125\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -855698.250000\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -711587.312500\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -784627.937500\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -716078.625000\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -717508.625000\n",
      "    epoch          : 140\n",
      "    loss           : -761935.9900990099\n",
      "    val_loss       : -759625.0228515625\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -733808.875000\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -792351.125000\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -715968.750000\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -767951.687500\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -762266.500000\n",
      "    epoch          : 141\n",
      "    loss           : -762237.6695544554\n",
      "    val_loss       : -760604.34453125\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -859008.625000\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -712640.062500\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -765805.500000\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -763477.062500\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -765911.312500\n",
      "    epoch          : 142\n",
      "    loss           : -762574.801980198\n",
      "    val_loss       : -758278.29296875\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -858725.687500\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -794470.312500\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -741189.875000\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -792957.625000\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -738102.125000\n",
      "    epoch          : 143\n",
      "    loss           : -764804.8688118812\n",
      "    val_loss       : -763357.05859375\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -859060.250000\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -796456.750000\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -725171.000000\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -769880.750000\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -769548.250000\n",
      "    epoch          : 144\n",
      "    loss           : -767241.3063118812\n",
      "    val_loss       : -763979.931640625\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -863016.000000\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -742718.875000\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -769386.125000\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -773979.000000\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -770536.437500\n",
      "    epoch          : 145\n",
      "    loss           : -768797.9121287129\n",
      "    val_loss       : -765049.0140625\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -864873.562500\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -737696.437500\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -792553.875000\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -774527.062500\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -773795.000000\n",
      "    epoch          : 146\n",
      "    loss           : -768619.9009900991\n",
      "    val_loss       : -767230.2271484375\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -866058.562500\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -745783.750000\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -777038.000000\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -798297.937500\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -778846.187500\n",
      "    epoch          : 147\n",
      "    loss           : -772009.0327970297\n",
      "    val_loss       : -768948.5244140625\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -868162.875000\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -748412.000000\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -722336.937500\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -868992.000000\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -773128.562500\n",
      "    epoch          : 148\n",
      "    loss           : -771318.1955445545\n",
      "    val_loss       : -768587.224609375\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -870201.250000\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -747436.250000\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -771912.937500\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -724314.250000\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -775648.000000\n",
      "    epoch          : 149\n",
      "    loss           : -771106.2840346535\n",
      "    val_loss       : -769745.7892578125\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -870748.625000\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -777306.500000\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -796612.250000\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -775462.500000\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -743375.750000\n",
      "    epoch          : 150\n",
      "    loss           : -774173.9189356435\n",
      "    val_loss       : -771693.7466796875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0803_215641/checkpoint-epoch150.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -869593.312500\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -750358.312500\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -731877.625000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -727117.875000\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -770676.812500\n",
      "    epoch          : 151\n",
      "    loss           : -774527.7561881188\n",
      "    val_loss       : -766981.131640625\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -868894.750000\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -746708.750000\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -802058.062500\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -799953.625000\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -781816.562500\n",
      "    epoch          : 152\n",
      "    loss           : -775609.1299504951\n",
      "    val_loss       : -774261.3263671875\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -874597.687500\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -751457.000000\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -800564.062500\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -876164.750000\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -782804.000000\n",
      "    epoch          : 153\n",
      "    loss           : -777795.9870049505\n",
      "    val_loss       : -774755.599609375\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -875682.125000\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -804843.375000\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -734475.437500\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -779624.125000\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -750012.312500\n",
      "    epoch          : 154\n",
      "    loss           : -778213.2865099009\n",
      "    val_loss       : -775648.3041015625\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -875350.375000\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -811603.625000\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -736744.812500\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -878004.937500\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -809559.375000\n",
      "    epoch          : 155\n",
      "    loss           : -781461.7209158416\n",
      "    val_loss       : -777192.0521484375\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -877285.875000\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -748516.187500\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -807040.875000\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -783055.750000\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -748349.250000\n",
      "    epoch          : 156\n",
      "    loss           : -780797.8044554455\n",
      "    val_loss       : -776843.994140625\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -878707.375000\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -817370.812500\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -784419.375000\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -741449.500000\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -787762.625000\n",
      "    epoch          : 157\n",
      "    loss           : -782098.9022277228\n",
      "    val_loss       : -777945.2984375\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -881461.000000\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -734726.687500\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -803050.312500\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -785724.500000\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -791073.000000\n",
      "    epoch          : 158\n",
      "    loss           : -783201.3892326732\n",
      "    val_loss       : -781198.6189453125\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -884833.312500\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -759938.500000\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -744224.750000\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -812611.375000\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -792457.437500\n",
      "    epoch          : 159\n",
      "    loss           : -785533.6577970297\n",
      "    val_loss       : -780710.538671875\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -743655.125000\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -763637.250000\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -742025.312500\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -755608.250000\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -756416.562500\n",
      "    epoch          : 160\n",
      "    loss           : -787231.6367574257\n",
      "    val_loss       : -783089.58359375\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -885639.062500\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -808294.875000\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -743760.000000\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -740587.500000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -787049.125000\n",
      "    epoch          : 161\n",
      "    loss           : -786843.7060643565\n",
      "    val_loss       : -782753.0181640625\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -885685.000000\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -758916.250000\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -744249.812500\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -887691.625000\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -757152.250000\n",
      "    epoch          : 162\n",
      "    loss           : -787800.3521039604\n",
      "    val_loss       : -785795.2349609375\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -886127.500000\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -761664.437500\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -744968.125000\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -889830.125000\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -792133.250000\n",
      "    epoch          : 163\n",
      "    loss           : -790330.7066831683\n",
      "    val_loss       : -785478.6275390625\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -889785.687500\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -811678.375000\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -812352.375000\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -796388.562500\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -793135.937500\n",
      "    epoch          : 164\n",
      "    loss           : -788696.7673267326\n",
      "    val_loss       : -785663.9986328125\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -889442.125000\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -757617.812500\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -748816.750000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -815537.625000\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -795583.687500\n",
      "    epoch          : 165\n",
      "    loss           : -789320.9028465346\n",
      "    val_loss       : -787450.8673828125\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -892740.750000\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -780348.625000\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -793639.875000\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -747964.812500\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -792515.187500\n",
      "    epoch          : 166\n",
      "    loss           : -790469.3224009901\n",
      "    val_loss       : -789272.4076171875\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -893161.125000\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -825328.562500\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -791509.812500\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -750004.500000\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -800015.687500\n",
      "    epoch          : 167\n",
      "    loss           : -793911.3075495049\n",
      "    val_loss       : -790562.4693359375\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -894482.562500\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -797432.687500\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -792853.937500\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -797919.000000\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -757105.750000\n",
      "    epoch          : 168\n",
      "    loss           : -793185.6658415842\n",
      "    val_loss       : -789211.18046875\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -891938.812500\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -759705.625000\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -820388.062500\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -897812.125000\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -804516.250000\n",
      "    epoch          : 169\n",
      "    loss           : -794422.1862623763\n",
      "    val_loss       : -792988.0546875\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -897117.062500\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -769208.250000\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -755407.937500\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -801014.937500\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -767957.250000\n",
      "    epoch          : 170\n",
      "    loss           : -798538.0080445545\n",
      "    val_loss       : -794227.9041015625\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -899378.500000\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -774664.875000\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -754508.687500\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -822524.250000\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -801625.250000\n",
      "    epoch          : 171\n",
      "    loss           : -798964.2741336634\n",
      "    val_loss       : -792638.0279296875\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -801191.000000\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -770236.125000\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -808872.062500\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -770613.437500\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -761623.562500\n",
      "    epoch          : 172\n",
      "    loss           : -799351.6008663366\n",
      "    val_loss       : -796193.734765625\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -902497.062500\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -834254.500000\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -756571.250000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -800513.875000\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -805385.687500\n",
      "    epoch          : 173\n",
      "    loss           : -801114.3316831683\n",
      "    val_loss       : -796102.4765625\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -769452.375000\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -770203.312500\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -751950.437500\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -823615.375000\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -808650.937500\n",
      "    epoch          : 174\n",
      "    loss           : -800463.2097772277\n",
      "    val_loss       : -796085.2009765625\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -836578.812500\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -772144.250000\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -752139.875000\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -825824.750000\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -804953.312500\n",
      "    epoch          : 175\n",
      "    loss           : -801150.7549504951\n",
      "    val_loss       : -797997.6892578125\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -903245.375000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -824376.750000\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -748034.625000\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -748904.250000\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -768407.625000\n",
      "    epoch          : 176\n",
      "    loss           : -801154.0191831683\n",
      "    val_loss       : -799265.7236328125\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -902441.000000\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -805714.750000\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -807508.000000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -814181.312500\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -808454.875000\n",
      "    epoch          : 177\n",
      "    loss           : -805313.875\n",
      "    val_loss       : -800932.34765625\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -905764.250000\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -778617.187500\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -758964.062500\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -808491.500000\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -759395.125000\n",
      "    epoch          : 178\n",
      "    loss           : -803834.3774752475\n",
      "    val_loss       : -800696.840234375\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -779796.125000\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -772007.500000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -772678.750000\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -835164.750000\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -810501.875000\n",
      "    epoch          : 179\n",
      "    loss           : -807116.0891089109\n",
      "    val_loss       : -802812.509765625\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -843518.875000\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -777954.687500\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -775423.875000\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -910101.187500\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -769128.000000\n",
      "    epoch          : 180\n",
      "    loss           : -807237.728960396\n",
      "    val_loss       : -802018.4771484375\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -909353.125000\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -765270.750000\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -830982.937500\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -813225.625000\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -815114.125000\n",
      "    epoch          : 181\n",
      "    loss           : -808794.4090346535\n",
      "    val_loss       : -804940.42734375\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -910823.125000\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -782852.750000\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -812626.437500\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -834552.750000\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -812983.000000\n",
      "    epoch          : 182\n",
      "    loss           : -809608.1509900991\n",
      "    val_loss       : -804778.3638671875\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -912940.750000\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -844950.875000\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -760372.187500\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -911678.750000\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -812153.312500\n",
      "    epoch          : 183\n",
      "    loss           : -809080.5439356435\n",
      "    val_loss       : -804283.2876953125\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -845163.312500\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -785747.250000\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -760993.812500\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -762750.562500\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -774619.750000\n",
      "    epoch          : 184\n",
      "    loss           : -809801.1862623763\n",
      "    val_loss       : -805748.317578125\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -912091.750000\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -783026.500000\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -768777.125000\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -915913.000000\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -780266.937500\n",
      "    epoch          : 185\n",
      "    loss           : -812981.1633663366\n",
      "    val_loss       : -808058.1462890625\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -914488.500000\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -844910.062500\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -765784.937500\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -814438.937500\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -815902.375000\n",
      "    epoch          : 186\n",
      "    loss           : -812488.3149752475\n",
      "    val_loss       : -807668.0822265625\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -781681.250000\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -780823.125000\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -817421.000000\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -915970.687500\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -820801.125000\n",
      "    epoch          : 187\n",
      "    loss           : -813932.6448019802\n",
      "    val_loss       : -810440.959375\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -918468.250000\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -784329.000000\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -818940.750000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -819366.750000\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -773212.000000\n",
      "    epoch          : 188\n",
      "    loss           : -815874.9461633663\n",
      "    val_loss       : -809804.4818359375\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -918387.375000\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -848483.250000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -772058.562500\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -769680.375000\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -822401.062500\n",
      "    epoch          : 189\n",
      "    loss           : -816256.3632425743\n",
      "    val_loss       : -811097.4509765625\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -920634.875000\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -786977.750000\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -840037.125000\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -770824.375000\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -774190.875000\n",
      "    epoch          : 190\n",
      "    loss           : -815968.7623762377\n",
      "    val_loss       : -811611.850390625\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -921972.375000\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -778469.375000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -834149.250000\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -814904.375000\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -823635.625000\n",
      "    epoch          : 191\n",
      "    loss           : -813673.0761138614\n",
      "    val_loss       : -810763.9494140625\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -782492.687500\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -841656.875000\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -773453.875000\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -776686.937500\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -826817.625000\n",
      "    epoch          : 192\n",
      "    loss           : -818340.5457920792\n",
      "    val_loss       : -814094.4396484375\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -924460.062500\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -838026.125000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -774757.062500\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -775229.625000\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -777603.687500\n",
      "    epoch          : 193\n",
      "    loss           : -819862.9028465346\n",
      "    val_loss       : -815608.8416015625\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -825548.250000\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -852885.375000\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -787028.375000\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -923895.625000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -826875.000000\n",
      "    epoch          : 194\n",
      "    loss           : -821048.3972772277\n",
      "    val_loss       : -815301.5630859375\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -924695.375000\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -773407.437500\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -775884.250000\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -847336.937500\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -827896.500000\n",
      "    epoch          : 195\n",
      "    loss           : -820956.2660891089\n",
      "    val_loss       : -816626.5734375\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -791947.875000\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -790556.375000\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -848121.937500\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -927140.437500\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -828259.000000\n",
      "    epoch          : 196\n",
      "    loss           : -821761.0779702971\n",
      "    val_loss       : -816501.567578125\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -925857.000000\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -789506.750000\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -827435.875000\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -780368.375000\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -842754.062500\n",
      "    epoch          : 197\n",
      "    loss           : -823089.3737623763\n",
      "    val_loss       : -815724.8408203125\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -926904.625000\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -789902.500000\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -782754.500000\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -774884.625000\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -777512.937500\n",
      "    epoch          : 198\n",
      "    loss           : -821546.2698019802\n",
      "    val_loss       : -818625.0943359375\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -927586.562500\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -775293.625000\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -820103.937500\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -775305.062500\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -787056.125000\n",
      "    epoch          : 199\n",
      "    loss           : -823354.5922029703\n",
      "    val_loss       : -817280.5314453125\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -930490.375000\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -791589.625000\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -852396.750000\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -829678.437500\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -769621.562500\n",
      "    epoch          : 200\n",
      "    loss           : -822980.4430693069\n",
      "    val_loss       : -819020.2357421875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0803_215641/checkpoint-epoch200.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -929308.187500\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -778293.625000\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -851851.750000\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -792499.437500\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -790592.125000\n",
      "    epoch          : 201\n",
      "    loss           : -825523.1868811881\n",
      "    val_loss       : -821358.2615234375\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -837067.812500\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -788284.687500\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -781096.625000\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -934599.562500\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -835708.812500\n",
      "    epoch          : 202\n",
      "    loss           : -828394.5464108911\n",
      "    val_loss       : -821106.7498046875\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -931114.375000\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -791266.625000\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -828877.687500\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -934039.625000\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -784992.500000\n",
      "    epoch          : 203\n",
      "    loss           : -827276.4108910891\n",
      "    val_loss       : -822297.7298828125\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -798133.000000\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -799518.625000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -831703.375000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -791004.812500\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -796446.312500\n",
      "    epoch          : 204\n",
      "    loss           : -829162.3787128713\n",
      "    val_loss       : -824522.641015625\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -936068.125000\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -857231.375000\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -859646.750000\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -781483.500000\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -793253.875000\n",
      "    epoch          : 205\n",
      "    loss           : -830215.0129950495\n",
      "    val_loss       : -824526.7998046875\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -935579.375000\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -800697.500000\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -855924.812500\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -787750.750000\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -837133.437500\n",
      "    epoch          : 206\n",
      "    loss           : -830726.9962871287\n",
      "    val_loss       : -825589.903515625\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -937888.375000\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -863493.875000\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -833941.375000\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -856468.687500\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -835722.625000\n",
      "    epoch          : 207\n",
      "    loss           : -831791.8075495049\n",
      "    val_loss       : -826022.7974609375\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -868672.625000\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -831663.250000\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -856341.687500\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -834642.375000\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -790327.250000\n",
      "    epoch          : 208\n",
      "    loss           : -831541.8254950495\n",
      "    val_loss       : -827230.0435546875\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -868950.562500\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -802349.625000\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -796119.687500\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -836309.562500\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -839607.375000\n",
      "    epoch          : 209\n",
      "    loss           : -833718.6417079208\n",
      "    val_loss       : -828382.8931640625\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -941062.562500\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -802063.375000\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -860467.812500\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -786916.250000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -837388.562500\n",
      "    epoch          : 210\n",
      "    loss           : -834163.9758663366\n",
      "    val_loss       : -826141.7935546875\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -940651.500000\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -797139.375000\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -779074.750000\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -790612.187500\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -789816.750000\n",
      "    epoch          : 211\n",
      "    loss           : -832433.9405940594\n",
      "    val_loss       : -829677.9162109375\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -942674.187500\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -794555.750000\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -780395.125000\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -796376.125000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -795420.625000\n",
      "    epoch          : 212\n",
      "    loss           : -835659.5860148515\n",
      "    val_loss       : -829465.9537109375\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -942107.437500\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -806133.937500\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -789847.562500\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -857405.687500\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -843642.500000\n",
      "    epoch          : 213\n",
      "    loss           : -836668.3972772277\n",
      "    val_loss       : -829784.7560546875\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -944202.750000\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -867693.187500\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -844669.312500\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -791608.187500\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -840223.687500\n",
      "    epoch          : 214\n",
      "    loss           : -837125.8094059406\n",
      "    val_loss       : -831104.2037109375\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -944865.875000\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -860298.312500\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -791088.125000\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -791237.062500\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -842014.125000\n",
      "    epoch          : 215\n",
      "    loss           : -837834.9387376237\n",
      "    val_loss       : -830972.58203125\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -942886.750000\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -780885.250000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -825071.250000\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -946459.625000\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -839452.812500\n",
      "    epoch          : 216\n",
      "    loss           : -834636.8607673268\n",
      "    val_loss       : -829947.6892578125\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -942095.312500\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -802034.875000\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -806563.500000\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -947927.437500\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -800985.375000\n",
      "    epoch          : 217\n",
      "    loss           : -838709.7660891089\n",
      "    val_loss       : -833679.09375\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -948370.437500\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -806232.062500\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -802317.500000\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -948083.250000\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -844277.062500\n",
      "    epoch          : 218\n",
      "    loss           : -840715.4356435643\n",
      "    val_loss       : -835241.1138671875\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -948595.375000\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -877205.500000\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -867366.562500\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -804249.500000\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -804766.187500\n",
      "    epoch          : 219\n",
      "    loss           : -841834.1602722772\n",
      "    val_loss       : -835921.5177734375\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -950618.062500\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -849393.187500\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -849369.125000\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -858605.625000\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -859512.625000\n",
      "    epoch          : 220\n",
      "    loss           : -839985.4641089109\n",
      "    val_loss       : -833643.1896484375\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -945148.250000\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -793224.062500\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -867114.750000\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -796477.125000\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -805814.187500\n",
      "    epoch          : 221\n",
      "    loss           : -840983.1497524752\n",
      "    val_loss       : -836412.044921875\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -951991.937500\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -812833.187500\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -812734.687500\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -847513.312500\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -848342.687500\n",
      "    epoch          : 222\n",
      "    loss           : -843475.8508663366\n",
      "    val_loss       : -837877.1837890625\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -949522.187500\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -867017.062500\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -795559.000000\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -802631.125000\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -847718.125000\n",
      "    epoch          : 223\n",
      "    loss           : -843036.4913366337\n",
      "    val_loss       : -838363.6\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -951777.312500\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -811202.625000\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -844780.687500\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -867087.687500\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -845643.750000\n",
      "    epoch          : 224\n",
      "    loss           : -842068.2004950495\n",
      "    val_loss       : -835290.194921875\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -949123.125000\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -813518.875000\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -793070.687500\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -805513.500000\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -810364.937500\n",
      "    epoch          : 225\n",
      "    loss           : -844094.4300742574\n",
      "    val_loss       : -838794.6548828125\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -952470.000000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -803496.687500\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -799481.625000\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -809844.062500\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -806655.000000\n",
      "    epoch          : 226\n",
      "    loss           : -847709.9554455446\n",
      "    val_loss       : -840443.503515625\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -880569.875000\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -796471.687500\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -818164.812500\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -950348.000000\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -845854.875000\n",
      "    epoch          : 227\n",
      "    loss           : -845681.8310643565\n",
      "    val_loss       : -840099.640625\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -954608.000000\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -814593.125000\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -795786.562500\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -868109.750000\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -851760.500000\n",
      "    epoch          : 228\n",
      "    loss           : -846705.8991336634\n",
      "    val_loss       : -840838.8162109375\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -958423.875000\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -817041.937500\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -797963.625000\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -848769.750000\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -803551.375000\n",
      "    epoch          : 229\n",
      "    loss           : -846281.1553217822\n",
      "    val_loss       : -841158.9212890625\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -955789.812500\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -794729.125000\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -806910.937500\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -804908.500000\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -849402.687500\n",
      "    epoch          : 230\n",
      "    loss           : -848142.1831683168\n",
      "    val_loss       : -842526.6826171875\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -957490.750000\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -853669.062500\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -857576.875000\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -876082.812500\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -811645.500000\n",
      "    epoch          : 231\n",
      "    loss           : -851053.1417079208\n",
      "    val_loss       : -844102.596484375\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -819012.750000\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -820716.250000\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -814778.000000\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -853277.437500\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -811539.250000\n",
      "    epoch          : 232\n",
      "    loss           : -850844.3743811881\n",
      "    val_loss       : -843193.989453125\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -959859.062500\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -881421.250000\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -809031.562500\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -958854.312500\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -852253.500000\n",
      "    epoch          : 233\n",
      "    loss           : -849771.5086633663\n",
      "    val_loss       : -842935.6537109375\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -884493.875000\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -812395.625000\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -811220.375000\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -870230.750000\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -811413.437500\n",
      "    epoch          : 234\n",
      "    loss           : -849696.1157178218\n",
      "    val_loss       : -843787.625390625\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -961329.250000\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -875235.000000\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -809584.750000\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -795853.312500\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -855658.375000\n",
      "    epoch          : 235\n",
      "    loss           : -848441.1856435643\n",
      "    val_loss       : -845077.9708984375\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -960831.625000\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -822243.312500\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -813309.250000\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -806573.250000\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -857800.562500\n",
      "    epoch          : 236\n",
      "    loss           : -853524.8446782178\n",
      "    val_loss       : -845183.219140625\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -824904.500000\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -887130.437500\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -878998.500000\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -814312.812500\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -805883.250000\n",
      "    epoch          : 237\n",
      "    loss           : -852048.9084158416\n",
      "    val_loss       : -844428.675\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -958873.875000\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -823198.750000\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -808873.375000\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -805093.250000\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -813554.500000\n",
      "    epoch          : 238\n",
      "    loss           : -851385.1002475248\n",
      "    val_loss       : -847226.53984375\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -965358.250000\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -889506.500000\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -879560.687500\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -964811.875000\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -859880.750000\n",
      "    epoch          : 239\n",
      "    loss           : -855440.9263613861\n",
      "    val_loss       : -847626.25546875\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -891259.062500\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -808182.812500\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -862867.687500\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -804858.375000\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -879365.250000\n",
      "    epoch          : 240\n",
      "    loss           : -854392.4430693069\n",
      "    val_loss       : -844305.420703125\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -961429.125000\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -817808.937500\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -875386.750000\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -810496.250000\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -812323.875000\n",
      "    epoch          : 241\n",
      "    loss           : -853030.8483910891\n",
      "    val_loss       : -848640.584375\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -890166.250000\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -823086.312500\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -815698.125000\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -814472.000000\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -865684.750000\n",
      "    epoch          : 242\n",
      "    loss           : -856768.0513613861\n",
      "    val_loss       : -848219.5013671875\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -966844.187500\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -804407.125000\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -878137.437500\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -965360.875000\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -854364.125000\n",
      "    epoch          : 243\n",
      "    loss           : -855042.2196782178\n",
      "    val_loss       : -848808.9185546875\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -965976.687500\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -822789.250000\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -807700.000000\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -878899.625000\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -817086.375000\n",
      "    epoch          : 244\n",
      "    loss           : -855991.6299504951\n",
      "    val_loss       : -849593.690625\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -966305.687500\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -825125.812500\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -801164.187500\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -816557.937500\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -860806.437500\n",
      "    epoch          : 245\n",
      "    loss           : -856075.406559406\n",
      "    val_loss       : -850771.82890625\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -969893.500000\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -891350.000000\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -811873.500000\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -866616.250000\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -812844.687500\n",
      "    epoch          : 246\n",
      "    loss           : -859056.1417079208\n",
      "    val_loss       : -850620.494921875\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -968739.437500\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -828906.812500\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -816074.875000\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -812397.687500\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -860215.250000\n",
      "    epoch          : 247\n",
      "    loss           : -856624.9084158416\n",
      "    val_loss       : -850235.0611328125\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -969537.750000\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -829781.125000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -811492.625000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -969872.125000\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -820451.250000\n",
      "    epoch          : 248\n",
      "    loss           : -859021.426980198\n",
      "    val_loss       : -852564.0802734375\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -968799.250000\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -859558.250000\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -809571.625000\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -861172.875000\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -815641.125000\n",
      "    epoch          : 249\n",
      "    loss           : -859515.9412128713\n",
      "    val_loss       : -851410.797265625\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -972156.500000\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -890510.312500\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -864418.500000\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -874834.000000\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -860322.687500\n",
      "    epoch          : 250\n",
      "    loss           : -857519.1367574257\n",
      "    val_loss       : -849626.744140625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0803_215641/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -891424.375000\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -805724.625000\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -855620.625000\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -972367.625000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -822509.125000\n",
      "    epoch          : 251\n",
      "    loss           : -856701.2728960396\n",
      "    val_loss       : -852879.881640625\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -973581.375000\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -828742.500000\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -809236.187500\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -864913.812500\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -858816.500000\n",
      "    epoch          : 252\n",
      "    loss           : -860333.2066831683\n",
      "    val_loss       : -851174.9181640625\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -971100.687500\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -892757.125000\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -821756.750000\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -869902.187500\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -826958.000000\n",
      "    epoch          : 253\n",
      "    loss           : -861008.6082920792\n",
      "    val_loss       : -854794.3705078125\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -894766.312500\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -865770.437500\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -867240.125000\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -973965.500000\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -823020.125000\n",
      "    epoch          : 254\n",
      "    loss           : -863421.8007425743\n",
      "    val_loss       : -854951.2421875\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -975180.437500\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -808360.750000\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -807826.562500\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -858845.250000\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -819108.000000\n",
      "    epoch          : 255\n",
      "    loss           : -861492.8818069306\n",
      "    val_loss       : -853456.789453125\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -973598.687500\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -896696.250000\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -863858.125000\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -826133.500000\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -869580.750000\n",
      "    epoch          : 256\n",
      "    loss           : -862286.5099009901\n",
      "    val_loss       : -854189.1515625\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -821723.187500\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -826901.437500\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -821097.625000\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -863448.875000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -820145.250000\n",
      "    epoch          : 257\n",
      "    loss           : -862072.0816831683\n",
      "    val_loss       : -854699.87421875\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -978698.000000\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -896571.625000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -863965.625000\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -973561.812500\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -866864.687500\n",
      "    epoch          : 258\n",
      "    loss           : -861759.0699257426\n",
      "    val_loss       : -852532.707421875\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -970151.625000\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -826783.937500\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -817661.937500\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -869545.812500\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -869538.687500\n",
      "    epoch          : 259\n",
      "    loss           : -862635.4851485149\n",
      "    val_loss       : -855821.774609375\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -975111.812500\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -832105.812500\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -886113.000000\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -889271.937500\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -870057.875000\n",
      "    epoch          : 260\n",
      "    loss           : -864676.0080445545\n",
      "    val_loss       : -856041.5244140625\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -974117.812500\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -800511.687500\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -818274.750000\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -974069.937500\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -863192.437500\n",
      "    epoch          : 261\n",
      "    loss           : -861629.6714108911\n",
      "    val_loss       : -855171.702734375\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -977043.187500\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -830867.562500\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -813963.187500\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -867797.625000\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -867690.125000\n",
      "    epoch          : 262\n",
      "    loss           : -864768.073019802\n",
      "    val_loss       : -856947.9119140625\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -977993.375000\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -814201.562500\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -887885.437500\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -820780.750000\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -871838.375000\n",
      "    epoch          : 263\n",
      "    loss           : -866825.031559406\n",
      "    val_loss       : -857661.58515625\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -977614.312500\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -832443.375000\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -816121.125000\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -870959.687500\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -871675.000000\n",
      "    epoch          : 264\n",
      "    loss           : -865673.2221534654\n",
      "    val_loss       : -856676.515234375\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -978548.500000\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -825702.187500\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -870483.812500\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -886854.500000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -868009.875000\n",
      "    epoch          : 265\n",
      "    loss           : -864113.8254950495\n",
      "    val_loss       : -854803.3849609375\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -977830.187500\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -835985.000000\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -816551.625000\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -885578.750000\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -872937.750000\n",
      "    epoch          : 266\n",
      "    loss           : -866128.5878712871\n",
      "    val_loss       : -858180.5423828125\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -979163.687500\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -897415.375000\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -885969.125000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -865345.125000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -872409.125000\n",
      "    epoch          : 267\n",
      "    loss           : -864887.1219059406\n",
      "    val_loss       : -857237.9591796875\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -977991.875000\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -901340.625000\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -829983.500000\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -872985.125000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -826733.750000\n",
      "    epoch          : 268\n",
      "    loss           : -867612.0804455446\n",
      "    val_loss       : -858424.114453125\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -980846.000000\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -902153.125000\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -875187.000000\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -826728.625000\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -873473.250000\n",
      "    epoch          : 269\n",
      "    loss           : -866318.6757425743\n",
      "    val_loss       : -857684.512890625\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -830717.375000\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -900155.500000\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -867787.750000\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -978782.875000\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -866204.375000\n",
      "    epoch          : 270\n",
      "    loss           : -864257.8075495049\n",
      "    val_loss       : -857912.2955078125\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -981415.937500\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -830931.000000\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -828788.750000\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -983475.375000\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -824459.312500\n",
      "    epoch          : 271\n",
      "    loss           : -868473.3551980198\n",
      "    val_loss       : -859403.92890625\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -981285.187500\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -898113.187500\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -826181.312500\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -830463.750000\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -826758.312500\n",
      "    epoch          : 272\n",
      "    loss           : -868878.2227722772\n",
      "    val_loss       : -860087.0853515625\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -901259.812500\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -833428.500000\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -829050.125000\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -983827.687500\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -874497.437500\n",
      "    epoch          : 273\n",
      "    loss           : -868961.2660891089\n",
      "    val_loss       : -858860.57421875\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -985961.187500\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -898925.125000\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -870393.375000\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -813991.687500\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -833135.625000\n",
      "    epoch          : 274\n",
      "    loss           : -867560.5142326732\n",
      "    val_loss       : -859228.44375\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -984662.375000\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -830259.500000\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -887751.562500\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -867446.062500\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -877615.625000\n",
      "    epoch          : 275\n",
      "    loss           : -866182.3168316832\n",
      "    val_loss       : -858396.0166015625\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -980776.250000\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -834402.250000\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -892918.187500\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -872381.750000\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -822992.125000\n",
      "    epoch          : 276\n",
      "    loss           : -869084.5228960396\n",
      "    val_loss       : -860224.7392578125\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -903722.937500\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -833687.562500\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -817475.500000\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -984708.375000\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -876291.125000\n",
      "    epoch          : 277\n",
      "    loss           : -867760.4084158416\n",
      "    val_loss       : -860056.7560546875\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -982518.875000\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -885552.937500\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -873690.875000\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -827882.000000\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -874487.125000\n",
      "    epoch          : 278\n",
      "    loss           : -869224.9096534654\n",
      "    val_loss       : -861119.619921875\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -983909.875000\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -896295.937500\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -832118.125000\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -888355.250000\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -832368.187500\n",
      "    epoch          : 279\n",
      "    loss           : -869534.4975247525\n",
      "    val_loss       : -860059.87890625\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -983105.500000\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -835032.750000\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -890770.812500\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -831109.250000\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -830277.437500\n",
      "    epoch          : 280\n",
      "    loss           : -869925.760519802\n",
      "    val_loss       : -861500.0166015625\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -984854.562500\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -817270.437500\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -892572.312500\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -986088.000000\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -830668.375000\n",
      "    epoch          : 281\n",
      "    loss           : -870663.3323019802\n",
      "    val_loss       : -861609.01875\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -987493.750000\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -824903.312500\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -817168.187500\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -816621.500000\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -829187.062500\n",
      "    epoch          : 282\n",
      "    loss           : -868124.7110148515\n",
      "    val_loss       : -861331.175\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -838413.375000\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -908071.500000\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -894505.250000\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -834260.375000\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -877253.750000\n",
      "    epoch          : 283\n",
      "    loss           : -872136.7580445545\n",
      "    val_loss       : -862369.01640625\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -907720.187500\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -838030.500000\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -835439.562500\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -835114.000000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -829957.062500\n",
      "    epoch          : 284\n",
      "    loss           : -872450.6516089109\n",
      "    val_loss       : -861653.8078125\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -988741.875000\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -836205.187500\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -829928.312500\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -877694.312500\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -825286.687500\n",
      "    epoch          : 285\n",
      "    loss           : -871532.3459158416\n",
      "    val_loss       : -862309.534765625\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -838227.875000\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -836585.750000\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -820426.500000\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -820283.750000\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -878848.937500\n",
      "    epoch          : 286\n",
      "    loss           : -871866.7370049505\n",
      "    val_loss       : -862916.365234375\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -988976.312500\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -836975.062500\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -810528.937500\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -890602.750000\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -874532.562500\n",
      "    epoch          : 287\n",
      "    loss           : -870885.9009900991\n",
      "    val_loss       : -861811.02265625\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -986038.625000\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -837537.625000\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -874892.937500\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -833579.437500\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -879430.625000\n",
      "    epoch          : 288\n",
      "    loss           : -871566.0736386139\n",
      "    val_loss       : -862579.4623046875\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -985740.250000\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -839858.937500\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -824914.250000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -876110.375000\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -879090.250000\n",
      "    epoch          : 289\n",
      "    loss           : -873338.395420792\n",
      "    val_loss       : -862699.8234375\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -988577.875000\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -838910.375000\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -893128.812500\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -882462.562500\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -870923.875000\n",
      "    epoch          : 290\n",
      "    loss           : -870791.5402227723\n",
      "    val_loss       : -860330.540234375\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -986546.062500\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -835746.687500\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -829021.625000\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -876644.750000\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -888578.750000\n",
      "    epoch          : 291\n",
      "    loss           : -870739.4801980198\n",
      "    val_loss       : -863131.8080078125\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -842826.375000\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -832011.750000\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -832605.687500\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -991157.687500\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -881240.625000\n",
      "    epoch          : 292\n",
      "    loss           : -873786.104579208\n",
      "    val_loss       : -864445.803125\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -988579.187500\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -892549.750000\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -836655.500000\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -893765.625000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -824934.500000\n",
      "    epoch          : 293\n",
      "    loss           : -873706.9981435643\n",
      "    val_loss       : -861800.8009765625\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -985957.937500\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -814940.125000\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -821403.375000\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -874051.375000\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -879332.625000\n",
      "    epoch          : 294\n",
      "    loss           : -871910.6763613861\n",
      "    val_loss       : -862674.859375\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -986183.812500\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -902771.125000\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -823590.750000\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -830650.375000\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -877765.125000\n",
      "    epoch          : 295\n",
      "    loss           : -873519.6534653465\n",
      "    val_loss       : -864117.15703125\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -991438.000000\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -820003.562500\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -835109.250000\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -990452.937500\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -876379.625000\n",
      "    epoch          : 296\n",
      "    loss           : -871854.406559406\n",
      "    val_loss       : -860565.0041015625\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -990332.250000\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -839260.812500\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -837776.812500\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -836962.687500\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -880981.125000\n",
      "    epoch          : 297\n",
      "    loss           : -873504.6398514851\n",
      "    val_loss       : -865720.2771484375\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -993421.312500\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -838363.625000\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -831260.750000\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -831989.375000\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -833124.375000\n",
      "    epoch          : 298\n",
      "    loss           : -874248.9771039604\n",
      "    val_loss       : -865523.0396484375\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -992608.875000\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -910693.125000\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -883721.562500\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -838333.500000\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -836092.187500\n",
      "    epoch          : 299\n",
      "    loss           : -876251.198019802\n",
      "    val_loss       : -864823.407421875\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -993283.937500\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -910228.062500\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -877843.750000\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -831672.625000\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -886467.187500\n",
      "    epoch          : 300\n",
      "    loss           : -872308.9393564357\n",
      "    val_loss       : -858810.15703125\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0803_215641/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -985443.187500\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -833920.625000\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -892179.875000\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -993569.687500\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -876506.375000\n",
      "    epoch          : 301\n",
      "    loss           : -873285.3285891089\n",
      "    val_loss       : -864800.5880859375\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -991995.750000\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -838348.500000\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -823420.000000\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -882221.750000\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -828779.062500\n",
      "    epoch          : 302\n",
      "    loss           : -875349.4313118812\n",
      "    val_loss       : -864792.05625\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -994784.125000\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -829387.000000\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -879505.375000\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -821482.187500\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -892420.750000\n",
      "    epoch          : 303\n",
      "    loss           : -874142.5525990099\n",
      "    val_loss       : -863297.8533203125\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -993567.312500\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -814100.437500\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -826430.187500\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -830646.937500\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -829612.000000\n",
      "    epoch          : 304\n",
      "    loss           : -873727.8669554455\n",
      "    val_loss       : -862365.6330078125\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -991851.687500\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -899670.750000\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -876721.875000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -994150.250000\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -876773.750000\n",
      "    epoch          : 305\n",
      "    loss           : -873225.323019802\n",
      "    val_loss       : -863883.4921875\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -995683.812500\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -889327.812500\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -842523.250000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -997331.625000\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -878994.625000\n",
      "    epoch          : 306\n",
      "    loss           : -874369.4133663366\n",
      "    val_loss       : -865973.80703125\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -992887.437500\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -902331.562500\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -884208.312500\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -822490.812500\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -829458.562500\n",
      "    epoch          : 307\n",
      "    loss           : -876496.5915841584\n",
      "    val_loss       : -865980.9501953125\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -838426.750000\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -844612.187500\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -827514.625000\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -835832.062500\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -876975.187500\n",
      "    epoch          : 308\n",
      "    loss           : -876805.551980198\n",
      "    val_loss       : -865383.6765625\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -993354.937500\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -912547.812500\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -879324.687500\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -994720.625000\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -883135.500000\n",
      "    epoch          : 309\n",
      "    loss           : -876793.8267326732\n",
      "    val_loss       : -863687.8330078125\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -910954.875000\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -837574.750000\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -882250.687500\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -839037.625000\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -830115.750000\n",
      "    epoch          : 310\n",
      "    loss           : -875100.0222772277\n",
      "    val_loss       : -865865.984375\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -997716.937500\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -841668.875000\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -821515.625000\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -994993.750000\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -883533.500000\n",
      "    epoch          : 311\n",
      "    loss           : -875694.1856435643\n",
      "    val_loss       : -865402.1779296875\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -994033.750000\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -837503.187500\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -894696.125000\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -994755.750000\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -835698.812500\n",
      "    epoch          : 312\n",
      "    loss           : -876157.2877475248\n",
      "    val_loss       : -866153.544921875\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -995764.875000\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -911243.312500\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -837284.000000\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -900322.750000\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -885163.500000\n",
      "    epoch          : 313\n",
      "    loss           : -878130.1813118812\n",
      "    val_loss       : -865537.3525390625\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -995736.500000\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -883120.937500\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -882805.375000\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -883116.562500\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -883051.125000\n",
      "    epoch          : 314\n",
      "    loss           : -876961.3774752475\n",
      "    val_loss       : -866057.85\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -997120.000000\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -825867.187500\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -879491.375000\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -995882.875000\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -879539.000000\n",
      "    epoch          : 315\n",
      "    loss           : -875842.8304455446\n",
      "    val_loss       : -865136.74140625\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -993428.875000\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -839710.562500\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -895497.625000\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -834607.375000\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -885378.875000\n",
      "    epoch          : 316\n",
      "    loss           : -876598.3360148515\n",
      "    val_loss       : -866392.7833984375\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -999079.125000\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -822671.875000\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -823023.375000\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -883237.125000\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -878718.562500\n",
      "    epoch          : 317\n",
      "    loss           : -877490.5705445545\n",
      "    val_loss       : -866172.34765625\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -996529.875000\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -844263.625000\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -831966.250000\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -996542.000000\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -881800.125000\n",
      "    epoch          : 318\n",
      "    loss           : -877559.729579208\n",
      "    val_loss       : -866496.275390625\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -999442.437500\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -913670.375000\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -883859.875000\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -836674.312500\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -878220.375000\n",
      "    epoch          : 319\n",
      "    loss           : -875460.4993811881\n",
      "    val_loss       : -865151.2701171875\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -996242.250000\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -841731.687500\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -843814.500000\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -842666.500000\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -836711.625000\n",
      "    epoch          : 320\n",
      "    loss           : -877831.8149752475\n",
      "    val_loss       : -867554.4705078125\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -996618.062500\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -843510.375000\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -829334.312500\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -893411.250000\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -835074.500000\n",
      "    epoch          : 321\n",
      "    loss           : -879940.3688118812\n",
      "    val_loss       : -867014.5525390625\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -996069.250000\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -914050.500000\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -896552.250000\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -833041.000000\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -898132.750000\n",
      "    epoch          : 322\n",
      "    loss           : -879902.4733910891\n",
      "    val_loss       : -865755.6966796875\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -998718.125000\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -843074.687500\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -838218.125000\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -998758.875000\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -836923.437500\n",
      "    epoch          : 323\n",
      "    loss           : -879182.7073019802\n",
      "    val_loss       : -865838.3712890625\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -997853.625000\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -838021.687500\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -835120.562500\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -825363.500000\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -877535.125000\n",
      "    epoch          : 324\n",
      "    loss           : -877395.6311881188\n",
      "    val_loss       : -866247.055078125\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -995993.562500\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -843761.875000\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -840264.875000\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -830805.062500\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -889713.750000\n",
      "    epoch          : 325\n",
      "    loss           : -878726.4443069306\n",
      "    val_loss       : -868178.27421875\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -914591.250000\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -845420.187500\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -828516.250000\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -884330.000000\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -886334.312500\n",
      "    epoch          : 326\n",
      "    loss           : -879102.031559406\n",
      "    val_loss       : -866021.2248046875\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -998804.875000\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -839443.562500\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -832499.437500\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -843037.687500\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -832479.875000\n",
      "    epoch          : 327\n",
      "    loss           : -878889.6887376237\n",
      "    val_loss       : -867236.5541015625\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -1001600.062500\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -847558.250000\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -895662.312500\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -883165.562500\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -885471.500000\n",
      "    epoch          : 328\n",
      "    loss           : -879145.8632425743\n",
      "    val_loss       : -867939.8888671875\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -1001130.375000\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -842264.000000\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -830275.250000\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -846103.812500\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -881768.125000\n",
      "    epoch          : 329\n",
      "    loss           : -880495.9659653465\n",
      "    val_loss       : -867034.4953125\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -1000068.937500\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -842238.312500\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -830650.062500\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -887288.875000\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -882433.437500\n",
      "    epoch          : 330\n",
      "    loss           : -880544.0117574257\n",
      "    val_loss       : -868328.4224609375\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -1000009.500000\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -842536.125000\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -886151.250000\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -825785.187500\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -881196.375000\n",
      "    epoch          : 331\n",
      "    loss           : -878857.510519802\n",
      "    val_loss       : -865648.9677734375\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -838599.875000\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -847650.187500\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -892539.812500\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -875485.125000\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -831428.125000\n",
      "    epoch          : 332\n",
      "    loss           : -877216.8496287129\n",
      "    val_loss       : -866748.9455078125\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -997177.625000\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -881929.187500\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -895085.312500\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -885943.625000\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -882960.312500\n",
      "    epoch          : 333\n",
      "    loss           : -878458.8737623763\n",
      "    val_loss       : -867764.046875\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -1000511.125000\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -847280.125000\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -836501.250000\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -848159.187500\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -834563.875000\n",
      "    epoch          : 334\n",
      "    loss           : -881322.5284653465\n",
      "    val_loss       : -868766.580078125\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -1001923.625000\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -910347.062500\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -836261.937500\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -886414.000000\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -887752.375000\n",
      "    epoch          : 335\n",
      "    loss           : -881099.396039604\n",
      "    val_loss       : -868092.0287109375\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -1000919.625000\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -848145.500000\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -889620.250000\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -888048.312500\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -833664.000000\n",
      "    epoch          : 336\n",
      "    loss           : -879106.073019802\n",
      "    val_loss       : -867330.5734375\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -830568.500000\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -919737.250000\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -847730.750000\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -896765.875000\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -887531.187500\n",
      "    epoch          : 337\n",
      "    loss           : -880695.0965346535\n",
      "    val_loss       : -867573.8470703125\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -1002911.250000\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -827937.125000\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -837702.562500\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -846636.562500\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -831806.125000\n",
      "    epoch          : 338\n",
      "    loss           : -878695.0167079208\n",
      "    val_loss       : -868543.0349609375\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -1000833.437500\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -845795.125000\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -899563.687500\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -1003633.312500\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -835652.562500\n",
      "    epoch          : 339\n",
      "    loss           : -881994.3836633663\n",
      "    val_loss       : -867445.1986328125\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -849444.187500\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -849279.375000\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -900089.000000\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -832651.250000\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -829897.000000\n",
      "    epoch          : 340\n",
      "    loss           : -882622.2555693069\n",
      "    val_loss       : -868336.251953125\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -1002112.000000\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -916443.937500\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -826535.375000\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -1003721.750000\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -882923.125000\n",
      "    epoch          : 341\n",
      "    loss           : -881984.0358910891\n",
      "    val_loss       : -868111.1908203125\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -1002271.375000\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -839888.812500\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -899559.500000\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -1002355.562500\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -882434.625000\n",
      "    epoch          : 342\n",
      "    loss           : -880446.8780940594\n",
      "    val_loss       : -865971.1701171875\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -914990.250000\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -915482.687500\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -845439.625000\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -887036.437500\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -884820.750000\n",
      "    epoch          : 343\n",
      "    loss           : -880164.2357673268\n",
      "    val_loss       : -868100.7140625\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -844863.875000\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -841650.875000\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -883281.250000\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -835065.125000\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -831689.125000\n",
      "    epoch          : 344\n",
      "    loss           : -880532.3298267326\n",
      "    val_loss       : -868850.884765625\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -1005736.125000\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -850140.875000\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -883470.937500\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -1002731.250000\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -830914.500000\n",
      "    epoch          : 345\n",
      "    loss           : -882348.5798267326\n",
      "    val_loss       : -868400.607421875\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -1002781.187500\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -851967.187500\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -883988.000000\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -897920.625000\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -881067.000000\n",
      "    epoch          : 346\n",
      "    loss           : -880380.6844059406\n",
      "    val_loss       : -867581.699609375\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -830311.625000\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -842916.500000\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -826152.312500\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -895693.250000\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -817767.750000\n",
      "    epoch          : 347\n",
      "    loss           : -879351.2574257426\n",
      "    val_loss       : -866467.148046875\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -1000788.312500\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -840927.750000\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -821948.250000\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -883277.375000\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -833951.562500\n",
      "    epoch          : 348\n",
      "    loss           : -880233.8743811881\n",
      "    val_loss       : -869569.71171875\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -1003077.750000\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -852379.312500\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -831421.000000\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -889850.187500\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -883138.125000\n",
      "    epoch          : 349\n",
      "    loss           : -882403.5556930694\n",
      "    val_loss       : -868791.084375\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -1005038.250000\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -915992.562500\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -894834.125000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -888633.437500\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -831798.250000\n",
      "    epoch          : 350\n",
      "    loss           : -881499.5878712871\n",
      "    val_loss       : -869605.421875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0803_215641/checkpoint-epoch350.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -1004638.312500\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -844484.062500\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -839173.875000\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -849886.750000\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -888632.375000\n",
      "    epoch          : 351\n",
      "    loss           : -883082.1503712871\n",
      "    val_loss       : -869444.7794921875\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -1003574.875000\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -848188.312500\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -895939.062500\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -849392.937500\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -831664.250000\n",
      "    epoch          : 352\n",
      "    loss           : -882939.2258663366\n",
      "    val_loss       : -867737.623046875\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -1000743.500000\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -907236.500000\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -848189.750000\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -1006067.125000\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -887992.125000\n",
      "    epoch          : 353\n",
      "    loss           : -881921.0482673268\n",
      "    val_loss       : -867767.033203125\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -916318.625000\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -890261.000000\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -887439.375000\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -891217.250000\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -879684.750000\n",
      "    epoch          : 354\n",
      "    loss           : -877875.3774752475\n",
      "    val_loss       : -865253.3537109375\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -1001892.062500\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -911856.687500\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -843669.562500\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -893983.375000\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -887368.000000\n",
      "    epoch          : 355\n",
      "    loss           : -881445.6627475248\n",
      "    val_loss       : -870195.975390625\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -1003605.500000\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -896000.312500\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -850040.562500\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -891535.125000\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -887503.687500\n",
      "    epoch          : 356\n",
      "    loss           : -882830.5903465346\n",
      "    val_loss       : -869400.9767578125\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -1003482.875000\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -849602.000000\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -886842.687500\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -904156.562500\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -888429.000000\n",
      "    epoch          : 357\n",
      "    loss           : -883685.9740099009\n",
      "    val_loss       : -869529.4890625\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -1002923.125000\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -914544.187500\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -833565.750000\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -1007526.750000\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -888459.750000\n",
      "    epoch          : 358\n",
      "    loss           : -882795.3799504951\n",
      "    val_loss       : -869307.1501953125\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -1005447.062500\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -846786.750000\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -886058.937500\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -846630.312500\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -829164.000000\n",
      "    epoch          : 359\n",
      "    loss           : -881194.2933168317\n",
      "    val_loss       : -868673.1298828125\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -1002547.125000\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -914341.875000\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -836170.500000\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -1004985.750000\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -886673.500000\n",
      "    epoch          : 360\n",
      "    loss           : -882035.4702970297\n",
      "    val_loss       : -868653.505078125\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -1005883.625000\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -844885.812500\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -830137.375000\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -844351.250000\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -890306.375000\n",
      "    epoch          : 361\n",
      "    loss           : -882337.7673267326\n",
      "    val_loss       : -869969.108203125\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -1006914.250000\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -852739.687500\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -888184.750000\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -835031.437500\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -890826.000000\n",
      "    epoch          : 362\n",
      "    loss           : -884187.0996287129\n",
      "    val_loss       : -869582.205859375\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -898834.875000\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -921140.125000\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -835979.750000\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -834311.125000\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -891890.000000\n",
      "    epoch          : 363\n",
      "    loss           : -885333.8483910891\n",
      "    val_loss       : -869866.2033203125\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -1005745.375000\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -832871.937500\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -836012.437500\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -891234.875000\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -889664.875000\n",
      "    epoch          : 364\n",
      "    loss           : -885403.9628712871\n",
      "    val_loss       : -869313.63359375\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -922260.250000\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -843028.750000\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -839751.062500\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -826802.875000\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -885325.625000\n",
      "    epoch          : 365\n",
      "    loss           : -880244.666460396\n",
      "    val_loss       : -868770.3830078125\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -1004771.500000\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -851015.375000\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -852993.437500\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -897234.750000\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -831337.750000\n",
      "    epoch          : 366\n",
      "    loss           : -881920.4882425743\n",
      "    val_loss       : -868853.8013671875\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -914547.875000\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -838325.250000\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -837719.625000\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -1008740.312500\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -890868.062500\n",
      "    epoch          : 367\n",
      "    loss           : -881997.6181930694\n",
      "    val_loss       : -870087.5884765625\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -1003912.812500\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -900669.625000\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -839473.500000\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -900860.875000\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -891332.500000\n",
      "    epoch          : 368\n",
      "    loss           : -884705.8131188119\n",
      "    val_loss       : -870113.3662109375\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -1007633.750000\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -913873.937500\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -888038.375000\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -899650.000000\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -829542.375000\n",
      "    epoch          : 369\n",
      "    loss           : -883074.041460396\n",
      "    val_loss       : -869160.5494140625\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -1003658.312500\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -913914.000000\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -899617.625000\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -856947.125000\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -838119.937500\n",
      "    epoch          : 370\n",
      "    loss           : -884351.75\n",
      "    val_loss       : -869381.821484375\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -1004483.312500\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -846679.375000\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -831605.250000\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -833583.250000\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -881790.562500\n",
      "    epoch          : 371\n",
      "    loss           : -882816.3056930694\n",
      "    val_loss       : -866992.5400390625\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -1006556.500000\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -840672.062500\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -835653.375000\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -851541.875000\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -840824.000000\n",
      "    epoch          : 372\n",
      "    loss           : -881231.7716584158\n",
      "    val_loss       : -870280.345703125\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -1007772.125000\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -829088.250000\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -829198.875000\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -837236.250000\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -836533.187500\n",
      "    epoch          : 373\n",
      "    loss           : -884069.416460396\n",
      "    val_loss       : -870415.239453125\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -1007373.062500\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -852787.000000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -903110.812500\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -892785.875000\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -837178.187500\n",
      "    epoch          : 374\n",
      "    loss           : -885927.968440594\n",
      "    val_loss       : -870329.769140625\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -1004678.000000\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -825215.125000\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -828769.562500\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -887175.937500\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -837723.625000\n",
      "    epoch          : 375\n",
      "    loss           : -884439.2110148515\n",
      "    val_loss       : -869688.430078125\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -1010029.750000\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -888049.812500\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -892451.375000\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -891482.187500\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -892576.562500\n",
      "    epoch          : 376\n",
      "    loss           : -886016.729579208\n",
      "    val_loss       : -870781.119140625\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -1007654.500000\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -851509.500000\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -890971.250000\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -887434.125000\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -848359.812500\n",
      "    epoch          : 377\n",
      "    loss           : -885113.8836633663\n",
      "    val_loss       : -869289.296875\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -1004958.125000\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -845860.125000\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -830527.625000\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -888391.062500\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -824927.312500\n",
      "    epoch          : 378\n",
      "    loss           : -883787.4733910891\n",
      "    val_loss       : -869470.421875\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -1007411.062500\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -917852.375000\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -900333.000000\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -851085.500000\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -887199.062500\n",
      "    epoch          : 379\n",
      "    loss           : -883610.5371287129\n",
      "    val_loss       : -869861.9837890625\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -1008432.312500\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -835540.375000\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -831063.187500\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -1005911.125000\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -831484.437500\n",
      "    epoch          : 380\n",
      "    loss           : -882384.5297029703\n",
      "    val_loss       : -869890.8744140625\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -1007978.312500\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -917800.187500\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -850727.187500\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -1008887.312500\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -894808.687500\n",
      "    epoch          : 381\n",
      "    loss           : -885179.3706683168\n",
      "    val_loss       : -870528.658984375\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -1009075.937500\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -895396.000000\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -889429.750000\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -832847.000000\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -889955.000000\n",
      "    epoch          : 382\n",
      "    loss           : -885018.6126237623\n",
      "    val_loss       : -869128.6962890625\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -1005508.125000\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -893096.625000\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -854647.812500\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -1008877.312500\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -892795.312500\n",
      "    epoch          : 383\n",
      "    loss           : -883315.385519802\n",
      "    val_loss       : -869113.79609375\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -841415.750000\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -852224.250000\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -837678.062500\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -853522.875000\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -893775.562500\n",
      "    epoch          : 384\n",
      "    loss           : -885713.906559406\n",
      "    val_loss       : -869503.812890625\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -1008931.250000\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -915199.500000\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -898338.562500\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -1009364.625000\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -839617.625000\n",
      "    epoch          : 385\n",
      "    loss           : -884491.3044554455\n",
      "    val_loss       : -870525.242578125\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -1008157.312500\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -835709.125000\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -842243.125000\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -837621.000000\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -891497.375000\n",
      "    epoch          : 386\n",
      "    loss           : -884928.7679455446\n",
      "    val_loss       : -869885.0859375\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -1007707.375000\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -854300.687500\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -891250.812500\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -901574.000000\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -890948.187500\n",
      "    epoch          : 387\n",
      "    loss           : -885773.2964108911\n",
      "    val_loss       : -870713.34609375\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -1008887.062500\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -919700.812500\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -853644.000000\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -1009645.312500\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -886114.062500\n",
      "    epoch          : 388\n",
      "    loss           : -886421.9096534654\n",
      "    val_loss       : -869780.8119140625\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -1004377.000000\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -916873.437500\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -836674.625000\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -1009464.000000\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -892556.625000\n",
      "    epoch          : 389\n",
      "    loss           : -883412.3125\n",
      "    val_loss       : -870352.922265625\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -1009902.250000\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -826568.437500\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -889279.187500\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -890203.500000\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -891731.062500\n",
      "    epoch          : 390\n",
      "    loss           : -883768.541460396\n",
      "    val_loss       : -868943.819140625\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -1005365.375000\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -919355.187500\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -835018.437500\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -899413.250000\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -836498.687500\n",
      "    epoch          : 391\n",
      "    loss           : -882733.1101485149\n",
      "    val_loss       : -869669.864453125\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -850706.625000\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -916238.812500\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -889647.437500\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -850811.250000\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -838307.812500\n",
      "    epoch          : 392\n",
      "    loss           : -884554.2462871287\n",
      "    val_loss       : -871613.821875\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -1008206.687500\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -918539.312500\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -833456.125000\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -884181.750000\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -854234.000000\n",
      "    epoch          : 393\n",
      "    loss           : -886153.7159653465\n",
      "    val_loss       : -869459.629296875\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -916453.375000\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -846216.000000\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -831415.625000\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -1010249.000000\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -855214.500000\n",
      "    epoch          : 394\n",
      "    loss           : -886611.5198019802\n",
      "    val_loss       : -871300.0966796875\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -846480.562500\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -853030.625000\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -895344.125000\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -1011032.937500\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -889933.000000\n",
      "    epoch          : 395\n",
      "    loss           : -887929.2716584158\n",
      "    val_loss       : -870342.8544921875\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -1009197.125000\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -851363.500000\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -822313.687500\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -837947.437500\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -839792.375000\n",
      "    epoch          : 396\n",
      "    loss           : -884799.8032178218\n",
      "    val_loss       : -870897.733203125\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -1009898.625000\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -848112.375000\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -902617.250000\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -901033.750000\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -857711.375000\n",
      "    epoch          : 397\n",
      "    loss           : -886597.1336633663\n",
      "    val_loss       : -871054.549609375\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -1009365.937500\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -902515.187500\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -834751.625000\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -892177.750000\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -896044.750000\n",
      "    epoch          : 398\n",
      "    loss           : -886876.875\n",
      "    val_loss       : -871292.881640625\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -1009714.062500\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -850136.562500\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -839442.375000\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -890806.687500\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -893763.437500\n",
      "    epoch          : 399\n",
      "    loss           : -887266.3094059406\n",
      "    val_loss       : -870701.7767578125\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -847995.937500\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -850980.187500\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -902585.062500\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -833382.250000\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -891901.187500\n",
      "    epoch          : 400\n",
      "    loss           : -886312.7858910891\n",
      "    val_loss       : -869497.8517578125\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0803_215641/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -1006992.000000\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -918853.875000\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -890816.562500\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -897321.062500\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -885498.625000\n",
      "    epoch          : 401\n",
      "    loss           : -885116.5018564357\n",
      "    val_loss       : -867093.7470703125\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -1006159.125000\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -848529.187500\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -901761.125000\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -842006.250000\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -859177.125000\n",
      "    epoch          : 402\n",
      "    loss           : -885489.6646039604\n",
      "    val_loss       : -870521.2404296875\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -1010754.000000\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -855062.625000\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -835770.625000\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -1011136.375000\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -886100.187500\n",
      "    epoch          : 403\n",
      "    loss           : -887284.7642326732\n",
      "    val_loss       : -870369.8041015625\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -1009235.062500\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -919685.187500\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -829537.875000\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -1009889.750000\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -854396.187500\n",
      "    epoch          : 404\n",
      "    loss           : -885817.4350247525\n",
      "    val_loss       : -869100.15390625\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -1007542.562500\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -849930.062500\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -829311.625000\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -858076.250000\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -892038.750000\n",
      "    epoch          : 405\n",
      "    loss           : -885655.3737623763\n",
      "    val_loss       : -871359.18359375\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -1010959.187500\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -848911.375000\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -854272.062500\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -891311.750000\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -885552.437500\n",
      "    epoch          : 406\n",
      "    loss           : -885969.9814356435\n",
      "    val_loss       : -869346.69921875\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -1007031.750000\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -918841.750000\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -898527.000000\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -858448.750000\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -837381.312500\n",
      "    epoch          : 407\n",
      "    loss           : -885844.0464108911\n",
      "    val_loss       : -870876.0765625\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -1012922.375000\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -899453.062500\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -904012.437500\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -856570.000000\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -893958.500000\n",
      "    epoch          : 408\n",
      "    loss           : -885718.4084158416\n",
      "    val_loss       : -869736.725390625\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -1006820.500000\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -900223.437500\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -893160.937500\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -895714.500000\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -889490.187500\n",
      "    epoch          : 409\n",
      "    loss           : -887332.0662128713\n",
      "    val_loss       : -871963.606640625\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -1010066.875000\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -854929.750000\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -842348.750000\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -897682.437500\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -890758.187500\n",
      "    epoch          : 410\n",
      "    loss           : -888428.7165841584\n",
      "    val_loss       : -871100.4365234375\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -1011503.375000\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -900595.625000\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -900914.375000\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -855475.250000\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -892508.750000\n",
      "    epoch          : 411\n",
      "    loss           : -887530.3972772277\n",
      "    val_loss       : -870348.2615234375\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -1007987.937500\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -849437.062500\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -837907.125000\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -905160.062500\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -841551.875000\n",
      "    epoch          : 412\n",
      "    loss           : -887034.5167079208\n",
      "    val_loss       : -870012.20859375\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -1007948.750000\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -846274.062500\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -830653.625000\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -891906.937500\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -890719.500000\n",
      "    epoch          : 413\n",
      "    loss           : -886264.5779702971\n",
      "    val_loss       : -870700.3380859375\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -1011215.125000\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -856087.687500\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -894938.250000\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -841231.875000\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -889504.375000\n",
      "    epoch          : 414\n",
      "    loss           : -887425.2970297029\n",
      "    val_loss       : -871459.5109375\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -1011724.312500\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -850875.750000\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -833980.875000\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -899419.437500\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -889018.937500\n",
      "    epoch          : 415\n",
      "    loss           : -886706.958539604\n",
      "    val_loss       : -870199.519921875\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -1009400.812500\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -844912.187500\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -836897.812500\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -1012951.250000\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -896730.250000\n",
      "    epoch          : 416\n",
      "    loss           : -884802.0816831683\n",
      "    val_loss       : -870969.1787109375\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -1012438.562500\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -834916.062500\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -905405.000000\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -857023.500000\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -893791.937500\n",
      "    epoch          : 417\n",
      "    loss           : -887385.208539604\n",
      "    val_loss       : -871484.120703125\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -1010601.500000\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -920601.750000\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -893583.750000\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -890201.875000\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -890698.187500\n",
      "    epoch          : 418\n",
      "    loss           : -888215.7852722772\n",
      "    val_loss       : -870172.834765625\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -1009017.625000\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -833177.562500\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -853219.937500\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -889180.750000\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -834679.750000\n",
      "    epoch          : 419\n",
      "    loss           : -887128.1491336634\n",
      "    val_loss       : -871411.67578125\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -1013558.000000\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -829758.625000\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -853741.125000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -892996.187500\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -894147.000000\n",
      "    epoch          : 420\n",
      "    loss           : -887539.6002475248\n",
      "    val_loss       : -870950.08828125\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -1009620.875000\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -853793.875000\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -832545.000000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -894593.062500\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -890823.562500\n",
      "    epoch          : 421\n",
      "    loss           : -887701.8316831683\n",
      "    val_loss       : -870264.6345703125\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -1012015.375000\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -918087.500000\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -838615.000000\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -855715.187500\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -900042.000000\n",
      "    epoch          : 422\n",
      "    loss           : -886851.3471534654\n",
      "    val_loss       : -869450.3216796875\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -1007421.875000\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -833700.625000\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -839933.187500\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -832525.875000\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -891589.125000\n",
      "    epoch          : 423\n",
      "    loss           : -884870.9115099009\n",
      "    val_loss       : -869790.701171875\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -1011239.562500\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -848893.937500\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -856092.875000\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -893618.937500\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -846613.250000\n",
      "    epoch          : 424\n",
      "    loss           : -887945.8391089109\n",
      "    val_loss       : -871443.80234375\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -1011540.875000\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -857715.812500\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -900583.250000\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -890020.437500\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -837684.625000\n",
      "    epoch          : 425\n",
      "    loss           : -889081.6584158416\n",
      "    val_loss       : -871416.595703125\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -839981.250000\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -836646.000000\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -901463.000000\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -904362.000000\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -836682.750000\n",
      "    epoch          : 426\n",
      "    loss           : -887844.9158415842\n",
      "    val_loss       : -871249.77109375\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -1013410.937500\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -917843.875000\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -836383.187500\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -892383.437500\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -891118.500000\n",
      "    epoch          : 427\n",
      "    loss           : -887779.1349009901\n",
      "    val_loss       : -871328.7814453125\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -1013396.500000\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -851204.375000\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -857123.000000\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -861425.312500\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -836559.500000\n",
      "    epoch          : 428\n",
      "    loss           : -888332.3155940594\n",
      "    val_loss       : -870896.57109375\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -1012210.812500\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -918488.000000\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -837090.687500\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -902053.750000\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -836515.250000\n",
      "    epoch          : 429\n",
      "    loss           : -887606.5160891089\n",
      "    val_loss       : -869472.7060546875\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -921797.000000\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -852214.750000\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -891015.500000\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -835295.437500\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -883091.000000\n",
      "    epoch          : 430\n",
      "    loss           : -885732.3044554455\n",
      "    val_loss       : -868885.089453125\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -1009729.375000\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -840899.500000\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -887642.125000\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -887851.125000\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -835437.312500\n",
      "    epoch          : 431\n",
      "    loss           : -885159.8323019802\n",
      "    val_loss       : -872026.265625\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -1014541.750000\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -839864.500000\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -857470.750000\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -894702.312500\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -897525.375000\n",
      "    epoch          : 432\n",
      "    loss           : -888857.4257425743\n",
      "    val_loss       : -872336.26015625\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -1012409.687500\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -839000.625000\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -895959.750000\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -1014323.375000\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -896500.062500\n",
      "    epoch          : 433\n",
      "    loss           : -888967.4455445545\n",
      "    val_loss       : -871671.6404296875\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -1011145.562500\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -849692.000000\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -894771.687500\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -891489.750000\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -893889.875000\n",
      "    epoch          : 434\n",
      "    loss           : -889272.9127475248\n",
      "    val_loss       : -871313.5541015625\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -1013569.937500\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -853100.312500\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -894950.000000\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -857968.125000\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -891923.125000\n",
      "    epoch          : 435\n",
      "    loss           : -888801.1850247525\n",
      "    val_loss       : -870420.05703125\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -1009163.500000\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -852418.312500\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -902231.375000\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -842687.062500\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -841148.625000\n",
      "    epoch          : 436\n",
      "    loss           : -889882.0841584158\n",
      "    val_loss       : -871507.9943359375\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -1012897.375000\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -853982.750000\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -891926.500000\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -891033.750000\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -838841.750000\n",
      "    epoch          : 437\n",
      "    loss           : -888305.2277227723\n",
      "    val_loss       : -870591.2115234375\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -1012351.375000\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -909205.875000\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -826405.687500\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -897757.937500\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -894781.125000\n",
      "    epoch          : 438\n",
      "    loss           : -884931.1967821782\n",
      "    val_loss       : -871870.9115234375\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -1010848.375000\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -903603.062500\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -857272.375000\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -1016477.562500\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -899017.750000\n",
      "    epoch          : 439\n",
      "    loss           : -889864.2728960396\n",
      "    val_loss       : -872609.3076171875\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -896954.312500\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -899575.000000\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -903151.750000\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -899052.625000\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -893676.062500\n",
      "    epoch          : 440\n",
      "    loss           : -890459.9096534654\n",
      "    val_loss       : -871277.62109375\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -1016694.875000\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -841160.000000\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -852283.875000\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -903568.875000\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -896859.437500\n",
      "    epoch          : 441\n",
      "    loss           : -888896.4573019802\n",
      "    val_loss       : -870264.599609375\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -1010237.375000\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -921437.000000\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -894775.875000\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -835303.125000\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -857473.937500\n",
      "    epoch          : 442\n",
      "    loss           : -887833.9771039604\n",
      "    val_loss       : -870858.900390625\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -1013031.500000\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -854615.500000\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -859428.625000\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -891560.625000\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -862543.625000\n",
      "    epoch          : 443\n",
      "    loss           : -888109.1342821782\n",
      "    val_loss       : -870466.6642578125\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -1011668.187500\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -854522.375000\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -888569.000000\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -899880.062500\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -890170.687500\n",
      "    epoch          : 444\n",
      "    loss           : -888240.353960396\n",
      "    val_loss       : -868409.040234375\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -1012008.562500\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -855440.750000\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -859254.875000\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -895476.625000\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -889005.375000\n",
      "    epoch          : 445\n",
      "    loss           : -887385.2246287129\n",
      "    val_loss       : -869579.773046875\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -1011739.625000\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -903864.437500\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -824999.750000\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -889677.125000\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -892748.625000\n",
      "    epoch          : 446\n",
      "    loss           : -885644.3304455446\n",
      "    val_loss       : -871550.400390625\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -923461.812500\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -836829.937500\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -903885.875000\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -1014172.125000\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -860985.687500\n",
      "    epoch          : 447\n",
      "    loss           : -889386.6299504951\n",
      "    val_loss       : -871211.1212890625\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -923031.750000\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -840211.812500\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -842499.437500\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -862619.187500\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -894190.000000\n",
      "    epoch          : 448\n",
      "    loss           : -889632.4573019802\n",
      "    val_loss       : -871972.651171875\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -1012674.000000\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -918943.062500\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -837759.125000\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -897507.750000\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -894159.812500\n",
      "    epoch          : 449\n",
      "    loss           : -888481.5655940594\n",
      "    val_loss       : -870783.767578125\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -1011001.437500\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -853436.812500\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -863652.000000\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -861072.187500\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -865472.625000\n",
      "    epoch          : 450\n",
      "    loss           : -891241.0798267326\n",
      "    val_loss       : -872216.1404296875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0803_215641/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -898904.125000\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -855332.375000\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -863194.375000\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -897421.375000\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -898803.125000\n",
      "    epoch          : 451\n",
      "    loss           : -891963.2388613861\n",
      "    val_loss       : -872954.7177734375\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -1014776.625000\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -840783.625000\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -895328.750000\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -1015037.375000\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -839342.625000\n",
      "    epoch          : 452\n",
      "    loss           : -891643.1386138614\n",
      "    val_loss       : -871860.678515625\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -1012654.000000\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -840440.875000\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -858972.375000\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -894781.250000\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -840854.875000\n",
      "    epoch          : 453\n",
      "    loss           : -890353.2698019802\n",
      "    val_loss       : -871393.0611328125\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -1011469.687500\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -836547.500000\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -864707.312500\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -903603.000000\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -834281.312500\n",
      "    epoch          : 454\n",
      "    loss           : -889545.8737623763\n",
      "    val_loss       : -870725.01640625\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -1015371.312500\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -923136.250000\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -891864.687500\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -893239.625000\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -840374.500000\n",
      "    epoch          : 455\n",
      "    loss           : -890419.9238861386\n",
      "    val_loss       : -871072.5220703125\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -1015020.875000\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -861057.250000\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -856632.687500\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -865738.125000\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -837748.812500\n",
      "    epoch          : 456\n",
      "    loss           : -891250.3285891089\n",
      "    val_loss       : -870971.9611328125\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -1013664.250000\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -904805.250000\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -902950.437500\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -891970.250000\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -886787.500000\n",
      "    epoch          : 457\n",
      "    loss           : -888731.979579208\n",
      "    val_loss       : -870422.944921875\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -1012295.375000\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -850077.875000\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -841491.000000\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -847131.187500\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -893944.000000\n",
      "    epoch          : 458\n",
      "    loss           : -889871.4548267326\n",
      "    val_loss       : -871928.1021484375\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -852024.750000\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -913247.625000\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -889358.562500\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -843056.125000\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -895379.625000\n",
      "    epoch          : 459\n",
      "    loss           : -890287.7141089109\n",
      "    val_loss       : -871001.189453125\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -1016238.375000\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -920080.812500\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -833662.687500\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -1012289.187500\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -899026.375000\n",
      "    epoch          : 460\n",
      "    loss           : -890118.4040841584\n",
      "    val_loss       : -872679.6025390625\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -1015594.687500\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -894070.250000\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -898095.937500\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -842135.187500\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -843576.000000\n",
      "    epoch          : 461\n",
      "    loss           : -890761.5693069306\n",
      "    val_loss       : -870165.0181640625\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -1011781.687500\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -892409.062500\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -839294.187500\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -844324.000000\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -843072.750000\n",
      "    epoch          : 462\n",
      "    loss           : -890646.8706683168\n",
      "    val_loss       : -871933.247265625\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -918460.750000\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -892563.687500\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -892642.187500\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -897958.500000\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -904752.250000\n",
      "    epoch          : 463\n",
      "    loss           : -889560.8892326732\n",
      "    val_loss       : -869959.04921875\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -1013019.625000\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -910735.000000\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -890850.250000\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -838679.500000\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -841391.437500\n",
      "    epoch          : 464\n",
      "    loss           : -888264.6930693069\n",
      "    val_loss       : -871601.7953125\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -1015699.812500\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -906315.625000\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -903660.750000\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -840029.625000\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -898104.625000\n",
      "    epoch          : 465\n",
      "    loss           : -890368.4832920792\n",
      "    val_loss       : -871599.128515625\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -1015278.312500\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -849121.000000\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -863449.312500\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -893251.875000\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -897836.625000\n",
      "    epoch          : 466\n",
      "    loss           : -890593.2636138614\n",
      "    val_loss       : -870865.655078125\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -1012701.812500\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -909159.625000\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -833741.812500\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -1013731.250000\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -894008.750000\n",
      "    epoch          : 467\n",
      "    loss           : -887045.3323019802\n",
      "    val_loss       : -871580.158203125\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -851535.562500\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -1010780.875000\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -838683.937500\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -864929.812500\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -897195.750000\n",
      "    epoch          : 468\n",
      "    loss           : -890917.4783415842\n",
      "    val_loss       : -871744.6974609375\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -1015068.562500\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -856287.500000\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -834401.375000\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -901202.062500\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -890775.375000\n",
      "    epoch          : 469\n",
      "    loss           : -887901.3966584158\n",
      "    val_loss       : -869857.13828125\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -924512.375000\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -904059.437500\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -833191.312500\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -892678.000000\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -894644.750000\n",
      "    epoch          : 470\n",
      "    loss           : -886949.9826732674\n",
      "    val_loss       : -871213.5873046875\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -1013576.812500\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -850279.437500\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -838955.687500\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -838430.937500\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -897151.187500\n",
      "    epoch          : 471\n",
      "    loss           : -888503.0847772277\n",
      "    val_loss       : -872521.632421875\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -924720.062500\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -853949.812500\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -837594.125000\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -844592.437500\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -898418.000000\n",
      "    epoch          : 472\n",
      "    loss           : -891426.3675742574\n",
      "    val_loss       : -872929.6548828125\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -926338.312500\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -842485.875000\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -901500.875000\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -904635.500000\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -888958.500000\n",
      "    epoch          : 473\n",
      "    loss           : -890799.9393564357\n",
      "    val_loss       : -871203.7955078125\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -1013228.125000\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -920978.500000\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -891694.625000\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -890200.000000\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -888253.687500\n",
      "    epoch          : 474\n",
      "    loss           : -888328.6311881188\n",
      "    val_loss       : -871807.66484375\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -1011759.625000\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -853577.937500\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -859869.500000\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -900178.562500\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -895650.375000\n",
      "    epoch          : 475\n",
      "    loss           : -891264.4653465346\n",
      "    val_loss       : -870683.2416015625\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -1016279.250000\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -848239.125000\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -844526.625000\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -1017101.687500\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -842115.312500\n",
      "    epoch          : 476\n",
      "    loss           : -891173.9771039604\n",
      "    val_loss       : -871825.703125\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -1018476.687500\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -858380.187500\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -843422.000000\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -839902.000000\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -897234.375000\n",
      "    epoch          : 477\n",
      "    loss           : -892226.2574257426\n",
      "    val_loss       : -871376.425390625\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -1013201.000000\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -857371.500000\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -829745.000000\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -900583.375000\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -897769.875000\n",
      "    epoch          : 478\n",
      "    loss           : -889690.8118811881\n",
      "    val_loss       : -870971.044140625\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -1014222.375000\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -857599.750000\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -904752.562500\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -893633.750000\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -894580.125000\n",
      "    epoch          : 479\n",
      "    loss           : -891726.8298267326\n",
      "    val_loss       : -871824.048828125\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -1015333.687500\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -853901.375000\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -911737.750000\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -894743.062500\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -891289.500000\n",
      "    epoch          : 480\n",
      "    loss           : -891811.6924504951\n",
      "    val_loss       : -871959.2923828125\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -1015838.562500\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -850602.437500\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -838121.562500\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -858773.875000\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -894466.125000\n",
      "    epoch          : 481\n",
      "    loss           : -890060.4511138614\n",
      "    val_loss       : -870416.0720703125\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -1014099.125000\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -924822.750000\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -844801.125000\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -898128.625000\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -892869.812500\n",
      "    epoch          : 482\n",
      "    loss           : -891558.3490099009\n",
      "    val_loss       : -871703.6189453125\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -1014438.625000\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -858605.750000\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -864544.375000\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -903803.562500\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -887916.937500\n",
      "    epoch          : 483\n",
      "    loss           : -890539.156559406\n",
      "    val_loss       : -871146.176171875\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -1014871.000000\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -853799.375000\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -859434.875000\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -904718.000000\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -838815.562500\n",
      "    epoch          : 484\n",
      "    loss           : -889847.051980198\n",
      "    val_loss       : -871786.9513671875\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -863301.750000\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -858406.875000\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -908527.875000\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -895403.937500\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -892896.375000\n",
      "    epoch          : 485\n",
      "    loss           : -892097.3452970297\n",
      "    val_loss       : -871706.9537109375\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -1013493.500000\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -842192.625000\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -861863.000000\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -898651.125000\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -895655.125000\n",
      "    epoch          : 486\n",
      "    loss           : -890737.6404702971\n",
      "    val_loss       : -870944.9486328125\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -1014919.500000\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -852075.625000\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -839618.812500\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -1013612.562500\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -894251.250000\n",
      "    epoch          : 487\n",
      "    loss           : -890807.7667079208\n",
      "    val_loss       : -871625.3853515625\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -1012699.500000\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -921303.000000\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -843221.125000\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -893708.000000\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -901197.125000\n",
      "    epoch          : 488\n",
      "    loss           : -890411.7939356435\n",
      "    val_loss       : -871330.702734375\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -1015250.187500\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -860863.875000\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -842973.250000\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -865902.375000\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -894409.500000\n",
      "    epoch          : 489\n",
      "    loss           : -890263.6175742574\n",
      "    val_loss       : -870930.859765625\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -1012818.500000\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -914204.937500\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -888893.312500\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -892570.375000\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -895302.312500\n",
      "    epoch          : 490\n",
      "    loss           : -887086.9034653465\n",
      "    val_loss       : -871534.690234375\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -1015077.125000\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -852900.875000\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -835496.687500\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -894678.437500\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -896974.625000\n",
      "    epoch          : 491\n",
      "    loss           : -889622.6231435643\n",
      "    val_loss       : -871211.89296875\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -1017183.312500\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -915365.437500\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -864965.625000\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -895764.562500\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -858330.750000\n",
      "    epoch          : 492\n",
      "    loss           : -889594.8038366337\n",
      "    val_loss       : -871823.623046875\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -1014775.500000\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -922661.437500\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -864217.375000\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -865552.750000\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -898136.500000\n",
      "    epoch          : 493\n",
      "    loss           : -891205.7747524752\n",
      "    val_loss       : -872178.40859375\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -1017281.750000\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -925592.375000\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -867506.875000\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -896528.437500\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -893777.500000\n",
      "    epoch          : 494\n",
      "    loss           : -892160.7741336634\n",
      "    val_loss       : -871880.832421875\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -1014339.875000\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -838199.375000\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -907234.000000\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -843850.750000\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -894175.812500\n",
      "    epoch          : 495\n",
      "    loss           : -892005.2623762377\n",
      "    val_loss       : -871859.29609375\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -925290.500000\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -840410.000000\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -899877.625000\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -841327.500000\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -843158.750000\n",
      "    epoch          : 496\n",
      "    loss           : -892792.3867574257\n",
      "    val_loss       : -872179.0751953125\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -1016053.625000\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -854148.937500\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -908562.937500\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -897919.437500\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -893421.500000\n",
      "    epoch          : 497\n",
      "    loss           : -892789.9542079208\n",
      "    val_loss       : -872672.760546875\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -1015581.000000\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -859362.187500\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -841697.375000\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -841856.750000\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -896883.500000\n",
      "    epoch          : 498\n",
      "    loss           : -890932.3941831683\n",
      "    val_loss       : -871352.4443359375\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -1015517.125000\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -856666.500000\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -842829.625000\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -1017772.000000\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -897296.937500\n",
      "    epoch          : 499\n",
      "    loss           : -892603.6602722772\n",
      "    val_loss       : -871867.078125\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -1013623.875000\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -898218.250000\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -897040.625000\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -895072.125000\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -892487.750000\n",
      "    epoch          : 500\n",
      "    loss           : -891876.4467821782\n",
      "    val_loss       : -868863.21171875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0803_215641/checkpoint-epoch500.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAECategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_3_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_9_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "  )\n",
       "  (guide_embedding): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (guide_confidences): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_distances): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=15, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJwAAAEuCAYAAAB7zXuHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAS0UlEQVR4nO3de2zU5Z7H8c9DizO1toVyaQuUFrkWuQjiakEssB7AZSmJN2QXKLDsygIS5RgpUi6N2FRXZGNsdl2IQIiYAp7gRmURUe67wEEuIhQseGqxtmILWtsZOkO/+0fHca6dcvtOx/N5Jb+k8/xuD503v7kEZoyIgEhLm3BPgP66MDhSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSFV0uCfQEjExMZV2uz0p3POIBFartcpmsyWHex7BmEj4TzTGGImEebYGxhiIiAn3PILhQyqpYnCkisGRKgZHqhgcqWJwpIrBkSoGR6oYHKlicKSKwZEqBkeqGBypYnCkisGRKgZHqhgcqWJwpIrBkSoGR6oYHKlicKSKwZEqBkeqGBypYnCkisGRKgZHqhgcqfrdBHf16lX0798flZWV173v0aNHkZ+fj4qKiqDbiAhycnKQmJiI+fPn38xUW6y4uBgJCQkYPny439yuXr2Kfv364YcfflCZyy0jIq1+aZpm895880155plnRESkrKxMYmNj/ZaoqCgZPXq0135nzpyRjh07ykMPPSQDBw6Uy5cvBzz+yZMnxWKxSGVlpdd4cXGxZGZmSkxMjGRlZXmtO3v2rGRnZ0vHjh2lffv2MnbsWCkpKXGvb2xslCVLlkiXLl0kPj5esrKy5NSpU17HaGhokMzMTFm9erXfnF599VVZuHCh15jrdxX2+yzYEvYJtGiSLQjunnvukf379wddf/LkSbnrrrtk586d7rHy8nJJT0+XNWvWSGNjoyxYsEBGjhwpNpvNb//du3dL165d/cZ37twpxcXFkp+f7xfcoUOHZO3atVJdXS0NDQ2Sl5cnffv2da8vLi6WlJQUOX/+vDidTsnNzZUhQ4b4nSMnJ0fy8vL8xsvLy6VDhw5it9vdYwzuFgaXlpYmBQUFkpGRIe3atZMZM2aIzWaTsrIysVqt4nA4/O4UEZGffvpJevfuLS+//LJ7rLq6WgYNGiQbN2702nbRokUyadIkcTqdXuOffvqppKamBjy+iMiaNWv8gvNVXV0tAOTHH38UEZHCwkJ58skn3etPnTolFovFb7+ZM2dKbm5uwGP26tVLdu/e7b7d2oOLuOdw7777Lnbs2IHz58/j3LlzWLlyJb788kvcfffdiI4O/AmyM2fORK9evbBkyRL3WGJiIk6cOIGpU6d6bVtYWIht27YhKirKPdbY2Ihdu3ahe/fuNzX3vXv3Ijk5GR06dAAAPP300ygtLcW5c+fgcDiwYcMGjB8/3m+/1NRUHDx4EHV1dX7rMjIycOLEiZual6aI+IxfT/Pnz0dqaioAYMmSJXj22WeRkZGBuLi4gNuvWrUKR48exRdffAFjrv+TSGtqapCSkoI77rgDn3zyyQ3P++LFi5g3bx7eeOMN91hKSgpGjhyJvn37IioqCqmpqfjss8/89n3uuefw3nvvIT4+Hlu2bMFjjz3mXhcXF4crV67c8Ly0RdwV7tfYACAtLQ0VFRVo3749amtr/bbdv38/li9fjq1btyIxMfGGzpeYmIi6ujrk5OTglVdeuaFjXLp0CWPHjsXcuXMxZcoU93h+fj6OHDmC8vJy2O12LF++HGPGjEF9fb3X/uvWrUN8fDxqamq8YgOA2tpatGvX7obmFQ4RF1x5ebn752+//RZdunTBoEGDcOHCBTidTve6qqoqTJ48Ga+//jqGDRt2U+eMjo7GxIkTcfr06eve9/Llyxg7diyys7O9HtIB4MSJE5g8eTK6deuG6OhozJgxA5cvX/Y7z5kzZzB69GgkJCT4Hf/MmTMYPHjwdc8rXCIuuKKiIly8eBE1NTUoKChw32G9e/fG4cOHAQDXrl3DlClTMGbMGMyZM+eWnNdisaChocFv/Nq1a7Db7XA6nWhsbITdbofD4QAA/Pzzzxg3bhxGjBiBwsJCv33vv/9+bNmyBVVVVWhsbMTGjRvhcDjQq1cvr+0cDgcsFovf/t999x1qamrw4IMP3pI/o4pwv2ppyYIAr1ITEhJk+vTpUldXJyIib731lsyZM0dERPbs2SMAJCYmxu+9uP79+wd8tRfKnj17JCUlxW983bp1AsBrycnJERGR9evXCwC58847veZQVlYmIiI2m03mzp0rycnJEhcXJ0OGDJHt27f7nWPq1KmydOlSv/HXXntNnn/+ea8xtPJXqWGfQIsm6RGc5/tonux2u2RkZEhFRUXA9Tfr7NmzEh0dLRcuXLgtxw+mvr5ehg4dKkVFRV7jdrtd+vbtK1VVVV7jrT24iHtIDcZiseD06dNISUm5Lcfv06cP5s2bh5EjR2LBggW35Ry+Nm/ejLS0NCQlJeGpp57yWmexWFBSUoLOnTurzOVWiahvoklPT8fatWvxyCOPhHtKrVZr/yaaiAqOQmvtwf1uHlIpMjA4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFIVER+bb7Vaq4wxSeGeRySwWq1V4Z5DcyLi8+FaG2PMNABjRWRauOcSafiQSqoYHKlicKSKwZEqBkeqGBypYnCkisGRKgZHqhgcqWJwpIrBkSoGR6oYHKlicGFijJlvjPmzMeaqMWZ9uOejJSL+AebvVAWAlQDGAYgJ81zUMLgwEZE/AYAxZhiAbmGejho+pJIqBkeqGBypYnCkii8awsQYE42m338UgChjjBWAU0Sc4Z3Z7cUrXPjkAbAByAUw1fVzXlhnpIBXuDARkRUAVoR5Gup4hSNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVv6D3xpwEUBfuSUQiIyLhnkNIMTExlXa7PSnc84gEVqu1ymazJYd7HsFERHDGGImEebYGxhiIiAn3PILhczhSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UnXTwRljLMaY08aY6/6IKGNMT2PMCmNM/1Dbdu7cGRMnTryxSV6n4uJiJCQkYPjw4aioqAi5/ahRo7B79+6bPq/dbkdBQQE+/vjjZrfbsmULevTogfbt29/0OVvKGHPZGFNmjHk6wLo3jDFzWnQgEbmpBcCzAP7T4/b7AP7LZ5ttAN7yGUsGcB7AHgAXAXQPcvw4AHLs2DHxNH78eImNjXUvbdu2lQEDBrjXHzt2TB566CGJj4+Xrl27Sn5+vgSyYsUKASA7d+70Gm9oaJDMzExZvXp1wP08ZWVlyeeffx5wne88Y2NjxWKxCAApKytzb+d0OiU7O1uGDRsm8fHxsn379qDnGzp0qN+8KioqZOLEiQLg1yVdvH+PrwP4GkAtgBIA033WjwHwBYCfAVwA8C/if1/MB3A8wHgKgHIAd/iu89s21AYhDwCcAjDC43YygGoAo123JwMoA3CXxzbxAI4BeNl1+48ATgPoEOD4aQDE4XAEvQNEmu50z6gyMjLkpZdeEqfTKaWlpZKcnCwffPCB1z6lpaUyYMAASUlJ8QtORCQnJ0fy8vKaPe+v5w4WnC+n0ylZWVkyffp0r/FZs2bJuHHjpL6+Xnbv3i3Jycly6NChgMdIT0/3m29lZaUUFRU1F1w+gH5oelR7AMBlAMNd69oC+AnAMwAMgPsB/AJgsM8xRgEol8Ad7ATwRKB1XtuF3AD4C4DFriAuA1gHwOpa1x2ADUC0zz4zAJS61lcBGO+xzgLgcwCLffaZC+AggFif8Z4A5Nq1a0HvxG+++UbatGkjFy5ccI/FxMTIV1995b79xBNPSEFBgdd+48ePl48++kjS0tICBjdz5kzJzc0Net5fXU9wL774ogwcOFDq6urcY7m5uZKdnS12u909dvDgQenZs6eUlJT4HaNbt26ya9eugMcPFpzvAuC/AfzR9XOSa587PdYfATDFZ5+HAXwf5HhLAKxr7pxyHcGdApAKIBHAAQArXesmAPgqyH47APwIYEOoczRzbgNgDoBm7kKR/Px8ycrK8hpbvHixLFq0SBoaGqSkpES6du0qhw8fdq/fvHmzZGdni4gEDW7ZsmXy8MMPyy+//NLs+Vsa3LZt2yQhIUHOnTsXcttgTp48KdHR0fL1118HXN+S4ADEAPje50KwCcA8AFEAMgH8ACDVZ7+7ATgB3BvgmI8B+CLYOd3bhdygKbg5Hrf/DsB518//COD/guyX5/qD/yHUOZo5948AHKGC69mzp6xbt85r7MCBA9KzZ0+JiooSALJs2TL3utraWunVq5f7ihgsuJqaGundu7e0adNG3n///aDnb0lwpaWl0q5dO9m6dWuz2zXn8ccfFwCycOHCoNu0MLgNAP4Hro/cdY1NdD0aOV3LPwfZ93XX8bf5jP8BwIVg53RvF3KDpuAmeNy+B4BNfovP7woHoDeAKwCK0PSJ321DnSfIudsAeLG54Pbt2yexsbFSW1vrHquurpa4uDjZsGGDOBwOKS8vlwceeECKiopERGThwoVez/eCBbdq1Sq577775MqVK0HPLxI6OJvNJvfee2+zobTUgQMHpG3btvL9998HXB8qOAD/BuAogHiPsX4A6gGMc/3O+6LpBcYEn307A2gAMDLAcW/bFe5RjytcN/g8h3M9DO4BsMI1+f8FkBfqPM2cvy8AaWxsDPgLnj17tkybNs1r7MiRI9KuXTuvsdWrV8uECRNERGTw4MHSoUMHSUpKkqSkJGnTpo20b99eCgsL/Y79wgsvBDyvp1DBzZo1S0aMGBHyhU9LJScny549ewKuay44NL1wOAWfF2cAngBwzGfs3+H/zsIIAJd8j+ta16LncC19H26eMaabMSYRwEsAitF02bno+pvwNx7b/iuAjgAKRKQRwD8BeNEY06+F5/J1FQCuXbvmt8Jms2HLli2YMWOG13ifPn0gIti0aRMaGxtRWVmJ4uJiDB48GACwa9cunDp1CsePH8fx48fRpUsXvP3225g3b57XcRwOBywWyw1Ou8k777yDDz/8EJs3b0Z09K35WgyLxYKGhga/cbvd7rWZMcb66w1jzGIA/4CmpzjVPrseA9DbGDPGNOkJ4O8BnPDZri1c90cAWQC2h5x8qCLh/Sr1Cpoe/z1fzcwD8B+un1Nd2zzoc4zlAPbB4zlDSxc0vdKVq1ev+v1t3rRpk3Tv3j3g1W/Xrl3u97SSkpJk9uzZXq8MPQV7SJ06daosXbo04D6emrvC9ejRQ6Kjo/3ei4uNjZW9e/eGPHawY+7YscNvHL9d3dyL/PZ7FFcsv3gsL3msfwpNV79aNL0v+iqANuJ9X/wtgG/F/z5Kce1z8+/DuYJ7pJn1FleMKaGOdSMLgDsByL59+27ozrlR9fX1MnToUPfzvuZcz9sit0JmZqasXLky4F80z8hu9YKm59OHA4yvAjC3RcdowUmaDU5jASBpaWkyadKklt4nN6W4uFg6deokjz76qFy6dCnk9trBbd++XQYMGCCdOnXyW3e7ggNwCcBX8Hkhcb1LyG+iMcb8BcBsEfm02Q1vo9b+TTTr16/HqFGjkJ6eHu6ptPpvouFXH/3OtPbg+M+TSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EjVrfkM0NvMarVWGWOSwj2PSGC1WqvCPYfmRMTHdbU2xphpAMaKyLRwzyXS8CGVVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDCxNjzHxjzJ+NMVeNMevDPR8tEfEvfn+nKgCsBDAOQEyY56KGwYWJiPwJAIwxwwB0C/N01PAhlVQxOFLF4EgVgyNVfNEQJsaYaDT9/qMARBljrACcIuIM78xuL17hwicPgA1ALoCprp/zwjojBbzChYmIrACwIszTUMcrHKlicKSKwZEqBkeqGBypYnCkisGRKgZHqhgcqWJwpIrB3ZgKACfCPYlIxA+VJlW8wpEqBkeqGBypYnCkisGRKgZHqhgcqWJwpIrBkSoGR6oYHKlicKSKwZEqBkeqGBypYnCkisGRKgZHqhgcqfp/UtuDHK9RLvkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJwAAAEuCAYAAAB7zXuHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAS0UlEQVR4nO3de2zU5Z7H8c9DizO1toVyaQuUFrkWuQjiakEssB7AZSmJN2QXKLDsygIS5RgpUi6N2FRXZGNsdl2IQIiYAp7gRmURUe67wEEuIhQseGqxtmILWtsZOkO/+0fHca6dcvtOx/N5Jb+k8/xuD503v7kEZoyIgEhLm3BPgP66MDhSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSFV0uCfQEjExMZV2uz0p3POIBFartcpmsyWHex7BmEj4TzTGGImEebYGxhiIiAn3PILhQyqpYnCkisGRKgZHqhgcqWJwpIrBkSoGR6oYHKlicKSKwZEqBkeqGBypYnCkisGRKgZHqhgcqWJwpIrBkSoGR6oYHKlicKSKwZEqBkeqGBypYnCkisGRKgZHqhgcqfrdBHf16lX0798flZWV173v0aNHkZ+fj4qKiqDbiAhycnKQmJiI+fPn38xUW6y4uBgJCQkYPny439yuXr2Kfv364YcfflCZyy0jIq1+aZpm895880155plnRESkrKxMYmNj/ZaoqCgZPXq0135nzpyRjh07ykMPPSQDBw6Uy5cvBzz+yZMnxWKxSGVlpdd4cXGxZGZmSkxMjGRlZXmtO3v2rGRnZ0vHjh2lffv2MnbsWCkpKXGvb2xslCVLlkiXLl0kPj5esrKy5NSpU17HaGhokMzMTFm9erXfnF599VVZuHCh15jrdxX2+yzYEvYJtGiSLQjunnvukf379wddf/LkSbnrrrtk586d7rHy8nJJT0+XNWvWSGNjoyxYsEBGjhwpNpvNb//du3dL165d/cZ37twpxcXFkp+f7xfcoUOHZO3atVJdXS0NDQ2Sl5cnffv2da8vLi6WlJQUOX/+vDidTsnNzZUhQ4b4nSMnJ0fy8vL8xsvLy6VDhw5it9vdYwzuFgaXlpYmBQUFkpGRIe3atZMZM2aIzWaTsrIysVqt4nA4/O4UEZGffvpJevfuLS+//LJ7rLq6WgYNGiQbN2702nbRokUyadIkcTqdXuOffvqppKamBjy+iMiaNWv8gvNVXV0tAOTHH38UEZHCwkJ58skn3etPnTolFovFb7+ZM2dKbm5uwGP26tVLdu/e7b7d2oOLuOdw7777Lnbs2IHz58/j3LlzWLlyJb788kvcfffdiI4O/AmyM2fORK9evbBkyRL3WGJiIk6cOIGpU6d6bVtYWIht27YhKirKPdbY2Ihdu3ahe/fuNzX3vXv3Ijk5GR06dAAAPP300ygtLcW5c+fgcDiwYcMGjB8/3m+/1NRUHDx4EHV1dX7rMjIycOLEiZual6aI+IxfT/Pnz0dqaioAYMmSJXj22WeRkZGBuLi4gNuvWrUKR48exRdffAFjrv+TSGtqapCSkoI77rgDn3zyyQ3P++LFi5g3bx7eeOMN91hKSgpGjhyJvn37IioqCqmpqfjss8/89n3uuefw3nvvIT4+Hlu2bMFjjz3mXhcXF4crV67c8Ly0RdwV7tfYACAtLQ0VFRVo3749amtr/bbdv38/li9fjq1btyIxMfGGzpeYmIi6ujrk5OTglVdeuaFjXLp0CWPHjsXcuXMxZcoU93h+fj6OHDmC8vJy2O12LF++HGPGjEF9fb3X/uvWrUN8fDxqamq8YgOA2tpatGvX7obmFQ4RF1x5ebn752+//RZdunTBoEGDcOHCBTidTve6qqoqTJ48Ga+//jqGDRt2U+eMjo7GxIkTcfr06eve9/Llyxg7diyys7O9HtIB4MSJE5g8eTK6deuG6OhozJgxA5cvX/Y7z5kzZzB69GgkJCT4Hf/MmTMYPHjwdc8rXCIuuKKiIly8eBE1NTUoKChw32G9e/fG4cOHAQDXrl3DlClTMGbMGMyZM+eWnNdisaChocFv/Nq1a7Db7XA6nWhsbITdbofD4QAA/Pzzzxg3bhxGjBiBwsJCv33vv/9+bNmyBVVVVWhsbMTGjRvhcDjQq1cvr+0cDgcsFovf/t999x1qamrw4IMP3pI/o4pwv2ppyYIAr1ITEhJk+vTpUldXJyIib731lsyZM0dERPbs2SMAJCYmxu+9uP79+wd8tRfKnj17JCUlxW983bp1AsBrycnJERGR9evXCwC58847veZQVlYmIiI2m03mzp0rycnJEhcXJ0OGDJHt27f7nWPq1KmydOlSv/HXXntNnn/+ea8xtPJXqWGfQIsm6RGc5/tonux2u2RkZEhFRUXA9Tfr7NmzEh0dLRcuXLgtxw+mvr5ehg4dKkVFRV7jdrtd+vbtK1VVVV7jrT24iHtIDcZiseD06dNISUm5Lcfv06cP5s2bh5EjR2LBggW35Ry+Nm/ejLS0NCQlJeGpp57yWmexWFBSUoLOnTurzOVWiahvoklPT8fatWvxyCOPhHtKrVZr/yaaiAqOQmvtwf1uHlIpMjA4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFIVER+bb7Vaq4wxSeGeRySwWq1V4Z5DcyLi8+FaG2PMNABjRWRauOcSafiQSqoYHKlicKSKwZEqBkeqGBypYnCkisGRKgZHqhgcqWJwpIrBkSoGR6oYHKlicGFijJlvjPmzMeaqMWZ9uOejJSL+AebvVAWAlQDGAYgJ81zUMLgwEZE/AYAxZhiAbmGejho+pJIqBkeqGBypYnCkii8awsQYE42m338UgChjjBWAU0Sc4Z3Z7cUrXPjkAbAByAUw1fVzXlhnpIBXuDARkRUAVoR5Gup4hSNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVv6D3xpwEUBfuSUQiIyLhnkNIMTExlXa7PSnc84gEVqu1ymazJYd7HsFERHDGGImEebYGxhiIiAn3PILhczhSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UnXTwRljLMaY08aY6/6IKGNMT2PMCmNM/1Dbdu7cGRMnTryxSV6n4uJiJCQkYPjw4aioqAi5/ahRo7B79+6bPq/dbkdBQQE+/vjjZrfbsmULevTogfbt29/0OVvKGHPZGFNmjHk6wLo3jDFzWnQgEbmpBcCzAP7T4/b7AP7LZ5ttAN7yGUsGcB7AHgAXAXQPcvw4AHLs2DHxNH78eImNjXUvbdu2lQEDBrjXHzt2TB566CGJj4+Xrl27Sn5+vgSyYsUKASA7d+70Gm9oaJDMzExZvXp1wP08ZWVlyeeffx5wne88Y2NjxWKxCAApKytzb+d0OiU7O1uGDRsm8fHxsn379qDnGzp0qN+8KioqZOLEiQLg1yVdvH+PrwP4GkAtgBIA033WjwHwBYCfAVwA8C/if1/MB3A8wHgKgHIAd/iu89s21AYhDwCcAjDC43YygGoAo123JwMoA3CXxzbxAI4BeNl1+48ATgPoEOD4aQDE4XAEvQNEmu50z6gyMjLkpZdeEqfTKaWlpZKcnCwffPCB1z6lpaUyYMAASUlJ8QtORCQnJ0fy8vKaPe+v5w4WnC+n0ylZWVkyffp0r/FZs2bJuHHjpL6+Xnbv3i3Jycly6NChgMdIT0/3m29lZaUUFRU1F1w+gH5oelR7AMBlAMNd69oC+AnAMwAMgPsB/AJgsM8xRgEol8Ad7ATwRKB1XtuF3AD4C4DFriAuA1gHwOpa1x2ADUC0zz4zAJS61lcBGO+xzgLgcwCLffaZC+AggFif8Z4A5Nq1a0HvxG+++UbatGkjFy5ccI/FxMTIV1995b79xBNPSEFBgdd+48ePl48++kjS0tICBjdz5kzJzc0Net5fXU9wL774ogwcOFDq6urcY7m5uZKdnS12u909dvDgQenZs6eUlJT4HaNbt26ya9eugMcPFpzvAuC/AfzR9XOSa587PdYfATDFZ5+HAXwf5HhLAKxr7pxyHcGdApAKIBHAAQArXesmAPgqyH47APwIYEOoczRzbgNgDoBm7kKR/Px8ycrK8hpbvHixLFq0SBoaGqSkpES6du0qhw8fdq/fvHmzZGdni4gEDW7ZsmXy8MMPyy+//NLs+Vsa3LZt2yQhIUHOnTsXcttgTp48KdHR0fL1118HXN+S4ADEAPje50KwCcA8AFEAMgH8ACDVZ7+7ATgB3BvgmI8B+CLYOd3bhdygKbg5Hrf/DsB518//COD/guyX5/qD/yHUOZo5948AHKGC69mzp6xbt85r7MCBA9KzZ0+JiooSALJs2TL3utraWunVq5f7ihgsuJqaGundu7e0adNG3n///aDnb0lwpaWl0q5dO9m6dWuz2zXn8ccfFwCycOHCoNu0MLgNAP4Hro/cdY1NdD0aOV3LPwfZ93XX8bf5jP8BwIVg53RvF3KDpuAmeNy+B4BNfovP7woHoDeAKwCK0PSJ321DnSfIudsAeLG54Pbt2yexsbFSW1vrHquurpa4uDjZsGGDOBwOKS8vlwceeECKiopERGThwoVez/eCBbdq1Sq577775MqVK0HPLxI6OJvNJvfee2+zobTUgQMHpG3btvL9998HXB8qOAD/BuAogHiPsX4A6gGMc/3O+6LpBcYEn307A2gAMDLAcW/bFe5RjytcN/g8h3M9DO4BsMI1+f8FkBfqPM2cvy8AaWxsDPgLnj17tkybNs1r7MiRI9KuXTuvsdWrV8uECRNERGTw4MHSoUMHSUpKkqSkJGnTpo20b99eCgsL/Y79wgsvBDyvp1DBzZo1S0aMGBHyhU9LJScny549ewKuay44NL1wOAWfF2cAngBwzGfs3+H/zsIIAJd8j+ta16LncC19H26eMaabMSYRwEsAitF02bno+pvwNx7b/iuAjgAKRKQRwD8BeNEY06+F5/J1FQCuXbvmt8Jms2HLli2YMWOG13ifPn0gIti0aRMaGxtRWVmJ4uJiDB48GACwa9cunDp1CsePH8fx48fRpUsXvP3225g3b57XcRwOBywWyw1Ou8k777yDDz/8EJs3b0Z09K35WgyLxYKGhga/cbvd7rWZMcb66w1jzGIA/4CmpzjVPrseA9DbGDPGNOkJ4O8BnPDZri1c90cAWQC2h5x8qCLh/Sr1Cpoe/z1fzcwD8B+un1Nd2zzoc4zlAPbB4zlDSxc0vdKVq1ev+v1t3rRpk3Tv3j3g1W/Xrl3u97SSkpJk9uzZXq8MPQV7SJ06daosXbo04D6emrvC9ejRQ6Kjo/3ei4uNjZW9e/eGPHawY+7YscNvHL9d3dyL/PZ7FFcsv3gsL3msfwpNV79aNL0v+iqANuJ9X/wtgG/F/z5Kce1z8+/DuYJ7pJn1FleMKaGOdSMLgDsByL59+27ozrlR9fX1MnToUPfzvuZcz9sit0JmZqasXLky4F80z8hu9YKm59OHA4yvAjC3RcdowUmaDU5jASBpaWkyadKklt4nN6W4uFg6deokjz76qFy6dCnk9trBbd++XQYMGCCdOnXyW3e7ggNwCcBX8Hkhcb1LyG+iMcb8BcBsEfm02Q1vo9b+TTTr16/HqFGjkJ6eHu6ptPpvouFXH/3OtPbg+M+TSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDI1UMjlQxOFLF4EjVrfkM0NvMarVWGWOSwj2PSGC1WqvCPYfmRMTHdbU2xphpAMaKyLRwzyXS8CGVVDE4UsXgSBWDI1UMjlQxOFLF4EgVgyNVDI5UMThSxeBIFYMjVQyOVDE4UsXgSBWDCxNjzHxjzJ+NMVeNMevDPR8tEfEvfn+nKgCsBDAOQEyY56KGwYWJiPwJAIwxwwB0C/N01PAhlVQxOFLF4EgVgyNVfNEQJsaYaDT9/qMARBljrACcIuIM78xuL17hwicPgA1ALoCprp/zwjojBbzChYmIrACwIszTUMcrHKlicKSKwZEqBkeqGBypYnCkisGRKgZHqhgcqWJwpIrB3ZgKACfCPYlIxA+VJlW8wpEqBkeqGBypYnCkisGRKgZHqhgcqWJwpIrBkSoGR6oYHKlicKSKwZEqBkeqGBypYnCkisGRKgZHqhgcqfp/UtuDHK9RLvkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASUUlEQVR4nO3dfbBcdX3H8fcnl5sH8iCJeTCEEJSHSgQb9IIotIZSEHGYoCMMmeokHWr8QyhOGZXBVqDTDikVH6t0omQID6JWRGKNLWkoRrRQLoghCgKmIY8mgQgkQZJ7b779Y0+czfXu2Zvds/fsze/zmrlzd8/3nD3f3dxPztk95+xPEYGZHf5GlN2AmQ0Nh90sEQ67WSIcdrNEOOxmiXDYzRLhsB9mJF0v6c4Gl/0jST+TtEvSXxfdW9EkHStpt6SOsnsZDhz2gkg6W9JPJb0saaekn0g6vey+DtEngQcjYnxEfKnsZuqJiA0RMS4i+sruZThw2AsgaQLw78CXgUnADOAGYG+ZfTVgFvCLWsV22oJKOqLM5Ycjh70YJwFExN0R0RcRv4uI+yNiDYCk4yU9IOlFSS9IukvSUQcWlrRe0ickrZG0R9KtkqZJ+mG2S/1fkiZm8x4nKSQtkrRF0lZJV9dqTNKZ2R7HS5J+LmlujfkeAM4B/iXbNT5J0m2SbpG0QtIe4BxJr5N0u6Qdkp6X9LeSRmSPsTDbo/l8tr51kt6VTd8oabukBTm9PijpRkn/m+0h3SdpUr/nfbmkDcADVdOOyOY5WtLybM/qOUkfqXrs6yV9R9Kdkl4BFg7qX/ZwEhH+afIHmAC8CCwD3gtM7Fc/ATgPGAVMAVYDX6iqrwceBqZR2SvYDjwOnJYt8wBwXTbvcUAAdwNjgVOBHcCfZ/XrgTuz2zOyvi6k8h/7edn9KTWex4PAX1Xdvw14GTgrW340cDtwHzA+6+UZ4PJs/oVAL/CXQAfwD8AG4CvZ8zgf2AWMy1n/ZuCU7LndU/VcDjzv27PamKppR2Tz/Aj4atbnnOx1ObfqdekBLs6ey5iy/26G/O+07AYOlx/g5Cwcm7I/+OXAtBrzXgz8rOr+euAvqu7fA9xSdf9K4HvZ7QN/4G+uqt8E3Jrdrg77p4A7+q37P4EFNfoaKOy3V93voPLWZHbVtI9SeZ9/IOzPVtVOzXqdVjXtRWBOzvoXV92fDezL1nvgeb+pqv77sAMzgT5gfFX9RuC2qtdlddl/J2X+eDe+IBHxVEQsjIhjqGyZjga+ACBpqqRvStqc7ULeCUzu9xDbqm7/boD74/rNv7Hq9vPZ+vqbBVyS7VK/JOkl4Gxg+iE8ter1TAZGZuurXveMqvv9+yYi6j2XWut7Hujk4NdqIwM7GtgZEbtyequ1bBIc9haIiKepbBVPySbdSGUL9NaImAB8CFCTq5lZdftYYMsA82yksmU/qupnbEQsPoT1VF8W+QKVXeFZ/da9+RAer57+z6snW+9A/VTbAkySND6nt6Qv8XTYCyDpzZKulnRMdn8mMJ/K+3CovL/dDbwkaQbwiQJW+3eSjpT0Firvkb81wDx3AhdJeo+kDkmjJc090Oehisohrm8D/yhpvKRZwN9k6ynKhyTNlnQk8PfAd2IQh9YiYiPwU+DG7Hm+FbgcuKvA3oY1h70Yu4B3AI9kn1o/DKwFDnxKfgPwNiofdv0A+G4B6/wR8BywCvhsRNzff4YsAPOAa6l8WLWRyn80zfy7XwnsAdYBDwHfAJY28Xj93UFlr+g3VD5oO5STe+ZTeR+/BbiXyoeaKwvsbVhT9uGFDROSjgP+D+iMiN5yuymWpAepfLj49bJ7ORx5y26WCIfdLBHejTdLhLfsZokY0osBRmpUjGbsUK7SLCmvsYd9sXfAcziavXLoAuCLVE5n/Hq9kzVGM5Z36NxmVmlmOR6JVTVrDe/GZ5c7foXKhR+zgfmSZjf6eGbWWs28Zz8DeC4i1kXEPuCbVE7gMLM21EzYZ3DwhQWbOPiiAwCy6667JXX3DLvvcjA7fDQT9oE+BPiD43gRsSQiuiKiq5NRTazOzJrRTNg3cfAVSscw8JVXZtYGmgn7o8CJkt4oaSRwGZUvbDCzNtTwobeI6JV0BZVvPukAlkZEzS8rNLNyNXWcPSJWACsK6sXMWsiny5olwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSKGdMhmK8GIjtzyng905dbvuPnm3PpV6z+YW993/m9r1mKvhwMbSt6ymyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJ8HH2w9zL80/PrT/8z/9a5xHG5VavmLEqt/6FCXNr1vp27KizbitSU2GXtB7YBfQBvRGRf4aGmZWmiC37ORHxQgGPY2Yt5PfsZoloNuwB3C/pMUmLBppB0iJJ3ZK6e/C50GZlaXY3/qyI2CJpKrBS0tMRsbp6hohYAiwBmKBJ0eT6zKxBTW3ZI2JL9ns7cC9wRhFNmVnxGg67pLGSxh+4DZwPrC2qMTMrVjO78dOAeyUdeJxvRMR/FNKVHZIRp7y5Zu3HN32lztL517u/un9fbv2G5z6QW59w9KjaRR9nH1INhz0i1gF/XGAvZtZCPvRmlgiH3SwRDrtZIhx2s0Q47GaJ8CWuw0Gdr4M+7Y5f1qx1Kn/ZZl0287Hc+sovnVyz1vvBKbnL+hLYYnnLbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwsfZh4GOcWNz61e+fkVONf+roOvZWecS1529+b3Nm/pEzdrSufNylx33bz7OXiRv2c0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRPg4+zAQfX259akdR7Zs3SMrXxVe0yljNuXWH959fM1az8KducvqeyNz69GTfw6AHcxbdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sET7OPgxsuCp/sNwO/aRl6+6LyK2v25f/3e+XTXykZu2io36Wu+y1388fDnrsBety63awult2SUslbZe0tmraJEkrJT2b/Z7Y2jbNrFmD2Y2/Dbig37RrgFURcSKwKrtvZm2sbtgjYjXQ/7zGecCy7PYy4OKC+zKzgjX6Ad20iNgKkP2eWmtGSYskdUvq7mFvg6szs2a1/NP4iFgSEV0R0dXJqFavzsxqaDTs2yRNB8h+by+uJTNrhUbDvhxYkN1eANxXTDtm1ip1j7NLuhuYC0yWtAm4DlgMfFvS5cAG4JJWNpm6yU/2lrbuX/VMyK3v7hudWz9StXufMzL/z+8Hp9yVW790xNm5dfbnfw9AauqGPSLm1yidW3AvZtZCPl3WLBEOu1kiHHazRDjsZolw2M0S4Utch4GxP/5Vbr0nah9i6lRH7rLL9+R/DfWd296ZW+/dn7+9uOx1tWsdyl92XJ0zLusNZd33yiu59dR4y26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcLH2YeB/Xt+l1v/4avja9Y6tD932c98eWFu/ahne3LrY3/5m9z61bd9sGbtvpO+n7tsvXMEXjvzpPzl7+/OrafGW3azRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBE+zj4MqDP/n2ln37iataXPn5W77NF3PZ1b73ux/zB/B6v3Jdd7bzq9Zq3z1vzj6PW8ffFjufU19zf18Icdb9nNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0T4OPtwcMKxueV3j6l9QHnxy+/JXXbMzvWNdDRoYx7KP47fjLccuTm3voajW7bu4ajull3SUknbJa2tmna9pM2Snsh+Lmxtm2bWrMHsxt8GXDDA9M9HxJzsZ0WxbZlZ0eqGPSJWA/nnTJpZ22vmA7orJK3JdvMn1ppJ0iJJ3ZK6e9jbxOrMrBmNhv0W4HhgDrAVuLnWjBGxJCK6IqKrs85AfWbWOg2FPSK2RURfROwHvgacUWxbZla0hsIuaXrV3fcDa2vNa2btoe5xdkl3A3OByZI2AdcBcyXNAQJYD3y0hT0m7/nP5P8zTemoXY9na1/rXpkhGmlp0PZ8Z3LLHnvKEfXGX/dx9mp1wx4R8weYfGsLejGzFvLpsmaJcNjNEuGwmyXCYTdLhMNulghf4joM/Mmxv86tj9HImrV/uvSO3GVv+fQJDfU0WKtPvbdlj/0/u09s2WMfjrxlN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4ePsw8Cn37Ayt96h2pexnti5I//BVedYdZ1LYEfMmZ2/PE/UqTfusXeNrzPHqy1b93DkLbtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulggfZx8GntyX/3XMxx7xWs3arCOUu+zmT74ztz7tvE259VWzv5Fbb8YFT78vtx6v5g/ZbAfzlt0sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S8RghmyeCdwOvAHYDyyJiC9KmgR8CziOyrDNl0bEb1vXarpu2XRObv19J/2wZm3ciNG5y6696qsN9VSEZ3r25Nbjz3wcvUiD2bL3AldHxMnAmcDHJM0GrgFWRcSJwKrsvpm1qbphj4itEfF4dnsX8BQwA5gHLMtmWwZc3Komzax5h/SeXdJxwGnAI8C0iNgKlf8QgKlFN2dmxRl02CWNA+4BPh4RrxzCcoskdUvq7mFvIz2aWQEGFXZJnVSCfldEfDebvE3S9Kw+Hdg+0LIRsSQiuiKiq5NRRfRsZg2oG3ZJAm4FnoqIz1WVlgMLstsLgPuKb8/MijKYS1zPAj4MPCnpwPcCXwssBr4t6XJgA3BJa1q0rbvqfWVy+9rQu7tm7cpZZw9hJ1Y37BHxEFDrouhzi23HzFrFZ9CZJcJhN0uEw26WCIfdLBEOu1kiHHazRPirpIeByRc9k1t/eF1fzdqZozuKbucgm3KOowN85FgfS28X3rKbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZonwcfbDwHVvenvNWseUKbnL7rjohNz61BXrcuu9v9mWW7f24S27WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIH2c/zPXt2JFbn7Q0v95bZDNWKm/ZzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNE1A27pJmS/lvSU5J+IemqbPr1kjZLeiL7ubD17ZpZowZzUk0vcHVEPC5pPPCYpJVZ7fMR8dnWtWdmRakb9ojYCmzNbu+S9BQwo9WNmVmxDuk9u6TjgNOAR7JJV0haI2mppIk1llkkqVtSdw97m2rWzBo36LBLGgfcA3w8Il4BbgGOB+ZQ2fLfPNByEbEkIroioquTUQW0bGaNGFTYJXVSCfpdEfFdgIjYFhF9EbEf+BpwRuvaNLNmDebTeAG3Ak9FxOeqpk+vmu39wNri2zOzogzm0/izgA8DT0p6Ipt2LTBf0hwggPXAR1vSoZkVYjCfxj8EaIDSiuLbMbNW8Rl0Zolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBGKiKFbmbQDeL5q0mTghSFr4NC0a2/t2he4t0YV2dusiJgyUGFIw/4HK5e6I6KrtAZytGtv7doXuLdGDVVv3o03S4TDbpaIssO+pOT152nX3tq1L3BvjRqS3kp9z25mQ6fsLbuZDRGH3SwRpYRd0gWSfiXpOUnXlNFDLZLWS3oyG4a6u+RelkraLmlt1bRJklZKejb7PeAYeyX11hbDeOcMM17qa1f28OdD/p5dUgfwDHAesAl4FJgfEb8c0kZqkLQe6IqI0k/AkPSnwG7g9og4JZt2E7AzIhZn/1FOjIhPtUlv1wO7yx7GOxutaHr1MOPAxcBCSnztcvq6lCF43crYsp8BPBcR6yJiH/BNYF4JfbS9iFgN7Ow3eR6wLLu9jMofy5Cr0VtbiIitEfF4dnsXcGCY8VJfu5y+hkQZYZ8BbKy6v4n2Gu89gPslPSZpUdnNDGBaRGyFyh8PMLXkfvqrO4z3UOo3zHjbvHaNDH/erDLCPtBQUu10/O+siHgb8F7gY9nuqg3OoIbxHioDDDPeFhod/rxZZYR9EzCz6v4xwJYS+hhQRGzJfm8H7qX9hqLedmAE3ez39pL7+b12GsZ7oGHGaYPXrszhz8sI+6PAiZLeKGkkcBmwvIQ+/oCksdkHJ0gaC5xP+w1FvRxYkN1eANxXYi8HaZdhvGsNM07Jr13pw59HxJD/ABdS+UT+18Cny+ihRl9vAn6e/fyi7N6Au6ns1vVQ2SO6HHg9sAp4Nvs9qY16uwN4ElhDJVjTS+rtbCpvDdcAT2Q/F5b92uX0NSSvm0+XNUuEz6AzS4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLx/wYjfwNS3Jl2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    path.draw()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
