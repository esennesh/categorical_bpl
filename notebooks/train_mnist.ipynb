{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 135120.406250\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -42563.011719\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -61543.453125\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -67316.367188\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -75196.820312\n",
      "    epoch          : 1\n",
      "    loss           : -53096.688263845914\n",
      "    val_loss       : -74768.88828125\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -59511.015625\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -74395.601562\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -86811.179688\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -93696.367188\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -93482.593750\n",
      "    epoch          : 2\n",
      "    loss           : -86687.9041615099\n",
      "    val_loss       : -99473.820703125\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -86922.765625\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -106492.789062\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -110220.335938\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -115635.843750\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -117949.859375\n",
      "    epoch          : 3\n",
      "    loss           : -110793.89866955446\n",
      "    val_loss       : -122952.207421875\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -118305.140625\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -127655.828125\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -134529.906250\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -130785.218750\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -142591.375000\n",
      "    epoch          : 4\n",
      "    loss           : -133760.77103960395\n",
      "    val_loss       : -145331.372265625\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -140862.671875\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -146927.203125\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -161998.343750\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -156671.875000\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -163023.218750\n",
      "    epoch          : 5\n",
      "    loss           : -155603.5083539604\n",
      "    val_loss       : -166510.6765625\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -165513.859375\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -167884.234375\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -172202.453125\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -182976.218750\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -183925.875000\n",
      "    epoch          : 6\n",
      "    loss           : -176249.1676980198\n",
      "    val_loss       : -186377.590234375\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -188826.718750\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -186192.421875\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -198555.984375\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -190689.500000\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -203729.593750\n",
      "    epoch          : 7\n",
      "    loss           : -195607.3785581683\n",
      "    val_loss       : -205097.719921875\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -211499.093750\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -208065.312500\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -218598.421875\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -210979.500000\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -224473.484375\n",
      "    epoch          : 8\n",
      "    loss           : -213815.84204826734\n",
      "    val_loss       : -222702.192578125\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -232418.921875\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -225222.500000\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -215643.468750\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -245180.000000\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -235819.093750\n",
      "    epoch          : 9\n",
      "    loss           : -230895.99412128713\n",
      "    val_loss       : -239185.662890625\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -252248.296875\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -244588.796875\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -248852.406250\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -249516.468750\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -255087.218750\n",
      "    epoch          : 10\n",
      "    loss           : -246876.270884901\n",
      "    val_loss       : -254697.180078125\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -270781.406250\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -254709.578125\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -239690.375000\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -282435.250000\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -269493.906250\n",
      "    epoch          : 11\n",
      "    loss           : -261911.50232054456\n",
      "    val_loss       : -269254.887890625\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -288036.687500\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -280633.312500\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -251393.796875\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -276081.750000\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -281966.593750\n",
      "    epoch          : 12\n",
      "    loss           : -276020.40129950497\n",
      "    val_loss       : -282603.04765625\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -304858.187500\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -282615.812500\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -276116.718750\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -263890.906250\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -292342.062500\n",
      "    epoch          : 13\n",
      "    loss           : -289278.77196782175\n",
      "    val_loss       : -295552.88984375\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -320705.250000\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -295186.875000\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -311267.843750\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -305442.843750\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -310811.250000\n",
      "    epoch          : 14\n",
      "    loss           : -301818.9795792079\n",
      "    val_loss       : -307771.08203125\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -335148.437500\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -316003.000000\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -324958.031250\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -313192.968750\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -318706.375000\n",
      "    epoch          : 15\n",
      "    loss           : -313617.0553836634\n",
      "    val_loss       : -319214.31796875\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -348655.750000\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -317840.781250\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -321295.343750\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -337471.937500\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -328528.250000\n",
      "    epoch          : 16\n",
      "    loss           : -324752.68842821784\n",
      "    val_loss       : -330010.5703125\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -316606.406250\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -327889.562500\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -299003.937500\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -350269.031250\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -343466.468750\n",
      "    epoch          : 17\n",
      "    loss           : -335294.9733910891\n",
      "    val_loss       : -340014.96953125\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -332537.937500\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -350990.625000\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -306720.500000\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -359812.812500\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -344586.312500\n",
      "    epoch          : 18\n",
      "    loss           : -345224.6547029703\n",
      "    val_loss       : -349573.378125\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -385429.906250\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -345487.593750\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -360156.031250\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -314972.750000\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -362619.062500\n",
      "    epoch          : 19\n",
      "    loss           : -354614.00495049503\n",
      "    val_loss       : -358918.80390625\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -396551.937500\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -354730.437500\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -345107.093750\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -346835.968750\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -363978.437500\n",
      "    epoch          : 20\n",
      "    loss           : -363444.0278465347\n",
      "    val_loss       : -367424.6234375\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -407080.812500\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -378640.375000\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -378289.093750\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -367299.125000\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -373245.187500\n",
      "    epoch          : 21\n",
      "    loss           : -371890.53248762374\n",
      "    val_loss       : -375504.3625\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -417651.187500\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -372546.375000\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -331251.781250\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -423088.031250\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -380345.562500\n",
      "    epoch          : 22\n",
      "    loss           : -379869.7896039604\n",
      "    val_loss       : -383588.275\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -426609.250000\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -377358.875000\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -342542.031250\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -387137.250000\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -386880.687500\n",
      "    epoch          : 23\n",
      "    loss           : -387487.01794554456\n",
      "    val_loss       : -390494.53359375\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -436593.250000\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -385163.031250\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -409911.062500\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -393328.468750\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -394418.031250\n",
      "    epoch          : 24\n",
      "    loss           : -394655.3626237624\n",
      "    val_loss       : -398019.61875\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -444597.250000\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -395675.687500\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -415895.500000\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -399222.000000\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -397031.250000\n",
      "    epoch          : 25\n",
      "    loss           : -401453.9588490099\n",
      "    val_loss       : -404285.07890625\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -452656.375000\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -397009.531250\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -414731.156250\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -357155.312500\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -419187.500000\n",
      "    epoch          : 26\n",
      "    loss           : -408064.3756188119\n",
      "    val_loss       : -410840.890625\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -459889.187500\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -404149.562500\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -432884.312500\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -431582.843750\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -411689.312500\n",
      "    epoch          : 27\n",
      "    loss           : -414217.3025990099\n",
      "    val_loss       : -417114.37109375\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -429837.000000\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -412066.843750\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -394930.718750\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -470847.375000\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -430395.687500\n",
      "    epoch          : 28\n",
      "    loss           : -420150.25\n",
      "    val_loss       : -422365.5625\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -473535.750000\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -438740.656250\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -445528.312500\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -478050.937500\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -421592.437500\n",
      "    epoch          : 29\n",
      "    loss           : -425823.5853960396\n",
      "    val_loss       : -428193.13125\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -480741.156250\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -421408.250000\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -409559.062500\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -439761.875000\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -429406.656250\n",
      "    epoch          : 30\n",
      "    loss           : -431249.94399752474\n",
      "    val_loss       : -433841.409375\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -487083.312500\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -423165.437500\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -456577.656250\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -415428.156250\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -411573.406250\n",
      "    epoch          : 31\n",
      "    loss           : -436535.39975247526\n",
      "    val_loss       : -438252.28046875\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -493292.406250\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -416920.062500\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -463060.375000\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -498550.312500\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -436852.156250\n",
      "    epoch          : 32\n",
      "    loss           : -441522.87747524754\n",
      "    val_loss       : -443491.5734375\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -499428.125000\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -434650.375000\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -441429.375000\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -502741.812500\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -443653.656250\n",
      "    epoch          : 33\n",
      "    loss           : -446298.57518564357\n",
      "    val_loss       : -448234.546875\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -504612.812500\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -466438.875000\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -444911.000000\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -444949.906250\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -451089.406250\n",
      "    epoch          : 34\n",
      "    loss           : -450982.44306930696\n",
      "    val_loss       : -452695.4671875\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -510120.218750\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -464160.281250\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -428041.718750\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -451776.312500\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -478554.500000\n",
      "    epoch          : 35\n",
      "    loss           : -455424.03558168316\n",
      "    val_loss       : -457348.32109375\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -517038.125000\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -476362.156250\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -431094.718750\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -480886.500000\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -458384.218750\n",
      "    epoch          : 36\n",
      "    loss           : -459749.099009901\n",
      "    val_loss       : -461402.59296875\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -520944.875000\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -478829.750000\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -398244.312500\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -487178.656250\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -476638.875000\n",
      "    epoch          : 37\n",
      "    loss           : -463895.6318069307\n",
      "    val_loss       : -465386.09140625\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -525359.687500\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -484096.437500\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -490455.375000\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -491053.875000\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -464378.937500\n",
      "    epoch          : 38\n",
      "    loss           : -467933.2691831683\n",
      "    val_loss       : -469408.17109375\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -529899.250000\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -458403.562500\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -492760.625000\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -443749.906250\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -485390.812500\n",
      "    epoch          : 39\n",
      "    loss           : -471780.59993811883\n",
      "    val_loss       : -473345.07734375\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -533507.812500\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -444824.531250\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -411611.125000\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -470350.750000\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -469447.312500\n",
      "    epoch          : 40\n",
      "    loss           : -475584.9368811881\n",
      "    val_loss       : -476844.5234375\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -495076.437500\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -500997.843750\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -448374.750000\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -472350.375000\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -473003.875000\n",
      "    epoch          : 41\n",
      "    loss           : -479214.7462871287\n",
      "    val_loss       : -480611.98671875\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -542890.187500\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -492241.875000\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -495758.468750\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -477604.500000\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -482125.000000\n",
      "    epoch          : 42\n",
      "    loss           : -482731.55136138614\n",
      "    val_loss       : -483925.63984375\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -499341.562500\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -478304.312500\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -480106.093750\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -460131.875000\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -481452.437500\n",
      "    epoch          : 43\n",
      "    loss           : -486770.4597772277\n",
      "    val_loss       : -490117.06796875\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -552699.375000\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -479692.718750\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -460771.187500\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -489218.312500\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -502735.437500\n",
      "    epoch          : 44\n",
      "    loss           : -491371.46627475246\n",
      "    val_loss       : -493413.61796875\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -556968.062500\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -482329.625000\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -519353.656250\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -463744.562500\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -493002.968750\n",
      "    epoch          : 45\n",
      "    loss           : -495981.8431311881\n",
      "    val_loss       : -497377.44921875\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -559799.687500\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -512516.531250\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -472863.031250\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -562985.000000\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -494950.875000\n",
      "    epoch          : 46\n",
      "    loss           : -499498.6271658416\n",
      "    val_loss       : -499850.5953125\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -563968.875000\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -524498.250000\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -528702.000000\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -499103.000000\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -512060.937500\n",
      "    epoch          : 47\n",
      "    loss           : -503471.192759901\n",
      "    val_loss       : -505065.990625\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -568825.625000\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -474253.531250\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -514861.875000\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -514094.187500\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -503147.531250\n",
      "    epoch          : 48\n",
      "    loss           : -506697.32549504953\n",
      "    val_loss       : -508391.9828125\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -572599.000000\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -519590.062500\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -457194.187500\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -504512.750000\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -502291.156250\n",
      "    epoch          : 49\n",
      "    loss           : -511168.885210396\n",
      "    val_loss       : -512905.5125\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -576569.937500\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -499819.062500\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -521543.937500\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -506376.062500\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -510714.187500\n",
      "    epoch          : 50\n",
      "    loss           : -515448.8511757426\n",
      "    val_loss       : -516708.640625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0714_125235/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -581018.250000\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -506826.656250\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -465550.281250\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -511159.156250\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -523958.687500\n",
      "    epoch          : 51\n",
      "    loss           : -518925.4668935643\n",
      "    val_loss       : -520220.865625\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -584860.500000\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -535498.437500\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -519769.531250\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -588192.625000\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -531025.625000\n",
      "    epoch          : 52\n",
      "    loss           : -522010.04610148515\n",
      "    val_loss       : -523028.15\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -538469.062500\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -511920.000000\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -515128.625000\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -470181.718750\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -520067.750000\n",
      "    epoch          : 53\n",
      "    loss           : -525091.0816831683\n",
      "    val_loss       : -527447.2890625\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -591125.250000\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -515591.281250\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -551475.000000\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -534961.125000\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -526547.500000\n",
      "    epoch          : 54\n",
      "    loss           : -529772.4975247525\n",
      "    val_loss       : -530136.53671875\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -595739.875000\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -520870.312500\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -539115.750000\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -556415.875000\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -527925.812500\n",
      "    epoch          : 55\n",
      "    loss           : -534341.1608910891\n",
      "    val_loss       : -535949.26484375\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -598480.125000\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -554047.562500\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -484736.500000\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -503688.687500\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -532367.312500\n",
      "    epoch          : 56\n",
      "    loss           : -537914.7407178218\n",
      "    val_loss       : -539890.72578125\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -602565.750000\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -511904.750000\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -490155.656250\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -535660.125000\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -533393.125000\n",
      "    epoch          : 57\n",
      "    loss           : -541686.3678836634\n",
      "    val_loss       : -542803.32109375\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -562086.750000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -534168.625000\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -567858.312500\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -568420.125000\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -538171.062500\n",
      "    epoch          : 58\n",
      "    loss           : -545487.7320544554\n",
      "    val_loss       : -544746.36953125\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -610522.625000\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -562035.875000\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -496360.031250\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -613506.500000\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -539723.562500\n",
      "    epoch          : 59\n",
      "    loss           : -548308.6299504951\n",
      "    val_loss       : -549951.30546875\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -613660.937500\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -570076.750000\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -519640.875000\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -554781.812500\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -555679.375000\n",
      "    epoch          : 60\n",
      "    loss           : -549521.4721534654\n",
      "    val_loss       : -550731.01328125\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -615969.562500\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -536525.312500\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -558578.875000\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -546913.750000\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -546907.500000\n",
      "    epoch          : 61\n",
      "    loss           : -554018.5250618812\n",
      "    val_loss       : -555351.78984375\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -620232.562500\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -544213.937500\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -561452.687500\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -553270.187500\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -552384.437500\n",
      "    epoch          : 62\n",
      "    loss           : -557757.4542079208\n",
      "    val_loss       : -558469.465625\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -622448.375000\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -528206.187500\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -525652.437500\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -625116.500000\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -564906.000000\n",
      "    epoch          : 63\n",
      "    loss           : -560185.6577970297\n",
      "    val_loss       : -558790.6078125\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -625926.812500\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -584742.625000\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -554483.812500\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -531664.187500\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -509993.187500\n",
      "    epoch          : 64\n",
      "    loss           : -562306.2852722772\n",
      "    val_loss       : -564265.47421875\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -628306.000000\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -529845.125000\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -560202.687500\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -558608.250000\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -561374.937500\n",
      "    epoch          : 65\n",
      "    loss           : -566012.4405940594\n",
      "    val_loss       : -567155.75546875\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -632627.437500\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -510878.000000\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -517506.625000\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -564674.250000\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -575426.250000\n",
      "    epoch          : 66\n",
      "    loss           : -568927.2685643565\n",
      "    val_loss       : -570272.19609375\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -635967.437500\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -550933.500000\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -576649.750000\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -539283.687500\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -565729.937500\n",
      "    epoch          : 67\n",
      "    loss           : -571774.3075495049\n",
      "    val_loss       : -572075.8375\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -640390.312500\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -600200.812500\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -565981.000000\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -641098.187500\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -568613.000000\n",
      "    epoch          : 68\n",
      "    loss           : -573933.0906559406\n",
      "    val_loss       : -574626.5375\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -642613.062500\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -560563.250000\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -574026.500000\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -640074.312500\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -561753.562500\n",
      "    epoch          : 69\n",
      "    loss           : -573845.1516089109\n",
      "    val_loss       : -572830.5265625\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -644207.625000\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -596369.875000\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -567991.062500\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -569893.187500\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -564543.125000\n",
      "    epoch          : 70\n",
      "    loss           : -576983.5758044554\n",
      "    val_loss       : -578303.425\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -647058.000000\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -563145.125000\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -605846.125000\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -650712.375000\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -585470.687500\n",
      "    epoch          : 71\n",
      "    loss           : -581635.6652227723\n",
      "    val_loss       : -582305.284375\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -606038.562500\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -551941.437500\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -577276.000000\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -576410.187500\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -575467.250000\n",
      "    epoch          : 72\n",
      "    loss           : -584263.9325495049\n",
      "    val_loss       : -584201.33828125\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -653004.875000\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -570424.500000\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -576035.750000\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -575575.000000\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -583400.000000\n",
      "    epoch          : 73\n",
      "    loss           : -586739.4412128713\n",
      "    val_loss       : -587191.68046875\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -654343.250000\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -573790.437500\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -580773.125000\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -533464.812500\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -591777.750000\n",
      "    epoch          : 74\n",
      "    loss           : -588930.6113861386\n",
      "    val_loss       : -587432.48828125\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -659799.812500\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -575580.875000\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -620643.250000\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -619906.687500\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -590559.125000\n",
      "    epoch          : 75\n",
      "    loss           : -591046.2871287129\n",
      "    val_loss       : -591231.284375\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -578470.250000\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -576954.875000\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -617297.437500\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -664033.125000\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -541478.250000\n",
      "    epoch          : 76\n",
      "    loss           : -593114.6410891089\n",
      "    val_loss       : -594269.334375\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -617585.812500\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -580420.500000\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -555935.125000\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -585416.000000\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -591803.000000\n",
      "    epoch          : 77\n",
      "    loss           : -595774.823019802\n",
      "    val_loss       : -596029.23515625\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -581511.687500\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -581002.812500\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -562005.375000\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -669186.750000\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -595002.625000\n",
      "    epoch          : 78\n",
      "    loss           : -598305.4876237623\n",
      "    val_loss       : -598620.0828125\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -623785.125000\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -583287.375000\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -602078.750000\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -588379.562500\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -589239.125000\n",
      "    epoch          : 79\n",
      "    loss           : -600223.8236386139\n",
      "    val_loss       : -600894.38828125\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -672326.687500\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -626027.250000\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -563120.562500\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -674234.125000\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -605291.687500\n",
      "    epoch          : 80\n",
      "    loss           : -601714.6379950495\n",
      "    val_loss       : -602268.65\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -675630.875000\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -624702.250000\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -635135.625000\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -548549.937500\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -596525.125000\n",
      "    epoch          : 81\n",
      "    loss           : -604312.7172029703\n",
      "    val_loss       : -602236.71015625\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -585659.500000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -598843.375000\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -567399.250000\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -602943.000000\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -545716.250000\n",
      "    epoch          : 82\n",
      "    loss           : -604508.4405940594\n",
      "    val_loss       : -604970.57265625\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -678707.625000\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -629098.625000\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -552371.062500\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -680580.500000\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -612116.750000\n",
      "    epoch          : 83\n",
      "    loss           : -608639.5952970297\n",
      "    val_loss       : -609367.275\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -681823.625000\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -575251.125000\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -639473.875000\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -682356.000000\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -558055.500000\n",
      "    epoch          : 84\n",
      "    loss           : -610874.8564356435\n",
      "    val_loss       : -611303.065625\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -683372.500000\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -642292.625000\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -643347.125000\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -604185.812500\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -609453.250000\n",
      "    epoch          : 85\n",
      "    loss           : -613397.4969059406\n",
      "    val_loss       : -613720.27734375\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -686843.187500\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -631000.187500\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -553316.625000\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -615540.437500\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -613921.812500\n",
      "    epoch          : 86\n",
      "    loss           : -613459.6225247525\n",
      "    val_loss       : -612725.784375\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -687137.750000\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -638564.750000\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -646932.312500\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -575255.937500\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -607149.625000\n",
      "    epoch          : 87\n",
      "    loss           : -616613.2271039604\n",
      "    val_loss       : -617115.771875\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -640181.687500\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -640254.250000\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -580832.937500\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -692968.437500\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -623221.562500\n",
      "    epoch          : 88\n",
      "    loss           : -619184.3063118812\n",
      "    val_loss       : -619774.33984375\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -694289.750000\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -645772.125000\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -580429.000000\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -608838.250000\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -617575.375000\n",
      "    epoch          : 89\n",
      "    loss           : -620941.3892326732\n",
      "    val_loss       : -621778.3421875\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -695549.250000\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -606649.375000\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -580878.312500\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -580801.562500\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -623514.250000\n",
      "    epoch          : 90\n",
      "    loss           : -622175.2438118812\n",
      "    val_loss       : -622783.92265625\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -589059.750000\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -649065.312500\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -615051.625000\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -613121.125000\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -620666.750000\n",
      "    epoch          : 91\n",
      "    loss           : -624719.8372524752\n",
      "    val_loss       : -624699.64609375\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -701552.125000\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -608052.562500\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -625175.375000\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -656547.187500\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -626473.437500\n",
      "    epoch          : 92\n",
      "    loss           : -626723.1646039604\n",
      "    val_loss       : -627041.13828125\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -701264.625000\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -650793.312500\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -615022.437500\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -573061.875000\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -628345.312500\n",
      "    epoch          : 93\n",
      "    loss           : -628940.4350247525\n",
      "    val_loss       : -629463.2078125\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -704406.125000\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -656266.000000\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -627627.312500\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -573472.750000\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -620770.375000\n",
      "    epoch          : 94\n",
      "    loss           : -630635.8997524752\n",
      "    val_loss       : -630581.97734375\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -707467.250000\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -615703.437500\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -593103.250000\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -576832.125000\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -632625.875000\n",
      "    epoch          : 95\n",
      "    loss           : -632615.3527227723\n",
      "    val_loss       : -633392.43203125\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -710036.375000\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -594496.250000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -594318.750000\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -579342.437500\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -619586.375000\n",
      "    epoch          : 96\n",
      "    loss           : -635247.1763613861\n",
      "    val_loss       : -633395.95078125\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -710812.187500\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -620276.625000\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -579081.750000\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -599585.750000\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -634595.750000\n",
      "    epoch          : 97\n",
      "    loss           : -636831.0470297029\n",
      "    val_loss       : -637045.96953125\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -713775.312500\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -617102.000000\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -578952.125000\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -601730.625000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -580384.562500\n",
      "    epoch          : 98\n",
      "    loss           : -638684.5903465346\n",
      "    val_loss       : -639143.30546875\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -716040.750000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -667997.750000\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -668795.937500\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -642948.125000\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -627413.375000\n",
      "    epoch          : 99\n",
      "    loss           : -640821.9925742574\n",
      "    val_loss       : -641942.884375\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -717173.687500\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -674206.812500\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -626841.687500\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -626969.125000\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -638861.250000\n",
      "    epoch          : 100\n",
      "    loss           : -642992.9907178218\n",
      "    val_loss       : -642837.1203125\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0714_125235/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -720639.500000\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -626907.500000\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -587988.812500\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -626639.187500\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -639357.375000\n",
      "    epoch          : 101\n",
      "    loss           : -644142.0154702971\n",
      "    val_loss       : -642815.2359375\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -723134.125000\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -661643.062500\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -601138.125000\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -717364.625000\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -645394.000000\n",
      "    epoch          : 102\n",
      "    loss           : -644206.2165841584\n",
      "    val_loss       : -645540.1625\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -725596.375000\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -634324.250000\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -652155.250000\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -725441.500000\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -654687.312500\n",
      "    epoch          : 103\n",
      "    loss           : -650129.0272277228\n",
      "    val_loss       : -650558.334375\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -727256.937500\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -676499.937500\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -654793.250000\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -594181.562500\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -594373.437500\n",
      "    epoch          : 104\n",
      "    loss           : -651815.0816831683\n",
      "    val_loss       : -652357.02890625\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -730234.000000\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -634036.062500\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -614366.250000\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -597218.500000\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -637303.750000\n",
      "    epoch          : 105\n",
      "    loss           : -654266.7654702971\n",
      "    val_loss       : -652287.55390625\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -728395.625000\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -680402.875000\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -682753.875000\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -599534.875000\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -658946.000000\n",
      "    epoch          : 106\n",
      "    loss           : -654383.5693069306\n",
      "    val_loss       : -656604.6734375\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -733894.062500\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -642031.750000\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -616738.625000\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -658684.625000\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -642969.625000\n",
      "    epoch          : 107\n",
      "    loss           : -657072.5878712871\n",
      "    val_loss       : -657519.0265625\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -686920.187500\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -643026.250000\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -617999.000000\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -603907.562500\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -641467.687500\n",
      "    epoch          : 108\n",
      "    loss           : -660038.9535891089\n",
      "    val_loss       : -659908.6671875\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -692104.750000\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -640752.125000\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -624147.062500\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -666650.000000\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -644980.500000\n",
      "    epoch          : 109\n",
      "    loss           : -662159.0946782178\n",
      "    val_loss       : -662498.95390625\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -739877.187500\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -694202.812500\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -601826.000000\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -741960.750000\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -659846.750000\n",
      "    epoch          : 110\n",
      "    loss           : -663298.7691831683\n",
      "    val_loss       : -663493.01328125\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -741520.312500\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -602930.562500\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -664903.437500\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -743567.000000\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -695362.312500\n",
      "    epoch          : 111\n",
      "    loss           : -663494.4220297029\n",
      "    val_loss       : -664779.25703125\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -743691.250000\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -649110.687500\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -609495.062500\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -744238.687500\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -672232.250000\n",
      "    epoch          : 112\n",
      "    loss           : -667299.2543316832\n",
      "    val_loss       : -666540.63046875\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -744180.187500\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -647419.250000\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -623859.375000\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -653520.000000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -653678.000000\n",
      "    epoch          : 113\n",
      "    loss           : -669814.4047029703\n",
      "    val_loss       : -669594.00546875\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -747631.812500\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -698929.500000\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -674281.625000\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -748014.125000\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -670924.312500\n",
      "    epoch          : 114\n",
      "    loss           : -671566.396039604\n",
      "    val_loss       : -671553.10234375\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -750535.250000\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -704533.375000\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -618534.312500\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -652681.000000\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -678846.500000\n",
      "    epoch          : 115\n",
      "    loss           : -673498.4925742574\n",
      "    val_loss       : -672565.14453125\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -652808.562500\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -654808.437500\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -707557.687500\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -631160.187500\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -672256.000000\n",
      "    epoch          : 116\n",
      "    loss           : -675818.9022277228\n",
      "    val_loss       : -675202.15859375\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -754892.250000\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -651576.500000\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -617494.062500\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -704161.375000\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -613838.500000\n",
      "    epoch          : 117\n",
      "    loss           : -673860.8917079208\n",
      "    val_loss       : -675516.0890625\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -756652.125000\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -657980.250000\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -678116.625000\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -658124.750000\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -624321.125000\n",
      "    epoch          : 118\n",
      "    loss           : -678727.6639851485\n",
      "    val_loss       : -678401.27890625\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -712567.125000\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -628879.687500\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -659603.562500\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -676878.625000\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -679389.625000\n",
      "    epoch          : 119\n",
      "    loss           : -676826.7419554455\n",
      "    val_loss       : -677574.64453125\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -760301.187500\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -658879.875000\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -717158.750000\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -716565.375000\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -686669.187500\n",
      "    epoch          : 120\n",
      "    loss           : -681313.9381188119\n",
      "    val_loss       : -681416.8265625\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -762109.250000\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -662559.687500\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -716303.750000\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -630145.250000\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -687716.625000\n",
      "    epoch          : 121\n",
      "    loss           : -684252.3050742574\n",
      "    val_loss       : -683892.384375\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -765150.000000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -627924.750000\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -629191.875000\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -633090.437500\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -693479.250000\n",
      "    epoch          : 122\n",
      "    loss           : -686303.5160891089\n",
      "    val_loss       : -686238.36015625\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -767254.375000\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -668260.875000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -720696.375000\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -687983.250000\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -692982.937500\n",
      "    epoch          : 123\n",
      "    loss           : -687623.8502475248\n",
      "    val_loss       : -687407.96796875\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -768496.875000\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -671021.312500\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -719048.937500\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -634094.625000\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -689926.625000\n",
      "    epoch          : 124\n",
      "    loss           : -690012.3966584158\n",
      "    val_loss       : -689621.72734375\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -722123.750000\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -667839.000000\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -687145.250000\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -770776.625000\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -692162.125000\n",
      "    epoch          : 125\n",
      "    loss           : -690491.8323019802\n",
      "    val_loss       : -689939.19921875\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -773270.500000\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -724744.625000\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -635688.187500\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -638635.250000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -672813.187500\n",
      "    epoch          : 126\n",
      "    loss           : -691526.2351485149\n",
      "    val_loss       : -689649.88515625\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -776747.125000\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -671711.625000\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -724322.750000\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -696678.000000\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -637291.250000\n",
      "    epoch          : 127\n",
      "    loss           : -693666.3310643565\n",
      "    val_loss       : -690504.78125\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -773216.875000\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -671603.625000\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -646790.937500\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -692616.500000\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -700716.750000\n",
      "    epoch          : 128\n",
      "    loss           : -693546.8991336634\n",
      "    val_loss       : -693858.9578125\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -729343.000000\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -671968.812500\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -639059.375000\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -698664.250000\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -675362.812500\n",
      "    epoch          : 129\n",
      "    loss           : -696322.6763613861\n",
      "    val_loss       : -698403.85390625\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -781027.312500\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -652886.875000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -649204.437500\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -784624.250000\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -678057.125000\n",
      "    epoch          : 130\n",
      "    loss           : -700661.4251237623\n",
      "    val_loss       : -699168.67578125\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -785649.062500\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -646178.500000\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -732178.812500\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -783390.750000\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -702907.875000\n",
      "    epoch          : 131\n",
      "    loss           : -701088.7308168317\n",
      "    val_loss       : -701118.83203125\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -787236.687500\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -656142.000000\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -733224.375000\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -702182.125000\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -704727.250000\n",
      "    epoch          : 132\n",
      "    loss           : -702391.5476485149\n",
      "    val_loss       : -700086.76640625\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -786957.000000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -734162.750000\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -677443.187500\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -647792.187500\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -650092.375000\n",
      "    epoch          : 133\n",
      "    loss           : -700262.853960396\n",
      "    val_loss       : -701114.38515625\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -790233.375000\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -734029.500000\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -738943.937500\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -738799.562500\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -651341.125000\n",
      "    epoch          : 134\n",
      "    loss           : -704513.4975247525\n",
      "    val_loss       : -705389.92734375\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -793246.562500\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -659959.187500\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -707938.437500\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -708787.562500\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -713633.625000\n",
      "    epoch          : 135\n",
      "    loss           : -707966.8403465346\n",
      "    val_loss       : -706745.54140625\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -743736.937500\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -741714.187500\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -742188.750000\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -794592.500000\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -714974.625000\n",
      "    epoch          : 136\n",
      "    loss           : -710189.8774752475\n",
      "    val_loss       : -709827.16015625\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -795897.312500\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -692499.375000\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -668264.875000\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -691696.375000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -681752.250000\n",
      "    epoch          : 137\n",
      "    loss           : -711442.6862623763\n",
      "    val_loss       : -708803.09921875\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -797544.625000\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -664486.750000\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -746551.375000\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -688368.312500\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -660378.062500\n",
      "    epoch          : 138\n",
      "    loss           : -713176.6633663366\n",
      "    val_loss       : -711448.565625\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -799807.625000\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -747979.500000\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -708756.250000\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -693805.875000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -714746.750000\n",
      "    epoch          : 139\n",
      "    loss           : -711747.6676980198\n",
      "    val_loss       : -711080.3765625\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -747092.000000\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -662051.187500\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -665668.000000\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -803041.812500\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -716288.875000\n",
      "    epoch          : 140\n",
      "    loss           : -715453.7035891089\n",
      "    val_loss       : -715030.1234375\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -750881.625000\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -667500.562500\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -714964.125000\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -696230.437500\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -692768.000000\n",
      "    epoch          : 141\n",
      "    loss           : -718084.3459158416\n",
      "    val_loss       : -717106.5734375\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -751702.500000\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -692344.000000\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -720326.812500\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -720219.375000\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -720428.500000\n",
      "    epoch          : 142\n",
      "    loss           : -718316.2363861386\n",
      "    val_loss       : -712689.8671875\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -804299.625000\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -699091.250000\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -699939.750000\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -808918.375000\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -717960.375000\n",
      "    epoch          : 143\n",
      "    loss           : -720187.4461633663\n",
      "    val_loss       : -721265.4921875\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -809927.750000\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -673901.125000\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -724781.562500\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -726989.750000\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -723823.375000\n",
      "    epoch          : 144\n",
      "    loss           : -723971.2846534654\n",
      "    val_loss       : -720806.2703125\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -809380.375000\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -754729.625000\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -757545.000000\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -812086.250000\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -726919.187500\n",
      "    epoch          : 145\n",
      "    loss           : -724211.9300742574\n",
      "    val_loss       : -723967.8875\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -758475.625000\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -755989.937500\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -757529.250000\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -680988.437500\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -704352.500000\n",
      "    epoch          : 146\n",
      "    loss           : -726482.2134900991\n",
      "    val_loss       : -722797.515625\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -812736.375000\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -670579.312500\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -671786.625000\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -703796.875000\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -704558.625000\n",
      "    epoch          : 147\n",
      "    loss           : -726541.1905940594\n",
      "    val_loss       : -727100.88515625\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -814881.000000\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -710505.375000\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -733855.250000\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -811992.625000\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -730738.125000\n",
      "    epoch          : 148\n",
      "    loss           : -729739.3681930694\n",
      "    val_loss       : -727478.965625\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -818519.875000\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -760322.375000\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -676720.625000\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -818157.625000\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -752873.187500\n",
      "    epoch          : 149\n",
      "    loss           : -730218.1132425743\n",
      "    val_loss       : -729487.86015625\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -817767.187500\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -677417.750000\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -685379.125000\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -816867.625000\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -707198.687500\n",
      "    epoch          : 150\n",
      "    loss           : -732141.3217821782\n",
      "    val_loss       : -732450.95390625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0714_125235/checkpoint-epoch150.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -822954.250000\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -762620.875000\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -723211.062500\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -730365.375000\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -733905.000000\n",
      "    epoch          : 151\n",
      "    loss           : -731448.4888613861\n",
      "    val_loss       : -732733.1859375\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -821289.875000\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -716028.625000\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -686924.312500\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -825011.625000\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -714767.187500\n",
      "    epoch          : 152\n",
      "    loss           : -736514.1268564357\n",
      "    val_loss       : -735678.10859375\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -825069.250000\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -713310.875000\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -769784.625000\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -678514.750000\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -736925.500000\n",
      "    epoch          : 153\n",
      "    loss           : -738215.6175742574\n",
      "    val_loss       : -737270.59453125\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -827062.062500\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -772387.250000\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -693078.625000\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -695338.750000\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -743725.312500\n",
      "    epoch          : 154\n",
      "    loss           : -740403.5662128713\n",
      "    val_loss       : -739748.2125\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -828406.312500\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -723296.625000\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -683388.687500\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -687224.312500\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -741531.875000\n",
      "    epoch          : 155\n",
      "    loss           : -742003.1806930694\n",
      "    val_loss       : -740835.91328125\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -830665.500000\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -774389.625000\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -771294.625000\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -828483.750000\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -743033.312500\n",
      "    epoch          : 156\n",
      "    loss           : -742658.4461633663\n",
      "    val_loss       : -742157.871875\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -833111.875000\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -780581.625000\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -699154.437500\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -689265.875000\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -720918.000000\n",
      "    epoch          : 157\n",
      "    loss           : -744894.5779702971\n",
      "    val_loss       : -741230.6515625\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -833566.750000\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -775719.937500\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -779439.875000\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -724651.125000\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -741105.062500\n",
      "    epoch          : 158\n",
      "    loss           : -745346.2042079208\n",
      "    val_loss       : -744414.23359375\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -835273.000000\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -777596.000000\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -780591.687500\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -748284.375000\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -721571.000000\n",
      "    epoch          : 159\n",
      "    loss           : -746855.1064356435\n",
      "    val_loss       : -747076.4125\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -837953.937500\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -779090.500000\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -779387.812500\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -698260.312500\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -742581.125000\n",
      "    epoch          : 160\n",
      "    loss           : -748645.1596534654\n",
      "    val_loss       : -747407.88125\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -839367.562500\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -725550.000000\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -699524.875000\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -750375.125000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -752171.062500\n",
      "    epoch          : 161\n",
      "    loss           : -749982.5513613861\n",
      "    val_loss       : -749490.2859375\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -842390.750000\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -784410.812500\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -731673.625000\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -701875.687500\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -751278.750000\n",
      "    epoch          : 162\n",
      "    loss           : -751880.6720297029\n",
      "    val_loss       : -749147.4359375\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -841763.375000\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -703553.875000\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -704491.500000\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -702213.125000\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -750826.187500\n",
      "    epoch          : 163\n",
      "    loss           : -752752.1002475248\n",
      "    val_loss       : -745666.4859375\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -841447.375000\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -782369.750000\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -690325.500000\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -697669.500000\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -752838.125000\n",
      "    epoch          : 164\n",
      "    loss           : -750120.8248762377\n",
      "    val_loss       : -752957.47578125\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -846768.312500\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -736532.875000\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -754896.500000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -706748.562500\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -759453.312500\n",
      "    epoch          : 165\n",
      "    loss           : -758183.6837871287\n",
      "    val_loss       : -755517.93359375\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -849645.000000\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -794253.562500\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -755934.250000\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -711824.687500\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -711291.125000\n",
      "    epoch          : 166\n",
      "    loss           : -758357.3700495049\n",
      "    val_loss       : -756658.88828125\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -847472.687500\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -734946.000000\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -711375.875000\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -752845.375000\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -755792.250000\n",
      "    epoch          : 167\n",
      "    loss           : -758897.968440594\n",
      "    val_loss       : -756941.93125\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -849549.312500\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -733662.125000\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -758677.500000\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -787975.750000\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -759724.000000\n",
      "    epoch          : 168\n",
      "    loss           : -757855.7803217822\n",
      "    val_loss       : -759459.22109375\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -797529.437500\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -737409.625000\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -790198.500000\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -764718.875000\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -759380.500000\n",
      "    epoch          : 169\n",
      "    loss           : -762058.5594059406\n",
      "    val_loss       : -759253.76640625\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -850551.000000\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -738890.750000\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -795890.812500\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -708353.312500\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -738578.562500\n",
      "    epoch          : 170\n",
      "    loss           : -761984.7506188119\n",
      "    val_loss       : -761504.8078125\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -852626.187500\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -744714.875000\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -769810.312500\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -767598.250000\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -756706.000000\n",
      "    epoch          : 171\n",
      "    loss           : -764394.542079208\n",
      "    val_loss       : -761163.37109375\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -855245.125000\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -712962.625000\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -763216.250000\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -855632.125000\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -767436.062500\n",
      "    epoch          : 172\n",
      "    loss           : -765471.1410891089\n",
      "    val_loss       : -764808.09765625\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -862083.750000\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -765247.312500\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -721600.625000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -764685.062500\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -766013.250000\n",
      "    epoch          : 173\n",
      "    loss           : -767616.6571782178\n",
      "    val_loss       : -764717.8109375\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -858666.875000\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -746483.187500\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -798528.187500\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -763949.812500\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -772668.312500\n",
      "    epoch          : 174\n",
      "    loss           : -768505.8143564357\n",
      "    val_loss       : -767958.5859375\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -861537.875000\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -724993.875000\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -804084.000000\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -723346.750000\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -766103.750000\n",
      "    epoch          : 175\n",
      "    loss           : -771698.6893564357\n",
      "    val_loss       : -768537.85\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -746786.500000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -803816.125000\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -718661.187500\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -796256.375000\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -775400.375000\n",
      "    epoch          : 176\n",
      "    loss           : -771270.4950495049\n",
      "    val_loss       : -769621.6828125\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -807198.312500\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -796872.562500\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -738117.187500\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -767556.625000\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -742623.000000\n",
      "    epoch          : 177\n",
      "    loss           : -770055.4133663366\n",
      "    val_loss       : -770681.7203125\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -809618.625000\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -749991.125000\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -804181.750000\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -799901.250000\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -778114.812500\n",
      "    epoch          : 178\n",
      "    loss           : -773414.1237623763\n",
      "    val_loss       : -768870.3421875\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -868893.125000\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -806290.875000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -802420.562500\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -869661.375000\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -743904.875000\n",
      "    epoch          : 179\n",
      "    loss           : -773345.926980198\n",
      "    val_loss       : -769682.36796875\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -867003.625000\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -798471.250000\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -802305.750000\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -769983.875000\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -766162.562500\n",
      "    epoch          : 180\n",
      "    loss           : -770476.8366336634\n",
      "    val_loss       : -773064.728125\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -808980.875000\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -730514.500000\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -808267.187500\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -779621.125000\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -746284.625000\n",
      "    epoch          : 181\n",
      "    loss           : -777821.6590346535\n",
      "    val_loss       : -774649.67265625\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -726629.187500\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -732420.125000\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -729126.000000\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -734712.250000\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -782240.187500\n",
      "    epoch          : 182\n",
      "    loss           : -778411.3013613861\n",
      "    val_loss       : -774061.40390625\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -752657.375000\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -813888.562500\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -808639.625000\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -775796.687500\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -776604.000000\n",
      "    epoch          : 183\n",
      "    loss           : -778019.6912128713\n",
      "    val_loss       : -776300.66484375\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -876464.625000\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -809254.250000\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -780734.062500\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -753074.375000\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -734654.000000\n",
      "    epoch          : 184\n",
      "    loss           : -778979.5241336634\n",
      "    val_loss       : -776069.59453125\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -874989.875000\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -772278.250000\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -774174.875000\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -779410.000000\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -775178.875000\n",
      "    epoch          : 185\n",
      "    loss           : -779591.9003712871\n",
      "    val_loss       : -778638.9421875\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -876784.250000\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -759995.500000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -757605.000000\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -754210.812500\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -786491.000000\n",
      "    epoch          : 186\n",
      "    loss           : -783492.6794554455\n",
      "    val_loss       : -780957.453125\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -880328.812500\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -760555.937500\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -780980.250000\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -779883.875000\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -783465.437500\n",
      "    epoch          : 187\n",
      "    loss           : -784663.7301980198\n",
      "    val_loss       : -781569.70859375\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -880978.750000\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -819954.625000\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -815480.812500\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -813512.250000\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -755873.750000\n",
      "    epoch          : 188\n",
      "    loss           : -786887.3199257426\n",
      "    val_loss       : -782731.0546875\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -880112.125000\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -763804.000000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -733080.062500\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -758394.500000\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -788514.750000\n",
      "    epoch          : 189\n",
      "    loss           : -785593.4220297029\n",
      "    val_loss       : -782898.2640625\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -882050.062500\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -767234.000000\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -734464.000000\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -804000.875000\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -758361.312500\n",
      "    epoch          : 190\n",
      "    loss           : -785670.6986386139\n",
      "    val_loss       : -784354.22109375\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -885927.312500\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -765042.750000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -787119.000000\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -820063.250000\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -781946.875000\n",
      "    epoch          : 191\n",
      "    loss           : -788317.8904702971\n",
      "    val_loss       : -781469.85625\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -884363.687500\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -760243.375000\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -740695.625000\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -762942.750000\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -791034.000000\n",
      "    epoch          : 192\n",
      "    loss           : -787373.8366336634\n",
      "    val_loss       : -787514.32109375\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -825272.312500\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -770027.750000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -763359.000000\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -791765.250000\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -797285.250000\n",
      "    epoch          : 193\n",
      "    loss           : -791850.6107673268\n",
      "    val_loss       : -788771.2984375\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -830281.250000\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -891651.750000\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -742506.812500\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -746530.937500\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -755802.750000\n",
      "    epoch          : 194\n",
      "    loss           : -792436.4826732674\n",
      "    val_loss       : -789290.47890625\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -769046.687500\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -829125.187500\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -821470.000000\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -891432.562500\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -765064.187500\n",
      "    epoch          : 195\n",
      "    loss           : -795316.6342821782\n",
      "    val_loss       : -792751.51640625\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -892494.750000\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -745431.687500\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -742999.312500\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -890867.000000\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -792738.500000\n",
      "    epoch          : 196\n",
      "    loss           : -796352.8013613861\n",
      "    val_loss       : -792323.9234375\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -893603.625000\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -742794.937500\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -738840.250000\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -791378.437500\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -788868.875000\n",
      "    epoch          : 197\n",
      "    loss           : -792538.2425742574\n",
      "    val_loss       : -792011.23046875\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -830047.312500\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -743112.625000\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -786390.937500\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -818306.062500\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -800532.375000\n",
      "    epoch          : 198\n",
      "    loss           : -793976.2735148515\n",
      "    val_loss       : -794656.65390625\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -896508.000000\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -752841.187500\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -749983.500000\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -822608.625000\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -797447.000000\n",
      "    epoch          : 199\n",
      "    loss           : -799402.7747524752\n",
      "    val_loss       : -796588.35078125\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -897957.500000\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -834127.687500\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -756541.187500\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -802624.437500\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -763913.062500\n",
      "    epoch          : 200\n",
      "    loss           : -799373.6516089109\n",
      "    val_loss       : -793832.5484375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0714_125235/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -895167.437500\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -800267.375000\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -773007.750000\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -823650.625000\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -793987.750000\n",
      "    epoch          : 201\n",
      "    loss           : -800344.6571782178\n",
      "    val_loss       : -798556.11171875\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -900363.187500\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -832866.875000\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -752068.250000\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -897865.875000\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -797538.000000\n",
      "    epoch          : 202\n",
      "    loss           : -802748.1775990099\n",
      "    val_loss       : -801155.07578125\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -760273.625000\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -752941.875000\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -775378.000000\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -772692.250000\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -806759.062500\n",
      "    epoch          : 203\n",
      "    loss           : -804330.1169554455\n",
      "    val_loss       : -799860.98359375\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -903248.500000\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -780020.500000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -758178.500000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -811737.250000\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -776947.875000\n",
      "    epoch          : 204\n",
      "    loss           : -805080.9121287129\n",
      "    val_loss       : -802475.12109375\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -902010.875000\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -839930.500000\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -802939.687500\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -903819.187500\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -778952.750000\n",
      "    epoch          : 205\n",
      "    loss           : -806196.4350247525\n",
      "    val_loss       : -802675.3390625\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -905306.625000\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -777495.750000\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -777331.750000\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -837407.812500\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -813940.500000\n",
      "    epoch          : 206\n",
      "    loss           : -807083.5946782178\n",
      "    val_loss       : -806252.8375\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -904884.250000\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -814439.562500\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -810206.062500\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -766201.750000\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -813670.687500\n",
      "    epoch          : 207\n",
      "    loss           : -810486.9622524752\n",
      "    val_loss       : -805953.490625\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -907790.750000\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -785820.375000\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -755225.250000\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -760924.000000\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -767249.375000\n",
      "    epoch          : 208\n",
      "    loss           : -806655.1120049505\n",
      "    val_loss       : -802594.46484375\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -906052.062500\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -788479.000000\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -840494.750000\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -750282.812500\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -781987.875000\n",
      "    epoch          : 209\n",
      "    loss           : -808375.0705445545\n",
      "    val_loss       : -808538.3203125\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -910253.625000\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -847755.437500\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -766176.250000\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -785471.500000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -782096.625000\n",
      "    epoch          : 210\n",
      "    loss           : -813133.573019802\n",
      "    val_loss       : -810772.9890625\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -912291.625000\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -807851.437500\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -757182.000000\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -839764.687500\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -812192.500000\n",
      "    epoch          : 211\n",
      "    loss           : -812639.2574257426\n",
      "    val_loss       : -810476.77578125\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -912138.562500\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -786794.312500\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -815212.000000\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -911702.125000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -804693.437500\n",
      "    epoch          : 212\n",
      "    loss           : -812433.2642326732\n",
      "    val_loss       : -810147.353125\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -913968.875000\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -850177.812500\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -822555.187500\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -769134.312500\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -816258.500000\n",
      "    epoch          : 213\n",
      "    loss           : -817383.4077970297\n",
      "    val_loss       : -814114.64765625\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -915370.437500\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -795205.500000\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -770299.937500\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -843306.250000\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -774753.750000\n",
      "    epoch          : 214\n",
      "    loss           : -817658.9040841584\n",
      "    val_loss       : -813673.01640625\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -914210.375000\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -851729.562500\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -822725.125000\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -774357.125000\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -836439.625000\n",
      "    epoch          : 215\n",
      "    loss           : -816896.3589108911\n",
      "    val_loss       : -810050.5265625\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -912164.437500\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -850649.875000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -852148.375000\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -823969.375000\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -819593.187500\n",
      "    epoch          : 216\n",
      "    loss           : -816984.2772277228\n",
      "    val_loss       : -811974.06015625\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -855613.875000\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -764103.625000\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -760921.500000\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -808019.062500\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -815286.812500\n",
      "    epoch          : 217\n",
      "    loss           : -813501.4313118812\n",
      "    val_loss       : -815524.61484375\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -918811.937500\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -796886.937500\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -828623.375000\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -852726.687500\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -786926.250000\n",
      "    epoch          : 218\n",
      "    loss           : -821858.3892326732\n",
      "    val_loss       : -817814.465625\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -921977.250000\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -798885.875000\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -852131.500000\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -923034.875000\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -826717.812500\n",
      "    epoch          : 219\n",
      "    loss           : -822975.7277227723\n",
      "    val_loss       : -818186.1796875\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -922149.875000\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -796165.750000\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -773805.750000\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -777840.625000\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -784401.750000\n",
      "    epoch          : 220\n",
      "    loss           : -819788.7035891089\n",
      "    val_loss       : -814142.4203125\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -921326.375000\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -850896.500000\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -776810.875000\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -852986.375000\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -791894.750000\n",
      "    epoch          : 221\n",
      "    loss           : -822495.1831683168\n",
      "    val_loss       : -819582.55078125\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -923925.125000\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -802434.812500\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -853557.000000\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -823980.250000\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -820424.500000\n",
      "    epoch          : 222\n",
      "    loss           : -825548.6676980198\n",
      "    val_loss       : -821635.9109375\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -926806.250000\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -804328.875000\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -859447.437500\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -852787.562500\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -789894.125000\n",
      "    epoch          : 223\n",
      "    loss           : -826748.6070544554\n",
      "    val_loss       : -823655.2703125\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -926729.687500\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -802776.187500\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -824057.750000\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -848792.625000\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -830126.750000\n",
      "    epoch          : 224\n",
      "    loss           : -824706.1441831683\n",
      "    val_loss       : -820799.29296875\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -926638.625000\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -824547.750000\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -755023.125000\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -919422.062500\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -821518.312500\n",
      "    epoch          : 225\n",
      "    loss           : -822179.8131188119\n",
      "    val_loss       : -824158.60390625\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -927444.875000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -774439.625000\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -786360.562500\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -821496.500000\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -796386.812500\n",
      "    epoch          : 226\n",
      "    loss           : -829054.0383663366\n",
      "    val_loss       : -826152.7046875\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -930406.062500\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -782089.750000\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -789141.125000\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -790768.312500\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -834124.250000\n",
      "    epoch          : 227\n",
      "    loss           : -829834.2450495049\n",
      "    val_loss       : -826311.95703125\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -933476.875000\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -866168.625000\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -794670.000000\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -829847.375000\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -777469.312500\n",
      "    epoch          : 228\n",
      "    loss           : -829209.0563118812\n",
      "    val_loss       : -826434.69375\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -931260.375000\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -856162.000000\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -787276.875000\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -786029.875000\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -799597.312500\n",
      "    epoch          : 229\n",
      "    loss           : -832618.4665841584\n",
      "    val_loss       : -827973.75546875\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -933250.187500\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -803911.687500\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -789296.125000\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -829225.625000\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -799764.312500\n",
      "    epoch          : 230\n",
      "    loss           : -832831.2029702971\n",
      "    val_loss       : -828728.1359375\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -934861.062500\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -808319.562500\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -863240.000000\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -790670.375000\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -783065.000000\n",
      "    epoch          : 231\n",
      "    loss           : -833305.1070544554\n",
      "    val_loss       : -826316.00625\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -871877.312500\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -790341.312500\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -781330.375000\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -934969.125000\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -794985.375000\n",
      "    epoch          : 232\n",
      "    loss           : -832431.0006188119\n",
      "    val_loss       : -826678.50390625\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -931680.312500\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -866699.187500\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -788428.437500\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -935696.750000\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -839726.625000\n",
      "    epoch          : 233\n",
      "    loss           : -832931.8391089109\n",
      "    val_loss       : -829563.6296875\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -801986.062500\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -783433.875000\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -786539.000000\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -794096.625000\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -840853.000000\n",
      "    epoch          : 234\n",
      "    loss           : -836319.0290841584\n",
      "    val_loss       : -831284.5953125\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -939344.187500\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -869285.500000\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -804214.375000\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -795893.500000\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -844046.375000\n",
      "    epoch          : 235\n",
      "    loss           : -837797.2722772277\n",
      "    val_loss       : -832575.30859375\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -939123.625000\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -789603.000000\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -864166.437500\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -937203.875000\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -838421.125000\n",
      "    epoch          : 236\n",
      "    loss           : -836553.8929455446\n",
      "    val_loss       : -832926.18203125\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -940869.187500\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -866975.312500\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -790038.875000\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -803034.687500\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -798696.312500\n",
      "    epoch          : 237\n",
      "    loss           : -838487.8533415842\n",
      "    val_loss       : -834605.13671875\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -941343.437500\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -870050.375000\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -780609.750000\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -937460.375000\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -806002.312500\n",
      "    epoch          : 238\n",
      "    loss           : -835077.5222772277\n",
      "    val_loss       : -834252.17734375\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -942567.375000\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -818967.437500\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -791686.375000\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -805871.437500\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -806634.187500\n",
      "    epoch          : 239\n",
      "    loss           : -840455.5482673268\n",
      "    val_loss       : -835661.45390625\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -943756.500000\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -805997.375000\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -811776.750000\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -838520.312500\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -794918.750000\n",
      "    epoch          : 240\n",
      "    loss           : -840351.2066831683\n",
      "    val_loss       : -835356.73046875\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -940389.875000\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -875032.375000\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -805235.000000\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -805167.125000\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -833332.750000\n",
      "    epoch          : 241\n",
      "    loss           : -841730.3298267326\n",
      "    val_loss       : -837085.0328125\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -944509.250000\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -815521.750000\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -796451.625000\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -943929.687500\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -793187.250000\n",
      "    epoch          : 242\n",
      "    loss           : -838031.8650990099\n",
      "    val_loss       : -832501.15078125\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -942752.437500\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -787081.375000\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -836923.000000\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -946218.812500\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -851352.875000\n",
      "    epoch          : 243\n",
      "    loss           : -841135.6806930694\n",
      "    val_loss       : -839630.40546875\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -947863.250000\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -797063.750000\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -801194.500000\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -870450.000000\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -851469.875000\n",
      "    epoch          : 244\n",
      "    loss           : -844291.3978960396\n",
      "    val_loss       : -838982.65625\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -948485.937500\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -815732.875000\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -867267.562500\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -815614.375000\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -835066.562500\n",
      "    epoch          : 245\n",
      "    loss           : -842302.7413366337\n",
      "    val_loss       : -836185.1578125\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -947726.437500\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -816655.687500\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -804285.812500\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -841117.500000\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -842737.875000\n",
      "    epoch          : 246\n",
      "    loss           : -844167.2073019802\n",
      "    val_loss       : -840849.28046875\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -948502.625000\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -878899.375000\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -802993.687500\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -871087.187500\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -838130.750000\n",
      "    epoch          : 247\n",
      "    loss           : -844612.5563118812\n",
      "    val_loss       : -838939.17578125\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -947140.625000\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -880112.375000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -804370.125000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -950472.437500\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -840637.125000\n",
      "    epoch          : 248\n",
      "    loss           : -846006.1751237623\n",
      "    val_loss       : -841088.90625\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -950643.500000\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -880168.125000\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -873145.000000\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -846917.375000\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -837216.875000\n",
      "    epoch          : 249\n",
      "    loss           : -845589.1695544554\n",
      "    val_loss       : -840762.490625\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -950730.875000\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -818000.250000\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -877374.062500\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -868390.625000\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -809605.875000\n",
      "    epoch          : 250\n",
      "    loss           : -845928.0897277228\n",
      "    val_loss       : -840813.22421875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0714_125235/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -953076.562500\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -817938.937500\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -797347.812500\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -852170.250000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -844685.562500\n",
      "    epoch          : 251\n",
      "    loss           : -847654.7159653465\n",
      "    val_loss       : -841845.5578125\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -953167.375000\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -796039.500000\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -853834.750000\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -872571.750000\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -805199.250000\n",
      "    epoch          : 252\n",
      "    loss           : -848226.6825495049\n",
      "    val_loss       : -843457.903125\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -954767.500000\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -809683.375000\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -808049.187500\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -855515.812500\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -851601.500000\n",
      "    epoch          : 253\n",
      "    loss           : -850624.7957920792\n",
      "    val_loss       : -844551.46328125\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -954482.250000\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -882114.437500\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -874758.750000\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -813443.562500\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -804730.625000\n",
      "    epoch          : 254\n",
      "    loss           : -849415.0160891089\n",
      "    val_loss       : -845001.2515625\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -953518.000000\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -877377.375000\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -847768.375000\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -801828.750000\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -851638.125000\n",
      "    epoch          : 255\n",
      "    loss           : -846251.416460396\n",
      "    val_loss       : -843697.8171875\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -957807.625000\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -851275.000000\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -843776.750000\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -882139.937500\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -856321.625000\n",
      "    epoch          : 256\n",
      "    loss           : -850226.0396039604\n",
      "    val_loss       : -846580.80078125\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -957706.625000\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -800837.937500\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -847142.250000\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -850159.625000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -848805.875000\n",
      "    epoch          : 257\n",
      "    loss           : -846132.3613861386\n",
      "    val_loss       : -844964.1328125\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -955000.937500\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -827836.812500\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -801649.937500\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -811543.000000\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -807380.750000\n",
      "    epoch          : 258\n",
      "    loss           : -852185.864480198\n",
      "    val_loss       : -847389.66640625\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -960684.562500\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -846905.500000\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -803982.437500\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -849607.250000\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -849945.312500\n",
      "    epoch          : 259\n",
      "    loss           : -853268.0983910891\n",
      "    val_loss       : -847068.7484375\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -959422.312500\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -886920.125000\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -800227.812500\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -957717.687500\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -854992.875000\n",
      "    epoch          : 260\n",
      "    loss           : -852522.5247524752\n",
      "    val_loss       : -847847.8421875\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -890559.750000\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -857197.375000\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -803709.187500\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -848491.750000\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -812138.000000\n",
      "    epoch          : 261\n",
      "    loss           : -853907.0290841584\n",
      "    val_loss       : -850198.8671875\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -961237.125000\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -892442.562500\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -817512.000000\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -815469.062500\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -852479.500000\n",
      "    epoch          : 262\n",
      "    loss           : -855424.3032178218\n",
      "    val_loss       : -850570.72421875\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -961825.500000\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -828823.875000\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -886349.937500\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -848362.375000\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -861269.625000\n",
      "    epoch          : 263\n",
      "    loss           : -855470.7469059406\n",
      "    val_loss       : -850384.64140625\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -889354.125000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -892881.375000\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -807691.687500\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -959973.375000\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -865420.625000\n",
      "    epoch          : 264\n",
      "    loss           : -856192.9220297029\n",
      "    val_loss       : -851564.81484375\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -962352.062500\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -887835.375000\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -884674.437500\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -962330.375000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -861217.500000\n",
      "    epoch          : 265\n",
      "    loss           : -856158.0235148515\n",
      "    val_loss       : -851365.09140625\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -963717.687500\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -801599.687500\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -812807.812500\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -961548.500000\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -861977.937500\n",
      "    epoch          : 266\n",
      "    loss           : -855314.666460396\n",
      "    val_loss       : -849285.80234375\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -964281.312500\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -828816.250000\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -855475.875000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -848529.250000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -808920.250000\n",
      "    epoch          : 267\n",
      "    loss           : -854253.0148514851\n",
      "    val_loss       : -851884.08125\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -964386.000000\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -880788.750000\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -808331.500000\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -806487.375000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -860453.250000\n",
      "    epoch          : 268\n",
      "    loss           : -857023.198019802\n",
      "    val_loss       : -852902.0078125\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -891984.875000\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -816056.750000\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -882965.562500\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -811466.500000\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -821804.562500\n",
      "    epoch          : 269\n",
      "    loss           : -858356.0569306931\n",
      "    val_loss       : -854592.48046875\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -967259.750000\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -832605.000000\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -815442.062500\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -821106.875000\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -850134.625000\n",
      "    epoch          : 270\n",
      "    loss           : -858448.1590346535\n",
      "    val_loss       : -852391.77578125\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -966600.312500\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -803410.125000\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -854079.250000\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -966047.312500\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -816481.437500\n",
      "    epoch          : 271\n",
      "    loss           : -857959.3923267326\n",
      "    val_loss       : -854958.70703125\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -967682.687500\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -823665.562500\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -823801.562500\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -824786.937500\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -818056.812500\n",
      "    epoch          : 272\n",
      "    loss           : -860029.2450495049\n",
      "    val_loss       : -854666.0765625\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -966538.625000\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -886988.125000\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -817098.375000\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -859793.937500\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -887936.125000\n",
      "    epoch          : 273\n",
      "    loss           : -859796.8397277228\n",
      "    val_loss       : -855487.62265625\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -896432.250000\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -833744.000000\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -887054.875000\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -856556.125000\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -865597.250000\n",
      "    epoch          : 274\n",
      "    loss           : -861547.3731435643\n",
      "    val_loss       : -856622.571875\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -896813.687500\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -813593.625000\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -885723.500000\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -853904.937500\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -819591.875000\n",
      "    epoch          : 275\n",
      "    loss           : -860410.3799504951\n",
      "    val_loss       : -856691.2984375\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -968840.937500\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -834645.000000\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -822132.875000\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -824166.875000\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -857846.562500\n",
      "    epoch          : 276\n",
      "    loss           : -862176.5191831683\n",
      "    val_loss       : -858027.00234375\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -969552.312500\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -813197.937500\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -812636.312500\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -887319.625000\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -869973.500000\n",
      "    epoch          : 277\n",
      "    loss           : -862758.3013613861\n",
      "    val_loss       : -857854.6015625\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -973384.375000\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -822169.875000\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -884489.687500\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -867890.000000\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -819251.625000\n",
      "    epoch          : 278\n",
      "    loss           : -863116.1757425743\n",
      "    val_loss       : -858394.9453125\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -971456.500000\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -831998.125000\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -819348.812500\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -865889.687500\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -860735.125000\n",
      "    epoch          : 279\n",
      "    loss           : -865522.3508663366\n",
      "    val_loss       : -859614.92109375\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -974020.750000\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -825416.250000\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -872513.375000\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -881026.437500\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -855823.937500\n",
      "    epoch          : 280\n",
      "    loss           : -864577.6274752475\n",
      "    val_loss       : -858271.05859375\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -971943.750000\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -887202.937500\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -790366.250000\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -797909.125000\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -866904.187500\n",
      "    epoch          : 281\n",
      "    loss           : -856319.780940594\n",
      "    val_loss       : -857381.3890625\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -971783.500000\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -837279.500000\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -857266.250000\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -822115.375000\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -872261.000000\n",
      "    epoch          : 282\n",
      "    loss           : -865213.0488861386\n",
      "    val_loss       : -861603.52421875\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -973811.375000\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -830239.812500\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -868477.000000\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -818294.625000\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -829512.250000\n",
      "    epoch          : 283\n",
      "    loss           : -867474.8038366337\n",
      "    val_loss       : -860820.36875\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -975022.312500\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -898781.062500\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -830547.000000\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -872378.500000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -862794.875000\n",
      "    epoch          : 284\n",
      "    loss           : -866864.9950495049\n",
      "    val_loss       : -860143.125\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -973796.937500\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -838880.875000\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -891807.375000\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -975332.937500\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -866859.062500\n",
      "    epoch          : 285\n",
      "    loss           : -866371.5643564357\n",
      "    val_loss       : -861740.19921875\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -975555.187500\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -902070.375000\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -874811.125000\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -884166.625000\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -864336.500000\n",
      "    epoch          : 286\n",
      "    loss           : -867362.7660891089\n",
      "    val_loss       : -858586.41640625\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -975101.187500\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -840960.750000\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -867066.875000\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -973749.000000\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -863614.750000\n",
      "    epoch          : 287\n",
      "    loss           : -867352.4133663366\n",
      "    val_loss       : -863309.0125\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -973711.000000\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -838330.437500\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -869108.437500\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -868413.500000\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -864732.000000\n",
      "    epoch          : 288\n",
      "    loss           : -867249.5228960396\n",
      "    val_loss       : -861634.6609375\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -976113.375000\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -820317.250000\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -830203.875000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -828970.625000\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -825252.125000\n",
      "    epoch          : 289\n",
      "    loss           : -865145.7629950495\n",
      "    val_loss       : -863535.19609375\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -903598.875000\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -841114.687500\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -896087.375000\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -821371.875000\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -868839.625000\n",
      "    epoch          : 290\n",
      "    loss           : -870680.6590346535\n",
      "    val_loss       : -865129.9515625\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -975492.500000\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -829178.000000\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -835025.875000\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -977933.500000\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -873582.125000\n",
      "    epoch          : 291\n",
      "    loss           : -871417.2221534654\n",
      "    val_loss       : -862930.92890625\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -978509.687500\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -844070.937500\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -819458.625000\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -870163.375000\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -865516.187500\n",
      "    epoch          : 292\n",
      "    loss           : -869069.4566831683\n",
      "    val_loss       : -864630.79453125\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -977827.375000\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -904427.500000\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -832285.125000\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -976534.625000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -832618.500000\n",
      "    epoch          : 293\n",
      "    loss           : -869591.8756188119\n",
      "    val_loss       : -864789.13515625\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -909653.750000\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -837142.875000\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -814418.812500\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -827633.437500\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -831931.125000\n",
      "    epoch          : 294\n",
      "    loss           : -868325.469059406\n",
      "    val_loss       : -865515.9328125\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -977974.125000\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -839231.625000\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -821368.125000\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -875769.562500\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -830347.437500\n",
      "    epoch          : 295\n",
      "    loss           : -870992.7202970297\n",
      "    val_loss       : -866703.25625\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -980268.875000\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -896616.812500\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -822717.125000\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -829959.562500\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -832018.687500\n",
      "    epoch          : 296\n",
      "    loss           : -871224.1150990099\n",
      "    val_loss       : -864559.809375\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -978823.125000\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -837324.375000\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -821875.250000\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -872889.875000\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -870256.625000\n",
      "    epoch          : 297\n",
      "    loss           : -871420.5160891089\n",
      "    val_loss       : -867606.75390625\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -898105.937500\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -899048.687500\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -842579.000000\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -834825.937500\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -872825.437500\n",
      "    epoch          : 298\n",
      "    loss           : -873844.4925742574\n",
      "    val_loss       : -868469.49296875\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -982055.750000\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -844572.125000\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -836063.937500\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -831384.250000\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -879803.750000\n",
      "    epoch          : 299\n",
      "    loss           : -873872.5185643565\n",
      "    val_loss       : -866837.62421875\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -982584.875000\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -841606.250000\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -830292.500000\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -895936.937500\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -879375.000000\n",
      "    epoch          : 300\n",
      "    loss           : -873187.6064356435\n",
      "    val_loss       : -868415.85234375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0714_125235/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -982831.875000\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -845371.250000\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -814882.875000\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -980271.125000\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -881178.250000\n",
      "    epoch          : 301\n",
      "    loss           : -871717.9851485149\n",
      "    val_loss       : -868658.0828125\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -982887.625000\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -900211.000000\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -865992.562500\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -978960.437500\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -834618.062500\n",
      "    epoch          : 302\n",
      "    loss           : -871469.2283415842\n",
      "    val_loss       : -865012.5546875\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -979446.875000\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -845072.937500\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -836051.187500\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -827642.875000\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -884859.750000\n",
      "    epoch          : 303\n",
      "    loss           : -872879.5575495049\n",
      "    val_loss       : -870361.8421875\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -847810.750000\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -907082.125000\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -840471.375000\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -883167.625000\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -836345.937500\n",
      "    epoch          : 304\n",
      "    loss           : -876713.4764851485\n",
      "    val_loss       : -869700.209375\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -983379.375000\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -907448.312500\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -883351.250000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -876163.000000\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -825675.125000\n",
      "    epoch          : 305\n",
      "    loss           : -876263.2004950495\n",
      "    val_loss       : -869949.83828125\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -984469.562500\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -879597.062500\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -885607.187500\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -983644.312500\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -876493.375000\n",
      "    epoch          : 306\n",
      "    loss           : -876465.7165841584\n",
      "    val_loss       : -869985.7375\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -983884.437500\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -850038.812500\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -842085.000000\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -984854.000000\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -837819.312500\n",
      "    epoch          : 307\n",
      "    loss           : -877579.7877475248\n",
      "    val_loss       : -871821.01328125\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -983439.750000\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -906768.000000\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -835847.375000\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -888945.937500\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -867820.500000\n",
      "    epoch          : 308\n",
      "    loss           : -876408.280940594\n",
      "    val_loss       : -869912.66953125\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -982618.687500\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -844124.000000\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -838272.687500\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -980789.250000\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -851284.250000\n",
      "    epoch          : 309\n",
      "    loss           : -872693.3620049505\n",
      "    val_loss       : -866910.03984375\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -838511.625000\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -847571.312500\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -875950.125000\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -982584.375000\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -879344.562500\n",
      "    epoch          : 310\n",
      "    loss           : -874865.4820544554\n",
      "    val_loss       : -870878.171875\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -915468.125000\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -884377.937500\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -878238.750000\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -875120.125000\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -881051.125000\n",
      "    epoch          : 311\n",
      "    loss           : -878137.9696782178\n",
      "    val_loss       : -872559.32265625\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -986975.500000\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -900139.000000\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -900307.125000\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -902920.000000\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -836521.375000\n",
      "    epoch          : 312\n",
      "    loss           : -878329.6806930694\n",
      "    val_loss       : -869813.7875\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -986255.125000\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -846644.187500\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -883099.250000\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -841089.625000\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -875447.000000\n",
      "    epoch          : 313\n",
      "    loss           : -877749.853960396\n",
      "    val_loss       : -871707.60703125\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -880493.875000\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -874108.750000\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -838976.500000\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -874042.875000\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -887330.562500\n",
      "    epoch          : 314\n",
      "    loss           : -878818.5099009901\n",
      "    val_loss       : -872654.8078125\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -986322.125000\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -881361.500000\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -842058.125000\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -988526.000000\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -868689.125000\n",
      "    epoch          : 315\n",
      "    loss           : -880463.3601485149\n",
      "    val_loss       : -871112.490625\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -987184.125000\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -881053.375000\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -831427.562500\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -875709.000000\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -831595.562500\n",
      "    epoch          : 316\n",
      "    loss           : -876060.583539604\n",
      "    val_loss       : -871227.2390625\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -986437.187500\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -846857.000000\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -836984.312500\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -889317.250000\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -883327.875000\n",
      "    epoch          : 317\n",
      "    loss           : -879271.4678217822\n",
      "    val_loss       : -873598.27578125\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -987047.125000\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -834312.937500\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -878414.250000\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -875163.562500\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -886936.000000\n",
      "    epoch          : 318\n",
      "    loss           : -880395.4071782178\n",
      "    val_loss       : -874178.7703125\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -914869.937500\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -847120.250000\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -832369.062500\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -990090.750000\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -880076.375000\n",
      "    epoch          : 319\n",
      "    loss           : -880446.1577970297\n",
      "    val_loss       : -872590.37890625\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -987514.750000\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -852385.875000\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -878310.625000\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -829715.875000\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -875441.375000\n",
      "    epoch          : 320\n",
      "    loss           : -878439.0507425743\n",
      "    val_loss       : -872064.53984375\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -990437.187500\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -907281.937500\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -836835.375000\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -880333.000000\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -880389.812500\n",
      "    epoch          : 321\n",
      "    loss           : -880083.9275990099\n",
      "    val_loss       : -874758.96875\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -914350.000000\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -885801.875000\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -845246.437500\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -897728.625000\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -878208.875000\n",
      "    epoch          : 322\n",
      "    loss           : -880739.2462871287\n",
      "    val_loss       : -873989.33125\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -992034.625000\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -853137.125000\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -837095.562500\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -988976.312500\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -838539.750000\n",
      "    epoch          : 323\n",
      "    loss           : -881990.8886138614\n",
      "    val_loss       : -874108.0859375\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -989874.312500\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -884188.437500\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -887426.187500\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -838328.000000\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -882030.625000\n",
      "    epoch          : 324\n",
      "    loss           : -881956.7524752475\n",
      "    val_loss       : -874724.52109375\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -988191.000000\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -854465.875000\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -838889.875000\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -907400.562500\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -883354.687500\n",
      "    epoch          : 325\n",
      "    loss           : -883523.8273514851\n",
      "    val_loss       : -875611.51953125\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -993176.750000\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -848479.000000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -847978.187500\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -878231.375000\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -839971.375000\n",
      "    epoch          : 326\n",
      "    loss           : -883481.4121287129\n",
      "    val_loss       : -876265.59453125\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -990356.812500\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -851621.375000\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -885147.000000\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -906566.437500\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -839411.875000\n",
      "    epoch          : 327\n",
      "    loss           : -882044.2865099009\n",
      "    val_loss       : -876963.39375\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -992746.250000\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -891193.375000\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -839477.000000\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -890268.812500\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -837325.937500\n",
      "    epoch          : 328\n",
      "    loss           : -883877.2741336634\n",
      "    val_loss       : -875475.1640625\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -917609.687500\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -852784.187500\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -844609.375000\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -837847.562500\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -882395.875000\n",
      "    epoch          : 329\n",
      "    loss           : -880000.4387376237\n",
      "    val_loss       : -874402.7296875\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -918138.125000\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -855128.125000\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -843200.937500\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -837240.500000\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -891468.125000\n",
      "    epoch          : 330\n",
      "    loss           : -883194.2543316832\n",
      "    val_loss       : -875967.14296875\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -993842.562500\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -851516.312500\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -904863.250000\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -849902.750000\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -880984.375000\n",
      "    epoch          : 331\n",
      "    loss           : -883651.0228960396\n",
      "    val_loss       : -877282.31484375\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -845514.000000\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -858692.562500\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -842998.937500\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -904432.437500\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -832912.875000\n",
      "    epoch          : 332\n",
      "    loss           : -882959.2611386139\n",
      "    val_loss       : -875825.57578125\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -839808.750000\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -920140.437500\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -907945.062500\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -888528.750000\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -841285.250000\n",
      "    epoch          : 333\n",
      "    loss           : -884215.9498762377\n",
      "    val_loss       : -877910.59375\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -991977.625000\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -917868.875000\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -850984.875000\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -906531.875000\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -891984.625000\n",
      "    epoch          : 334\n",
      "    loss           : -886120.6602722772\n",
      "    val_loss       : -878256.12890625\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -991926.312500\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -859289.625000\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -882901.937500\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -993170.562500\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -893160.937500\n",
      "    epoch          : 335\n",
      "    loss           : -886100.614480198\n",
      "    val_loss       : -877216.82265625\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -991847.437500\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -917964.000000\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -839668.375000\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -847992.812500\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -874018.687500\n",
      "    epoch          : 336\n",
      "    loss           : -882729.792079208\n",
      "    val_loss       : -871290.63203125\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -991321.687500\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -847599.500000\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -885451.750000\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -888527.062500\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -838978.875000\n",
      "    epoch          : 337\n",
      "    loss           : -881986.4554455446\n",
      "    val_loss       : -877274.79140625\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -996705.437500\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -848237.937500\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -893658.375000\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -893832.625000\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -880583.375000\n",
      "    epoch          : 338\n",
      "    loss           : -885936.3607673268\n",
      "    val_loss       : -877632.61328125\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -994771.312500\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -913306.000000\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -832313.000000\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -880738.750000\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -890124.875000\n",
      "    epoch          : 339\n",
      "    loss           : -884372.6243811881\n",
      "    val_loss       : -878661.5359375\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -993291.250000\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -837888.812500\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -851348.500000\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -992863.750000\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -844206.125000\n",
      "    epoch          : 340\n",
      "    loss           : -886734.5538366337\n",
      "    val_loss       : -879395.97734375\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -997216.687500\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -907587.125000\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -842001.875000\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -995162.687500\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -891458.875000\n",
      "    epoch          : 341\n",
      "    loss           : -886274.3001237623\n",
      "    val_loss       : -878651.78984375\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -856698.687500\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -914897.500000\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -889817.000000\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -887095.812500\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -851634.500000\n",
      "    epoch          : 342\n",
      "    loss           : -886189.926980198\n",
      "    val_loss       : -877750.6015625\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -994414.250000\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -833429.312500\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -887111.062500\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -849665.125000\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -845539.625000\n",
      "    epoch          : 343\n",
      "    loss           : -887136.6076732674\n",
      "    val_loss       : -879625.54140625\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -918976.312500\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -857456.125000\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -891106.687500\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -995586.625000\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -892890.062500\n",
      "    epoch          : 344\n",
      "    loss           : -887893.1367574257\n",
      "    val_loss       : -879607.875\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -996686.750000\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -847051.125000\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -846691.375000\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -910195.500000\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -886456.375000\n",
      "    epoch          : 345\n",
      "    loss           : -888629.4622524752\n",
      "    val_loss       : -879538.45390625\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -997836.187500\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -841320.000000\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -905845.062500\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -886238.375000\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -882100.875000\n",
      "    epoch          : 346\n",
      "    loss           : -885362.4158415842\n",
      "    val_loss       : -873191.234375\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -831832.000000\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -839011.000000\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -839534.500000\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -881557.875000\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -878105.312500\n",
      "    epoch          : 347\n",
      "    loss           : -880616.4523514851\n",
      "    val_loss       : -872605.9234375\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -993947.125000\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -917279.000000\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -840228.437500\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -995707.250000\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -883745.437500\n",
      "    epoch          : 348\n",
      "    loss           : -884577.6553217822\n",
      "    val_loss       : -880978.04140625\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -997024.687500\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -889664.125000\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -843545.625000\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -905052.625000\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -896257.625000\n",
      "    epoch          : 349\n",
      "    loss           : -888318.3929455446\n",
      "    val_loss       : -880947.74609375\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -924207.250000\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -919781.687500\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -886999.375000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -912821.875000\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -893741.687500\n",
      "    epoch          : 350\n",
      "    loss           : -887861.7871287129\n",
      "    val_loss       : -880640.925\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0714_125235/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -999652.875000\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -921186.937500\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -858486.500000\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -910095.750000\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -848843.750000\n",
      "    epoch          : 351\n",
      "    loss           : -890508.4783415842\n",
      "    val_loss       : -881549.75546875\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -999991.687500\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -889001.625000\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -841078.062500\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -893205.125000\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -879254.187500\n",
      "    epoch          : 352\n",
      "    loss           : -886787.7988861386\n",
      "    val_loss       : -879111.91171875\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -996623.250000\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -850944.625000\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -840189.750000\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -905884.625000\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -855475.937500\n",
      "    epoch          : 353\n",
      "    loss           : -887442.5884900991\n",
      "    val_loss       : -880697.5796875\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -1000214.500000\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -923401.125000\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -845068.875000\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -912049.187500\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -846361.750000\n",
      "    epoch          : 354\n",
      "    loss           : -890266.2883663366\n",
      "    val_loss       : -882028.325\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -926499.812500\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -909458.000000\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -844856.000000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -909093.062500\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -841360.000000\n",
      "    epoch          : 355\n",
      "    loss           : -888970.9084158416\n",
      "    val_loss       : -876802.22265625\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -998781.187500\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -861457.500000\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -909179.875000\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -911905.937500\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -888747.125000\n",
      "    epoch          : 356\n",
      "    loss           : -889320.5247524752\n",
      "    val_loss       : -881275.26171875\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -999095.125000\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -924991.625000\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -892261.875000\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -906616.000000\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -840013.562500\n",
      "    epoch          : 357\n",
      "    loss           : -888555.8948019802\n",
      "    val_loss       : -879756.778125\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -995368.062500\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -897974.875000\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -915467.062500\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -850169.625000\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -899470.875000\n",
      "    epoch          : 358\n",
      "    loss           : -890952.8638613861\n",
      "    val_loss       : -881726.84765625\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -1000801.375000\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -924183.062500\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -898379.937500\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -846695.125000\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -846047.562500\n",
      "    epoch          : 359\n",
      "    loss           : -891704.7017326732\n",
      "    val_loss       : -882071.4046875\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -999866.875000\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -923153.562500\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -846239.000000\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -894079.500000\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -846800.750000\n",
      "    epoch          : 360\n",
      "    loss           : -890686.1120049505\n",
      "    val_loss       : -881519.15703125\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -861780.125000\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -922441.375000\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -908963.250000\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -911897.312500\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -889049.125000\n",
      "    epoch          : 361\n",
      "    loss           : -890902.6113861386\n",
      "    val_loss       : -882503.74375\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -1000937.750000\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -918239.062500\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -911516.812500\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -846563.625000\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -897112.000000\n",
      "    epoch          : 362\n",
      "    loss           : -889463.0334158416\n",
      "    val_loss       : -882985.83828125\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -999631.125000\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -870734.875000\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -911522.375000\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -1000918.437500\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -901989.187500\n",
      "    epoch          : 363\n",
      "    loss           : -892922.1206683168\n",
      "    val_loss       : -883214.13359375\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -1000199.687500\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -899373.875000\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -910685.500000\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -892249.625000\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -837044.375000\n",
      "    epoch          : 364\n",
      "    loss           : -891155.1844059406\n",
      "    val_loss       : -880117.86640625\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -1000085.250000\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -922545.875000\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -891296.875000\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -890885.375000\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -897565.625000\n",
      "    epoch          : 365\n",
      "    loss           : -890629.7332920792\n",
      "    val_loss       : -882732.08671875\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -926543.000000\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -860522.000000\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -862402.875000\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -850026.625000\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -850164.750000\n",
      "    epoch          : 366\n",
      "    loss           : -893358.9108910891\n",
      "    val_loss       : -883349.2875\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -1003061.000000\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -861139.812500\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -899323.125000\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -910487.562500\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -846517.562500\n",
      "    epoch          : 367\n",
      "    loss           : -892683.6974009901\n",
      "    val_loss       : -882122.74765625\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -1002230.750000\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -859968.875000\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -907138.750000\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -850835.375000\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -841613.125000\n",
      "    epoch          : 368\n",
      "    loss           : -889409.8910891089\n",
      "    val_loss       : -882594.35390625\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -1002730.625000\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -922145.125000\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -847940.687500\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -892204.625000\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -897449.125000\n",
      "    epoch          : 369\n",
      "    loss           : -892209.7060643565\n",
      "    val_loss       : -883765.0296875\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -1000783.375000\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -857868.625000\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -913636.500000\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -856234.500000\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -900461.312500\n",
      "    epoch          : 370\n",
      "    loss           : -894305.7419554455\n",
      "    val_loss       : -884025.51796875\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -928211.812500\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -858777.062500\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -857657.187500\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -847533.375000\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -854883.625000\n",
      "    epoch          : 371\n",
      "    loss           : -894702.323019802\n",
      "    val_loss       : -884398.16875\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -1005798.812500\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -865547.000000\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -911682.875000\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -1005158.875000\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -899306.562500\n",
      "    epoch          : 372\n",
      "    loss           : -894325.9257425743\n",
      "    val_loss       : -884347.08828125\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -1001947.000000\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -858354.750000\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -845170.500000\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -860686.875000\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -847524.562500\n",
      "    epoch          : 373\n",
      "    loss           : -892456.7271039604\n",
      "    val_loss       : -883379.25234375\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -1002468.125000\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -847878.875000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -850299.000000\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -890048.312500\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -847678.500000\n",
      "    epoch          : 374\n",
      "    loss           : -891431.707920792\n",
      "    val_loss       : -883100.1953125\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -1003357.750000\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -916455.062500\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -908111.125000\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -858123.125000\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -858767.312500\n",
      "    epoch          : 375\n",
      "    loss           : -891134.6410891089\n",
      "    val_loss       : -884073.46171875\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -1002886.750000\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -861217.125000\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -883753.687500\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -833457.687500\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -888689.000000\n",
      "    epoch          : 376\n",
      "    loss           : -886582.9913366337\n",
      "    val_loss       : -881206.38203125\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -1003006.125000\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -854069.500000\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -851912.750000\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -862977.125000\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -852221.875000\n",
      "    epoch          : 377\n",
      "    loss           : -893045.228960396\n",
      "    val_loss       : -883288.5578125\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -1003477.250000\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -855299.125000\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -915185.312500\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -916889.562500\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -894862.375000\n",
      "    epoch          : 378\n",
      "    loss           : -895241.6806930694\n",
      "    val_loss       : -884982.97578125\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -1005268.250000\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -928671.750000\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -894927.625000\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -845875.687500\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -890880.250000\n",
      "    epoch          : 379\n",
      "    loss           : -892658.6639851485\n",
      "    val_loss       : -883790.1640625\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -1004700.312500\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -857595.625000\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -891201.250000\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -843997.500000\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -895560.500000\n",
      "    epoch          : 380\n",
      "    loss           : -892978.2896039604\n",
      "    val_loss       : -885194.7828125\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -1005156.250000\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -866564.312500\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -901263.312500\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -904199.312500\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -852056.875000\n",
      "    epoch          : 381\n",
      "    loss           : -895357.7611386139\n",
      "    val_loss       : -885034.8859375\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -865639.187500\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -931375.625000\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -898918.500000\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -862150.937500\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -900814.312500\n",
      "    epoch          : 382\n",
      "    loss           : -895697.6621287129\n",
      "    val_loss       : -883536.6578125\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -1003411.500000\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -857778.750000\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -890232.750000\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -904811.375000\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -874025.500000\n",
      "    epoch          : 383\n",
      "    loss           : -888160.4777227723\n",
      "    val_loss       : -879319.94609375\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -1002566.250000\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -921933.750000\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -855853.750000\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -864304.687500\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -853211.625000\n",
      "    epoch          : 384\n",
      "    loss           : -893247.9548267326\n",
      "    val_loss       : -886244.9515625\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -1006203.000000\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -849533.750000\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -853570.437500\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -864192.500000\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -846938.812500\n",
      "    epoch          : 385\n",
      "    loss           : -897043.5402227723\n",
      "    val_loss       : -886442.32734375\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -1005807.875000\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -932742.250000\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -915464.750000\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -917553.625000\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -897034.750000\n",
      "    epoch          : 386\n",
      "    loss           : -897755.4956683168\n",
      "    val_loss       : -885999.99921875\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -1007109.750000\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -926182.187500\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -862176.375000\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -848041.875000\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -892505.937500\n",
      "    epoch          : 387\n",
      "    loss           : -895156.3657178218\n",
      "    val_loss       : -883722.04375\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -1003486.750000\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -849744.500000\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -866478.562500\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -855014.562500\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -850641.125000\n",
      "    epoch          : 388\n",
      "    loss           : -895094.2654702971\n",
      "    val_loss       : -883606.34375\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -1005168.375000\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -864090.437500\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -852321.500000\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -894070.812500\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -894631.125000\n",
      "    epoch          : 389\n",
      "    loss           : -893136.3415841584\n",
      "    val_loss       : -884834.62890625\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -930474.812500\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -850151.437500\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -894532.625000\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -854800.250000\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -904071.937500\n",
      "    epoch          : 390\n",
      "    loss           : -896000.8681930694\n",
      "    val_loss       : -886929.5640625\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -1006521.250000\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -866460.750000\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -862770.000000\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -899721.125000\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -853787.875000\n",
      "    epoch          : 391\n",
      "    loss           : -897660.8879950495\n",
      "    val_loss       : -886697.23125\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -1006913.062500\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -918692.500000\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -893647.750000\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -912574.125000\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -901432.250000\n",
      "    epoch          : 392\n",
      "    loss           : -896284.4721534654\n",
      "    val_loss       : -886509.35390625\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -1007242.500000\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -897599.437500\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -852695.125000\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -1002989.875000\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -847504.562500\n",
      "    epoch          : 393\n",
      "    loss           : -895177.5445544554\n",
      "    val_loss       : -885926.12890625\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -1008438.125000\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -900480.250000\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -865041.375000\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -901131.000000\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -847306.062500\n",
      "    epoch          : 394\n",
      "    loss           : -894054.7425742574\n",
      "    val_loss       : -885931.93515625\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -1006368.187500\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -930022.125000\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -865629.562500\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -906614.750000\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -893616.312500\n",
      "    epoch          : 395\n",
      "    loss           : -896929.3378712871\n",
      "    val_loss       : -884397.81640625\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -856758.750000\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -925864.000000\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -911219.375000\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -893184.000000\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -852917.687500\n",
      "    epoch          : 396\n",
      "    loss           : -893763.7512376237\n",
      "    val_loss       : -884611.53828125\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -929395.687500\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -927535.625000\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -912078.625000\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -1004935.812500\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -897465.625000\n",
      "    epoch          : 397\n",
      "    loss           : -894311.2673267326\n",
      "    val_loss       : -886432.81796875\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -1008495.875000\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -854308.625000\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -864915.125000\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -903298.812500\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -850314.375000\n",
      "    epoch          : 398\n",
      "    loss           : -897972.8663366337\n",
      "    val_loss       : -886052.0203125\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -1006893.000000\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -869211.187500\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -905342.062500\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -869855.062500\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -896335.375000\n",
      "    epoch          : 399\n",
      "    loss           : -897609.1293316832\n",
      "    val_loss       : -886945.73515625\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -1008576.625000\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -933642.812500\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -861220.875000\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -865949.375000\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -841957.437500\n",
      "    epoch          : 400\n",
      "    loss           : -897629.9245049505\n",
      "    val_loss       : -886365.49453125\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0714_125235/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -1006599.000000\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -930939.000000\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -915742.437500\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -901622.937500\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -903325.250000\n",
      "    epoch          : 401\n",
      "    loss           : -898275.0259900991\n",
      "    val_loss       : -888170.21640625\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -870822.250000\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -929405.312500\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -910115.000000\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -846710.562500\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -900764.500000\n",
      "    epoch          : 402\n",
      "    loss           : -894795.1658415842\n",
      "    val_loss       : -887672.21171875\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -1004463.312500\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -871775.625000\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -854642.062500\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -853997.937500\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -896903.125000\n",
      "    epoch          : 403\n",
      "    loss           : -899644.4610148515\n",
      "    val_loss       : -888185.24453125\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -1010229.250000\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -931256.375000\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -856438.437500\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -856649.062500\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -904249.500000\n",
      "    epoch          : 404\n",
      "    loss           : -900256.2413366337\n",
      "    val_loss       : -887989.04765625\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -1009610.750000\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -865657.937500\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -868631.062500\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -866758.000000\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -848780.375000\n",
      "    epoch          : 405\n",
      "    loss           : -898746.6547029703\n",
      "    val_loss       : -886876.015625\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -1007094.312500\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -922824.875000\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -850110.250000\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -896706.625000\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -895614.062500\n",
      "    epoch          : 406\n",
      "    loss           : -897817.4146039604\n",
      "    val_loss       : -886980.7484375\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -1010112.125000\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -847096.750000\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -867482.875000\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -862320.625000\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -835593.750000\n",
      "    epoch          : 407\n",
      "    loss           : -891569.9152227723\n",
      "    val_loss       : -882439.78984375\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -1005618.875000\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -860629.875000\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -853786.375000\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -912722.625000\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -849571.125000\n",
      "    epoch          : 408\n",
      "    loss           : -895540.9641089109\n",
      "    val_loss       : -887102.053125\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -1005757.062500\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -910785.750000\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -849413.312500\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -862207.625000\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -902221.687500\n",
      "    epoch          : 409\n",
      "    loss           : -896383.9275990099\n",
      "    val_loss       : -888298.8859375\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -932401.625000\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -873286.125000\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -919955.500000\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -901604.125000\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -901217.625000\n",
      "    epoch          : 410\n",
      "    loss           : -900375.6701732674\n",
      "    val_loss       : -888041.7953125\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -1008005.500000\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -866162.375000\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -900838.625000\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -901807.437500\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -898584.875000\n",
      "    epoch          : 411\n",
      "    loss           : -901453.5736386139\n",
      "    val_loss       : -887909.84296875\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -1009787.750000\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -868520.000000\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -851929.187500\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -1007416.437500\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -870420.062500\n",
      "    epoch          : 412\n",
      "    loss           : -896955.542079208\n",
      "    val_loss       : -886260.52578125\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -855075.062500\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -869419.375000\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -918941.562500\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -854721.750000\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -898399.500000\n",
      "    epoch          : 413\n",
      "    loss           : -900101.6775990099\n",
      "    val_loss       : -887314.759375\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -1010962.937500\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -916884.000000\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -913008.000000\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -918131.312500\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -897705.812500\n",
      "    epoch          : 414\n",
      "    loss           : -897249.8719059406\n",
      "    val_loss       : -887145.4546875\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -1008853.375000\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -929610.062500\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -854847.000000\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -920426.625000\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -886785.812500\n",
      "    epoch          : 415\n",
      "    loss           : -898143.4938118812\n",
      "    val_loss       : -885621.0359375\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -854713.250000\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -863798.875000\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -901620.000000\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -1008117.625000\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -850967.312500\n",
      "    epoch          : 416\n",
      "    loss           : -898837.0216584158\n",
      "    val_loss       : -888841.3390625\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -1012742.937500\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -903660.000000\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -898433.250000\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -852307.937500\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -895052.750000\n",
      "    epoch          : 417\n",
      "    loss           : -897962.0532178218\n",
      "    val_loss       : -888293.8328125\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -1009862.875000\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -848224.062500\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -868924.250000\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -860654.500000\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -852191.937500\n",
      "    epoch          : 418\n",
      "    loss           : -899679.7846534654\n",
      "    val_loss       : -887105.47734375\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -1010474.375000\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -858187.000000\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -855090.562500\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -903189.500000\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -902292.437500\n",
      "    epoch          : 419\n",
      "    loss           : -900157.0693069306\n",
      "    val_loss       : -888865.15\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -1010802.375000\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -917920.875000\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -852945.625000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -857334.812500\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -898282.375000\n",
      "    epoch          : 420\n",
      "    loss           : -900153.7840346535\n",
      "    val_loss       : -889397.4203125\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -1010768.437500\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -866298.312500\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -907473.375000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -907368.000000\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -904694.750000\n",
      "    epoch          : 421\n",
      "    loss           : -901544.9418316832\n",
      "    val_loss       : -887889.48125\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -1011263.375000\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -848646.500000\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -913800.750000\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -861499.312500\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -904356.812500\n",
      "    epoch          : 422\n",
      "    loss           : -896754.9492574257\n",
      "    val_loss       : -888719.71953125\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -1011753.375000\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -869253.125000\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -859906.125000\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -902005.312500\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -909887.437500\n",
      "    epoch          : 423\n",
      "    loss           : -901861.3780940594\n",
      "    val_loss       : -888843.26015625\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -1010823.187500\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -868933.625000\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -858745.000000\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -908875.000000\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -855847.625000\n",
      "    epoch          : 424\n",
      "    loss           : -901304.5773514851\n",
      "    val_loss       : -889134.93125\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -936042.312500\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -873737.812500\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -909272.125000\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -910054.125000\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -901046.000000\n",
      "    epoch          : 425\n",
      "    loss           : -901427.073019802\n",
      "    val_loss       : -887772.07578125\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -857320.500000\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -935414.125000\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -856883.250000\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -904970.375000\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -860586.500000\n",
      "    epoch          : 426\n",
      "    loss           : -900725.4529702971\n",
      "    val_loss       : -890296.0859375\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -1008861.375000\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -937685.250000\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -872593.625000\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -915577.125000\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -856481.687500\n",
      "    epoch          : 427\n",
      "    loss           : -902606.3879950495\n",
      "    val_loss       : -888926.9953125\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -1011835.375000\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -856700.062500\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -853123.437500\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -899882.000000\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -863476.937500\n",
      "    epoch          : 428\n",
      "    loss           : -901668.5569306931\n",
      "    val_loss       : -887966.78671875\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -1010185.250000\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -932005.250000\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -870547.312500\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -864300.250000\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -898059.937500\n",
      "    epoch          : 429\n",
      "    loss           : -898884.4839108911\n",
      "    val_loss       : -888773.225\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -1013645.687500\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -870323.937500\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -903667.750000\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -870231.000000\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -910693.437500\n",
      "    epoch          : 430\n",
      "    loss           : -902326.8100247525\n",
      "    val_loss       : -889831.6375\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -1011788.875000\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -907580.375000\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -854494.000000\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -906733.187500\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -850092.437500\n",
      "    epoch          : 431\n",
      "    loss           : -900518.4436881188\n",
      "    val_loss       : -888342.7234375\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -1011071.937500\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -865378.937500\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -854228.687500\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -865914.812500\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -857806.812500\n",
      "    epoch          : 432\n",
      "    loss           : -899732.4003712871\n",
      "    val_loss       : -889220.31484375\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -1011828.625000\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -871193.625000\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -908781.375000\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -909637.000000\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -870197.125000\n",
      "    epoch          : 433\n",
      "    loss           : -902221.8199257426\n",
      "    val_loss       : -887947.18125\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -1010093.437500\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -922414.312500\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -851413.437500\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -1011848.250000\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -904426.562500\n",
      "    epoch          : 434\n",
      "    loss           : -900029.4461633663\n",
      "    val_loss       : -887999.74375\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -1013087.875000\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -937920.250000\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -900666.125000\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -913908.875000\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -910588.500000\n",
      "    epoch          : 435\n",
      "    loss           : -898679.4300742574\n",
      "    val_loss       : -886591.16484375\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -1009980.125000\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -933383.375000\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -919862.000000\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -902776.500000\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -919637.062500\n",
      "    epoch          : 436\n",
      "    loss           : -902119.7722772277\n",
      "    val_loss       : -890142.11015625\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -1010615.250000\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -925937.875000\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -851809.062500\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -854687.250000\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -870568.250000\n",
      "    epoch          : 437\n",
      "    loss           : -898900.082920792\n",
      "    val_loss       : -889735.56875\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -1014081.812500\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -859953.000000\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -856638.750000\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -872023.250000\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -857840.500000\n",
      "    epoch          : 438\n",
      "    loss           : -903360.8162128713\n",
      "    val_loss       : -890582.59296875\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -1012773.437500\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -871001.875000\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -910719.000000\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -902309.937500\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -871423.875000\n",
      "    epoch          : 439\n",
      "    loss           : -903580.4876237623\n",
      "    val_loss       : -889970.471875\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -873542.250000\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -872140.750000\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -870484.062500\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -907773.375000\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -894581.500000\n",
      "    epoch          : 440\n",
      "    loss           : -901632.1324257426\n",
      "    val_loss       : -887549.58984375\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -934594.562500\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -868970.375000\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -903199.187500\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -908229.500000\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -900931.125000\n",
      "    epoch          : 441\n",
      "    loss           : -900226.5674504951\n",
      "    val_loss       : -889217.0359375\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -1011607.375000\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -935866.937500\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -858085.875000\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -919560.437500\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -904743.312500\n",
      "    epoch          : 442\n",
      "    loss           : -902742.4257425743\n",
      "    val_loss       : -890014.12421875\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -935982.625000\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -873110.687500\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -873936.750000\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -855380.500000\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -858425.937500\n",
      "    epoch          : 443\n",
      "    loss           : -904126.8533415842\n",
      "    val_loss       : -890566.76015625\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -1013177.125000\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -870987.750000\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -862314.125000\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -903031.687500\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -906038.250000\n",
      "    epoch          : 444\n",
      "    loss           : -903438.8459158416\n",
      "    val_loss       : -889395.96328125\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -860371.000000\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -937958.000000\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -918708.500000\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -922654.000000\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -909899.375000\n",
      "    epoch          : 445\n",
      "    loss           : -903536.2568069306\n",
      "    val_loss       : -890589.5265625\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -1011094.812500\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -876020.687500\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -923234.125000\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -902138.437500\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -850394.375000\n",
      "    epoch          : 446\n",
      "    loss           : -902157.7240099009\n",
      "    val_loss       : -889010.3328125\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -1012148.875000\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -870670.312500\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -862283.562500\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -905079.812500\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -905160.250000\n",
      "    epoch          : 447\n",
      "    loss           : -903295.0643564357\n",
      "    val_loss       : -890822.6484375\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -1014339.437500\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -934568.062500\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -859020.250000\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -868738.750000\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -908839.125000\n",
      "    epoch          : 448\n",
      "    loss           : -903126.2840346535\n",
      "    val_loss       : -890231.03359375\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -1011823.125000\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -871760.500000\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -913696.562500\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -912697.750000\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -906519.875000\n",
      "    epoch          : 449\n",
      "    loss           : -905407.5024752475\n",
      "    val_loss       : -891274.859375\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -1013477.125000\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -855359.750000\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -859683.625000\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -869207.375000\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -900540.187500\n",
      "    epoch          : 450\n",
      "    loss           : -903709.780940594\n",
      "    val_loss       : -890144.80859375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0714_125235/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -1012896.062500\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -874394.875000\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -852431.875000\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -907420.625000\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -898733.187500\n",
      "    epoch          : 451\n",
      "    loss           : -901548.7493811881\n",
      "    val_loss       : -889180.06484375\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -1012168.500000\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -868964.625000\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -857392.125000\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -908593.062500\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -869081.250000\n",
      "    epoch          : 452\n",
      "    loss           : -902109.2407178218\n",
      "    val_loss       : -889721.6859375\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -1014917.250000\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -873598.625000\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -876112.937500\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -854781.687500\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -904039.625000\n",
      "    epoch          : 453\n",
      "    loss           : -903753.021039604\n",
      "    val_loss       : -891438.80859375\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -1014843.250000\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -934402.250000\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -865370.000000\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -1011683.187500\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -860395.625000\n",
      "    epoch          : 454\n",
      "    loss           : -903028.4573019802\n",
      "    val_loss       : -889893.153125\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -1015427.625000\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -868863.000000\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -857505.937500\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -891198.687500\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -871376.250000\n",
      "    epoch          : 455\n",
      "    loss           : -901695.3162128713\n",
      "    val_loss       : -889309.67890625\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -1014116.625000\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -866187.062500\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -869460.375000\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -893552.812500\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -909710.687500\n",
      "    epoch          : 456\n",
      "    loss           : -902167.3459158416\n",
      "    val_loss       : -891115.171875\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -1012883.062500\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -855347.000000\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -870698.687500\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -921265.875000\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -860260.250000\n",
      "    epoch          : 457\n",
      "    loss           : -903241.1757425743\n",
      "    val_loss       : -890559.83515625\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -1016655.250000\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -854517.000000\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -920878.812500\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -907312.500000\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -874221.500000\n",
      "    epoch          : 458\n",
      "    loss           : -904668.614480198\n",
      "    val_loss       : -891415.9671875\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -1015586.250000\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -933138.812500\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -862794.500000\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -1015208.750000\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -858209.125000\n",
      "    epoch          : 459\n",
      "    loss           : -904620.5228960396\n",
      "    val_loss       : -889169.090625\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -1014394.812500\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -868126.062500\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -891010.625000\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -859194.250000\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -848774.500000\n",
      "    epoch          : 460\n",
      "    loss           : -896632.9820544554\n",
      "    val_loss       : -889879.1921875\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -1013314.250000\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -935872.625000\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -909118.687500\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -907299.250000\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -902242.875000\n",
      "    epoch          : 461\n",
      "    loss           : -903554.6311881188\n",
      "    val_loss       : -891027.16875\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -1012248.250000\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -934976.687500\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -922743.500000\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -1016440.187500\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -905369.875000\n",
      "    epoch          : 462\n",
      "    loss           : -904916.1794554455\n",
      "    val_loss       : -890848.64140625\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -925467.125000\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -871300.875000\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -861816.500000\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -873625.687500\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -853882.250000\n",
      "    epoch          : 463\n",
      "    loss           : -904140.1683168317\n",
      "    val_loss       : -890897.7796875\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -1016057.250000\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -874505.625000\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -902598.625000\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -1013136.500000\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -902809.062500\n",
      "    epoch          : 464\n",
      "    loss           : -904146.3582920792\n",
      "    val_loss       : -890482.1140625\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -1015907.000000\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -917776.125000\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -867768.875000\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -1012684.812500\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -905397.562500\n",
      "    epoch          : 465\n",
      "    loss           : -902344.9665841584\n",
      "    val_loss       : -890640.1828125\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -1015554.875000\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -863819.187500\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -859853.375000\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -1013961.375000\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -874247.750000\n",
      "    epoch          : 466\n",
      "    loss           : -904649.8613861386\n",
      "    val_loss       : -890796.77109375\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -1015368.625000\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -922994.687500\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -906496.812500\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -872740.687500\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -907279.000000\n",
      "    epoch          : 467\n",
      "    loss           : -904603.4152227723\n",
      "    val_loss       : -891424.80234375\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -1016892.937500\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -941407.437500\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -873527.500000\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -862336.875000\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -906395.875000\n",
      "    epoch          : 468\n",
      "    loss           : -905160.7790841584\n",
      "    val_loss       : -890463.665625\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -1013987.875000\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -874973.125000\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -876035.187500\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -897595.375000\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -907486.500000\n",
      "    epoch          : 469\n",
      "    loss           : -905162.0922029703\n",
      "    val_loss       : -891267.7484375\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -1017414.000000\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -939321.375000\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -907035.312500\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -859295.312500\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -907711.250000\n",
      "    epoch          : 470\n",
      "    loss           : -905784.6528465346\n",
      "    val_loss       : -891786.78984375\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -1016670.312500\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -878199.000000\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -860202.875000\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -905268.750000\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -861612.625000\n",
      "    epoch          : 471\n",
      "    loss           : -906825.1825495049\n",
      "    val_loss       : -891907.10234375\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -1016185.500000\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -874345.750000\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -919981.687500\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -876179.000000\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -914356.562500\n",
      "    epoch          : 472\n",
      "    loss           : -906427.5990099009\n",
      "    val_loss       : -890731.76484375\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -1017471.187500\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -867701.937500\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -920769.250000\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -856333.687500\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -908326.250000\n",
      "    epoch          : 473\n",
      "    loss           : -905209.0278465346\n",
      "    val_loss       : -891462.09375\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -1016410.437500\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -904249.750000\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -925424.937500\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -876651.500000\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -903278.437500\n",
      "    epoch          : 474\n",
      "    loss           : -904786.7363861386\n",
      "    val_loss       : -891344.0921875\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -1016333.062500\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -870224.250000\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -860910.562500\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -863542.500000\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -856946.312500\n",
      "    epoch          : 475\n",
      "    loss           : -906110.3527227723\n",
      "    val_loss       : -891944.01875\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -1015805.000000\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -858739.625000\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -910907.750000\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -910841.000000\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -858510.750000\n",
      "    epoch          : 476\n",
      "    loss           : -904933.0897277228\n",
      "    val_loss       : -889760.846875\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -940261.062500\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -871904.500000\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -856041.625000\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -907869.500000\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -908507.687500\n",
      "    epoch          : 477\n",
      "    loss           : -904912.7722772277\n",
      "    val_loss       : -891919.5625\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -1017219.625000\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -874017.000000\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -924994.187500\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -922947.375000\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -909715.812500\n",
      "    epoch          : 478\n",
      "    loss           : -906114.1206683168\n",
      "    val_loss       : -891536.42265625\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -943350.375000\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -873978.375000\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -927354.000000\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -913566.437500\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -912876.125000\n",
      "    epoch          : 479\n",
      "    loss           : -906301.4610148515\n",
      "    val_loss       : -891288.1734375\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -1015886.250000\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -874584.312500\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -923511.750000\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -1018177.750000\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -915676.500000\n",
      "    epoch          : 480\n",
      "    loss           : -907474.4294554455\n",
      "    val_loss       : -891832.13671875\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -1016592.125000\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -877803.000000\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -925468.500000\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -908186.375000\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -909223.000000\n",
      "    epoch          : 481\n",
      "    loss           : -905916.5092821782\n",
      "    val_loss       : -892048.19296875\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -1017005.437500\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -872687.750000\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -875831.125000\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -903545.187500\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -908307.500000\n",
      "    epoch          : 482\n",
      "    loss           : -906305.3626237623\n",
      "    val_loss       : -891315.3546875\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -1016764.375000\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -931650.000000\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -875803.187500\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -923660.187500\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -909136.562500\n",
      "    epoch          : 483\n",
      "    loss           : -905979.3149752475\n",
      "    val_loss       : -890890.27734375\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -1017927.000000\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -870188.500000\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -862136.062500\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -1015689.000000\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -906227.625000\n",
      "    epoch          : 484\n",
      "    loss           : -904330.9504950495\n",
      "    val_loss       : -891300.08203125\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -1014957.437500\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -869273.937500\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -857765.250000\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -911242.187500\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -908622.500000\n",
      "    epoch          : 485\n",
      "    loss           : -904970.2202970297\n",
      "    val_loss       : -891667.02265625\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -1016582.437500\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -862777.750000\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -910337.437500\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -875272.562500\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -906295.875000\n",
      "    epoch          : 486\n",
      "    loss           : -906970.5922029703\n",
      "    val_loss       : -891734.84453125\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -1017203.750000\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -936503.937500\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -856595.750000\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -909649.500000\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -907377.312500\n",
      "    epoch          : 487\n",
      "    loss           : -905093.4801980198\n",
      "    val_loss       : -892062.77734375\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -1018076.000000\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -874983.437500\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -923503.750000\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -1018767.187500\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -911189.375000\n",
      "    epoch          : 488\n",
      "    loss           : -908115.8118811881\n",
      "    val_loss       : -891977.6796875\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -940228.500000\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -924303.375000\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -927303.312500\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -1015009.000000\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -908497.250000\n",
      "    epoch          : 489\n",
      "    loss           : -906840.6602722772\n",
      "    val_loss       : -891744.18671875\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -1016886.125000\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -873246.875000\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -859486.250000\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -874395.875000\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -910055.687500\n",
      "    epoch          : 490\n",
      "    loss           : -905042.7945544554\n",
      "    val_loss       : -890256.2609375\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -1014459.687500\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -874078.125000\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -923781.375000\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -1016956.062500\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -858187.125000\n",
      "    epoch          : 491\n",
      "    loss           : -905908.792079208\n",
      "    val_loss       : -890822.13515625\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -1017964.437500\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -877365.500000\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -859912.375000\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -858686.687500\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -910233.875000\n",
      "    epoch          : 492\n",
      "    loss           : -906442.1695544554\n",
      "    val_loss       : -891607.6578125\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -1015981.625000\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -872915.250000\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -865254.000000\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -906120.125000\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -914328.187500\n",
      "    epoch          : 493\n",
      "    loss           : -904687.406559406\n",
      "    val_loss       : -891960.8625\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -1016330.437500\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -877998.812500\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -879906.250000\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -882250.750000\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -909661.875000\n",
      "    epoch          : 494\n",
      "    loss           : -908403.9077970297\n",
      "    val_loss       : -893513.096875\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -1017761.500000\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -877370.937500\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -921303.125000\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -865816.062500\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -913270.750000\n",
      "    epoch          : 495\n",
      "    loss           : -907674.4566831683\n",
      "    val_loss       : -893274.5890625\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -1018469.500000\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -876962.000000\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -920881.000000\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -907227.625000\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -909305.000000\n",
      "    epoch          : 496\n",
      "    loss           : -904512.1181930694\n",
      "    val_loss       : -891032.3234375\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -1016266.875000\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -866050.250000\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -857625.750000\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -908123.375000\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -869696.125000\n",
      "    epoch          : 497\n",
      "    loss           : -902950.8626237623\n",
      "    val_loss       : -890428.9703125\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -1014462.937500\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -869345.500000\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -910170.062500\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -1018392.500000\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -907549.312500\n",
      "    epoch          : 498\n",
      "    loss           : -905093.3007425743\n",
      "    val_loss       : -892444.9859375\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -1018967.125000\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -939880.125000\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -912321.125000\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -904405.812500\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -908775.687500\n",
      "    epoch          : 499\n",
      "    loss           : -907436.3310643565\n",
      "    val_loss       : -891720.21328125\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -1018691.312500\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -882242.500000\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -867316.500000\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -1016021.687500\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -913684.375000\n",
      "    epoch          : 500\n",
      "    loss           : -908249.6701732674\n",
      "    val_loss       : -892700.61171875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0714_125235/checkpoint-epoch500.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAECategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_3_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_9_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "  )\n",
       "  (guide_embedding): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (guide_confidences): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_distances): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=15, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASVklEQVR4nO3dfbBcdX3H8fcnlySQJ8kVEkPIA2oEI9pgr2CJrTAUVCwSZXRI1QmWGmcqtlbGytB2DJ12yPiAQqXUAJFEIuDwmFZawFDMIDX1ijGExkoSAwkJCZhiEpDk5t5v/9hz7eaye/be3XN3N/l9XjM7d/d8z9nz3c1+cs7u2bM/RQRmduQb0eoGzKw5HHazRDjsZolw2M0S4bCbJcJhN0uEw36EkbRI0q11LnuypJ9K2ivpz4vurWiSpkvaJ6mj1b0cDhz2gkh6l6THJP1a0m5JP5T0jlb3NUR/BTwSEeMj4rpWN1NLRDwTEeMiorfVvRwOHPYCSJoA/Cvwj0AnMBW4Ctjfyr7qMAN4slqxnbagko5q5fKHI4e9GG8CiIjbIqI3In4TEQ9GxDoASW+Q9LCkX0l6QdIKScf2Lyxpi6TPS1on6SVJN0uaLOnfsl3q70uamM07U1JIWihpu6Qdki6v1pikd2Z7HC9K+pmks6rM9zBwNvCNbNf4TZJukXSDpPslvQScLek1kpZLel7S05L+RtKI7D4uyfZovpatb7OkM7PpWyXtkrQgp9dHJF0t6b+yPaT7JHUOeNyXSnoGeLhs2lHZPCdIWpntWW2U9Mmy+14k6U5Jt0raA1wyqH/ZI0lE+NLgBZgA/ApYBrwPmDig/kbgXGA0cDywGvh6WX0L8CNgMqW9gl3A48Bp2TIPA1/M5p0JBHAbMBZ4K/A88IdZfRFwa3Z9atbX+ZT+Yz83u318lcfxCPCnZbdvAX4NzM2WPxpYDtwHjM96+QVwaTb/JcBB4BNAB/D3wDPA9dnjOA/YC4zLWf+zwKnZY7ur7LH0P+7lWe2YsmlHZfP8APinrM852fNyTtnz0gPMyx7LMa1+3TT9ddrqBo6UC/DmLBzbshf8SmBylXnnAT8tu70F+GjZ7buAG8pufwa4N7ve/wI/paz+JeDm7Hp52L8AfHvAuh8AFlTpq1LYl5fd7qD01mR22bRPUXqf3x/2p8pqb816nVw27VfAnJz1Ly67PRs4kK23/3G/vqz+27AD04BeYHxZ/WrglrLnZXWrXyetvHg3viARsSEiLomIEyltmU4Avg4gaZKk2yU9m+1C3gocN+AudpZd/02F2+MGzL+17PrT2foGmgF8ONulflHSi8C7gClDeGjl6zkOGJWtr3zdU8tuD+ybiKj1WKqt72lgJIc+V1up7ARgd0Tszemt2rJJcNiHQUT8nNJW8dRs0tWUtkBvi4gJwMcANbiaaWXXpwPbK8yzldKW/diyy9iIWDyE9ZSfFvkCpV3hGQPW/ewQ7q+WgY+rJ1tvpX7KbQc6JY3P6S3pUzwd9gJIOkXS5ZJOzG5PA+ZTeh8Opfe3+4AXJU0FPl/Aav9W0hhJb6H0HvmOCvPcClwg6T2SOiQdLems/j6HKkqHuL4L/IOk8ZJmAJ/L1lOUj0maLWkM8HfAnTGIQ2sRsRV4DLg6e5xvAy4FVhTY22HNYS/GXuAMYE32qfWPgPVA/6fkVwFvp/Rh1/eAuwtY5w+AjcAq4CsR8eDAGbIAXAhcSenDqq2U/qNp5N/9M8BLwGbgUeA7wNIG7m+gb1PaK3qO0gdtQ/lyz3xK7+O3A/dQ+lDzoQJ7O6wp+/DCDhOSZgK/BEZGxMHWdlMsSY9Q+nDxplb3ciTylt0sEQ67WSK8G2+WCG/ZzRLR1JMBRml0HM3YZq7SLCmv8BIHYn/F73A0eubQe4FrKX2d8aZaX9Y4mrGcoXMaWaWZ5VgTq6rW6t6Nz053vJ7SiR+zgfmSZtd7f2Y2vBp5z346sDEiNkfEAeB2Sl/gMLM21EjYp3LoiQXbOPSkAwCy8667JXX3HHa/5WB25Ggk7JU+BHjVcbyIWBIRXRHRNZLRDazOzBrRSNi3cegZSidS+cwrM2sDjYT9x8AsSSdJGgVcTOkHG8ysDdV96C0iDkq6jNIvn3QASyOi6o8VmllrNXScPSLuB+4vqBczG0b+uqxZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyWioVFcrTk2rTgtt77u3d+sWhszYlTR7RyiN/py6/e+dGzV2k1vm527bN8rr9TVk1XWUNglbQH2Ar3AwYjoKqIpMyteEVv2syPihQLux8yGkd+zmyWi0bAH8KCkn0haWGkGSQsldUvq7mF/g6szs3o1uhs/NyK2S5oEPCTp5xGxunyGiFgCLAGYoM5ocH1mVqeGtuwRsT37uwu4Bzi9iKbMrHh1h13SWEnj+68D5wHri2rMzIrVyG78ZOAeSf33852I+PdCukrM5RufzK2fN2ZtjXsY3mPpeTqUv724aNyeqrV5mx7LXfaCd1+UW+/d+Mvcuh2q7rBHxGbgdwrsxcyGkQ+9mSXCYTdLhMNulgiH3SwRDrtZInyKazOUDk9WtXrfKbn1WSP/M7e++eBrqtYWL/h47rKjNj2XW4+xx+TW33HXL3LrVx1f/bBircN2X/7+d3Lrn5v5e7l1O5S37GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhTRvB+PmaDOOEPnNG19R4wax+lp4r/hUPWe/faqte+vWJq77P7oya1/YOo76urpSLYmVrEndld8wXjLbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwuezHw7a+Dh6LZsu7qh72b19BwrsxLxlN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4ePs1pCOiRNz6xv/6Js51fxtzbmLP59bn0T+kM92qJpbdklLJe2StL5sWqekhyQ9lf3N/xc3s5YbzG78LcB7B0y7AlgVEbOAVdltM2tjNcMeEauB3QMmXwgsy64vA+YV3JeZFazeD+gmR8QOgOzvpGozSlooqVtSdw/761ydmTVq2D+Nj4glEdEVEV0jGT3cqzOzKuoN+05JUwCyv7uKa8nMhkO9YV8JLMiuLwDuK6YdMxsuNY+zS7oNOAs4TtI24IvAYuC7ki4FngE+PJxN2vDRUfkvgZffX/133wH+5fprc+sdqj6++46D+3KXnXS9j6MXqWbYI2J+lZJHezA7jPjrsmaJcNjNEuGwmyXCYTdLhMNulgif4nokyBnSecRbTs5d9Pgbt+fWvzX9n3PrfYzKrX/gqYHnUP2//Wc/n7ss9Nao21B4y26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcLH2Y8AR50wpWptxre25C577Qk/rHHv1Y/hA5z66Cdy6zMvfqJ68TAeivpw5C27WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIH2c/ArxySvXj7Bd1PpC7bE80ds74mTN+mVt/bnT1UYD6XnmloXXb0HjLbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslQtHEc4onqDPOkAd/LVrHazur1jb9Zf7vxp905jO59TvedGdufZyqH0cH2NX7ctXaJTN+P3dZn+8+dGtiFXtid8UfIai5ZZe0VNIuSevLpi2S9Kyktdnl/CIbNrPiDWY3/hag0rAeX4uIOdnl/mLbMrOi1Qx7RKwGdjehFzMbRo18QHeZpHXZbv7EajNJWiipW1J3D/sbWJ2ZNaLesN8AvAGYA+wAvlptxohYEhFdEdE1kvwPc8xs+NQV9ojYGRG9EdEH3AicXmxbZla0usIuqfycyg8C66vNa2btoeZxdkm3AWcBxwE7gS9mt+cAAWwBPhURO2qtzMfZ21DO2O5Qe3z3lQ+syK2PVEfV2vdePjp32eveeEpu3V4t7zh7zR+viIj5FSbf3HBXZtZU/rqsWSIcdrNEOOxmiXDYzRLhsJslwj8lnboah1771v88t37yPX+WW9/8oW9Wrb1/TP5PSV+XW7Wh8pbdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEj7NbQ2Zdtia3/vK8A1VrY0aMyl32xB+Ny61ve+e+3Lodylt2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRPs5uw+qPN11QtXbvrAdyl715+qO59fcwp66eUuUtu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WiJrH2SVNA5YDrwP6gCURca2kTuAOYCalYZs/EhH/O3yt2uHoupPuyqnmn6/eG33FNpO4wWzZDwKXR8SbgXcCn5Y0G7gCWBURs4BV2W0za1M1wx4ROyLi8ez6XmADMBW4EFiWzbYMmDdcTZpZ44b0nl3STOA0YA0wOSJ2QOk/BGBS0c2ZWXEGHXZJ44C7gM9GxJ4hLLdQUrek7h7219OjmRVgUGGXNJJS0FdExN3Z5J2SpmT1KcCuSstGxJKI6IqIrpGMLqJnM6tDzbBLEnAzsCEirikrrQQWZNcXAPcV356ZFWUwp7jOBT4OPCFpbTbtSmAx8F1JlwLPAB8enhaPAFJuuefc382td7x8MLc+4rEnqhf7enOXbdSzV5yZW59+1Nrcep5Tl1yWf988Vvd9p6hm2CPiUaDaq/WcYtsxs+Hib9CZJcJhN0uEw26WCIfdLBEOu1kiHHazRPinpNvAyC88l1u/5+S7c+t5fnog/5/42u3n5tavmZ7/XakTGziOvj96cuvTr/Jx9CJ5y26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcLH2ZshIrc84nPj85f/Xn55zIhRVWtzj85fdu7rV+XPUOPnnmt5ofelqrWPTpvb0H3b0HjLbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwsfZ20Dfzzbk1j80709y63fce2PV2mtGHFNXT/1e7juQWz/9G5/NrU9d7HPS24W37GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhQ1zrWWNA1YDrwO6AOWRMS1khYBnwSez2a9MiLuz7uvCeqMM+RRns2Gy5pYxZ7YXXGI9cF8qeYgcHlEPC5pPPATSQ9lta9FxFeKatTMhk/NsEfEDmBHdn2vpA3A1OFuzMyKNaT37JJmAqcBa7JJl0laJ2mppIlVllkoqVtSdw/7G2rWzOo36LBLGgfcBXw2IvYANwBvAOZQ2vJ/tdJyEbEkIroiomskowto2czqMaiwSxpJKegrIuJugIjYGRG9EdEH3AicPnxtmlmjaoZdkoCbgQ0RcU3Z9Clls30QWF98e2ZWlMF8Gj8X+DjwhKT+8XmvBOZLmgMEsAX41LB0aGaFGMyn8Y8ClY7b5R5TN7P24m/QmSXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0TU/CnpQlcmPQ88XTbpOOCFpjUwNO3aW7v2Be6tXkX2NiMijq9UaGrYX7VyqTsiulrWQI527a1d+wL3Vq9m9ebdeLNEOOxmiWh12Je0eP152rW3du0L3Fu9mtJbS9+zm1nztHrLbmZN4rCbJaIlYZf0Xkn/I2mjpCta0UM1krZIekLSWkndLe5lqaRdktaXTeuU9JCkp7K/FcfYa1FviyQ9mz13ayWd36Lepkn6D0kbJD0p6S+y6S197nL6asrz1vT37JI6gF8A5wLbgB8D8yPiv5vaSBWStgBdEdHyL2BI+gNgH7A8Ik7Npn0J2B0Ri7P/KCdGxBfapLdFwL5WD+OdjVY0pXyYcWAecAktfO5y+voITXjeWrFlPx3YGBGbI+IAcDtwYQv6aHsRsRrYPWDyhcCy7PoySi+WpqvSW1uIiB0R8Xh2fS/QP8x4S5+7nL6aohVhnwpsLbu9jfYa7z2AByX9RNLCVjdTweSI2AGlFw8wqcX9DFRzGO9mGjDMeNs8d/UMf96oVoS90lBS7XT8b25EvB14H/DpbHfVBmdQw3g3S4VhxttCvcOfN6oVYd8GTCu7fSKwvQV9VBQR27O/u4B7aL+hqHf2j6Cb/d3V4n5+q52G8a40zDht8Ny1cvjzVoT9x8AsSSdJGgVcDKxsQR+vImls9sEJksYC59F+Q1GvBBZk1xcA97Wwl0O0yzDe1YYZp8XPXcuHP4+Ipl+A8yl9Ir8J+OtW9FClr9cDP8suT7a6N+A2Srt1PZT2iC4FXgusAp7K/na2UW/fBp4A1lEK1pQW9fYuSm8N1wFrs8v5rX7ucvpqyvPmr8uaJcLfoDNLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEvF/mpSSzXGtA60AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAARCklEQVR4nO3df7BU5X3H8fcHuIL8MIEqFIGAGtNITYLJjdrCtDpUo850MNMxE6ZxsEOKM9W0aRwbx7YjZtLKZGJ+tE3tkMgIYjRO1EoS20oxhrFJiFdFwKLBEuRnQCVGRAMX+PaPPTjLdffce3fP7ll4Pq+Znbt7nnP2+e6yH87Z82MfRQRmduIbUnYBZtYeDrtZIhx2s0Q47GaJcNjNEuGwmyXCYT/BSFooaXmDy/6OpGck7ZP0l0XXVjRJ75H0hqShZddyPHDYCyJplqQfS/q1pL2S/kfSR8uua5D+Bng8IsZExD+VXUx/ImJrRIyOiMNl13I8cNgLIOkU4PvAPwPjgEnArcCBMutqwFTguXqNnbQGlTSszOWPRw57Md4HEBH3RsThiHgrIh6NiHUAks6S9JikVyW9IukeSe8+urCkLZJulLRO0n5Jd0qaIOk/sk3q/5Y0Npt3mqSQtEDSTkm7JN1QrzBJF2ZbHK9JelbSRXXmewy4GPiXbNP4fZLuknSHpEck7QculvQuScskvSzpJUl/J2lI9hzXZFs0X8362yzp97Pp2yTtkTQvp9bHJd0m6WfZFtLDksb1ed3zJW0FHquaNiyb53RJK7Itqxcl/XnVcy+U9F1JyyW9DlwzoH/ZE0lE+NbkDTgFeBVYClwOjO3T/l7gEmA4cBqwGvhaVfsW4KfABCpbBXuAp4HzsmUeA27J5p0GBHAvMAr4APAy8EdZ+0JgeXZ/UlbXFVT+Y78ke3xandfxOPDpqsd3Ab8GZmbLjwCWAQ8DY7Jafg7Mz+a/BjgE/BkwFPgisBX4RvY6LgX2AaNz+t8BnJu9tgeqXsvR170sazu5atqwbJ4fAf+a1Tkje19mV70vvcCV2Ws5uezPTds/p2UXcKLcgHOycGzPPvArgAl15r0SeKbq8RbgT6sePwDcUfX4M8C/Z/ePfsDfX9X+JeDO7H512D8P3N2n7/8C5tWpq1bYl1U9Hkrlq8n0qmnXUvmefzTsm6raPpDVOqFq2qvAjJz+F1U9ng4czPo9+rrPrGp/O+zAFOAwMKaq/Tbgrqr3ZXXZn5Myb96ML0hEbIyIayJiMpU10+nA1wAkjZd0n6Qd2SbkcuDUPk+xu+r+WzUej+4z/7aq+y9l/fU1Fbgq26R+TdJrwCxg4iBeWnU/pwInZf1V9z2p6nHfuomI/l5Lvf5eAro49r3aRm2nA3sjYl9ObfWWTYLD3gIR8TyVteK52aTbqKyBPhgRpwCfAtRkN1Oq7r8H2Fljnm1U1uzvrrqNiohFg+in+rLIV6hsCk/t0/eOQTxff/q+rt6s31r1VNsJjJM0Jqe2pC/xdNgLIOn9km6QNDl7PAWYS+V7OFS+374BvCZpEnBjAd3+vaSRkn6Xynfk79SYZznwx5I+JmmopBGSLjpa52BF5RDX/cA/SBojaSrwuayfonxK0nRJI4EvAN+NARxai4htwI+B27LX+UFgPnBPgbUd1xz2YuwDLgDWZHutfwpsAI7uJb8V+DCVnV0/AB4soM8fAS8Cq4AvR8SjfWfIAjAHuJnKzqptVP6jaebf/TPAfmAz8ATwbWBJE8/X191Utop+SWVH22BO7plL5Xv8TuAhKjs1VxZY23FN2c4LO05Imgb8AuiKiEPlVlMsSY9T2bn4rbJrORF5zW6WCIfdLBHejDdLhNfsZolo68UAJ2l4jGBUO7s0S8pv2M/BOFDzHI5mrxy6DPg6ldMZv9XfyRojGMUFmt1Ml2aWY02sqtvW8GZ8drnjN6hc+DEdmCtpeqPPZ2at1cx39vOBFyNic0QcBO6jcgKHmXWgZsI+iWMvLNjOsRcdAJBdd90jqaf3uPstB7MTRzNhr7UT4B3H8SJicUR0R0R3F8Ob6M7MmtFM2Ldz7BVKk6l95ZWZdYBmwv4kcLakMySdBHySyg82mFkHavjQW0QcknQ9lV8+GQosiYi6P1ZoZuVq6jh7RDwCPFJQLWbWQj5d1iwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEtHWIZutQao5Au/b5m7cUbft6jG/zF32sufzh+cbMntbbrsdP7xmN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4ePsx4EhJ5+c27679105rfnH2Vee873c9o8N+UhuO0cO57dbx2gq7JK2APuAw8ChiOguoigzK14Ra/aLI+KVAp7HzFrI39nNEtFs2AN4VNJTkhbUmkHSAkk9knp6OdBkd2bWqGY342dGxE5J44GVkp6PiNXVM0TEYmAxwCkaF032Z2YNamrNHhE7s797gIeA84soysyK13DYJY2SNObofeBSYENRhZlZsZrZjJ8APKTKtdbDgG9HxH8WUpUd48ibb+a2/7B7XN22M9aPzV32T0b9Krd92LQpue2HNm/JbbfO0XDYI2Iz8KECazGzFvKhN7NEOOxmiXDYzRLhsJslwmE3S4Qi2ndS2ykaFxdodtv6MxgycmRu+/c3PZHbvuVQ/mG/v5g6a9A1WeusiVW8Hntr/va41+xmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSL8U9InuP4ujz1C/nkWZ3WNLrIcK5HX7GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZInycPXFdGtrU8vF7+T8wrJ8829TzW3G8ZjdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuHj7Il748hvcttHDxmR2/6Fe5bktt9y5kcGXZO1Rr9rdklLJO2RtKFq2jhJKyVtyv7mDwJuZqUbyGb8XcBlfabdBKyKiLOBVdljM+tg/YY9IlYDe/tMngMsze4vBa4suC4zK1ijO+gmRMQugOzv+HozSlogqUdSTy8HGuzOzJrV8r3xEbE4IrojoruL4a3uzszqaDTsuyVNBMj+7imuJDNrhUbDvgKYl92fBzxcTDlm1ir9HmeXdC9wEXCqpO3ALcAi4H5J84GtwFWtLNJaZ9ErH81t/+L49bntk4e9VWQ51kL9hj0i5tZpml1wLWbWQj5d1iwRDrtZIhx2s0Q47GaJcNjNEuFLXBO35vp+LkG9P//Q2yh5fXG88L+UWSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIH2dPXNeGX+S298bh3PaRQ7qKLMdayGt2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRPs6euDjYm9vepaH9PEN++5FZM+q2DXlibT/PbUXymt0sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4SPsycuDh5s6fPfvvzf6rbdOO3ClvZtx+p3zS5piaQ9kjZUTVsoaYektdntitaWaWbNGshm/F3AZTWmfzUiZmS3R4oty8yK1m/YI2I1sLcNtZhZCzWzg+56Seuyzfyx9WaStEBSj6SeXg400Z2ZNaPRsN8BnAXMAHYBt9ebMSIWR0R3RHR3MbzB7sysWQ2FPSJ2R8ThiDgCfBM4v9iyzKxoDYVd0sSqhx8HNtSb18w6Q7/H2SXdC1wEnCppO3ALcJGkGUAAW4BrW1ijtVAcOpTbfiDyr3cfrvzfjT+ny78r3yn6DXtEzK0x+c4W1GJmLeTTZc0S4bCbJcJhN0uEw26WCIfdLBG+xNVy9RzI/6nomSOaeHIpvz2iiSe3vrxmN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4ePslusfL56T2/6Dn3wvt703DtdtGzJyZO6yR/bvz223wfGa3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhI+zW65DW7c3tfxw1f+Iadrk/IWfe6Gpvu1YXrObJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZokYyJDNU4BlwG8DR4DFEfF1SeOA7wDTqAzb/ImI+FXrSrUyaGj+78a/eeRgbvvIISfVbXvh02Nzl33vX+c22yANZM1+CLghIs4BLgSukzQduAlYFRFnA6uyx2bWofoNe0Tsioins/v7gI3AJGAOsDSbbSlwZauKNLPmDeo7u6RpwHnAGmBCROyCyn8IwPiiizOz4gw47JJGAw8An42I1wex3AJJPZJ6ejnQSI1mVoABhV1SF5Wg3xMRD2aTd0uamLVPBPbUWjYiFkdEd0R0dzG8iJrNrAH9hl2SgDuBjRHxlaqmFcC87P484OHiyzOzogzkEteZwNXAeklrs2k3A4uA+yXNB7YCV7WmRCvT0Mmn57Z3Kf/QXJ7LZz2T276p4We2WvoNe0Q8AdQbSHt2seWYWav4DDqzRDjsZolw2M0S4bCbJcJhN0uEw26WCP+UtOU69NK23PY//Nx1ue0Lbn2wbtv6Wz+Uu+wIfpbbboPjNbtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulghFRNs6O0Xj4gL5qtikqN7V0UAbP3upWBOreD321nzTvWY3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLh69mttXwsvWN4zW6WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJaLfsEuaIumHkjZKek7SX2XTF0raIWltdrui9eWaWaMGclLNIeCGiHha0hjgKUkrs7avRsSXW1eemRWl37BHxC5gV3Z/n6SNwKRWF2ZmxRrUd3ZJ04DzgDXZpOslrZO0RNLYOssskNQjqaeXA00Va2aNG3DYJY0GHgA+GxGvA3cAZwEzqKz5b6+1XEQsjojuiOjuYngBJZtZIwYUdkldVIJ+T0Q8CBARuyPicEQcAb4JnN+6Ms2sWQPZGy/gTmBjRHylavrEqtk+DmwovjwzK8pA9sbPBK4G1ktam027GZgraQYQwBbg2pZUaGaFGMje+CeAWr9D/Ujx5ZhZq/gMOrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIRRuH1JX0MvBS1aRTgVfaVsDgdGptnVoXuLZGFVnb1Ig4rVZDW8P+js6lnojoLq2AHJ1aW6fWBa6tUe2qzZvxZolw2M0SUXbYF5fcf55Ora1T6wLX1qi21Fbqd3Yza5+y1+xm1iYOu1kiSgm7pMskvSDpRUk3lVFDPZK2SFqfDUPdU3ItSyTtkbShato4SSslbcr+1hxjr6TaOmIY75xhxkt978oe/rzt39klDQV+DlwCbAeeBOZGxP+2tZA6JG0BuiOi9BMwJP0B8AawLCLOzaZ9CdgbEYuy/yjHRsTnO6S2hcAbZQ/jnY1WNLF6mHHgSuAaSnzvcur6BG1438pYs58PvBgRmyPiIHAfMKeEOjpeRKwG9vaZPAdYmt1fSuXD0nZ1ausIEbErIp7O7u8Djg4zXup7l1NXW5QR9knAtqrH2+ms8d4DeFTSU5IWlF1MDRMiYhdUPjzA+JLr6avfYbzbqc8w4x3z3jUy/Hmzygh7raGkOun438yI+DBwOXBdtrlqAzOgYbzbpcYw4x2h0eHPm1VG2LcDU6oeTwZ2llBHTRGxM/u7B3iIzhuKevfREXSzv3tKrudtnTSMd61hxumA967M4c/LCPuTwNmSzpB0EvBJYEUJdbyDpFHZjhMkjQIupfOGol4BzMvuzwMeLrGWY3TKMN71hhmn5Peu9OHPI6LtN+AKKnvk/w/42zJqqFPXmcCz2e25smsD7qWyWddLZYtoPvBbwCpgU/Z3XAfVdjewHlhHJVgTS6ptFpWvhuuAtdntirLfu5y62vK++XRZs0T4DDqzRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBH/DylXMjmYp2hEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    path.draw()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
