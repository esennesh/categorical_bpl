{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f3b65f8e220>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)\n",
    "model.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.5,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [256/54000 (0%)] Loss: 3501.937012\n",
      "Train Epoch: 1 [4352/54000 (8%)] Loss: -804.064880\n",
      "Train Epoch: 1 [8448/54000 (16%)] Loss: -13401.763672\n",
      "Train Epoch: 1 [12544/54000 (23%)] Loss: -19313.343750\n",
      "Train Epoch: 1 [16640/54000 (31%)] Loss: -20011.144531\n",
      "Train Epoch: 1 [20736/54000 (38%)] Loss: -37868.046875\n",
      "Train Epoch: 1 [24832/54000 (46%)] Loss: -51490.582031\n",
      "Train Epoch: 1 [28928/54000 (54%)] Loss: -20753.392578\n",
      "Train Epoch: 1 [33024/54000 (61%)] Loss: -96649.625000\n",
      "Train Epoch: 1 [37120/54000 (69%)] Loss: -43546.960938\n",
      "Train Epoch: 1 [41216/54000 (76%)] Loss: -91124.828125\n",
      "Train Epoch: 1 [45312/54000 (84%)] Loss: -95424.000000\n",
      "Train Epoch: 1 [49408/54000 (91%)] Loss: -144047.625000\n",
      "    epoch          : 1\n",
      "    loss           : -60782.528678307164\n",
      "    val_loss       : -124264.6130859375\n",
      "Train Epoch: 2 [256/54000 (0%)] Loss: -103154.570312\n",
      "Train Epoch: 2 [4352/54000 (8%)] Loss: -97057.210938\n",
      "Train Epoch: 2 [8448/54000 (16%)] Loss: -69887.468750\n",
      "Train Epoch: 2 [12544/54000 (23%)] Loss: -123865.515625\n",
      "Train Epoch: 2 [16640/54000 (31%)] Loss: -236234.062500\n",
      "Train Epoch: 2 [20736/54000 (38%)] Loss: -107573.781250\n",
      "Train Epoch: 2 [24832/54000 (46%)] Loss: -249539.546875\n",
      "Train Epoch: 2 [28928/54000 (54%)] Loss: -29043.175781\n",
      "Train Epoch: 2 [33024/54000 (61%)] Loss: -146017.312500\n",
      "Train Epoch: 2 [37120/54000 (69%)] Loss: -84782.671875\n",
      "Train Epoch: 2 [41216/54000 (76%)] Loss: -179947.593750\n",
      "Train Epoch: 2 [45312/54000 (84%)] Loss: -127626.609375\n",
      "Train Epoch: 2 [49408/54000 (91%)] Loss: -164584.390625\n",
      "    epoch          : 2\n",
      "    loss           : -131627.87810809797\n",
      "    val_loss       : -137779.498828125\n",
      "Train Epoch: 3 [256/54000 (0%)] Loss: -214575.609375\n",
      "Train Epoch: 3 [4352/54000 (8%)] Loss: -200844.312500\n",
      "Train Epoch: 3 [8448/54000 (16%)] Loss: -65346.078125\n",
      "Train Epoch: 3 [12544/54000 (23%)] Loss: -169612.156250\n",
      "Train Epoch: 3 [16640/54000 (31%)] Loss: -168486.671875\n",
      "Train Epoch: 3 [20736/54000 (38%)] Loss: -103389.679688\n",
      "Train Epoch: 3 [24832/54000 (46%)] Loss: -143720.359375\n",
      "Train Epoch: 3 [28928/54000 (54%)] Loss: -187617.093750\n",
      "Train Epoch: 3 [33024/54000 (61%)] Loss: -111958.015625\n",
      "Train Epoch: 3 [37120/54000 (69%)] Loss: -67359.734375\n",
      "Train Epoch: 3 [41216/54000 (76%)] Loss: -166931.328125\n",
      "Train Epoch: 3 [45312/54000 (84%)] Loss: -105441.679688\n",
      "Train Epoch: 3 [49408/54000 (91%)] Loss: -128872.179688\n",
      "    epoch          : 3\n",
      "    loss           : -148782.31896033653\n",
      "    val_loss       : -166427.6111328125\n",
      "Train Epoch: 4 [256/54000 (0%)] Loss: -76969.867188\n",
      "Train Epoch: 4 [4352/54000 (8%)] Loss: -179520.734375\n",
      "Train Epoch: 4 [8448/54000 (16%)] Loss: -152667.781250\n",
      "Train Epoch: 4 [12544/54000 (23%)] Loss: -153003.968750\n",
      "Train Epoch: 4 [16640/54000 (31%)] Loss: -122639.375000\n",
      "Train Epoch: 4 [20736/54000 (38%)] Loss: -163837.937500\n",
      "Train Epoch: 4 [24832/54000 (46%)] Loss: -166861.312500\n",
      "Train Epoch: 4 [28928/54000 (54%)] Loss: -163598.265625\n",
      "Train Epoch: 4 [33024/54000 (61%)] Loss: -133680.390625\n",
      "Train Epoch: 4 [37120/54000 (69%)] Loss: -147606.937500\n",
      "Train Epoch: 4 [41216/54000 (76%)] Loss: -210615.687500\n",
      "Train Epoch: 4 [45312/54000 (84%)] Loss: -152439.875000\n",
      "Train Epoch: 4 [49408/54000 (91%)] Loss: -217450.000000\n",
      "    epoch          : 4\n",
      "    loss           : -157456.34119121844\n",
      "    val_loss       : -166442.2388671875\n",
      "Train Epoch: 5 [256/54000 (0%)] Loss: -184453.234375\n",
      "Train Epoch: 5 [4352/54000 (8%)] Loss: -132194.515625\n",
      "Train Epoch: 5 [8448/54000 (16%)] Loss: -247834.890625\n",
      "Train Epoch: 5 [12544/54000 (23%)] Loss: -179518.625000\n",
      "Train Epoch: 5 [16640/54000 (31%)] Loss: -159007.093750\n",
      "Train Epoch: 5 [20736/54000 (38%)] Loss: -260524.328125\n",
      "Train Epoch: 5 [24832/54000 (46%)] Loss: -273008.812500\n",
      "Train Epoch: 5 [28928/54000 (54%)] Loss: -126377.640625\n",
      "Train Epoch: 5 [33024/54000 (61%)] Loss: -149946.437500\n",
      "Train Epoch: 5 [37120/54000 (69%)] Loss: -218019.796875\n",
      "Train Epoch: 5 [41216/54000 (76%)] Loss: -107440.132812\n",
      "Train Epoch: 5 [45312/54000 (84%)] Loss: -75168.898438\n",
      "Train Epoch: 5 [49408/54000 (91%)] Loss: -186544.796875\n",
      "    epoch          : 5\n",
      "    loss           : -165604.24047851562\n",
      "    val_loss       : -172334.621484375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 6 [256/54000 (0%)] Loss: -113478.523438\n",
      "Train Epoch: 6 [4352/54000 (8%)] Loss: -196810.703125\n",
      "Train Epoch: 6 [8448/54000 (16%)] Loss: -199747.765625\n",
      "Train Epoch: 6 [12544/54000 (23%)] Loss: -238193.093750\n",
      "Train Epoch: 6 [16640/54000 (31%)] Loss: -240096.875000\n",
      "Train Epoch: 6 [20736/54000 (38%)] Loss: -122263.554688\n",
      "Train Epoch: 6 [24832/54000 (46%)] Loss: -131182.765625\n",
      "Train Epoch: 6 [28928/54000 (54%)] Loss: -142364.359375\n",
      "Train Epoch: 6 [33024/54000 (61%)] Loss: -169524.218750\n",
      "Train Epoch: 6 [37120/54000 (69%)] Loss: -126608.164062\n",
      "Train Epoch: 6 [41216/54000 (76%)] Loss: -90670.546875\n",
      "Train Epoch: 6 [45312/54000 (84%)] Loss: -76915.859375\n",
      "Train Epoch: 6 [49408/54000 (91%)] Loss: -271054.500000\n",
      "    epoch          : 6\n",
      "    loss           : -173856.30784254806\n",
      "    val_loss       : -180222.00859375\n",
      "Train Epoch: 7 [256/54000 (0%)] Loss: -117846.617188\n",
      "Train Epoch: 7 [4352/54000 (8%)] Loss: -188485.921875\n",
      "Train Epoch: 7 [8448/54000 (16%)] Loss: -172535.859375\n",
      "Train Epoch: 7 [12544/54000 (23%)] Loss: -259553.109375\n",
      "Train Epoch: 7 [16640/54000 (31%)] Loss: -220656.875000\n",
      "Train Epoch: 7 [20736/54000 (38%)] Loss: -139959.234375\n",
      "Train Epoch: 7 [24832/54000 (46%)] Loss: -226825.140625\n",
      "Train Epoch: 7 [28928/54000 (54%)] Loss: -254181.312500\n",
      "Train Epoch: 7 [33024/54000 (61%)] Loss: -112751.570312\n",
      "Train Epoch: 7 [37120/54000 (69%)] Loss: -184536.156250\n",
      "Train Epoch: 7 [41216/54000 (76%)] Loss: -149536.046875\n",
      "Train Epoch: 7 [45312/54000 (84%)] Loss: -212337.328125\n",
      "Train Epoch: 7 [49408/54000 (91%)] Loss: -182859.046875\n",
      "    epoch          : 7\n",
      "    loss           : -177504.01079852766\n",
      "    val_loss       : -188600.446875\n",
      "Train Epoch: 8 [256/54000 (0%)] Loss: -111258.695312\n",
      "Train Epoch: 8 [4352/54000 (8%)] Loss: -232894.000000\n",
      "Train Epoch: 8 [8448/54000 (16%)] Loss: -234643.546875\n",
      "Train Epoch: 8 [12544/54000 (23%)] Loss: -258379.234375\n",
      "Train Epoch: 8 [16640/54000 (31%)] Loss: -149084.171875\n",
      "Train Epoch: 8 [20736/54000 (38%)] Loss: -143639.156250\n",
      "Train Epoch: 8 [24832/54000 (46%)] Loss: -239776.703125\n",
      "Train Epoch: 8 [28928/54000 (54%)] Loss: -103808.554688\n",
      "Train Epoch: 8 [33024/54000 (61%)] Loss: -248240.734375\n",
      "Train Epoch: 8 [37120/54000 (69%)] Loss: -147458.984375\n",
      "Train Epoch: 8 [41216/54000 (76%)] Loss: -159528.859375\n",
      "Train Epoch: 8 [45312/54000 (84%)] Loss: -221570.078125\n",
      "Train Epoch: 8 [49408/54000 (91%)] Loss: -193197.406250\n",
      "    epoch          : 8\n",
      "    loss           : -185226.15211838944\n",
      "    val_loss       : -185752.104296875\n",
      "Train Epoch: 9 [256/54000 (0%)] Loss: -279741.125000\n",
      "Train Epoch: 9 [4352/54000 (8%)] Loss: -214112.718750\n",
      "Train Epoch: 9 [8448/54000 (16%)] Loss: -202926.718750\n",
      "Train Epoch: 9 [12544/54000 (23%)] Loss: -134723.593750\n",
      "Train Epoch: 9 [16640/54000 (31%)] Loss: -127520.101562\n",
      "Train Epoch: 9 [20736/54000 (38%)] Loss: -129767.757812\n",
      "Train Epoch: 9 [24832/54000 (46%)] Loss: -164633.921875\n",
      "Train Epoch: 9 [28928/54000 (54%)] Loss: -213877.859375\n",
      "Train Epoch: 9 [33024/54000 (61%)] Loss: -244463.875000\n",
      "Train Epoch: 9 [37120/54000 (69%)] Loss: -108142.789062\n",
      "Train Epoch: 9 [41216/54000 (76%)] Loss: -173549.265625\n",
      "Train Epoch: 9 [45312/54000 (84%)] Loss: -273435.968750\n",
      "Train Epoch: 9 [49408/54000 (91%)] Loss: -190873.828125\n",
      "    epoch          : 9\n",
      "    loss           : -194845.3596379207\n",
      "    val_loss       : -182707.335546875\n",
      "Train Epoch: 10 [256/54000 (0%)] Loss: -173438.687500\n",
      "Train Epoch: 10 [4352/54000 (8%)] Loss: -179864.671875\n",
      "Train Epoch: 10 [8448/54000 (16%)] Loss: -282925.062500\n",
      "Train Epoch: 10 [12544/54000 (23%)] Loss: -272568.937500\n",
      "Train Epoch: 10 [16640/54000 (31%)] Loss: -192802.796875\n",
      "Train Epoch: 10 [20736/54000 (38%)] Loss: -184601.906250\n",
      "Train Epoch: 10 [24832/54000 (46%)] Loss: -197468.609375\n",
      "Train Epoch: 10 [28928/54000 (54%)] Loss: -199239.015625\n",
      "Train Epoch: 10 [33024/54000 (61%)] Loss: -99772.195312\n",
      "Train Epoch: 10 [37120/54000 (69%)] Loss: -308622.312500\n",
      "Train Epoch: 10 [41216/54000 (76%)] Loss: -293696.718750\n",
      "Train Epoch: 10 [45312/54000 (84%)] Loss: -157694.359375\n",
      "Train Epoch: 10 [49408/54000 (91%)] Loss: -178555.656250\n",
      "    epoch          : 10\n",
      "    loss           : -202957.28857421875\n",
      "    val_loss       : -206510.77734375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 11 [256/54000 (0%)] Loss: -167374.703125\n",
      "Train Epoch: 11 [4352/54000 (8%)] Loss: -195168.703125\n",
      "Train Epoch: 11 [8448/54000 (16%)] Loss: -201141.031250\n",
      "Train Epoch: 11 [12544/54000 (23%)] Loss: -310198.687500\n",
      "Train Epoch: 11 [16640/54000 (31%)] Loss: -249170.484375\n",
      "Train Epoch: 11 [20736/54000 (38%)] Loss: -199108.171875\n",
      "Train Epoch: 11 [24832/54000 (46%)] Loss: -175870.515625\n",
      "Train Epoch: 11 [28928/54000 (54%)] Loss: -302967.718750\n",
      "Train Epoch: 11 [33024/54000 (61%)] Loss: -205550.156250\n",
      "Train Epoch: 11 [37120/54000 (69%)] Loss: -179622.828125\n",
      "Train Epoch: 11 [41216/54000 (76%)] Loss: -194541.765625\n",
      "Train Epoch: 11 [45312/54000 (84%)] Loss: -287278.031250\n",
      "Train Epoch: 11 [49408/54000 (91%)] Loss: -196810.640625\n",
      "    epoch          : 11\n",
      "    loss           : -207938.15692608172\n",
      "    val_loss       : -200180.0484375\n",
      "Train Epoch: 12 [256/54000 (0%)] Loss: -204761.203125\n",
      "Train Epoch: 12 [4352/54000 (8%)] Loss: -207041.281250\n",
      "Train Epoch: 12 [8448/54000 (16%)] Loss: -192932.578125\n",
      "Train Epoch: 12 [12544/54000 (23%)] Loss: -245244.078125\n",
      "Train Epoch: 12 [16640/54000 (31%)] Loss: -213993.734375\n",
      "Train Epoch: 12 [20736/54000 (38%)] Loss: -206735.359375\n",
      "Train Epoch: 12 [24832/54000 (46%)] Loss: -205136.078125\n",
      "Train Epoch: 12 [28928/54000 (54%)] Loss: -184860.187500\n",
      "Train Epoch: 12 [33024/54000 (61%)] Loss: -191483.437500\n",
      "Train Epoch: 12 [37120/54000 (69%)] Loss: -140841.812500\n",
      "Train Epoch: 12 [41216/54000 (76%)] Loss: -210324.171875\n",
      "Train Epoch: 12 [45312/54000 (84%)] Loss: -174471.625000\n",
      "Train Epoch: 12 [49408/54000 (91%)] Loss: -200409.375000\n",
      "    epoch          : 12\n",
      "    loss           : -205469.0332782452\n",
      "    val_loss       : -208979.728125\n",
      "Train Epoch: 13 [256/54000 (0%)] Loss: -169528.000000\n",
      "Train Epoch: 13 [4352/54000 (8%)] Loss: -271009.687500\n",
      "Train Epoch: 13 [8448/54000 (16%)] Loss: -308706.375000\n",
      "Train Epoch: 13 [12544/54000 (23%)] Loss: -168281.609375\n",
      "Train Epoch: 13 [16640/54000 (31%)] Loss: -177002.734375\n",
      "Train Epoch: 13 [20736/54000 (38%)] Loss: -204609.906250\n",
      "Train Epoch: 13 [24832/54000 (46%)] Loss: -274657.593750\n",
      "Train Epoch: 13 [28928/54000 (54%)] Loss: -258324.484375\n",
      "Train Epoch: 13 [33024/54000 (61%)] Loss: -273364.093750\n",
      "Train Epoch: 13 [37120/54000 (69%)] Loss: -191477.593750\n",
      "Train Epoch: 13 [41216/54000 (76%)] Loss: -174345.359375\n",
      "Train Epoch: 13 [45312/54000 (84%)] Loss: -149554.437500\n",
      "Train Epoch: 13 [49408/54000 (91%)] Loss: -151763.687500\n",
      "    epoch          : 13\n",
      "    loss           : -204606.8137394832\n",
      "    val_loss       : -212069.7296875\n",
      "Train Epoch: 14 [256/54000 (0%)] Loss: -210688.234375\n",
      "Train Epoch: 14 [4352/54000 (8%)] Loss: -169158.984375\n",
      "Train Epoch: 14 [8448/54000 (16%)] Loss: -189389.875000\n",
      "Train Epoch: 14 [12544/54000 (23%)] Loss: -179032.000000\n",
      "Train Epoch: 14 [16640/54000 (31%)] Loss: -254661.281250\n",
      "Train Epoch: 14 [20736/54000 (38%)] Loss: -203792.078125\n",
      "Train Epoch: 14 [24832/54000 (46%)] Loss: -227271.875000\n",
      "Train Epoch: 14 [28928/54000 (54%)] Loss: -160388.812500\n",
      "Train Epoch: 14 [33024/54000 (61%)] Loss: -184732.093750\n",
      "Train Epoch: 14 [37120/54000 (69%)] Loss: -303445.000000\n",
      "Train Epoch: 14 [41216/54000 (76%)] Loss: -149327.312500\n",
      "Train Epoch: 14 [45312/54000 (84%)] Loss: -174851.312500\n",
      "Train Epoch: 14 [49408/54000 (91%)] Loss: -203089.500000\n",
      "    epoch          : 14\n",
      "    loss           : -212158.11673677884\n",
      "    val_loss       : -215545.56171875\n",
      "Train Epoch: 15 [256/54000 (0%)] Loss: -176852.453125\n",
      "Train Epoch: 15 [4352/54000 (8%)] Loss: -173110.234375\n",
      "Train Epoch: 15 [8448/54000 (16%)] Loss: -262102.359375\n",
      "Train Epoch: 15 [12544/54000 (23%)] Loss: -236989.125000\n",
      "Train Epoch: 15 [16640/54000 (31%)] Loss: -267112.250000\n",
      "Train Epoch: 15 [20736/54000 (38%)] Loss: -323543.562500\n",
      "Train Epoch: 15 [24832/54000 (46%)] Loss: -190252.046875\n",
      "Train Epoch: 15 [28928/54000 (54%)] Loss: -247856.734375\n",
      "Train Epoch: 15 [33024/54000 (61%)] Loss: -197672.046875\n",
      "Train Epoch: 15 [37120/54000 (69%)] Loss: -175943.890625\n",
      "Train Epoch: 15 [41216/54000 (76%)] Loss: -210834.234375\n",
      "Train Epoch: 15 [45312/54000 (84%)] Loss: -178689.515625\n",
      "Train Epoch: 15 [49408/54000 (91%)] Loss: -263079.593750\n",
      "    epoch          : 15\n",
      "    loss           : -223013.50424429087\n",
      "    val_loss       : -219910.21171875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch15.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 16 [256/54000 (0%)] Loss: -203287.187500\n",
      "Train Epoch: 16 [4352/54000 (8%)] Loss: -262989.593750\n",
      "Train Epoch: 16 [8448/54000 (16%)] Loss: -315529.656250\n",
      "Train Epoch: 16 [12544/54000 (23%)] Loss: -181763.062500\n",
      "Train Epoch: 16 [16640/54000 (31%)] Loss: -276851.500000\n",
      "Train Epoch: 16 [20736/54000 (38%)] Loss: -214613.796875\n",
      "Train Epoch: 16 [24832/54000 (46%)] Loss: -254983.562500\n",
      "Train Epoch: 16 [28928/54000 (54%)] Loss: -301746.500000\n",
      "Train Epoch: 16 [33024/54000 (61%)] Loss: -161131.875000\n",
      "Train Epoch: 16 [37120/54000 (69%)] Loss: -200933.343750\n",
      "Train Epoch: 16 [41216/54000 (76%)] Loss: -204268.765625\n",
      "Train Epoch: 16 [45312/54000 (84%)] Loss: -217752.281250\n",
      "Train Epoch: 16 [49408/54000 (91%)] Loss: -178801.765625\n",
      "    epoch          : 16\n",
      "    loss           : -221096.53925030047\n",
      "    val_loss       : -210213.79453125\n",
      "Train Epoch: 17 [256/54000 (0%)] Loss: -150663.765625\n",
      "Train Epoch: 17 [4352/54000 (8%)] Loss: -219890.156250\n",
      "Train Epoch: 17 [8448/54000 (16%)] Loss: -187440.937500\n",
      "Train Epoch: 17 [12544/54000 (23%)] Loss: -327624.625000\n",
      "Train Epoch: 17 [16640/54000 (31%)] Loss: -258738.359375\n",
      "Train Epoch: 17 [20736/54000 (38%)] Loss: -173434.250000\n",
      "Train Epoch: 17 [24832/54000 (46%)] Loss: -294451.812500\n",
      "Train Epoch: 17 [28928/54000 (54%)] Loss: -156482.406250\n",
      "Train Epoch: 17 [33024/54000 (61%)] Loss: -270264.312500\n",
      "Train Epoch: 17 [37120/54000 (69%)] Loss: -182014.468750\n",
      "Train Epoch: 17 [41216/54000 (76%)] Loss: -186224.015625\n",
      "Train Epoch: 17 [45312/54000 (84%)] Loss: -264305.562500\n",
      "Train Epoch: 17 [49408/54000 (91%)] Loss: -198413.406250\n",
      "    epoch          : 17\n",
      "    loss           : -228226.41650390625\n",
      "    val_loss       : -223271.7296875\n",
      "Train Epoch: 18 [256/54000 (0%)] Loss: -208970.609375\n",
      "Train Epoch: 18 [4352/54000 (8%)] Loss: -271950.531250\n",
      "Train Epoch: 18 [8448/54000 (16%)] Loss: -202389.765625\n",
      "Train Epoch: 18 [12544/54000 (23%)] Loss: -264216.312500\n",
      "Train Epoch: 18 [16640/54000 (31%)] Loss: -204410.156250\n",
      "Train Epoch: 18 [20736/54000 (38%)] Loss: -177221.140625\n",
      "Train Epoch: 18 [24832/54000 (46%)] Loss: -291043.750000\n",
      "Train Epoch: 18 [28928/54000 (54%)] Loss: -157246.125000\n",
      "Train Epoch: 18 [33024/54000 (61%)] Loss: -262060.734375\n",
      "Train Epoch: 18 [37120/54000 (69%)] Loss: -217443.890625\n",
      "Train Epoch: 18 [41216/54000 (76%)] Loss: -154211.796875\n",
      "Train Epoch: 18 [45312/54000 (84%)] Loss: -200258.031250\n",
      "Train Epoch: 18 [49408/54000 (91%)] Loss: -203538.812500\n",
      "    epoch          : 18\n",
      "    loss           : -228963.28290264422\n",
      "    val_loss       : -221523.503125\n",
      "Train Epoch: 19 [256/54000 (0%)] Loss: -219112.078125\n",
      "Train Epoch: 19 [4352/54000 (8%)] Loss: -197600.000000\n",
      "Train Epoch: 19 [8448/54000 (16%)] Loss: -207514.328125\n",
      "Train Epoch: 19 [12544/54000 (23%)] Loss: -225316.281250\n",
      "Train Epoch: 19 [16640/54000 (31%)] Loss: -215169.421875\n",
      "Train Epoch: 19 [20736/54000 (38%)] Loss: -156445.296875\n",
      "Train Epoch: 19 [24832/54000 (46%)] Loss: -179275.984375\n",
      "Train Epoch: 19 [28928/54000 (54%)] Loss: -311695.125000\n",
      "Train Epoch: 19 [33024/54000 (61%)] Loss: -201335.968750\n",
      "Train Epoch: 19 [37120/54000 (69%)] Loss: -221624.234375\n",
      "Train Epoch: 19 [41216/54000 (76%)] Loss: -286132.750000\n",
      "Train Epoch: 19 [45312/54000 (84%)] Loss: -217270.531250\n",
      "Train Epoch: 19 [49408/54000 (91%)] Loss: -338403.781250\n",
      "    epoch          : 19\n",
      "    loss           : -227356.9717548077\n",
      "    val_loss       : -205439.309375\n",
      "Train Epoch: 20 [256/54000 (0%)] Loss: -259035.890625\n",
      "Train Epoch: 20 [4352/54000 (8%)] Loss: -307989.437500\n",
      "Train Epoch: 20 [8448/54000 (16%)] Loss: -314373.187500\n",
      "Train Epoch: 20 [12544/54000 (23%)] Loss: -231586.515625\n",
      "Train Epoch: 20 [16640/54000 (31%)] Loss: -328752.156250\n",
      "Train Epoch: 20 [20736/54000 (38%)] Loss: -164937.359375\n",
      "Train Epoch: 20 [24832/54000 (46%)] Loss: -272841.937500\n",
      "Train Epoch: 20 [28928/54000 (54%)] Loss: -220713.171875\n",
      "Train Epoch: 20 [33024/54000 (61%)] Loss: -289809.562500\n",
      "Train Epoch: 20 [37120/54000 (69%)] Loss: -226787.937500\n",
      "Train Epoch: 20 [41216/54000 (76%)] Loss: -326561.781250\n",
      "Train Epoch: 20 [45312/54000 (84%)] Loss: -233104.968750\n",
      "Train Epoch: 20 [49408/54000 (91%)] Loss: -293762.875000\n",
      "    epoch          : 20\n",
      "    loss           : -232756.36260516828\n",
      "    val_loss       : -219731.107421875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch20.pth ...\n",
      "Train Epoch: 21 [256/54000 (0%)] Loss: -182376.578125\n",
      "Train Epoch: 21 [4352/54000 (8%)] Loss: -191410.203125\n",
      "Train Epoch: 21 [8448/54000 (16%)] Loss: -328332.625000\n",
      "Train Epoch: 21 [12544/54000 (23%)] Loss: -217686.984375\n",
      "Train Epoch: 21 [16640/54000 (31%)] Loss: -226579.406250\n",
      "Train Epoch: 21 [20736/54000 (38%)] Loss: -153674.328125\n",
      "Train Epoch: 21 [24832/54000 (46%)] Loss: -214355.906250\n",
      "Train Epoch: 21 [28928/54000 (54%)] Loss: -280430.875000\n",
      "Train Epoch: 21 [33024/54000 (61%)] Loss: -234203.203125\n",
      "Train Epoch: 21 [37120/54000 (69%)] Loss: -186470.609375\n",
      "Train Epoch: 21 [41216/54000 (76%)] Loss: -286319.406250\n",
      "Train Epoch: 21 [45312/54000 (84%)] Loss: -229847.328125\n",
      "Train Epoch: 21 [49408/54000 (91%)] Loss: -278770.218750\n",
      "    epoch          : 21\n",
      "    loss           : -235340.37672776444\n",
      "    val_loss       : -227910.08203125\n",
      "Train Epoch: 22 [256/54000 (0%)] Loss: -171973.437500\n",
      "Train Epoch: 22 [4352/54000 (8%)] Loss: -274531.187500\n",
      "Train Epoch: 22 [8448/54000 (16%)] Loss: -207028.703125\n",
      "Train Epoch: 22 [12544/54000 (23%)] Loss: -216290.984375\n",
      "Train Epoch: 22 [16640/54000 (31%)] Loss: -273717.937500\n",
      "Train Epoch: 22 [20736/54000 (38%)] Loss: -159696.953125\n",
      "Train Epoch: 22 [24832/54000 (46%)] Loss: -289757.062500\n",
      "Train Epoch: 22 [28928/54000 (54%)] Loss: -216955.750000\n",
      "Train Epoch: 22 [33024/54000 (61%)] Loss: -282486.593750\n",
      "Train Epoch: 22 [37120/54000 (69%)] Loss: -224836.750000\n",
      "Train Epoch: 22 [41216/54000 (76%)] Loss: -194566.093750\n",
      "Train Epoch: 22 [45312/54000 (84%)] Loss: -266757.375000\n",
      "Train Epoch: 22 [49408/54000 (91%)] Loss: -211624.937500\n",
      "    epoch          : 22\n",
      "    loss           : -233354.28695913462\n",
      "    val_loss       : -217784.9859375\n",
      "Train Epoch: 23 [256/54000 (0%)] Loss: -191008.531250\n",
      "Train Epoch: 23 [4352/54000 (8%)] Loss: -239790.625000\n",
      "Train Epoch: 23 [8448/54000 (16%)] Loss: -342671.187500\n",
      "Train Epoch: 23 [12544/54000 (23%)] Loss: -339262.312500\n",
      "Train Epoch: 23 [16640/54000 (31%)] Loss: -238583.875000\n",
      "Train Epoch: 23 [20736/54000 (38%)] Loss: -195105.921875\n",
      "Train Epoch: 23 [24832/54000 (46%)] Loss: -187163.312500\n",
      "Train Epoch: 23 [28928/54000 (54%)] Loss: -296446.750000\n",
      "Train Epoch: 23 [33024/54000 (61%)] Loss: -284824.812500\n",
      "Train Epoch: 23 [37120/54000 (69%)] Loss: -223195.578125\n",
      "Train Epoch: 23 [41216/54000 (76%)] Loss: -299284.937500\n",
      "Train Epoch: 23 [45312/54000 (84%)] Loss: -234348.531250\n",
      "Train Epoch: 23 [49408/54000 (91%)] Loss: -202932.140625\n",
      "    epoch          : 23\n",
      "    loss           : -234284.12762920672\n",
      "    val_loss       : -230733.99296875\n",
      "Train Epoch: 24 [256/54000 (0%)] Loss: -273811.437500\n",
      "Train Epoch: 24 [4352/54000 (8%)] Loss: -159246.921875\n",
      "Train Epoch: 24 [8448/54000 (16%)] Loss: -213610.796875\n",
      "Train Epoch: 24 [12544/54000 (23%)] Loss: -167370.937500\n",
      "Train Epoch: 24 [16640/54000 (31%)] Loss: -251163.234375\n",
      "Train Epoch: 24 [20736/54000 (38%)] Loss: -167389.109375\n",
      "Train Epoch: 24 [24832/54000 (46%)] Loss: -225210.750000\n",
      "Train Epoch: 24 [28928/54000 (54%)] Loss: -276331.875000\n",
      "Train Epoch: 24 [33024/54000 (61%)] Loss: -220275.203125\n",
      "Train Epoch: 24 [37120/54000 (69%)] Loss: -292464.750000\n",
      "Train Epoch: 24 [41216/54000 (76%)] Loss: -209957.703125\n",
      "Train Epoch: 24 [45312/54000 (84%)] Loss: -238685.031250\n",
      "Train Epoch: 24 [49408/54000 (91%)] Loss: -190164.578125\n",
      "    epoch          : 24\n",
      "    loss           : -239092.52020733172\n",
      "    val_loss       : -241739.8265625\n",
      "Train Epoch: 25 [256/54000 (0%)] Loss: -228882.359375\n",
      "Train Epoch: 25 [4352/54000 (8%)] Loss: -200314.531250\n",
      "Train Epoch: 25 [8448/54000 (16%)] Loss: -225503.468750\n",
      "Train Epoch: 25 [12544/54000 (23%)] Loss: -255630.140625\n",
      "Train Epoch: 25 [16640/54000 (31%)] Loss: -209712.171875\n",
      "Train Epoch: 25 [20736/54000 (38%)] Loss: -197105.296875\n",
      "Train Epoch: 25 [24832/54000 (46%)] Loss: -208107.546875\n",
      "Train Epoch: 25 [28928/54000 (54%)] Loss: -196232.687500\n",
      "Train Epoch: 25 [33024/54000 (61%)] Loss: -193637.140625\n",
      "Train Epoch: 25 [37120/54000 (69%)] Loss: -328484.000000\n",
      "Train Epoch: 25 [41216/54000 (76%)] Loss: -215536.390625\n",
      "Train Epoch: 25 [45312/54000 (84%)] Loss: -234020.093750\n",
      "Train Epoch: 25 [49408/54000 (91%)] Loss: -333753.187500\n",
      "    epoch          : 25\n",
      "    loss           : -240029.22693810097\n",
      "    val_loss       : -243325.04296875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch25.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 26 [256/54000 (0%)] Loss: -209102.234375\n",
      "Train Epoch: 26 [4352/54000 (8%)] Loss: -214159.484375\n",
      "Train Epoch: 26 [8448/54000 (16%)] Loss: -336169.812500\n",
      "Train Epoch: 26 [12544/54000 (23%)] Loss: -187202.281250\n",
      "Train Epoch: 26 [16640/54000 (31%)] Loss: -280384.375000\n",
      "Train Epoch: 26 [20736/54000 (38%)] Loss: -213241.265625\n",
      "Train Epoch: 26 [24832/54000 (46%)] Loss: -281225.468750\n",
      "Train Epoch: 26 [28928/54000 (54%)] Loss: -346500.031250\n",
      "Train Epoch: 26 [33024/54000 (61%)] Loss: -218992.875000\n",
      "Train Epoch: 26 [37120/54000 (69%)] Loss: -170457.578125\n",
      "Train Epoch: 26 [41216/54000 (76%)] Loss: -336330.781250\n",
      "Train Epoch: 26 [45312/54000 (84%)] Loss: -228366.750000\n",
      "Train Epoch: 26 [49408/54000 (91%)] Loss: -169384.921875\n",
      "    epoch          : 26\n",
      "    loss           : -239508.92243840144\n",
      "    val_loss       : -248585.6359375\n",
      "Train Epoch: 27 [256/54000 (0%)] Loss: -253510.328125\n",
      "Train Epoch: 27 [4352/54000 (8%)] Loss: -234272.015625\n",
      "Train Epoch: 27 [8448/54000 (16%)] Loss: -228203.578125\n",
      "Train Epoch: 27 [12544/54000 (23%)] Loss: -262766.218750\n",
      "Train Epoch: 27 [16640/54000 (31%)] Loss: -266125.468750\n",
      "Train Epoch: 27 [20736/54000 (38%)] Loss: -343256.750000\n",
      "Train Epoch: 27 [24832/54000 (46%)] Loss: -274924.593750\n",
      "Train Epoch: 27 [28928/54000 (54%)] Loss: -195913.609375\n",
      "Train Epoch: 27 [33024/54000 (61%)] Loss: -214374.203125\n",
      "Train Epoch: 27 [37120/54000 (69%)] Loss: -344977.906250\n",
      "Train Epoch: 27 [41216/54000 (76%)] Loss: -215664.296875\n",
      "Train Epoch: 27 [45312/54000 (84%)] Loss: -291384.750000\n",
      "Train Epoch: 27 [49408/54000 (91%)] Loss: -202934.890625\n",
      "    epoch          : 27\n",
      "    loss           : -241216.22310697116\n",
      "    val_loss       : -234816.45546875\n",
      "Train Epoch: 28 [256/54000 (0%)] Loss: -242861.953125\n",
      "Train Epoch: 28 [4352/54000 (8%)] Loss: -214572.265625\n",
      "Train Epoch: 28 [8448/54000 (16%)] Loss: -300825.250000\n",
      "Train Epoch: 28 [12544/54000 (23%)] Loss: -292393.656250\n",
      "Train Epoch: 28 [16640/54000 (31%)] Loss: -184547.093750\n",
      "Train Epoch: 28 [20736/54000 (38%)] Loss: -337038.093750\n",
      "Train Epoch: 28 [24832/54000 (46%)] Loss: -204582.796875\n",
      "Train Epoch: 28 [28928/54000 (54%)] Loss: -269355.656250\n",
      "Train Epoch: 28 [33024/54000 (61%)] Loss: -235160.781250\n",
      "Train Epoch: 28 [37120/54000 (69%)] Loss: -215547.500000\n",
      "Train Epoch: 28 [41216/54000 (76%)] Loss: -219749.078125\n",
      "Train Epoch: 28 [45312/54000 (84%)] Loss: -226283.390625\n",
      "Train Epoch: 28 [49408/54000 (91%)] Loss: -232474.203125\n",
      "    epoch          : 28\n",
      "    loss           : -240560.11730018028\n",
      "    val_loss       : -237488.40234375\n",
      "Train Epoch: 29 [256/54000 (0%)] Loss: -203106.265625\n",
      "Train Epoch: 29 [4352/54000 (8%)] Loss: -181652.234375\n",
      "Train Epoch: 29 [8448/54000 (16%)] Loss: -203410.687500\n",
      "Train Epoch: 29 [12544/54000 (23%)] Loss: -252481.515625\n",
      "Train Epoch: 29 [16640/54000 (31%)] Loss: -188204.453125\n",
      "Train Epoch: 29 [20736/54000 (38%)] Loss: -218938.593750\n",
      "Train Epoch: 29 [24832/54000 (46%)] Loss: -162433.734375\n",
      "Train Epoch: 29 [28928/54000 (54%)] Loss: -203883.546875\n",
      "Train Epoch: 29 [33024/54000 (61%)] Loss: -306012.531250\n",
      "Train Epoch: 29 [37120/54000 (69%)] Loss: -261042.531250\n",
      "Train Epoch: 29 [41216/54000 (76%)] Loss: -211462.234375\n",
      "Train Epoch: 29 [45312/54000 (84%)] Loss: -348171.125000\n",
      "Train Epoch: 29 [49408/54000 (91%)] Loss: -155254.015625\n",
      "    epoch          : 29\n",
      "    loss           : -245408.26607572116\n",
      "    val_loss       : -239860.04375\n",
      "Train Epoch: 30 [256/54000 (0%)] Loss: -216013.765625\n",
      "Train Epoch: 30 [4352/54000 (8%)] Loss: -213670.312500\n",
      "Train Epoch: 30 [8448/54000 (16%)] Loss: -309191.906250\n",
      "Train Epoch: 30 [12544/54000 (23%)] Loss: -230790.625000\n",
      "Train Epoch: 30 [16640/54000 (31%)] Loss: -240406.515625\n",
      "Train Epoch: 30 [20736/54000 (38%)] Loss: -199188.156250\n",
      "Train Epoch: 30 [24832/54000 (46%)] Loss: -260948.078125\n",
      "Train Epoch: 30 [28928/54000 (54%)] Loss: -202342.906250\n",
      "Train Epoch: 30 [33024/54000 (61%)] Loss: -186030.109375\n",
      "Train Epoch: 30 [37120/54000 (69%)] Loss: -309309.156250\n",
      "Train Epoch: 30 [41216/54000 (76%)] Loss: -202398.812500\n",
      "Train Epoch: 30 [45312/54000 (84%)] Loss: -232347.781250\n",
      "Train Epoch: 30 [49408/54000 (91%)] Loss: -179770.171875\n",
      "    epoch          : 30\n",
      "    loss           : -247491.50022536056\n",
      "    val_loss       : -246831.02578125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch30.pth ...\n",
      "Train Epoch: 31 [256/54000 (0%)] Loss: -317201.218750\n",
      "Train Epoch: 31 [4352/54000 (8%)] Loss: -279401.437500\n",
      "Train Epoch: 31 [8448/54000 (16%)] Loss: -318367.250000\n",
      "Train Epoch: 31 [12544/54000 (23%)] Loss: -205213.093750\n",
      "Train Epoch: 31 [16640/54000 (31%)] Loss: -286373.218750\n",
      "Train Epoch: 31 [20736/54000 (38%)] Loss: -299035.093750\n",
      "Train Epoch: 31 [24832/54000 (46%)] Loss: -279563.437500\n",
      "Train Epoch: 31 [28928/54000 (54%)] Loss: -250427.890625\n",
      "Train Epoch: 31 [33024/54000 (61%)] Loss: -215263.203125\n",
      "Train Epoch: 31 [37120/54000 (69%)] Loss: -244338.921875\n",
      "Train Epoch: 31 [41216/54000 (76%)] Loss: -311652.843750\n",
      "Train Epoch: 31 [45312/54000 (84%)] Loss: -310140.656250\n",
      "Train Epoch: 31 [49408/54000 (91%)] Loss: -229984.046875\n",
      "    epoch          : 31\n",
      "    loss           : -246290.73895733172\n",
      "    val_loss       : -239940.7828125\n",
      "Train Epoch: 32 [256/54000 (0%)] Loss: -197336.937500\n",
      "Train Epoch: 32 [4352/54000 (8%)] Loss: -307471.468750\n",
      "Train Epoch: 32 [8448/54000 (16%)] Loss: -204274.328125\n",
      "Train Epoch: 32 [12544/54000 (23%)] Loss: -283767.562500\n",
      "Train Epoch: 32 [16640/54000 (31%)] Loss: -309439.500000\n",
      "Train Epoch: 32 [20736/54000 (38%)] Loss: -156655.968750\n",
      "Train Epoch: 32 [24832/54000 (46%)] Loss: -298269.750000\n",
      "Train Epoch: 32 [28928/54000 (54%)] Loss: -175638.359375\n",
      "Train Epoch: 32 [33024/54000 (61%)] Loss: -229613.078125\n",
      "Train Epoch: 32 [37120/54000 (69%)] Loss: -199717.828125\n",
      "Train Epoch: 32 [41216/54000 (76%)] Loss: -308467.750000\n",
      "Train Epoch: 32 [45312/54000 (84%)] Loss: -323658.531250\n",
      "Train Epoch: 32 [49408/54000 (91%)] Loss: -303032.562500\n",
      "    epoch          : 32\n",
      "    loss           : -247714.99151141828\n",
      "    val_loss       : -235566.2828125\n",
      "Train Epoch: 33 [256/54000 (0%)] Loss: -206656.187500\n",
      "Train Epoch: 33 [4352/54000 (8%)] Loss: -200542.562500\n",
      "Train Epoch: 33 [8448/54000 (16%)] Loss: -291557.562500\n",
      "Train Epoch: 33 [12544/54000 (23%)] Loss: -322381.531250\n",
      "Train Epoch: 33 [16640/54000 (31%)] Loss: -285887.125000\n",
      "Train Epoch: 33 [20736/54000 (38%)] Loss: -284868.343750\n",
      "Train Epoch: 33 [24832/54000 (46%)] Loss: -269763.875000\n",
      "Train Epoch: 33 [28928/54000 (54%)] Loss: -298992.281250\n",
      "Train Epoch: 33 [33024/54000 (61%)] Loss: -163996.046875\n",
      "Train Epoch: 33 [37120/54000 (69%)] Loss: -216035.515625\n",
      "Train Epoch: 33 [41216/54000 (76%)] Loss: -292608.562500\n",
      "Train Epoch: 33 [45312/54000 (84%)] Loss: -313456.125000\n",
      "Train Epoch: 33 [49408/54000 (91%)] Loss: -213562.984375\n",
      "    epoch          : 33\n",
      "    loss           : -244649.37026742788\n",
      "    val_loss       : -247621.6\n",
      "Train Epoch: 34 [256/54000 (0%)] Loss: -234955.062500\n",
      "Train Epoch: 34 [4352/54000 (8%)] Loss: -206655.156250\n",
      "Train Epoch: 34 [8448/54000 (16%)] Loss: -293646.500000\n",
      "Train Epoch: 34 [12544/54000 (23%)] Loss: -305941.937500\n",
      "Train Epoch: 34 [16640/54000 (31%)] Loss: -290333.375000\n",
      "Train Epoch: 34 [20736/54000 (38%)] Loss: -311156.187500\n",
      "Train Epoch: 34 [24832/54000 (46%)] Loss: -292888.250000\n",
      "Train Epoch: 34 [28928/54000 (54%)] Loss: -215634.421875\n",
      "Train Epoch: 34 [33024/54000 (61%)] Loss: -249082.171875\n",
      "Train Epoch: 34 [37120/54000 (69%)] Loss: -201319.187500\n",
      "Train Epoch: 34 [41216/54000 (76%)] Loss: -338322.906250\n",
      "Train Epoch: 34 [45312/54000 (84%)] Loss: -189390.406250\n",
      "Train Epoch: 34 [49408/54000 (91%)] Loss: -307033.000000\n",
      "    epoch          : 34\n",
      "    loss           : -246391.04822716347\n",
      "    val_loss       : -236218.791015625\n",
      "Train Epoch: 35 [256/54000 (0%)] Loss: -214378.734375\n",
      "Train Epoch: 35 [4352/54000 (8%)] Loss: -209194.296875\n",
      "Train Epoch: 35 [8448/54000 (16%)] Loss: -292037.343750\n",
      "Train Epoch: 35 [12544/54000 (23%)] Loss: -207936.703125\n",
      "Train Epoch: 35 [16640/54000 (31%)] Loss: -251150.515625\n",
      "Train Epoch: 35 [20736/54000 (38%)] Loss: -179405.437500\n",
      "Train Epoch: 35 [24832/54000 (46%)] Loss: -300811.718750\n",
      "Train Epoch: 35 [28928/54000 (54%)] Loss: -174771.359375\n",
      "Train Epoch: 35 [33024/54000 (61%)] Loss: -294948.781250\n",
      "Train Epoch: 35 [37120/54000 (69%)] Loss: -239793.765625\n",
      "Train Epoch: 35 [41216/54000 (76%)] Loss: -251757.453125\n",
      "Train Epoch: 35 [45312/54000 (84%)] Loss: -192045.500000\n",
      "Train Epoch: 35 [49408/54000 (91%)] Loss: -347756.375000\n",
      "    epoch          : 35\n",
      "    loss           : -250387.6983173077\n",
      "    val_loss       : -252629.90234375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch35.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 36 [256/54000 (0%)] Loss: -270798.750000\n",
      "Train Epoch: 36 [4352/54000 (8%)] Loss: -204391.203125\n",
      "Train Epoch: 36 [8448/54000 (16%)] Loss: -255042.390625\n",
      "Train Epoch: 36 [12544/54000 (23%)] Loss: -226040.718750\n",
      "Train Epoch: 36 [16640/54000 (31%)] Loss: -290404.687500\n",
      "Train Epoch: 36 [20736/54000 (38%)] Loss: -194025.828125\n",
      "Train Epoch: 36 [24832/54000 (46%)] Loss: -249768.640625\n",
      "Train Epoch: 36 [28928/54000 (54%)] Loss: -335393.687500\n",
      "Train Epoch: 36 [33024/54000 (61%)] Loss: -210677.500000\n",
      "Train Epoch: 36 [37120/54000 (69%)] Loss: -352358.187500\n",
      "Train Epoch: 36 [41216/54000 (76%)] Loss: -327965.437500\n",
      "Train Epoch: 36 [45312/54000 (84%)] Loss: -285658.781250\n",
      "Train Epoch: 36 [49408/54000 (91%)] Loss: -235662.062500\n",
      "    epoch          : 36\n",
      "    loss           : -251033.8184344952\n",
      "    val_loss       : -234710.64296875\n",
      "Train Epoch: 37 [256/54000 (0%)] Loss: -218164.453125\n",
      "Train Epoch: 37 [4352/54000 (8%)] Loss: -173093.906250\n",
      "Train Epoch: 37 [8448/54000 (16%)] Loss: -337688.687500\n",
      "Train Epoch: 37 [12544/54000 (23%)] Loss: -204204.500000\n",
      "Train Epoch: 37 [16640/54000 (31%)] Loss: -301347.812500\n",
      "Train Epoch: 37 [20736/54000 (38%)] Loss: -201403.906250\n",
      "Train Epoch: 37 [24832/54000 (46%)] Loss: -325402.937500\n",
      "Train Epoch: 37 [28928/54000 (54%)] Loss: -212014.375000\n",
      "Train Epoch: 37 [33024/54000 (61%)] Loss: -314193.625000\n",
      "Train Epoch: 37 [37120/54000 (69%)] Loss: -310007.250000\n",
      "Train Epoch: 37 [41216/54000 (76%)] Loss: -218037.046875\n",
      "Train Epoch: 37 [45312/54000 (84%)] Loss: -207137.296875\n",
      "Train Epoch: 37 [49408/54000 (91%)] Loss: -211030.640625\n",
      "    epoch          : 37\n",
      "    loss           : -250403.4775390625\n",
      "    val_loss       : -237376.61171875\n",
      "Train Epoch: 38 [256/54000 (0%)] Loss: -231679.718750\n",
      "Train Epoch: 38 [4352/54000 (8%)] Loss: -210895.890625\n",
      "Train Epoch: 38 [8448/54000 (16%)] Loss: -237317.562500\n",
      "Train Epoch: 38 [12544/54000 (23%)] Loss: -289953.093750\n",
      "Train Epoch: 38 [16640/54000 (31%)] Loss: -253000.187500\n",
      "Train Epoch: 38 [20736/54000 (38%)] Loss: -234683.796875\n",
      "Train Epoch: 38 [24832/54000 (46%)] Loss: -200159.250000\n",
      "Train Epoch: 38 [28928/54000 (54%)] Loss: -221306.390625\n",
      "Train Epoch: 38 [33024/54000 (61%)] Loss: -205032.062500\n",
      "Train Epoch: 38 [37120/54000 (69%)] Loss: -220466.984375\n",
      "Train Epoch: 38 [41216/54000 (76%)] Loss: -287911.968750\n",
      "Train Epoch: 38 [45312/54000 (84%)] Loss: -233322.359375\n",
      "Train Epoch: 38 [49408/54000 (91%)] Loss: -175657.281250\n",
      "    epoch          : 38\n",
      "    loss           : -250196.4079402043\n",
      "    val_loss       : -240690.33515625\n",
      "Train Epoch: 39 [256/54000 (0%)] Loss: -306498.937500\n",
      "Train Epoch: 39 [4352/54000 (8%)] Loss: -225536.406250\n",
      "Train Epoch: 39 [8448/54000 (16%)] Loss: -204904.265625\n",
      "Train Epoch: 39 [12544/54000 (23%)] Loss: -204019.531250\n",
      "Train Epoch: 39 [16640/54000 (31%)] Loss: -200601.406250\n",
      "Train Epoch: 39 [20736/54000 (38%)] Loss: -298734.250000\n",
      "Train Epoch: 39 [24832/54000 (46%)] Loss: -309886.906250\n",
      "Train Epoch: 39 [28928/54000 (54%)] Loss: -220185.343750\n",
      "Train Epoch: 39 [33024/54000 (61%)] Loss: -208223.234375\n",
      "Train Epoch: 39 [37120/54000 (69%)] Loss: -183476.578125\n",
      "Train Epoch: 39 [41216/54000 (76%)] Loss: -339949.593750\n",
      "Train Epoch: 39 [45312/54000 (84%)] Loss: -200920.328125\n",
      "Train Epoch: 39 [49408/54000 (91%)] Loss: -225069.375000\n",
      "    epoch          : 39\n",
      "    loss           : -247967.78568209134\n",
      "    val_loss       : -221445.22578125\n",
      "Train Epoch: 40 [256/54000 (0%)] Loss: -244424.703125\n",
      "Train Epoch: 40 [4352/54000 (8%)] Loss: -310451.656250\n",
      "Train Epoch: 40 [8448/54000 (16%)] Loss: -238765.359375\n",
      "Train Epoch: 40 [12544/54000 (23%)] Loss: -321131.500000\n",
      "Train Epoch: 40 [16640/54000 (31%)] Loss: -204421.125000\n",
      "Train Epoch: 40 [20736/54000 (38%)] Loss: -322382.875000\n",
      "Train Epoch: 40 [24832/54000 (46%)] Loss: -236497.078125\n",
      "Train Epoch: 40 [28928/54000 (54%)] Loss: -238099.515625\n",
      "Train Epoch: 40 [33024/54000 (61%)] Loss: -195856.625000\n",
      "Train Epoch: 40 [37120/54000 (69%)] Loss: -307741.875000\n",
      "Train Epoch: 40 [41216/54000 (76%)] Loss: -294150.625000\n",
      "Train Epoch: 40 [45312/54000 (84%)] Loss: -215481.265625\n",
      "Train Epoch: 40 [49408/54000 (91%)] Loss: -184754.406250\n",
      "    epoch          : 40\n",
      "    loss           : -254099.78793569712\n",
      "    val_loss       : -211483.98984375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch40.pth ...\n",
      "Train Epoch: 41 [256/54000 (0%)] Loss: -233892.546875\n",
      "Train Epoch: 41 [4352/54000 (8%)] Loss: -204082.328125\n",
      "Train Epoch: 41 [8448/54000 (16%)] Loss: -268391.843750\n",
      "Train Epoch: 41 [12544/54000 (23%)] Loss: -346504.625000\n",
      "Train Epoch: 41 [16640/54000 (31%)] Loss: -234836.265625\n",
      "Train Epoch: 41 [20736/54000 (38%)] Loss: -211471.390625\n",
      "Train Epoch: 41 [24832/54000 (46%)] Loss: -220065.859375\n",
      "Train Epoch: 41 [28928/54000 (54%)] Loss: -223827.843750\n",
      "Train Epoch: 41 [33024/54000 (61%)] Loss: -182962.984375\n",
      "Train Epoch: 41 [37120/54000 (69%)] Loss: -329619.093750\n",
      "Train Epoch: 41 [41216/54000 (76%)] Loss: -214052.203125\n",
      "Train Epoch: 41 [45312/54000 (84%)] Loss: -206214.406250\n",
      "Train Epoch: 41 [49408/54000 (91%)] Loss: -211046.062500\n",
      "    epoch          : 41\n",
      "    loss           : -254345.47836538462\n",
      "    val_loss       : -252340.2625\n",
      "Train Epoch: 42 [256/54000 (0%)] Loss: -241386.109375\n",
      "Train Epoch: 42 [4352/54000 (8%)] Loss: -297074.781250\n",
      "Train Epoch: 42 [8448/54000 (16%)] Loss: -227818.609375\n",
      "Train Epoch: 42 [12544/54000 (23%)] Loss: -321794.781250\n",
      "Train Epoch: 42 [16640/54000 (31%)] Loss: -245913.015625\n",
      "Train Epoch: 42 [20736/54000 (38%)] Loss: -198887.062500\n",
      "Train Epoch: 42 [24832/54000 (46%)] Loss: -226870.046875\n",
      "Train Epoch: 42 [28928/54000 (54%)] Loss: -235716.171875\n",
      "Train Epoch: 42 [33024/54000 (61%)] Loss: -227871.984375\n",
      "Train Epoch: 42 [37120/54000 (69%)] Loss: -319805.687500\n",
      "Train Epoch: 42 [41216/54000 (76%)] Loss: -198834.562500\n",
      "Train Epoch: 42 [45312/54000 (84%)] Loss: -206699.437500\n",
      "Train Epoch: 42 [49408/54000 (91%)] Loss: -121212.460938\n",
      "    epoch          : 42\n",
      "    loss           : -252466.80209585337\n",
      "    val_loss       : -245993.6015625\n",
      "Train Epoch: 43 [256/54000 (0%)] Loss: -238553.843750\n",
      "Train Epoch: 43 [4352/54000 (8%)] Loss: -222007.953125\n",
      "Train Epoch: 43 [8448/54000 (16%)] Loss: -186221.875000\n",
      "Train Epoch: 43 [12544/54000 (23%)] Loss: -193557.671875\n",
      "Train Epoch: 43 [16640/54000 (31%)] Loss: -315685.687500\n",
      "Train Epoch: 43 [20736/54000 (38%)] Loss: -229550.234375\n",
      "Train Epoch: 43 [24832/54000 (46%)] Loss: -341284.093750\n",
      "Train Epoch: 43 [28928/54000 (54%)] Loss: -294389.437500\n",
      "Train Epoch: 43 [33024/54000 (61%)] Loss: -332544.500000\n",
      "Train Epoch: 43 [37120/54000 (69%)] Loss: -312810.968750\n",
      "Train Epoch: 43 [41216/54000 (76%)] Loss: -203381.468750\n",
      "Train Epoch: 43 [45312/54000 (84%)] Loss: -297665.375000\n",
      "Train Epoch: 43 [49408/54000 (91%)] Loss: -237717.671875\n",
      "    epoch          : 43\n",
      "    loss           : -254817.33563701922\n",
      "    val_loss       : -225204.690234375\n",
      "Train Epoch: 44 [256/54000 (0%)] Loss: -182728.859375\n",
      "Train Epoch: 44 [4352/54000 (8%)] Loss: -256729.843750\n",
      "Train Epoch: 44 [8448/54000 (16%)] Loss: -212630.953125\n",
      "Train Epoch: 44 [12544/54000 (23%)] Loss: -203595.781250\n",
      "Train Epoch: 44 [16640/54000 (31%)] Loss: -313257.218750\n",
      "Train Epoch: 44 [20736/54000 (38%)] Loss: -210055.578125\n",
      "Train Epoch: 44 [24832/54000 (46%)] Loss: -212760.234375\n",
      "Train Epoch: 44 [28928/54000 (54%)] Loss: -297984.343750\n",
      "Train Epoch: 44 [33024/54000 (61%)] Loss: -346033.187500\n",
      "Train Epoch: 44 [37120/54000 (69%)] Loss: -203754.109375\n",
      "Train Epoch: 44 [41216/54000 (76%)] Loss: -213237.000000\n",
      "Train Epoch: 44 [45312/54000 (84%)] Loss: -291885.937500\n",
      "Train Epoch: 44 [49408/54000 (91%)] Loss: -244804.953125\n",
      "    epoch          : 44\n",
      "    loss           : -255112.0721905048\n",
      "    val_loss       : -249214.7265625\n",
      "Train Epoch: 45 [256/54000 (0%)] Loss: -224674.250000\n",
      "Train Epoch: 45 [4352/54000 (8%)] Loss: -280564.531250\n",
      "Train Epoch: 45 [8448/54000 (16%)] Loss: -221497.968750\n",
      "Train Epoch: 45 [12544/54000 (23%)] Loss: -212135.796875\n",
      "Train Epoch: 45 [16640/54000 (31%)] Loss: -179825.640625\n",
      "Train Epoch: 45 [20736/54000 (38%)] Loss: -311972.062500\n",
      "Train Epoch: 45 [24832/54000 (46%)] Loss: -198560.000000\n",
      "Train Epoch: 45 [28928/54000 (54%)] Loss: -239539.453125\n",
      "Train Epoch: 45 [33024/54000 (61%)] Loss: -200386.281250\n",
      "Train Epoch: 45 [37120/54000 (69%)] Loss: -250867.062500\n",
      "Train Epoch: 45 [41216/54000 (76%)] Loss: -216150.234375\n",
      "Train Epoch: 45 [45312/54000 (84%)] Loss: -286631.843750\n",
      "Train Epoch: 45 [49408/54000 (91%)] Loss: -231817.078125\n",
      "    epoch          : 45\n",
      "    loss           : -256224.0116436298\n",
      "    val_loss       : -257257.09765625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch45.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 46 [256/54000 (0%)] Loss: -241169.140625\n",
      "Train Epoch: 46 [4352/54000 (8%)] Loss: -309991.656250\n",
      "Train Epoch: 46 [8448/54000 (16%)] Loss: -214178.656250\n",
      "Train Epoch: 46 [12544/54000 (23%)] Loss: -303758.531250\n",
      "Train Epoch: 46 [16640/54000 (31%)] Loss: -209067.984375\n",
      "Train Epoch: 46 [20736/54000 (38%)] Loss: -202076.921875\n",
      "Train Epoch: 46 [24832/54000 (46%)] Loss: -215094.484375\n",
      "Train Epoch: 46 [28928/54000 (54%)] Loss: -304530.625000\n",
      "Train Epoch: 46 [33024/54000 (61%)] Loss: -198892.296875\n",
      "Train Epoch: 46 [37120/54000 (69%)] Loss: -204066.593750\n",
      "Train Epoch: 46 [41216/54000 (76%)] Loss: -324538.093750\n",
      "Train Epoch: 46 [45312/54000 (84%)] Loss: -276304.750000\n",
      "Train Epoch: 46 [49408/54000 (91%)] Loss: -192172.812500\n",
      "    epoch          : 46\n",
      "    loss           : -254741.41650390625\n",
      "    val_loss       : -259893.80078125\n",
      "Train Epoch: 47 [256/54000 (0%)] Loss: -261902.156250\n",
      "Train Epoch: 47 [4352/54000 (8%)] Loss: -300132.656250\n",
      "Train Epoch: 47 [8448/54000 (16%)] Loss: -298514.281250\n",
      "Train Epoch: 47 [12544/54000 (23%)] Loss: -327941.531250\n",
      "Train Epoch: 47 [16640/54000 (31%)] Loss: -266541.156250\n",
      "Train Epoch: 47 [20736/54000 (38%)] Loss: -342182.250000\n",
      "Train Epoch: 47 [24832/54000 (46%)] Loss: -297912.687500\n",
      "Train Epoch: 47 [28928/54000 (54%)] Loss: -318164.250000\n",
      "Train Epoch: 47 [33024/54000 (61%)] Loss: -345641.593750\n",
      "Train Epoch: 47 [37120/54000 (69%)] Loss: -286227.500000\n",
      "Train Epoch: 47 [41216/54000 (76%)] Loss: -241440.562500\n",
      "Train Epoch: 47 [45312/54000 (84%)] Loss: -188365.531250\n",
      "Train Epoch: 47 [49408/54000 (91%)] Loss: -344490.375000\n",
      "    epoch          : 47\n",
      "    loss           : -257194.34292367788\n",
      "    val_loss       : -250645.2265625\n",
      "Train Epoch: 48 [256/54000 (0%)] Loss: -213584.390625\n",
      "Train Epoch: 48 [4352/54000 (8%)] Loss: -178321.046875\n",
      "Train Epoch: 48 [8448/54000 (16%)] Loss: -347122.750000\n",
      "Train Epoch: 48 [12544/54000 (23%)] Loss: -207936.296875\n",
      "Train Epoch: 48 [16640/54000 (31%)] Loss: -191756.921875\n",
      "Train Epoch: 48 [20736/54000 (38%)] Loss: -213852.359375\n",
      "Train Epoch: 48 [24832/54000 (46%)] Loss: -186044.078125\n",
      "Train Epoch: 48 [28928/54000 (54%)] Loss: -223429.437500\n",
      "Train Epoch: 48 [33024/54000 (61%)] Loss: -193025.484375\n",
      "Train Epoch: 48 [37120/54000 (69%)] Loss: -217838.359375\n",
      "Train Epoch: 48 [41216/54000 (76%)] Loss: -240853.359375\n",
      "Train Epoch: 48 [45312/54000 (84%)] Loss: -226261.765625\n",
      "Train Epoch: 48 [49408/54000 (91%)] Loss: -182879.203125\n",
      "    epoch          : 48\n",
      "    loss           : -259130.74586838944\n",
      "    val_loss       : -252342.053125\n",
      "Train Epoch: 49 [256/54000 (0%)] Loss: -236767.953125\n",
      "Train Epoch: 49 [4352/54000 (8%)] Loss: -315851.187500\n",
      "Train Epoch: 49 [8448/54000 (16%)] Loss: -222041.968750\n",
      "Train Epoch: 49 [12544/54000 (23%)] Loss: -207764.500000\n",
      "Train Epoch: 49 [16640/54000 (31%)] Loss: -346832.250000\n",
      "Train Epoch: 49 [20736/54000 (38%)] Loss: -230773.500000\n",
      "Train Epoch: 49 [24832/54000 (46%)] Loss: -327594.875000\n",
      "Train Epoch: 49 [28928/54000 (54%)] Loss: -188598.046875\n",
      "Train Epoch: 49 [33024/54000 (61%)] Loss: -289345.937500\n",
      "Train Epoch: 49 [37120/54000 (69%)] Loss: -330717.750000\n",
      "Train Epoch: 49 [41216/54000 (76%)] Loss: -217094.140625\n",
      "Train Epoch: 49 [45312/54000 (84%)] Loss: -195068.156250\n",
      "Train Epoch: 49 [49408/54000 (91%)] Loss: -195317.640625\n",
      "    epoch          : 49\n",
      "    loss           : -258444.81588040866\n",
      "    val_loss       : -248861.01640625\n",
      "Train Epoch: 50 [256/54000 (0%)] Loss: -300765.093750\n",
      "Train Epoch: 50 [4352/54000 (8%)] Loss: -216144.484375\n",
      "Train Epoch: 50 [8448/54000 (16%)] Loss: -190700.937500\n",
      "Train Epoch: 50 [12544/54000 (23%)] Loss: -324206.875000\n",
      "Train Epoch: 50 [16640/54000 (31%)] Loss: -301183.437500\n",
      "Train Epoch: 50 [20736/54000 (38%)] Loss: -226849.203125\n",
      "Train Epoch: 50 [24832/54000 (46%)] Loss: -239845.093750\n",
      "Train Epoch: 50 [28928/54000 (54%)] Loss: -120192.710938\n",
      "Train Epoch: 50 [33024/54000 (61%)] Loss: -300869.531250\n",
      "Train Epoch: 50 [37120/54000 (69%)] Loss: -327106.250000\n",
      "Train Epoch: 50 [41216/54000 (76%)] Loss: -179518.296875\n",
      "Train Epoch: 50 [45312/54000 (84%)] Loss: -291927.437500\n",
      "Train Epoch: 50 [49408/54000 (91%)] Loss: -229208.312500\n",
      "    epoch          : 50\n",
      "    loss           : -257688.49091045672\n",
      "    val_loss       : -260481.79921875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [256/54000 (0%)] Loss: -319201.312500\n",
      "Train Epoch: 51 [4352/54000 (8%)] Loss: -292833.031250\n",
      "Train Epoch: 51 [8448/54000 (16%)] Loss: -222022.046875\n",
      "Train Epoch: 51 [12544/54000 (23%)] Loss: -284532.812500\n",
      "Train Epoch: 51 [16640/54000 (31%)] Loss: -317709.875000\n",
      "Train Epoch: 51 [20736/54000 (38%)] Loss: -220246.093750\n",
      "Train Epoch: 51 [24832/54000 (46%)] Loss: -310957.312500\n",
      "Train Epoch: 51 [28928/54000 (54%)] Loss: -188643.390625\n",
      "Train Epoch: 51 [33024/54000 (61%)] Loss: -299416.906250\n",
      "Train Epoch: 51 [37120/54000 (69%)] Loss: -361484.468750\n",
      "Train Epoch: 51 [41216/54000 (76%)] Loss: -210602.484375\n",
      "Train Epoch: 51 [45312/54000 (84%)] Loss: -200380.218750\n",
      "Train Epoch: 51 [49408/54000 (91%)] Loss: -329909.187500\n",
      "    epoch          : 51\n",
      "    loss           : -258441.42514272837\n",
      "    val_loss       : -249816.50625\n",
      "Train Epoch: 52 [256/54000 (0%)] Loss: -191702.390625\n",
      "Train Epoch: 52 [4352/54000 (8%)] Loss: -270790.562500\n",
      "Train Epoch: 52 [8448/54000 (16%)] Loss: -319015.968750\n",
      "Train Epoch: 52 [12544/54000 (23%)] Loss: -213166.359375\n",
      "Train Epoch: 52 [16640/54000 (31%)] Loss: -313007.343750\n",
      "Train Epoch: 52 [20736/54000 (38%)] Loss: -251016.828125\n",
      "Train Epoch: 52 [24832/54000 (46%)] Loss: -353273.875000\n",
      "Train Epoch: 52 [28928/54000 (54%)] Loss: -177725.250000\n",
      "Train Epoch: 52 [33024/54000 (61%)] Loss: -326188.343750\n",
      "Train Epoch: 52 [37120/54000 (69%)] Loss: -219716.671875\n",
      "Train Epoch: 52 [41216/54000 (76%)] Loss: -135866.515625\n",
      "Train Epoch: 52 [45312/54000 (84%)] Loss: -228710.593750\n",
      "Train Epoch: 52 [49408/54000 (91%)] Loss: -332334.250000\n",
      "    epoch          : 52\n",
      "    loss           : -259467.97397085337\n",
      "    val_loss       : -263879.43515625\n",
      "Train Epoch: 53 [256/54000 (0%)] Loss: -237495.984375\n",
      "Train Epoch: 53 [4352/54000 (8%)] Loss: -240960.609375\n",
      "Train Epoch: 53 [8448/54000 (16%)] Loss: -206872.437500\n",
      "Train Epoch: 53 [12544/54000 (23%)] Loss: -336957.437500\n",
      "Train Epoch: 53 [16640/54000 (31%)] Loss: -319879.937500\n",
      "Train Epoch: 53 [20736/54000 (38%)] Loss: -174930.796875\n",
      "Train Epoch: 53 [24832/54000 (46%)] Loss: -268843.500000\n",
      "Train Epoch: 53 [28928/54000 (54%)] Loss: -182743.343750\n",
      "Train Epoch: 53 [33024/54000 (61%)] Loss: -296862.156250\n",
      "Train Epoch: 53 [37120/54000 (69%)] Loss: -238025.218750\n",
      "Train Epoch: 53 [41216/54000 (76%)] Loss: -300064.437500\n",
      "Train Epoch: 53 [45312/54000 (84%)] Loss: -153085.343750\n",
      "Train Epoch: 53 [49408/54000 (91%)] Loss: -213300.484375\n",
      "    epoch          : 53\n",
      "    loss           : -260490.22509765625\n",
      "    val_loss       : -239928.509375\n",
      "Train Epoch: 54 [256/54000 (0%)] Loss: -215193.906250\n",
      "Train Epoch: 54 [4352/54000 (8%)] Loss: -333354.218750\n",
      "Train Epoch: 54 [8448/54000 (16%)] Loss: -283710.843750\n",
      "Train Epoch: 54 [12544/54000 (23%)] Loss: -228108.359375\n",
      "Train Epoch: 54 [16640/54000 (31%)] Loss: -220046.562500\n",
      "Train Epoch: 54 [20736/54000 (38%)] Loss: -221200.203125\n",
      "Train Epoch: 54 [24832/54000 (46%)] Loss: -207519.656250\n",
      "Train Epoch: 54 [28928/54000 (54%)] Loss: -241379.062500\n",
      "Train Epoch: 54 [33024/54000 (61%)] Loss: -197914.203125\n",
      "Train Epoch: 54 [37120/54000 (69%)] Loss: -319019.593750\n",
      "Train Epoch: 54 [41216/54000 (76%)] Loss: -235612.296875\n",
      "Train Epoch: 54 [45312/54000 (84%)] Loss: -207904.953125\n",
      "Train Epoch: 54 [49408/54000 (91%)] Loss: -322586.187500\n",
      "    epoch          : 54\n",
      "    loss           : -261800.4668344351\n",
      "    val_loss       : -261948.25078125\n",
      "Train Epoch: 55 [256/54000 (0%)] Loss: -235837.515625\n",
      "Train Epoch: 55 [4352/54000 (8%)] Loss: -323381.093750\n",
      "Train Epoch: 55 [8448/54000 (16%)] Loss: -304297.218750\n",
      "Train Epoch: 55 [12544/54000 (23%)] Loss: -346332.593750\n",
      "Train Epoch: 55 [16640/54000 (31%)] Loss: -236237.062500\n",
      "Train Epoch: 55 [20736/54000 (38%)] Loss: -327648.125000\n",
      "Train Epoch: 55 [24832/54000 (46%)] Loss: -221747.203125\n",
      "Train Epoch: 55 [28928/54000 (54%)] Loss: -338073.312500\n",
      "Train Epoch: 55 [33024/54000 (61%)] Loss: -210867.078125\n",
      "Train Epoch: 55 [37120/54000 (69%)] Loss: -200457.671875\n",
      "Train Epoch: 55 [41216/54000 (76%)] Loss: -361375.437500\n",
      "Train Epoch: 55 [45312/54000 (84%)] Loss: -358693.531250\n",
      "Train Epoch: 55 [49408/54000 (91%)] Loss: -323698.906250\n",
      "    epoch          : 55\n",
      "    loss           : -261043.1923076923\n",
      "    val_loss       : -261684.95234375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch55.pth ...\n",
      "Train Epoch: 56 [256/54000 (0%)] Loss: -290323.625000\n",
      "Train Epoch: 56 [4352/54000 (8%)] Loss: -223592.453125\n",
      "Train Epoch: 56 [8448/54000 (16%)] Loss: -333000.625000\n",
      "Train Epoch: 56 [12544/54000 (23%)] Loss: -260172.921875\n",
      "Train Epoch: 56 [16640/54000 (31%)] Loss: -163293.515625\n",
      "Train Epoch: 56 [20736/54000 (38%)] Loss: -320094.625000\n",
      "Train Epoch: 56 [24832/54000 (46%)] Loss: -198694.593750\n",
      "Train Epoch: 56 [28928/54000 (54%)] Loss: -172488.968750\n",
      "Train Epoch: 56 [33024/54000 (61%)] Loss: -216395.609375\n",
      "Train Epoch: 56 [37120/54000 (69%)] Loss: -240481.156250\n",
      "Train Epoch: 56 [41216/54000 (76%)] Loss: -212977.234375\n",
      "Train Epoch: 56 [45312/54000 (84%)] Loss: -214366.828125\n",
      "Train Epoch: 56 [49408/54000 (91%)] Loss: -239840.234375\n",
      "    epoch          : 56\n",
      "    loss           : -262782.3511117789\n",
      "    val_loss       : -258082.81484375\n",
      "Train Epoch: 57 [256/54000 (0%)] Loss: -238597.890625\n",
      "Train Epoch: 57 [4352/54000 (8%)] Loss: -216960.328125\n",
      "Train Epoch: 57 [8448/54000 (16%)] Loss: -215102.828125\n",
      "Train Epoch: 57 [12544/54000 (23%)] Loss: -223482.953125\n",
      "Train Epoch: 57 [16640/54000 (31%)] Loss: -213314.671875\n",
      "Train Epoch: 57 [20736/54000 (38%)] Loss: -312727.843750\n",
      "Train Epoch: 57 [24832/54000 (46%)] Loss: -331613.031250\n",
      "Train Epoch: 57 [28928/54000 (54%)] Loss: -312968.218750\n",
      "Train Epoch: 57 [33024/54000 (61%)] Loss: -212153.718750\n",
      "Train Epoch: 57 [37120/54000 (69%)] Loss: -238608.578125\n",
      "Train Epoch: 57 [41216/54000 (76%)] Loss: -214965.859375\n",
      "Train Epoch: 57 [45312/54000 (84%)] Loss: -198432.406250\n",
      "Train Epoch: 57 [49408/54000 (91%)] Loss: -229058.984375\n",
      "    epoch          : 57\n",
      "    loss           : -264228.7700570914\n",
      "    val_loss       : -254811.13671875\n",
      "Train Epoch: 58 [256/54000 (0%)] Loss: -320700.718750\n",
      "Train Epoch: 58 [4352/54000 (8%)] Loss: -120380.351562\n",
      "Train Epoch: 58 [8448/54000 (16%)] Loss: -325711.812500\n",
      "Train Epoch: 58 [12544/54000 (23%)] Loss: -234300.078125\n",
      "Train Epoch: 58 [16640/54000 (31%)] Loss: -207840.234375\n",
      "Train Epoch: 58 [20736/54000 (38%)] Loss: -350353.750000\n",
      "Train Epoch: 58 [24832/54000 (46%)] Loss: -233212.265625\n",
      "Train Epoch: 58 [28928/54000 (54%)] Loss: -299102.281250\n",
      "Train Epoch: 58 [33024/54000 (61%)] Loss: -242099.625000\n",
      "Train Epoch: 58 [37120/54000 (69%)] Loss: -242828.296875\n",
      "Train Epoch: 58 [41216/54000 (76%)] Loss: -327962.718750\n",
      "Train Epoch: 58 [45312/54000 (84%)] Loss: -240561.890625\n",
      "Train Epoch: 58 [49408/54000 (91%)] Loss: -180893.109375\n",
      "    epoch          : 58\n",
      "    loss           : -262938.7006460336\n",
      "    val_loss       : -253193.28984375\n",
      "Train Epoch: 59 [256/54000 (0%)] Loss: -210259.453125\n",
      "Train Epoch: 59 [4352/54000 (8%)] Loss: -273294.750000\n",
      "Train Epoch: 59 [8448/54000 (16%)] Loss: -173729.421875\n",
      "Train Epoch: 59 [12544/54000 (23%)] Loss: -333257.750000\n",
      "Train Epoch: 59 [16640/54000 (31%)] Loss: -220884.890625\n",
      "Train Epoch: 59 [20736/54000 (38%)] Loss: -351337.343750\n",
      "Train Epoch: 59 [24832/54000 (46%)] Loss: -274614.031250\n",
      "Train Epoch: 59 [28928/54000 (54%)] Loss: -334214.656250\n",
      "Train Epoch: 59 [33024/54000 (61%)] Loss: -195897.328125\n",
      "Train Epoch: 59 [37120/54000 (69%)] Loss: -339397.406250\n",
      "Train Epoch: 59 [41216/54000 (76%)] Loss: -331179.812500\n",
      "Train Epoch: 59 [45312/54000 (84%)] Loss: -229727.453125\n",
      "Train Epoch: 59 [49408/54000 (91%)] Loss: -305944.500000\n",
      "    epoch          : 59\n",
      "    loss           : -265120.7205528846\n",
      "    val_loss       : -262202.05625\n",
      "Train Epoch: 60 [256/54000 (0%)] Loss: -273462.000000\n",
      "Train Epoch: 60 [4352/54000 (8%)] Loss: -349114.000000\n",
      "Train Epoch: 60 [8448/54000 (16%)] Loss: -235416.781250\n",
      "Train Epoch: 60 [12544/54000 (23%)] Loss: -227585.171875\n",
      "Train Epoch: 60 [16640/54000 (31%)] Loss: -358313.000000\n",
      "Train Epoch: 60 [20736/54000 (38%)] Loss: -212919.015625\n",
      "Train Epoch: 60 [24832/54000 (46%)] Loss: -216603.500000\n",
      "Train Epoch: 60 [28928/54000 (54%)] Loss: -303109.875000\n",
      "Train Epoch: 60 [33024/54000 (61%)] Loss: -212031.000000\n",
      "Train Epoch: 60 [37120/54000 (69%)] Loss: -290828.093750\n",
      "Train Epoch: 60 [41216/54000 (76%)] Loss: -324351.000000\n",
      "Train Epoch: 60 [45312/54000 (84%)] Loss: -310900.187500\n",
      "Train Epoch: 60 [49408/54000 (91%)] Loss: -227499.812500\n",
      "    epoch          : 60\n",
      "    loss           : -263316.93825120194\n",
      "    val_loss       : -252709.665625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch60.pth ...\n",
      "Train Epoch: 61 [256/54000 (0%)] Loss: -236797.859375\n",
      "Train Epoch: 61 [4352/54000 (8%)] Loss: -215672.953125\n",
      "Train Epoch: 61 [8448/54000 (16%)] Loss: -269503.218750\n",
      "Train Epoch: 61 [12544/54000 (23%)] Loss: -212953.000000\n",
      "Train Epoch: 61 [16640/54000 (31%)] Loss: -216049.593750\n",
      "Train Epoch: 61 [20736/54000 (38%)] Loss: -237043.828125\n",
      "Train Epoch: 61 [24832/54000 (46%)] Loss: -331778.718750\n",
      "Train Epoch: 61 [28928/54000 (54%)] Loss: -216885.187500\n",
      "Train Epoch: 61 [33024/54000 (61%)] Loss: -283167.687500\n",
      "Train Epoch: 61 [37120/54000 (69%)] Loss: -238243.453125\n",
      "Train Epoch: 61 [41216/54000 (76%)] Loss: -214741.156250\n",
      "Train Epoch: 61 [45312/54000 (84%)] Loss: -259902.875000\n",
      "Train Epoch: 61 [49408/54000 (91%)] Loss: -308013.875000\n",
      "    epoch          : 61\n",
      "    loss           : -265868.72393329325\n",
      "    val_loss       : -252809.03828125\n",
      "Train Epoch: 62 [256/54000 (0%)] Loss: -325612.312500\n",
      "Train Epoch: 62 [4352/54000 (8%)] Loss: -334908.750000\n",
      "Train Epoch: 62 [8448/54000 (16%)] Loss: -229606.406250\n",
      "Train Epoch: 62 [12544/54000 (23%)] Loss: -221791.578125\n",
      "Train Epoch: 62 [16640/54000 (31%)] Loss: -215967.453125\n",
      "Train Epoch: 62 [20736/54000 (38%)] Loss: -353295.687500\n",
      "Train Epoch: 62 [24832/54000 (46%)] Loss: -216412.296875\n",
      "Train Epoch: 62 [28928/54000 (54%)] Loss: -324474.093750\n",
      "Train Epoch: 62 [33024/54000 (61%)] Loss: -207138.328125\n",
      "Train Epoch: 62 [37120/54000 (69%)] Loss: -207693.593750\n",
      "Train Epoch: 62 [41216/54000 (76%)] Loss: -350607.281250\n",
      "Train Epoch: 62 [45312/54000 (84%)] Loss: -211731.265625\n",
      "Train Epoch: 62 [49408/54000 (91%)] Loss: -220286.390625\n",
      "    epoch          : 62\n",
      "    loss           : -263344.8147160457\n",
      "    val_loss       : -264156.034375\n",
      "Train Epoch: 63 [256/54000 (0%)] Loss: -181335.296875\n",
      "Train Epoch: 63 [4352/54000 (8%)] Loss: -356313.500000\n",
      "Train Epoch: 63 [8448/54000 (16%)] Loss: -314056.062500\n",
      "Train Epoch: 63 [12544/54000 (23%)] Loss: -323547.656250\n",
      "Train Epoch: 63 [16640/54000 (31%)] Loss: -286239.687500\n",
      "Train Epoch: 63 [20736/54000 (38%)] Loss: -247839.265625\n",
      "Train Epoch: 63 [24832/54000 (46%)] Loss: -279332.000000\n",
      "Train Epoch: 63 [28928/54000 (54%)] Loss: -235203.046875\n",
      "Train Epoch: 63 [33024/54000 (61%)] Loss: -318033.062500\n",
      "Train Epoch: 63 [37120/54000 (69%)] Loss: -322184.312500\n",
      "Train Epoch: 63 [41216/54000 (76%)] Loss: -355932.437500\n",
      "Train Epoch: 63 [45312/54000 (84%)] Loss: -184037.890625\n",
      "Train Epoch: 63 [49408/54000 (91%)] Loss: -237747.625000\n",
      "    epoch          : 63\n",
      "    loss           : -264799.69807316706\n",
      "    val_loss       : -262573.68125\n",
      "Train Epoch: 64 [256/54000 (0%)] Loss: -357031.093750\n",
      "Train Epoch: 64 [4352/54000 (8%)] Loss: -187716.750000\n",
      "Train Epoch: 64 [8448/54000 (16%)] Loss: -216530.046875\n",
      "Train Epoch: 64 [12544/54000 (23%)] Loss: -248014.421875\n",
      "Train Epoch: 64 [16640/54000 (31%)] Loss: -239716.640625\n",
      "Train Epoch: 64 [20736/54000 (38%)] Loss: -229366.718750\n",
      "Train Epoch: 64 [24832/54000 (46%)] Loss: -349993.843750\n",
      "Train Epoch: 64 [28928/54000 (54%)] Loss: -325632.500000\n",
      "Train Epoch: 64 [33024/54000 (61%)] Loss: -238096.187500\n",
      "Train Epoch: 64 [37120/54000 (69%)] Loss: -211499.984375\n",
      "Train Epoch: 64 [41216/54000 (76%)] Loss: -249118.312500\n",
      "Train Epoch: 64 [45312/54000 (84%)] Loss: -307749.187500\n",
      "Train Epoch: 64 [49408/54000 (91%)] Loss: -185697.453125\n",
      "    epoch          : 64\n",
      "    loss           : -265051.06837815506\n",
      "    val_loss       : -261751.69765625\n",
      "Train Epoch: 65 [256/54000 (0%)] Loss: -176585.187500\n",
      "Train Epoch: 65 [4352/54000 (8%)] Loss: -338578.125000\n",
      "Train Epoch: 65 [8448/54000 (16%)] Loss: -220028.843750\n",
      "Train Epoch: 65 [12544/54000 (23%)] Loss: -327298.218750\n",
      "Train Epoch: 65 [16640/54000 (31%)] Loss: -209987.843750\n",
      "Train Epoch: 65 [20736/54000 (38%)] Loss: -273802.187500\n",
      "Train Epoch: 65 [24832/54000 (46%)] Loss: -338704.312500\n",
      "Train Epoch: 65 [28928/54000 (54%)] Loss: -221605.343750\n",
      "Train Epoch: 65 [33024/54000 (61%)] Loss: -328124.500000\n",
      "Train Epoch: 65 [37120/54000 (69%)] Loss: -223900.796875\n",
      "Train Epoch: 65 [41216/54000 (76%)] Loss: -322112.531250\n",
      "Train Epoch: 65 [45312/54000 (84%)] Loss: -198832.218750\n",
      "Train Epoch: 65 [49408/54000 (91%)] Loss: -244288.406250\n",
      "    epoch          : 65\n",
      "    loss           : -266619.54033954325\n",
      "    val_loss       : -267076.3203125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch65.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 66 [256/54000 (0%)] Loss: -316844.937500\n",
      "Train Epoch: 66 [4352/54000 (8%)] Loss: -323942.875000\n",
      "Train Epoch: 66 [8448/54000 (16%)] Loss: -308408.750000\n",
      "Train Epoch: 66 [12544/54000 (23%)] Loss: -342871.843750\n",
      "Train Epoch: 66 [16640/54000 (31%)] Loss: -318896.218750\n",
      "Train Epoch: 66 [20736/54000 (38%)] Loss: -245251.640625\n",
      "Train Epoch: 66 [24832/54000 (46%)] Loss: -277014.281250\n",
      "Train Epoch: 66 [28928/54000 (54%)] Loss: -341776.906250\n",
      "Train Epoch: 66 [33024/54000 (61%)] Loss: -227704.734375\n",
      "Train Epoch: 66 [37120/54000 (69%)] Loss: -225472.015625\n",
      "Train Epoch: 66 [41216/54000 (76%)] Loss: -249197.453125\n",
      "Train Epoch: 66 [45312/54000 (84%)] Loss: -235121.000000\n",
      "Train Epoch: 66 [49408/54000 (91%)] Loss: -340162.375000\n",
      "    epoch          : 66\n",
      "    loss           : -265342.68381911056\n",
      "    val_loss       : -260368.79296875\n",
      "Train Epoch: 67 [256/54000 (0%)] Loss: -278233.625000\n",
      "Train Epoch: 67 [4352/54000 (8%)] Loss: -224437.234375\n",
      "Train Epoch: 67 [8448/54000 (16%)] Loss: -280655.375000\n",
      "Train Epoch: 67 [12544/54000 (23%)] Loss: -211746.609375\n",
      "Train Epoch: 67 [16640/54000 (31%)] Loss: -248206.484375\n",
      "Train Epoch: 67 [20736/54000 (38%)] Loss: -214948.625000\n",
      "Train Epoch: 67 [24832/54000 (46%)] Loss: -326116.718750\n",
      "Train Epoch: 67 [28928/54000 (54%)] Loss: -218089.125000\n",
      "Train Epoch: 67 [33024/54000 (61%)] Loss: -220369.515625\n",
      "Train Epoch: 67 [37120/54000 (69%)] Loss: -207368.015625\n",
      "Train Epoch: 67 [41216/54000 (76%)] Loss: -220319.437500\n",
      "Train Epoch: 67 [45312/54000 (84%)] Loss: -318781.250000\n",
      "Train Epoch: 67 [49408/54000 (91%)] Loss: -214388.343750\n",
      "    epoch          : 67\n",
      "    loss           : -267982.0943321815\n",
      "    val_loss       : -259911.18046875\n",
      "Train Epoch: 68 [256/54000 (0%)] Loss: -340101.343750\n",
      "Train Epoch: 68 [4352/54000 (8%)] Loss: -203293.453125\n",
      "Train Epoch: 68 [8448/54000 (16%)] Loss: -362502.125000\n",
      "Train Epoch: 68 [12544/54000 (23%)] Loss: -334048.687500\n",
      "Train Epoch: 68 [16640/54000 (31%)] Loss: -249894.718750\n",
      "Train Epoch: 68 [20736/54000 (38%)] Loss: -309920.312500\n",
      "Train Epoch: 68 [24832/54000 (46%)] Loss: -211805.703125\n",
      "Train Epoch: 68 [28928/54000 (54%)] Loss: -220280.937500\n",
      "Train Epoch: 68 [33024/54000 (61%)] Loss: -220653.968750\n",
      "Train Epoch: 68 [37120/54000 (69%)] Loss: -217582.578125\n",
      "Train Epoch: 68 [41216/54000 (76%)] Loss: -345778.125000\n",
      "Train Epoch: 68 [45312/54000 (84%)] Loss: -238461.203125\n",
      "Train Epoch: 68 [49408/54000 (91%)] Loss: -248442.609375\n",
      "    epoch          : 68\n",
      "    loss           : -269955.77223557694\n",
      "    val_loss       : -268815.46640625\n",
      "Train Epoch: 69 [256/54000 (0%)] Loss: -293466.968750\n",
      "Train Epoch: 69 [4352/54000 (8%)] Loss: -225800.296875\n",
      "Train Epoch: 69 [8448/54000 (16%)] Loss: -314690.218750\n",
      "Train Epoch: 69 [12544/54000 (23%)] Loss: -274732.250000\n",
      "Train Epoch: 69 [16640/54000 (31%)] Loss: -228544.328125\n",
      "Train Epoch: 69 [20736/54000 (38%)] Loss: -203069.265625\n",
      "Train Epoch: 69 [24832/54000 (46%)] Loss: -221400.078125\n",
      "Train Epoch: 69 [28928/54000 (54%)] Loss: -318973.875000\n",
      "Train Epoch: 69 [33024/54000 (61%)] Loss: -228677.562500\n",
      "Train Epoch: 69 [37120/54000 (69%)] Loss: -262786.937500\n",
      "Train Epoch: 69 [41216/54000 (76%)] Loss: -332272.437500\n",
      "Train Epoch: 69 [45312/54000 (84%)] Loss: -310070.000000\n",
      "Train Epoch: 69 [49408/54000 (91%)] Loss: -364391.156250\n",
      "    epoch          : 69\n",
      "    loss           : -266653.0520019531\n",
      "    val_loss       : -269073.09296875\n",
      "Train Epoch: 70 [256/54000 (0%)] Loss: -222256.140625\n",
      "Train Epoch: 70 [4352/54000 (8%)] Loss: -290721.281250\n",
      "Train Epoch: 70 [8448/54000 (16%)] Loss: -202400.203125\n",
      "Train Epoch: 70 [12544/54000 (23%)] Loss: -291220.437500\n",
      "Train Epoch: 70 [16640/54000 (31%)] Loss: -305757.437500\n",
      "Train Epoch: 70 [20736/54000 (38%)] Loss: -220619.859375\n",
      "Train Epoch: 70 [24832/54000 (46%)] Loss: -237266.078125\n",
      "Train Epoch: 70 [28928/54000 (54%)] Loss: -351567.187500\n",
      "Train Epoch: 70 [33024/54000 (61%)] Loss: -263655.656250\n",
      "Train Epoch: 70 [37120/54000 (69%)] Loss: -174951.828125\n",
      "Train Epoch: 70 [41216/54000 (76%)] Loss: -250209.453125\n",
      "Train Epoch: 70 [45312/54000 (84%)] Loss: -355093.187500\n",
      "Train Epoch: 70 [49408/54000 (91%)] Loss: -239968.656250\n",
      "    epoch          : 70\n",
      "    loss           : -269290.8692908654\n",
      "    val_loss       : -260576.55703125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch70.pth ...\n",
      "Train Epoch: 71 [256/54000 (0%)] Loss: -276512.125000\n",
      "Train Epoch: 71 [4352/54000 (8%)] Loss: -200277.828125\n",
      "Train Epoch: 71 [8448/54000 (16%)] Loss: -220911.765625\n",
      "Train Epoch: 71 [12544/54000 (23%)] Loss: -312397.687500\n",
      "Train Epoch: 71 [16640/54000 (31%)] Loss: -193731.828125\n",
      "Train Epoch: 71 [20736/54000 (38%)] Loss: -208567.250000\n",
      "Train Epoch: 71 [24832/54000 (46%)] Loss: -222216.640625\n",
      "Train Epoch: 71 [28928/54000 (54%)] Loss: -243200.203125\n",
      "Train Epoch: 71 [33024/54000 (61%)] Loss: -215998.812500\n",
      "Train Epoch: 71 [37120/54000 (69%)] Loss: -211055.406250\n",
      "Train Epoch: 71 [41216/54000 (76%)] Loss: -320899.093750\n",
      "Train Epoch: 71 [45312/54000 (84%)] Loss: -264376.437500\n",
      "Train Epoch: 71 [49408/54000 (91%)] Loss: -250770.921875\n",
      "    epoch          : 71\n",
      "    loss           : -267331.71942608175\n",
      "    val_loss       : -264065.771875\n",
      "Train Epoch: 72 [256/54000 (0%)] Loss: -248850.515625\n",
      "Train Epoch: 72 [4352/54000 (8%)] Loss: -357534.656250\n",
      "Train Epoch: 72 [8448/54000 (16%)] Loss: -312743.312500\n",
      "Train Epoch: 72 [12544/54000 (23%)] Loss: -325237.875000\n",
      "Train Epoch: 72 [16640/54000 (31%)] Loss: -355901.781250\n",
      "Train Epoch: 72 [20736/54000 (38%)] Loss: -194171.500000\n",
      "Train Epoch: 72 [24832/54000 (46%)] Loss: -218308.593750\n",
      "Train Epoch: 72 [28928/54000 (54%)] Loss: -352975.875000\n",
      "Train Epoch: 72 [33024/54000 (61%)] Loss: -261470.359375\n",
      "Train Epoch: 72 [37120/54000 (69%)] Loss: -320297.687500\n",
      "Train Epoch: 72 [41216/54000 (76%)] Loss: -321915.531250\n",
      "Train Epoch: 72 [45312/54000 (84%)] Loss: -358611.781250\n",
      "Train Epoch: 72 [49408/54000 (91%)] Loss: -200073.703125\n",
      "    epoch          : 72\n",
      "    loss           : -268401.65887920675\n",
      "    val_loss       : -264999.35234375\n",
      "Train Epoch: 73 [256/54000 (0%)] Loss: -226802.453125\n",
      "Train Epoch: 73 [4352/54000 (8%)] Loss: -225958.250000\n",
      "Train Epoch: 73 [8448/54000 (16%)] Loss: -340808.437500\n",
      "Train Epoch: 73 [12544/54000 (23%)] Loss: -199421.000000\n",
      "Train Epoch: 73 [16640/54000 (31%)] Loss: -240352.859375\n",
      "Train Epoch: 73 [20736/54000 (38%)] Loss: -201398.796875\n",
      "Train Epoch: 73 [24832/54000 (46%)] Loss: -264097.281250\n",
      "Train Epoch: 73 [28928/54000 (54%)] Loss: -192332.734375\n",
      "Train Epoch: 73 [33024/54000 (61%)] Loss: -224524.593750\n",
      "Train Epoch: 73 [37120/54000 (69%)] Loss: -231087.500000\n",
      "Train Epoch: 73 [41216/54000 (76%)] Loss: -354158.625000\n",
      "Train Epoch: 73 [45312/54000 (84%)] Loss: -264213.281250\n",
      "Train Epoch: 73 [49408/54000 (91%)] Loss: -204967.562500\n",
      "    epoch          : 73\n",
      "    loss           : -268285.21963266225\n",
      "    val_loss       : -270598.615625\n",
      "Train Epoch: 74 [256/54000 (0%)] Loss: -261115.875000\n",
      "Train Epoch: 74 [4352/54000 (8%)] Loss: -320658.812500\n",
      "Train Epoch: 74 [8448/54000 (16%)] Loss: -312333.468750\n",
      "Train Epoch: 74 [12544/54000 (23%)] Loss: -292189.906250\n",
      "Train Epoch: 74 [16640/54000 (31%)] Loss: -217794.312500\n",
      "Train Epoch: 74 [20736/54000 (38%)] Loss: -322838.625000\n",
      "Train Epoch: 74 [24832/54000 (46%)] Loss: -227443.109375\n",
      "Train Epoch: 74 [28928/54000 (54%)] Loss: -210283.484375\n",
      "Train Epoch: 74 [33024/54000 (61%)] Loss: -211225.921875\n",
      "Train Epoch: 74 [37120/54000 (69%)] Loss: -238435.812500\n",
      "Train Epoch: 74 [41216/54000 (76%)] Loss: -215523.437500\n",
      "Train Epoch: 74 [45312/54000 (84%)] Loss: -302383.375000\n",
      "Train Epoch: 74 [49408/54000 (91%)] Loss: -366881.218750\n",
      "    epoch          : 74\n",
      "    loss           : -267145.28575721156\n",
      "    val_loss       : -266169.06875\n",
      "Train Epoch: 75 [256/54000 (0%)] Loss: -231271.171875\n",
      "Train Epoch: 75 [4352/54000 (8%)] Loss: -264436.031250\n",
      "Train Epoch: 75 [8448/54000 (16%)] Loss: -225854.468750\n",
      "Train Epoch: 75 [12544/54000 (23%)] Loss: -325739.812500\n",
      "Train Epoch: 75 [16640/54000 (31%)] Loss: -210655.046875\n",
      "Train Epoch: 75 [20736/54000 (38%)] Loss: -248439.718750\n",
      "Train Epoch: 75 [24832/54000 (46%)] Loss: -222209.015625\n",
      "Train Epoch: 75 [28928/54000 (54%)] Loss: -342642.750000\n",
      "Train Epoch: 75 [33024/54000 (61%)] Loss: -356127.843750\n",
      "Train Epoch: 75 [37120/54000 (69%)] Loss: -326134.156250\n",
      "Train Epoch: 75 [41216/54000 (76%)] Loss: -342785.375000\n",
      "Train Epoch: 75 [45312/54000 (84%)] Loss: -319782.781250\n",
      "Train Epoch: 75 [49408/54000 (91%)] Loss: -356863.718750\n",
      "    epoch          : 75\n",
      "    loss           : -268598.31663161056\n",
      "    val_loss       : -269333.115625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch75.pth ...\n",
      "Train Epoch: 76 [256/54000 (0%)] Loss: -241795.000000\n",
      "Train Epoch: 76 [4352/54000 (8%)] Loss: -196516.953125\n",
      "Train Epoch: 76 [8448/54000 (16%)] Loss: -228645.500000\n",
      "Train Epoch: 76 [12544/54000 (23%)] Loss: -345732.000000\n",
      "Train Epoch: 76 [16640/54000 (31%)] Loss: -242849.156250\n",
      "Train Epoch: 76 [20736/54000 (38%)] Loss: -344130.500000\n",
      "Train Epoch: 76 [24832/54000 (46%)] Loss: -316261.718750\n",
      "Train Epoch: 76 [28928/54000 (54%)] Loss: -358899.875000\n",
      "Train Epoch: 76 [33024/54000 (61%)] Loss: -325384.875000\n",
      "Train Epoch: 76 [37120/54000 (69%)] Loss: -283117.343750\n",
      "Train Epoch: 76 [41216/54000 (76%)] Loss: -364929.062500\n",
      "Train Epoch: 76 [45312/54000 (84%)] Loss: -309113.812500\n",
      "Train Epoch: 76 [49408/54000 (91%)] Loss: -207325.296875\n",
      "    epoch          : 76\n",
      "    loss           : -265351.321101262\n",
      "    val_loss       : -274012.465625\n",
      "Train Epoch: 77 [256/54000 (0%)] Loss: -356013.500000\n",
      "Train Epoch: 77 [4352/54000 (8%)] Loss: -332315.437500\n",
      "Train Epoch: 77 [8448/54000 (16%)] Loss: -235128.984375\n",
      "Train Epoch: 77 [12544/54000 (23%)] Loss: -264155.062500\n",
      "Train Epoch: 77 [16640/54000 (31%)] Loss: -221301.109375\n",
      "Train Epoch: 77 [20736/54000 (38%)] Loss: -353051.593750\n",
      "Train Epoch: 77 [24832/54000 (46%)] Loss: -310403.531250\n",
      "Train Epoch: 77 [28928/54000 (54%)] Loss: -218008.484375\n",
      "Train Epoch: 77 [33024/54000 (61%)] Loss: -271089.812500\n",
      "Train Epoch: 77 [37120/54000 (69%)] Loss: -367019.312500\n",
      "Train Epoch: 77 [41216/54000 (76%)] Loss: -321568.875000\n",
      "Train Epoch: 77 [45312/54000 (84%)] Loss: -284041.687500\n",
      "Train Epoch: 77 [49408/54000 (91%)] Loss: -45289.644531\n",
      "    epoch          : 77\n",
      "    loss           : -270156.44919996994\n",
      "    val_loss       : -267884.93984375\n",
      "Train Epoch: 78 [256/54000 (0%)] Loss: -230886.546875\n",
      "Train Epoch: 78 [4352/54000 (8%)] Loss: -242523.515625\n",
      "Train Epoch: 78 [8448/54000 (16%)] Loss: -248329.421875\n",
      "Train Epoch: 78 [12544/54000 (23%)] Loss: -223517.812500\n",
      "Train Epoch: 78 [16640/54000 (31%)] Loss: -58544.675781\n",
      "Train Epoch: 78 [20736/54000 (38%)] Loss: -326398.375000\n",
      "Train Epoch: 78 [24832/54000 (46%)] Loss: -358991.000000\n",
      "Train Epoch: 78 [28928/54000 (54%)] Loss: -199064.171875\n",
      "Train Epoch: 78 [33024/54000 (61%)] Loss: -310440.718750\n",
      "Train Epoch: 78 [37120/54000 (69%)] Loss: -139736.484375\n",
      "Train Epoch: 78 [41216/54000 (76%)] Loss: -360141.250000\n",
      "Train Epoch: 78 [45312/54000 (84%)] Loss: -279972.156250\n",
      "Train Epoch: 78 [49408/54000 (91%)] Loss: -189100.375000\n",
      "    epoch          : 78\n",
      "    loss           : -268840.1422588642\n",
      "    val_loss       : -267453.96328125\n",
      "Train Epoch: 79 [256/54000 (0%)] Loss: -342552.500000\n",
      "Train Epoch: 79 [4352/54000 (8%)] Loss: -353771.906250\n",
      "Train Epoch: 79 [8448/54000 (16%)] Loss: -230234.750000\n",
      "Train Epoch: 79 [12544/54000 (23%)] Loss: -281200.406250\n",
      "Train Epoch: 79 [16640/54000 (31%)] Loss: -294182.062500\n",
      "Train Epoch: 79 [20736/54000 (38%)] Loss: -341754.250000\n",
      "Train Epoch: 79 [24832/54000 (46%)] Loss: -257559.281250\n",
      "Train Epoch: 79 [28928/54000 (54%)] Loss: -177282.484375\n",
      "Train Epoch: 79 [33024/54000 (61%)] Loss: -290507.437500\n",
      "Train Epoch: 79 [37120/54000 (69%)] Loss: -354290.875000\n",
      "Train Epoch: 79 [41216/54000 (76%)] Loss: -334878.562500\n",
      "Train Epoch: 79 [45312/54000 (84%)] Loss: -223243.953125\n",
      "Train Epoch: 79 [49408/54000 (91%)] Loss: -225715.937500\n",
      "    epoch          : 79\n",
      "    loss           : -271580.2061298077\n",
      "    val_loss       : -268914.45390625\n",
      "Train Epoch: 80 [256/54000 (0%)] Loss: -351429.093750\n",
      "Train Epoch: 80 [4352/54000 (8%)] Loss: -331136.750000\n",
      "Train Epoch: 80 [8448/54000 (16%)] Loss: -228374.343750\n",
      "Train Epoch: 80 [12544/54000 (23%)] Loss: -307341.500000\n",
      "Train Epoch: 80 [16640/54000 (31%)] Loss: -239421.515625\n",
      "Train Epoch: 80 [20736/54000 (38%)] Loss: -221420.890625\n",
      "Train Epoch: 80 [24832/54000 (46%)] Loss: -348653.562500\n",
      "Train Epoch: 80 [28928/54000 (54%)] Loss: -244779.140625\n",
      "Train Epoch: 80 [33024/54000 (61%)] Loss: -210432.296875\n",
      "Train Epoch: 80 [37120/54000 (69%)] Loss: -327326.375000\n",
      "Train Epoch: 80 [41216/54000 (76%)] Loss: -246844.359375\n",
      "Train Epoch: 80 [45312/54000 (84%)] Loss: -294232.312500\n",
      "Train Epoch: 80 [49408/54000 (91%)] Loss: -358699.218750\n",
      "    epoch          : 80\n",
      "    loss           : -271350.06501652644\n",
      "    val_loss       : -269697.75859375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [256/54000 (0%)] Loss: -234138.609375\n",
      "Train Epoch: 81 [4352/54000 (8%)] Loss: -306119.562500\n",
      "Train Epoch: 81 [8448/54000 (16%)] Loss: -228833.796875\n",
      "Train Epoch: 81 [12544/54000 (23%)] Loss: -341605.437500\n",
      "Train Epoch: 81 [16640/54000 (31%)] Loss: -246598.406250\n",
      "Train Epoch: 81 [20736/54000 (38%)] Loss: -283113.718750\n",
      "Train Epoch: 81 [24832/54000 (46%)] Loss: -209879.000000\n",
      "Train Epoch: 81 [28928/54000 (54%)] Loss: -217932.890625\n",
      "Train Epoch: 81 [33024/54000 (61%)] Loss: -206472.609375\n",
      "Train Epoch: 81 [37120/54000 (69%)] Loss: -245577.828125\n",
      "Train Epoch: 81 [41216/54000 (76%)] Loss: -280822.281250\n",
      "Train Epoch: 81 [45312/54000 (84%)] Loss: -336820.031250\n",
      "Train Epoch: 81 [49408/54000 (91%)] Loss: -254923.484375\n",
      "    epoch          : 81\n",
      "    loss           : -274744.9325420673\n",
      "    val_loss       : -267946.61875\n",
      "Train Epoch: 82 [256/54000 (0%)] Loss: -262368.156250\n",
      "Train Epoch: 82 [4352/54000 (8%)] Loss: -363118.218750\n",
      "Train Epoch: 82 [8448/54000 (16%)] Loss: -325136.125000\n",
      "Train Epoch: 82 [12544/54000 (23%)] Loss: -223138.562500\n",
      "Train Epoch: 82 [16640/54000 (31%)] Loss: -318309.125000\n",
      "Train Epoch: 82 [20736/54000 (38%)] Loss: -362118.062500\n",
      "Train Epoch: 82 [24832/54000 (46%)] Loss: -313595.000000\n",
      "Train Epoch: 82 [28928/54000 (54%)] Loss: -348723.250000\n",
      "Train Epoch: 82 [33024/54000 (61%)] Loss: -193305.687500\n",
      "Train Epoch: 82 [37120/54000 (69%)] Loss: -366203.468750\n",
      "Train Epoch: 82 [41216/54000 (76%)] Loss: -350939.437500\n",
      "Train Epoch: 82 [45312/54000 (84%)] Loss: -287208.656250\n",
      "Train Epoch: 82 [49408/54000 (91%)] Loss: -239839.359375\n",
      "    epoch          : 82\n",
      "    loss           : -273216.96266526444\n",
      "    val_loss       : -247370.8611328125\n",
      "Train Epoch: 83 [256/54000 (0%)] Loss: -360781.375000\n",
      "Train Epoch: 83 [4352/54000 (8%)] Loss: -138776.296875\n",
      "Train Epoch: 83 [8448/54000 (16%)] Loss: -319009.500000\n",
      "Train Epoch: 83 [12544/54000 (23%)] Loss: -238187.046875\n",
      "Train Epoch: 83 [16640/54000 (31%)] Loss: -243892.828125\n",
      "Train Epoch: 83 [20736/54000 (38%)] Loss: -281926.281250\n",
      "Train Epoch: 83 [24832/54000 (46%)] Loss: -228705.546875\n",
      "Train Epoch: 83 [28928/54000 (54%)] Loss: -246104.671875\n",
      "Train Epoch: 83 [33024/54000 (61%)] Loss: -250565.671875\n",
      "Train Epoch: 83 [37120/54000 (69%)] Loss: -275957.937500\n",
      "Train Epoch: 83 [41216/54000 (76%)] Loss: -358680.125000\n",
      "Train Epoch: 83 [45312/54000 (84%)] Loss: -243874.453125\n",
      "Train Epoch: 83 [49408/54000 (91%)] Loss: -231682.703125\n",
      "    epoch          : 83\n",
      "    loss           : -274905.6786358173\n",
      "    val_loss       : -270700.8515625\n",
      "Train Epoch: 84 [256/54000 (0%)] Loss: -219279.843750\n",
      "Train Epoch: 84 [4352/54000 (8%)] Loss: -329696.312500\n",
      "Train Epoch: 84 [8448/54000 (16%)] Loss: -305867.250000\n",
      "Train Epoch: 84 [12544/54000 (23%)] Loss: -327744.937500\n",
      "Train Epoch: 84 [16640/54000 (31%)] Loss: -227738.890625\n",
      "Train Epoch: 84 [20736/54000 (38%)] Loss: -204617.796875\n",
      "Train Epoch: 84 [24832/54000 (46%)] Loss: -239097.093750\n",
      "Train Epoch: 84 [28928/54000 (54%)] Loss: -226915.421875\n",
      "Train Epoch: 84 [33024/54000 (61%)] Loss: -267242.906250\n",
      "Train Epoch: 84 [37120/54000 (69%)] Loss: -336805.875000\n",
      "Train Epoch: 84 [41216/54000 (76%)] Loss: -215415.015625\n",
      "Train Epoch: 84 [45312/54000 (84%)] Loss: -339104.375000\n",
      "Train Epoch: 84 [49408/54000 (91%)] Loss: -320172.968750\n",
      "    epoch          : 84\n",
      "    loss           : -273993.28252704325\n",
      "    val_loss       : -267349.70703125\n",
      "Train Epoch: 85 [256/54000 (0%)] Loss: -207188.250000\n",
      "Train Epoch: 85 [4352/54000 (8%)] Loss: -214899.312500\n",
      "Train Epoch: 85 [8448/54000 (16%)] Loss: -222064.921875\n",
      "Train Epoch: 85 [12544/54000 (23%)] Loss: -283896.406250\n",
      "Train Epoch: 85 [16640/54000 (31%)] Loss: -360094.500000\n",
      "Train Epoch: 85 [20736/54000 (38%)] Loss: -218391.203125\n",
      "Train Epoch: 85 [24832/54000 (46%)] Loss: -377059.906250\n",
      "Train Epoch: 85 [28928/54000 (54%)] Loss: -357444.156250\n",
      "Train Epoch: 85 [33024/54000 (61%)] Loss: -370165.875000\n",
      "Train Epoch: 85 [37120/54000 (69%)] Loss: -212678.859375\n",
      "Train Epoch: 85 [41216/54000 (76%)] Loss: -247061.265625\n",
      "Train Epoch: 85 [45312/54000 (84%)] Loss: -204932.671875\n",
      "Train Epoch: 85 [49408/54000 (91%)] Loss: -245612.140625\n",
      "    epoch          : 85\n",
      "    loss           : -273691.7338491586\n",
      "    val_loss       : -269422.28359375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch85.pth ...\n",
      "Train Epoch: 86 [256/54000 (0%)] Loss: -351737.687500\n",
      "Train Epoch: 86 [4352/54000 (8%)] Loss: -254055.812500\n",
      "Train Epoch: 86 [8448/54000 (16%)] Loss: -241387.171875\n",
      "Train Epoch: 86 [12544/54000 (23%)] Loss: -313293.437500\n",
      "Train Epoch: 86 [16640/54000 (31%)] Loss: -231437.296875\n",
      "Train Epoch: 86 [20736/54000 (38%)] Loss: -227661.562500\n",
      "Train Epoch: 86 [24832/54000 (46%)] Loss: -206993.718750\n",
      "Train Epoch: 86 [28928/54000 (54%)] Loss: -216380.546875\n",
      "Train Epoch: 86 [33024/54000 (61%)] Loss: -345483.312500\n",
      "Train Epoch: 86 [37120/54000 (69%)] Loss: -276677.125000\n",
      "Train Epoch: 86 [41216/54000 (76%)] Loss: -213437.859375\n",
      "Train Epoch: 86 [45312/54000 (84%)] Loss: -229446.140625\n",
      "Train Epoch: 86 [49408/54000 (91%)] Loss: -207078.031250\n",
      "    epoch          : 86\n",
      "    loss           : -272954.03455528844\n",
      "    val_loss       : -269421.2828125\n",
      "Train Epoch: 87 [256/54000 (0%)] Loss: -357604.593750\n",
      "Train Epoch: 87 [4352/54000 (8%)] Loss: -221204.484375\n",
      "Train Epoch: 87 [8448/54000 (16%)] Loss: -248959.250000\n",
      "Train Epoch: 87 [12544/54000 (23%)] Loss: -197137.468750\n",
      "Train Epoch: 87 [16640/54000 (31%)] Loss: -317138.750000\n",
      "Train Epoch: 87 [20736/54000 (38%)] Loss: -239424.406250\n",
      "Train Epoch: 87 [24832/54000 (46%)] Loss: -344984.500000\n",
      "Train Epoch: 87 [28928/54000 (54%)] Loss: -268766.500000\n",
      "Train Epoch: 87 [33024/54000 (61%)] Loss: -229843.421875\n",
      "Train Epoch: 87 [37120/54000 (69%)] Loss: -330543.375000\n",
      "Train Epoch: 87 [41216/54000 (76%)] Loss: -180064.515625\n",
      "Train Epoch: 87 [45312/54000 (84%)] Loss: -237630.984375\n",
      "Train Epoch: 87 [49408/54000 (91%)] Loss: -340857.625000\n",
      "    epoch          : 87\n",
      "    loss           : -271956.2734750601\n",
      "    val_loss       : -257607.57109375\n",
      "Train Epoch: 88 [256/54000 (0%)] Loss: -243602.687500\n",
      "Train Epoch: 88 [4352/54000 (8%)] Loss: -283481.937500\n",
      "Train Epoch: 88 [8448/54000 (16%)] Loss: -336684.937500\n",
      "Train Epoch: 88 [12544/54000 (23%)] Loss: -221958.625000\n",
      "Train Epoch: 88 [16640/54000 (31%)] Loss: -232642.281250\n",
      "Train Epoch: 88 [20736/54000 (38%)] Loss: -329074.000000\n",
      "Train Epoch: 88 [24832/54000 (46%)] Loss: -242631.734375\n",
      "Train Epoch: 88 [28928/54000 (54%)] Loss: -239525.984375\n",
      "Train Epoch: 88 [33024/54000 (61%)] Loss: -225594.328125\n",
      "Train Epoch: 88 [37120/54000 (69%)] Loss: -310829.312500\n",
      "Train Epoch: 88 [41216/54000 (76%)] Loss: -210359.375000\n",
      "Train Epoch: 88 [45312/54000 (84%)] Loss: -228602.984375\n",
      "Train Epoch: 88 [49408/54000 (91%)] Loss: -182483.312500\n",
      "    epoch          : 88\n",
      "    loss           : -274201.3683143029\n",
      "    val_loss       : -270971.41015625\n",
      "Train Epoch: 89 [256/54000 (0%)] Loss: -240474.468750\n",
      "Train Epoch: 89 [4352/54000 (8%)] Loss: -367710.875000\n",
      "Train Epoch: 89 [8448/54000 (16%)] Loss: -229380.890625\n",
      "Train Epoch: 89 [12544/54000 (23%)] Loss: -218353.250000\n",
      "Train Epoch: 89 [16640/54000 (31%)] Loss: -317826.968750\n",
      "Train Epoch: 89 [20736/54000 (38%)] Loss: -240511.015625\n",
      "Train Epoch: 89 [24832/54000 (46%)] Loss: -333609.687500\n",
      "Train Epoch: 89 [28928/54000 (54%)] Loss: -227096.812500\n",
      "Train Epoch: 89 [33024/54000 (61%)] Loss: -354047.500000\n",
      "Train Epoch: 89 [37120/54000 (69%)] Loss: -356547.656250\n",
      "Train Epoch: 89 [41216/54000 (76%)] Loss: -244565.421875\n",
      "Train Epoch: 89 [45312/54000 (84%)] Loss: -222601.578125\n",
      "Train Epoch: 89 [49408/54000 (91%)] Loss: -222329.531250\n",
      "    epoch          : 89\n",
      "    loss           : -275953.17653245194\n",
      "    val_loss       : -268378.4125\n",
      "Train Epoch: 90 [256/54000 (0%)] Loss: -329539.125000\n",
      "Train Epoch: 90 [4352/54000 (8%)] Loss: -214711.296875\n",
      "Train Epoch: 90 [8448/54000 (16%)] Loss: -252333.234375\n",
      "Train Epoch: 90 [12544/54000 (23%)] Loss: -319797.156250\n",
      "Train Epoch: 90 [16640/54000 (31%)] Loss: -254530.421875\n",
      "Train Epoch: 90 [20736/54000 (38%)] Loss: -164118.515625\n",
      "Train Epoch: 90 [24832/54000 (46%)] Loss: -203430.671875\n",
      "Train Epoch: 90 [28928/54000 (54%)] Loss: -248030.937500\n",
      "Train Epoch: 90 [33024/54000 (61%)] Loss: -224121.906250\n",
      "Train Epoch: 90 [37120/54000 (69%)] Loss: -245008.296875\n",
      "Train Epoch: 90 [41216/54000 (76%)] Loss: -229235.937500\n",
      "Train Epoch: 90 [45312/54000 (84%)] Loss: -362705.437500\n",
      "Train Epoch: 90 [49408/54000 (91%)] Loss: -249651.468750\n",
      "    epoch          : 90\n",
      "    loss           : -275787.26600060094\n",
      "    val_loss       : -259024.878125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch90.pth ...\n",
      "Train Epoch: 91 [256/54000 (0%)] Loss: -216946.890625\n",
      "Train Epoch: 91 [4352/54000 (8%)] Loss: -323544.906250\n",
      "Train Epoch: 91 [8448/54000 (16%)] Loss: -220595.718750\n",
      "Train Epoch: 91 [12544/54000 (23%)] Loss: -327332.125000\n",
      "Train Epoch: 91 [16640/54000 (31%)] Loss: -220458.656250\n",
      "Train Epoch: 91 [20736/54000 (38%)] Loss: -352814.281250\n",
      "Train Epoch: 91 [24832/54000 (46%)] Loss: -297538.593750\n",
      "Train Epoch: 91 [28928/54000 (54%)] Loss: -213662.843750\n",
      "Train Epoch: 91 [33024/54000 (61%)] Loss: -318955.750000\n",
      "Train Epoch: 91 [37120/54000 (69%)] Loss: -336065.875000\n",
      "Train Epoch: 91 [41216/54000 (76%)] Loss: -254524.609375\n",
      "Train Epoch: 91 [45312/54000 (84%)] Loss: -215594.906250\n",
      "Train Epoch: 91 [49408/54000 (91%)] Loss: -352007.031250\n",
      "    epoch          : 91\n",
      "    loss           : -272040.3185096154\n",
      "    val_loss       : -267815.925\n",
      "Train Epoch: 92 [256/54000 (0%)] Loss: -273159.812500\n",
      "Train Epoch: 92 [4352/54000 (8%)] Loss: -225035.000000\n",
      "Train Epoch: 92 [8448/54000 (16%)] Loss: -357135.843750\n",
      "Train Epoch: 92 [12544/54000 (23%)] Loss: -234343.640625\n",
      "Train Epoch: 92 [16640/54000 (31%)] Loss: -243940.796875\n",
      "Train Epoch: 92 [20736/54000 (38%)] Loss: -297928.375000\n",
      "Train Epoch: 92 [24832/54000 (46%)] Loss: -178258.703125\n",
      "Train Epoch: 92 [28928/54000 (54%)] Loss: -348949.687500\n",
      "Train Epoch: 92 [33024/54000 (61%)] Loss: -238660.218750\n",
      "Train Epoch: 92 [37120/54000 (69%)] Loss: -247770.843750\n",
      "Train Epoch: 92 [41216/54000 (76%)] Loss: -236053.937500\n",
      "Train Epoch: 92 [45312/54000 (84%)] Loss: -326397.031250\n",
      "Train Epoch: 92 [49408/54000 (91%)] Loss: -225500.078125\n",
      "    epoch          : 92\n",
      "    loss           : -275065.8660794772\n",
      "    val_loss       : -269093.578125\n",
      "Train Epoch: 93 [256/54000 (0%)] Loss: -240711.296875\n",
      "Train Epoch: 93 [4352/54000 (8%)] Loss: -224777.343750\n",
      "Train Epoch: 93 [8448/54000 (16%)] Loss: -214754.171875\n",
      "Train Epoch: 93 [12544/54000 (23%)] Loss: -235077.937500\n",
      "Train Epoch: 93 [16640/54000 (31%)] Loss: -252846.609375\n",
      "Train Epoch: 93 [20736/54000 (38%)] Loss: -316935.500000\n",
      "Train Epoch: 93 [24832/54000 (46%)] Loss: -224819.953125\n",
      "Train Epoch: 93 [28928/54000 (54%)] Loss: -175153.843750\n",
      "Train Epoch: 93 [33024/54000 (61%)] Loss: -236345.484375\n",
      "Train Epoch: 93 [37120/54000 (69%)] Loss: -249914.984375\n",
      "Train Epoch: 93 [41216/54000 (76%)] Loss: -221858.031250\n",
      "Train Epoch: 93 [45312/54000 (84%)] Loss: -228794.593750\n",
      "Train Epoch: 93 [49408/54000 (91%)] Loss: -241154.953125\n",
      "    epoch          : 93\n",
      "    loss           : -273805.17901141825\n",
      "    val_loss       : -263808.473828125\n",
      "Train Epoch: 94 [256/54000 (0%)] Loss: -295090.906250\n",
      "Train Epoch: 94 [4352/54000 (8%)] Loss: -318486.906250\n",
      "Train Epoch: 94 [8448/54000 (16%)] Loss: -140684.078125\n",
      "Train Epoch: 94 [12544/54000 (23%)] Loss: -240028.031250\n",
      "Train Epoch: 94 [16640/54000 (31%)] Loss: -342622.406250\n",
      "Train Epoch: 94 [20736/54000 (38%)] Loss: -356891.781250\n",
      "Train Epoch: 94 [24832/54000 (46%)] Loss: -339047.625000\n",
      "Train Epoch: 94 [28928/54000 (54%)] Loss: -249590.765625\n",
      "Train Epoch: 94 [33024/54000 (61%)] Loss: -324270.250000\n",
      "Train Epoch: 94 [37120/54000 (69%)] Loss: -224880.656250\n",
      "Train Epoch: 94 [41216/54000 (76%)] Loss: -176813.687500\n",
      "Train Epoch: 94 [45312/54000 (84%)] Loss: -224324.843750\n",
      "Train Epoch: 94 [49408/54000 (91%)] Loss: -298952.093750\n",
      "    epoch          : 94\n",
      "    loss           : -274354.70368840144\n",
      "    val_loss       : -269357.44296875\n",
      "Train Epoch: 95 [256/54000 (0%)] Loss: -253305.703125\n",
      "Train Epoch: 95 [4352/54000 (8%)] Loss: -321264.250000\n",
      "Train Epoch: 95 [8448/54000 (16%)] Loss: -341658.437500\n",
      "Train Epoch: 95 [12544/54000 (23%)] Loss: -361567.812500\n",
      "Train Epoch: 95 [16640/54000 (31%)] Loss: -247104.453125\n",
      "Train Epoch: 95 [20736/54000 (38%)] Loss: -243379.015625\n",
      "Train Epoch: 95 [24832/54000 (46%)] Loss: -225909.796875\n",
      "Train Epoch: 95 [28928/54000 (54%)] Loss: -342160.250000\n",
      "Train Epoch: 95 [33024/54000 (61%)] Loss: -222995.015625\n",
      "Train Epoch: 95 [37120/54000 (69%)] Loss: -230357.656250\n",
      "Train Epoch: 95 [41216/54000 (76%)] Loss: -349281.375000\n",
      "Train Epoch: 95 [45312/54000 (84%)] Loss: -290727.781250\n",
      "Train Epoch: 95 [49408/54000 (91%)] Loss: -234096.828125\n",
      "    epoch          : 95\n",
      "    loss           : -275931.8614032452\n",
      "    val_loss       : -275644.04453125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch95.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 96 [256/54000 (0%)] Loss: -225092.500000\n",
      "Train Epoch: 96 [4352/54000 (8%)] Loss: -310573.375000\n",
      "Train Epoch: 96 [8448/54000 (16%)] Loss: -371917.593750\n",
      "Train Epoch: 96 [12544/54000 (23%)] Loss: -323458.031250\n",
      "Train Epoch: 96 [16640/54000 (31%)] Loss: -276377.812500\n",
      "Train Epoch: 96 [20736/54000 (38%)] Loss: -215485.671875\n",
      "Train Epoch: 96 [24832/54000 (46%)] Loss: -239909.109375\n",
      "Train Epoch: 96 [28928/54000 (54%)] Loss: -218791.218750\n",
      "Train Epoch: 96 [33024/54000 (61%)] Loss: -223431.328125\n",
      "Train Epoch: 96 [37120/54000 (69%)] Loss: -349591.312500\n",
      "Train Epoch: 96 [41216/54000 (76%)] Loss: -359766.687500\n",
      "Train Epoch: 96 [45312/54000 (84%)] Loss: -225669.328125\n",
      "Train Epoch: 96 [49408/54000 (91%)] Loss: -203500.609375\n",
      "    epoch          : 96\n",
      "    loss           : -273854.8990760216\n",
      "    val_loss       : -270701.39140625\n",
      "Train Epoch: 97 [256/54000 (0%)] Loss: -349518.375000\n",
      "Train Epoch: 97 [4352/54000 (8%)] Loss: -219611.140625\n",
      "Train Epoch: 97 [8448/54000 (16%)] Loss: -353536.187500\n",
      "Train Epoch: 97 [12544/54000 (23%)] Loss: -272367.218750\n",
      "Train Epoch: 97 [16640/54000 (31%)] Loss: -225922.750000\n",
      "Train Epoch: 97 [20736/54000 (38%)] Loss: -203651.578125\n",
      "Train Epoch: 97 [24832/54000 (46%)] Loss: -311269.093750\n",
      "Train Epoch: 97 [28928/54000 (54%)] Loss: -346316.156250\n",
      "Train Epoch: 97 [33024/54000 (61%)] Loss: -285480.968750\n",
      "Train Epoch: 97 [37120/54000 (69%)] Loss: -221696.375000\n",
      "Train Epoch: 97 [41216/54000 (76%)] Loss: -254256.125000\n",
      "Train Epoch: 97 [45312/54000 (84%)] Loss: -284623.062500\n",
      "Train Epoch: 97 [49408/54000 (91%)] Loss: -250872.312500\n",
      "    epoch          : 97\n",
      "    loss           : -276251.47288161056\n",
      "    val_loss       : -269006.95\n",
      "Train Epoch: 98 [256/54000 (0%)] Loss: -313427.312500\n",
      "Train Epoch: 98 [4352/54000 (8%)] Loss: -220396.046875\n",
      "Train Epoch: 98 [8448/54000 (16%)] Loss: -253412.765625\n",
      "Train Epoch: 98 [12544/54000 (23%)] Loss: -216371.343750\n",
      "Train Epoch: 98 [16640/54000 (31%)] Loss: -252919.937500\n",
      "Train Epoch: 98 [20736/54000 (38%)] Loss: -319766.562500\n",
      "Train Epoch: 98 [24832/54000 (46%)] Loss: -254932.281250\n",
      "Train Epoch: 98 [28928/54000 (54%)] Loss: -231707.671875\n",
      "Train Epoch: 98 [33024/54000 (61%)] Loss: -245502.375000\n",
      "Train Epoch: 98 [37120/54000 (69%)] Loss: -227477.359375\n",
      "Train Epoch: 98 [41216/54000 (76%)] Loss: -231047.234375\n",
      "Train Epoch: 98 [45312/54000 (84%)] Loss: -341270.625000\n",
      "Train Epoch: 98 [49408/54000 (91%)] Loss: -353312.031250\n",
      "    epoch          : 98\n",
      "    loss           : -279495.62537560094\n",
      "    val_loss       : -271761.09140625\n",
      "Train Epoch: 99 [256/54000 (0%)] Loss: -248149.328125\n",
      "Train Epoch: 99 [4352/54000 (8%)] Loss: -336716.375000\n",
      "Train Epoch: 99 [8448/54000 (16%)] Loss: -330374.187500\n",
      "Train Epoch: 99 [12544/54000 (23%)] Loss: -290613.843750\n",
      "Train Epoch: 99 [16640/54000 (31%)] Loss: -221937.484375\n",
      "Train Epoch: 99 [20736/54000 (38%)] Loss: -228870.140625\n",
      "Train Epoch: 99 [24832/54000 (46%)] Loss: -298635.937500\n",
      "Train Epoch: 99 [28928/54000 (54%)] Loss: -247789.765625\n",
      "Train Epoch: 99 [33024/54000 (61%)] Loss: -331875.125000\n",
      "Train Epoch: 99 [37120/54000 (69%)] Loss: -323617.187500\n",
      "Train Epoch: 99 [41216/54000 (76%)] Loss: -333781.843750\n",
      "Train Epoch: 99 [45312/54000 (84%)] Loss: -276976.812500\n",
      "Train Epoch: 99 [49408/54000 (91%)] Loss: -188386.921875\n",
      "    epoch          : 99\n",
      "    loss           : -277364.8233924279\n",
      "    val_loss       : -270813.88828125\n",
      "Train Epoch: 100 [256/54000 (0%)] Loss: -220968.921875\n",
      "Train Epoch: 100 [4352/54000 (8%)] Loss: -361397.875000\n",
      "Train Epoch: 100 [8448/54000 (16%)] Loss: -256048.109375\n",
      "Train Epoch: 100 [12544/54000 (23%)] Loss: -321578.000000\n",
      "Train Epoch: 100 [16640/54000 (31%)] Loss: -362713.656250\n",
      "Train Epoch: 100 [20736/54000 (38%)] Loss: -315722.500000\n",
      "Train Epoch: 100 [24832/54000 (46%)] Loss: -350655.375000\n",
      "Train Epoch: 100 [28928/54000 (54%)] Loss: -319914.656250\n",
      "Train Epoch: 100 [33024/54000 (61%)] Loss: -347204.687500\n",
      "Train Epoch: 100 [37120/54000 (69%)] Loss: -327565.250000\n",
      "Train Epoch: 100 [41216/54000 (76%)] Loss: -235285.046875\n",
      "Train Epoch: 100 [45312/54000 (84%)] Loss: -242901.437500\n",
      "Train Epoch: 100 [49408/54000 (91%)] Loss: -348272.500000\n",
      "    epoch          : 100\n",
      "    loss           : -278849.74814077525\n",
      "    val_loss       : -276459.34375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [256/54000 (0%)] Loss: -349643.656250\n",
      "Train Epoch: 101 [4352/54000 (8%)] Loss: -252202.265625\n",
      "Train Epoch: 101 [8448/54000 (16%)] Loss: -238622.906250\n",
      "Train Epoch: 101 [12544/54000 (23%)] Loss: -210626.328125\n",
      "Train Epoch: 101 [16640/54000 (31%)] Loss: -333867.031250\n",
      "Train Epoch: 101 [20736/54000 (38%)] Loss: -251121.203125\n",
      "Train Epoch: 101 [24832/54000 (46%)] Loss: -321790.093750\n",
      "Train Epoch: 101 [28928/54000 (54%)] Loss: -247559.890625\n",
      "Train Epoch: 101 [33024/54000 (61%)] Loss: -308473.687500\n",
      "Train Epoch: 101 [37120/54000 (69%)] Loss: -230100.578125\n",
      "Train Epoch: 101 [41216/54000 (76%)] Loss: -214706.765625\n",
      "Train Epoch: 101 [45312/54000 (84%)] Loss: -257968.921875\n",
      "Train Epoch: 101 [49408/54000 (91%)] Loss: -250950.953125\n",
      "    epoch          : 101\n",
      "    loss           : -275608.81939462514\n",
      "    val_loss       : -263970.11015625\n",
      "Train Epoch: 102 [256/54000 (0%)] Loss: -217250.921875\n",
      "Train Epoch: 102 [4352/54000 (8%)] Loss: -361910.250000\n",
      "Train Epoch: 102 [8448/54000 (16%)] Loss: -234046.156250\n",
      "Train Epoch: 102 [12544/54000 (23%)] Loss: -225560.296875\n",
      "Train Epoch: 102 [16640/54000 (31%)] Loss: -245147.828125\n",
      "Train Epoch: 102 [20736/54000 (38%)] Loss: -238877.265625\n",
      "Train Epoch: 102 [24832/54000 (46%)] Loss: -210450.312500\n",
      "Train Epoch: 102 [28928/54000 (54%)] Loss: -325909.062500\n",
      "Train Epoch: 102 [33024/54000 (61%)] Loss: -217997.531250\n",
      "Train Epoch: 102 [37120/54000 (69%)] Loss: -224002.765625\n",
      "Train Epoch: 102 [41216/54000 (76%)] Loss: -354761.093750\n",
      "Train Epoch: 102 [45312/54000 (84%)] Loss: -210577.984375\n",
      "Train Epoch: 102 [49408/54000 (91%)] Loss: -245933.078125\n",
      "    epoch          : 102\n",
      "    loss           : -278639.2169095553\n",
      "    val_loss       : -273372.13515625\n",
      "Train Epoch: 103 [256/54000 (0%)] Loss: -223965.328125\n",
      "Train Epoch: 103 [4352/54000 (8%)] Loss: -362540.125000\n",
      "Train Epoch: 103 [8448/54000 (16%)] Loss: -330437.875000\n",
      "Train Epoch: 103 [12544/54000 (23%)] Loss: -367690.093750\n",
      "Train Epoch: 103 [16640/54000 (31%)] Loss: -223777.843750\n",
      "Train Epoch: 103 [20736/54000 (38%)] Loss: -367749.375000\n",
      "Train Epoch: 103 [24832/54000 (46%)] Loss: -291795.062500\n",
      "Train Epoch: 103 [28928/54000 (54%)] Loss: -241833.421875\n",
      "Train Epoch: 103 [33024/54000 (61%)] Loss: -212024.687500\n",
      "Train Epoch: 103 [37120/54000 (69%)] Loss: -358974.156250\n",
      "Train Epoch: 103 [41216/54000 (76%)] Loss: -251905.796875\n",
      "Train Epoch: 103 [45312/54000 (84%)] Loss: -271705.750000\n",
      "Train Epoch: 103 [49408/54000 (91%)] Loss: -235498.718750\n",
      "    epoch          : 103\n",
      "    loss           : -278984.94704026444\n",
      "    val_loss       : -270533.70078125\n",
      "Train Epoch: 104 [256/54000 (0%)] Loss: -234623.453125\n",
      "Train Epoch: 104 [4352/54000 (8%)] Loss: -329596.156250\n",
      "Train Epoch: 104 [8448/54000 (16%)] Loss: -243633.421875\n",
      "Train Epoch: 104 [12544/54000 (23%)] Loss: -225947.625000\n",
      "Train Epoch: 104 [16640/54000 (31%)] Loss: -323572.750000\n",
      "Train Epoch: 104 [20736/54000 (38%)] Loss: -183078.250000\n",
      "Train Epoch: 104 [24832/54000 (46%)] Loss: -322538.875000\n",
      "Train Epoch: 104 [28928/54000 (54%)] Loss: -250633.234375\n",
      "Train Epoch: 104 [33024/54000 (61%)] Loss: -223257.484375\n",
      "Train Epoch: 104 [37120/54000 (69%)] Loss: -232007.109375\n",
      "Train Epoch: 104 [41216/54000 (76%)] Loss: -214284.765625\n",
      "Train Epoch: 104 [45312/54000 (84%)] Loss: -245093.453125\n",
      "Train Epoch: 104 [49408/54000 (91%)] Loss: -368513.156250\n",
      "    epoch          : 104\n",
      "    loss           : -277735.62353515625\n",
      "    val_loss       : -275046.93125\n",
      "Train Epoch: 105 [256/54000 (0%)] Loss: -227942.515625\n",
      "Train Epoch: 105 [4352/54000 (8%)] Loss: -230747.984375\n",
      "Train Epoch: 105 [8448/54000 (16%)] Loss: -224876.937500\n",
      "Train Epoch: 105 [12544/54000 (23%)] Loss: -242140.718750\n",
      "Train Epoch: 105 [16640/54000 (31%)] Loss: -202774.921875\n",
      "Train Epoch: 105 [20736/54000 (38%)] Loss: -224174.812500\n",
      "Train Epoch: 105 [24832/54000 (46%)] Loss: -224959.750000\n",
      "Train Epoch: 105 [28928/54000 (54%)] Loss: -297842.718750\n",
      "Train Epoch: 105 [33024/54000 (61%)] Loss: -255715.484375\n",
      "Train Epoch: 105 [37120/54000 (69%)] Loss: -242717.968750\n",
      "Train Epoch: 105 [41216/54000 (76%)] Loss: -223335.343750\n",
      "Train Epoch: 105 [45312/54000 (84%)] Loss: -256394.734375\n",
      "Train Epoch: 105 [49408/54000 (91%)] Loss: -218497.546875\n",
      "    epoch          : 105\n",
      "    loss           : -277198.8876201923\n",
      "    val_loss       : -267186.58828125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch105.pth ...\n",
      "Train Epoch: 106 [256/54000 (0%)] Loss: -239733.640625\n",
      "Train Epoch: 106 [4352/54000 (8%)] Loss: -229538.109375\n",
      "Train Epoch: 106 [8448/54000 (16%)] Loss: -343515.062500\n",
      "Train Epoch: 106 [12544/54000 (23%)] Loss: -215127.593750\n",
      "Train Epoch: 106 [16640/54000 (31%)] Loss: -328738.906250\n",
      "Train Epoch: 106 [20736/54000 (38%)] Loss: -312859.625000\n",
      "Train Epoch: 106 [24832/54000 (46%)] Loss: -254177.718750\n",
      "Train Epoch: 106 [28928/54000 (54%)] Loss: -211671.062500\n",
      "Train Epoch: 106 [33024/54000 (61%)] Loss: -274176.000000\n",
      "Train Epoch: 106 [37120/54000 (69%)] Loss: -361582.812500\n",
      "Train Epoch: 106 [41216/54000 (76%)] Loss: -207035.546875\n",
      "Train Epoch: 106 [45312/54000 (84%)] Loss: -327755.000000\n",
      "Train Epoch: 106 [49408/54000 (91%)] Loss: -251351.765625\n",
      "    epoch          : 106\n",
      "    loss           : -280427.66579026444\n",
      "    val_loss       : -273176.25703125\n",
      "Train Epoch: 107 [256/54000 (0%)] Loss: -213966.625000\n",
      "Train Epoch: 107 [4352/54000 (8%)] Loss: -247659.140625\n",
      "Train Epoch: 107 [8448/54000 (16%)] Loss: -227627.703125\n",
      "Train Epoch: 107 [12544/54000 (23%)] Loss: -349261.062500\n",
      "Train Epoch: 107 [16640/54000 (31%)] Loss: -340276.437500\n",
      "Train Epoch: 107 [20736/54000 (38%)] Loss: -259131.234375\n",
      "Train Epoch: 107 [24832/54000 (46%)] Loss: -339713.687500\n",
      "Train Epoch: 107 [28928/54000 (54%)] Loss: -224895.187500\n",
      "Train Epoch: 107 [33024/54000 (61%)] Loss: -337654.781250\n",
      "Train Epoch: 107 [37120/54000 (69%)] Loss: -230286.515625\n",
      "Train Epoch: 107 [41216/54000 (76%)] Loss: -256873.343750\n",
      "Train Epoch: 107 [45312/54000 (84%)] Loss: -349150.375000\n",
      "Train Epoch: 107 [49408/54000 (91%)] Loss: -349906.656250\n",
      "    epoch          : 107\n",
      "    loss           : -278345.96123798075\n",
      "    val_loss       : -271340.23125\n",
      "Train Epoch: 108 [256/54000 (0%)] Loss: -254484.890625\n",
      "Train Epoch: 108 [4352/54000 (8%)] Loss: -347537.687500\n",
      "Train Epoch: 108 [8448/54000 (16%)] Loss: -364536.062500\n",
      "Train Epoch: 108 [12544/54000 (23%)] Loss: -309277.937500\n",
      "Train Epoch: 108 [16640/54000 (31%)] Loss: -216882.140625\n",
      "Train Epoch: 108 [20736/54000 (38%)] Loss: -237527.234375\n",
      "Train Epoch: 108 [24832/54000 (46%)] Loss: -236195.187500\n",
      "Train Epoch: 108 [28928/54000 (54%)] Loss: -350428.156250\n",
      "Train Epoch: 108 [33024/54000 (61%)] Loss: -296174.281250\n",
      "Train Epoch: 108 [37120/54000 (69%)] Loss: -328742.156250\n",
      "Train Epoch: 108 [41216/54000 (76%)] Loss: -253549.828125\n",
      "Train Epoch: 108 [45312/54000 (84%)] Loss: -296498.468750\n",
      "Train Epoch: 108 [49408/54000 (91%)] Loss: -230381.703125\n",
      "    epoch          : 108\n",
      "    loss           : -281058.49060997594\n",
      "    val_loss       : -272682.75234375\n",
      "Train Epoch: 109 [256/54000 (0%)] Loss: -214399.562500\n",
      "Train Epoch: 109 [4352/54000 (8%)] Loss: -276524.343750\n",
      "Train Epoch: 109 [8448/54000 (16%)] Loss: -368597.687500\n",
      "Train Epoch: 109 [12544/54000 (23%)] Loss: -339995.250000\n",
      "Train Epoch: 109 [16640/54000 (31%)] Loss: -373196.218750\n",
      "Train Epoch: 109 [20736/54000 (38%)] Loss: -225503.437500\n",
      "Train Epoch: 109 [24832/54000 (46%)] Loss: -363242.062500\n",
      "Train Epoch: 109 [28928/54000 (54%)] Loss: -359022.218750\n",
      "Train Epoch: 109 [33024/54000 (61%)] Loss: -253644.140625\n",
      "Train Epoch: 109 [37120/54000 (69%)] Loss: -293404.468750\n",
      "Train Epoch: 109 [41216/54000 (76%)] Loss: -291248.562500\n",
      "Train Epoch: 109 [45312/54000 (84%)] Loss: -233803.140625\n",
      "Train Epoch: 109 [49408/54000 (91%)] Loss: -232827.468750\n",
      "    epoch          : 109\n",
      "    loss           : -279334.26759690506\n",
      "    val_loss       : -268547.253125\n",
      "Train Epoch: 110 [256/54000 (0%)] Loss: -224435.578125\n",
      "Train Epoch: 110 [4352/54000 (8%)] Loss: -318049.687500\n",
      "Train Epoch: 110 [8448/54000 (16%)] Loss: -239744.546875\n",
      "Train Epoch: 110 [12544/54000 (23%)] Loss: -231691.390625\n",
      "Train Epoch: 110 [16640/54000 (31%)] Loss: -317227.187500\n",
      "Train Epoch: 110 [20736/54000 (38%)] Loss: -204334.890625\n",
      "Train Epoch: 110 [24832/54000 (46%)] Loss: -232897.359375\n",
      "Train Epoch: 110 [28928/54000 (54%)] Loss: -232840.265625\n",
      "Train Epoch: 110 [33024/54000 (61%)] Loss: -363313.437500\n",
      "Train Epoch: 110 [37120/54000 (69%)] Loss: -327159.500000\n",
      "Train Epoch: 110 [41216/54000 (76%)] Loss: -230712.187500\n",
      "Train Epoch: 110 [45312/54000 (84%)] Loss: -307365.250000\n",
      "Train Epoch: 110 [49408/54000 (91%)] Loss: -341410.656250\n",
      "    epoch          : 110\n",
      "    loss           : -280922.9785907452\n",
      "    val_loss       : -268504.670703125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch110.pth ...\n",
      "Train Epoch: 111 [256/54000 (0%)] Loss: -216389.031250\n",
      "Train Epoch: 111 [4352/54000 (8%)] Loss: -253135.671875\n",
      "Train Epoch: 111 [8448/54000 (16%)] Loss: -238149.890625\n",
      "Train Epoch: 111 [12544/54000 (23%)] Loss: -236829.375000\n",
      "Train Epoch: 111 [16640/54000 (31%)] Loss: -215508.109375\n",
      "Train Epoch: 111 [20736/54000 (38%)] Loss: -252102.078125\n",
      "Train Epoch: 111 [24832/54000 (46%)] Loss: -364479.312500\n",
      "Train Epoch: 111 [28928/54000 (54%)] Loss: -218334.859375\n",
      "Train Epoch: 111 [33024/54000 (61%)] Loss: -232557.546875\n",
      "Train Epoch: 111 [37120/54000 (69%)] Loss: -209812.703125\n",
      "Train Epoch: 111 [41216/54000 (76%)] Loss: -251925.750000\n",
      "Train Epoch: 111 [45312/54000 (84%)] Loss: -326638.656250\n",
      "Train Epoch: 111 [49408/54000 (91%)] Loss: -200621.156250\n",
      "    epoch          : 111\n",
      "    loss           : -277732.56768329325\n",
      "    val_loss       : -263233.47109375\n",
      "Train Epoch: 112 [256/54000 (0%)] Loss: -227575.359375\n",
      "Train Epoch: 112 [4352/54000 (8%)] Loss: -314726.625000\n",
      "Train Epoch: 112 [8448/54000 (16%)] Loss: -230978.703125\n",
      "Train Epoch: 112 [12544/54000 (23%)] Loss: -246226.625000\n",
      "Train Epoch: 112 [16640/54000 (31%)] Loss: -325037.687500\n",
      "Train Epoch: 112 [20736/54000 (38%)] Loss: -358285.375000\n",
      "Train Epoch: 112 [24832/54000 (46%)] Loss: -170273.515625\n",
      "Train Epoch: 112 [28928/54000 (54%)] Loss: -327084.718750\n",
      "Train Epoch: 112 [33024/54000 (61%)] Loss: -222293.921875\n",
      "Train Epoch: 112 [37120/54000 (69%)] Loss: -367279.031250\n",
      "Train Epoch: 112 [41216/54000 (76%)] Loss: -352201.562500\n",
      "Train Epoch: 112 [45312/54000 (84%)] Loss: -251482.515625\n",
      "Train Epoch: 112 [49408/54000 (91%)] Loss: -329861.531250\n",
      "    epoch          : 112\n",
      "    loss           : -281595.1170372596\n",
      "    val_loss       : -276303.63671875\n",
      "Train Epoch: 113 [256/54000 (0%)] Loss: -232215.781250\n",
      "Train Epoch: 113 [4352/54000 (8%)] Loss: -360849.781250\n",
      "Train Epoch: 113 [8448/54000 (16%)] Loss: -258416.734375\n",
      "Train Epoch: 113 [12544/54000 (23%)] Loss: -295214.437500\n",
      "Train Epoch: 113 [16640/54000 (31%)] Loss: -363663.187500\n",
      "Train Epoch: 113 [20736/54000 (38%)] Loss: -281827.125000\n",
      "Train Epoch: 113 [24832/54000 (46%)] Loss: -338136.875000\n",
      "Train Epoch: 113 [28928/54000 (54%)] Loss: -241417.843750\n",
      "Train Epoch: 113 [33024/54000 (61%)] Loss: -337080.406250\n",
      "Train Epoch: 113 [37120/54000 (69%)] Loss: -218675.125000\n",
      "Train Epoch: 113 [41216/54000 (76%)] Loss: -350038.250000\n",
      "Train Epoch: 113 [45312/54000 (84%)] Loss: -356332.937500\n",
      "Train Epoch: 113 [49408/54000 (91%)] Loss: -238995.734375\n",
      "    epoch          : 113\n",
      "    loss           : -279612.2134164664\n",
      "    val_loss       : -274465.8375\n",
      "Train Epoch: 114 [256/54000 (0%)] Loss: -242332.140625\n",
      "Train Epoch: 114 [4352/54000 (8%)] Loss: -350460.656250\n",
      "Train Epoch: 114 [8448/54000 (16%)] Loss: -296608.468750\n",
      "Train Epoch: 114 [12544/54000 (23%)] Loss: -210256.750000\n",
      "Train Epoch: 114 [16640/54000 (31%)] Loss: -212486.859375\n",
      "Train Epoch: 114 [20736/54000 (38%)] Loss: -255133.406250\n",
      "Train Epoch: 114 [24832/54000 (46%)] Loss: -332519.375000\n",
      "Train Epoch: 114 [28928/54000 (54%)] Loss: -220815.140625\n",
      "Train Epoch: 114 [33024/54000 (61%)] Loss: -228735.656250\n",
      "Train Epoch: 114 [37120/54000 (69%)] Loss: -360440.093750\n",
      "Train Epoch: 114 [41216/54000 (76%)] Loss: -228335.265625\n",
      "Train Epoch: 114 [45312/54000 (84%)] Loss: -297357.625000\n",
      "Train Epoch: 114 [49408/54000 (91%)] Loss: -363987.375000\n",
      "    epoch          : 114\n",
      "    loss           : -280236.9900465745\n",
      "    val_loss       : -272103.9984375\n",
      "Train Epoch: 115 [256/54000 (0%)] Loss: -347789.812500\n",
      "Train Epoch: 115 [4352/54000 (8%)] Loss: -239942.796875\n",
      "Train Epoch: 115 [8448/54000 (16%)] Loss: -217417.968750\n",
      "Train Epoch: 115 [12544/54000 (23%)] Loss: -253403.390625\n",
      "Train Epoch: 115 [16640/54000 (31%)] Loss: -313081.625000\n",
      "Train Epoch: 115 [20736/54000 (38%)] Loss: -302854.343750\n",
      "Train Epoch: 115 [24832/54000 (46%)] Loss: -364032.156250\n",
      "Train Epoch: 115 [28928/54000 (54%)] Loss: -326648.593750\n",
      "Train Epoch: 115 [33024/54000 (61%)] Loss: -259071.046875\n",
      "Train Epoch: 115 [37120/54000 (69%)] Loss: -184528.859375\n",
      "Train Epoch: 115 [41216/54000 (76%)] Loss: -367026.312500\n",
      "Train Epoch: 115 [45312/54000 (84%)] Loss: -244190.140625\n",
      "Train Epoch: 115 [49408/54000 (91%)] Loss: -228234.703125\n",
      "    epoch          : 115\n",
      "    loss           : -280713.3416466346\n",
      "    val_loss       : -273480.06015625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch115.pth ...\n",
      "Train Epoch: 116 [256/54000 (0%)] Loss: -236600.390625\n",
      "Train Epoch: 116 [4352/54000 (8%)] Loss: -231322.562500\n",
      "Train Epoch: 116 [8448/54000 (16%)] Loss: -242678.703125\n",
      "Train Epoch: 116 [12544/54000 (23%)] Loss: -225083.265625\n",
      "Train Epoch: 116 [16640/54000 (31%)] Loss: -296325.156250\n",
      "Train Epoch: 116 [20736/54000 (38%)] Loss: -229039.921875\n",
      "Train Epoch: 116 [24832/54000 (46%)] Loss: -213987.906250\n",
      "Train Epoch: 116 [28928/54000 (54%)] Loss: -237951.218750\n",
      "Train Epoch: 116 [33024/54000 (61%)] Loss: -240250.343750\n",
      "Train Epoch: 116 [37120/54000 (69%)] Loss: -312572.250000\n",
      "Train Epoch: 116 [41216/54000 (76%)] Loss: -234653.796875\n",
      "Train Epoch: 116 [45312/54000 (84%)] Loss: -332666.750000\n",
      "Train Epoch: 116 [49408/54000 (91%)] Loss: -237711.312500\n",
      "    epoch          : 116\n",
      "    loss           : -280252.80205829325\n",
      "    val_loss       : -275074.8609375\n",
      "Train Epoch: 117 [256/54000 (0%)] Loss: -227054.062500\n",
      "Train Epoch: 117 [4352/54000 (8%)] Loss: -266270.687500\n",
      "Train Epoch: 117 [8448/54000 (16%)] Loss: -233664.609375\n",
      "Train Epoch: 117 [12544/54000 (23%)] Loss: -349468.343750\n",
      "Train Epoch: 117 [16640/54000 (31%)] Loss: -340140.281250\n",
      "Train Epoch: 117 [20736/54000 (38%)] Loss: -367932.156250\n",
      "Train Epoch: 117 [24832/54000 (46%)] Loss: -327746.750000\n",
      "Train Epoch: 117 [28928/54000 (54%)] Loss: -229930.437500\n",
      "Train Epoch: 117 [33024/54000 (61%)] Loss: -230893.906250\n",
      "Train Epoch: 117 [37120/54000 (69%)] Loss: -254318.109375\n",
      "Train Epoch: 117 [41216/54000 (76%)] Loss: -183249.359375\n",
      "Train Epoch: 117 [45312/54000 (84%)] Loss: -330118.562500\n",
      "Train Epoch: 117 [49408/54000 (91%)] Loss: -365586.000000\n",
      "    epoch          : 117\n",
      "    loss           : -279986.1476504986\n",
      "    val_loss       : -275198.16171875\n",
      "Train Epoch: 118 [256/54000 (0%)] Loss: -229832.734375\n",
      "Train Epoch: 118 [4352/54000 (8%)] Loss: -232684.984375\n",
      "Train Epoch: 118 [8448/54000 (16%)] Loss: -333873.062500\n",
      "Train Epoch: 118 [12544/54000 (23%)] Loss: -254124.406250\n",
      "Train Epoch: 118 [16640/54000 (31%)] Loss: -214035.328125\n",
      "Train Epoch: 118 [20736/54000 (38%)] Loss: -235604.750000\n",
      "Train Epoch: 118 [24832/54000 (46%)] Loss: -255134.046875\n",
      "Train Epoch: 118 [28928/54000 (54%)] Loss: -265586.750000\n",
      "Train Epoch: 118 [33024/54000 (61%)] Loss: -259891.062500\n",
      "Train Epoch: 118 [37120/54000 (69%)] Loss: -367429.062500\n",
      "Train Epoch: 118 [41216/54000 (76%)] Loss: -207964.500000\n",
      "Train Epoch: 118 [45312/54000 (84%)] Loss: -209928.734375\n",
      "Train Epoch: 118 [49408/54000 (91%)] Loss: -235988.921875\n",
      "    epoch          : 118\n",
      "    loss           : -280387.2603102464\n",
      "    val_loss       : -270312.6078125\n",
      "Train Epoch: 119 [256/54000 (0%)] Loss: -368494.906250\n",
      "Train Epoch: 119 [4352/54000 (8%)] Loss: -331166.093750\n",
      "Train Epoch: 119 [8448/54000 (16%)] Loss: -188852.296875\n",
      "Train Epoch: 119 [12544/54000 (23%)] Loss: -235313.640625\n",
      "Train Epoch: 119 [16640/54000 (31%)] Loss: -225207.750000\n",
      "Train Epoch: 119 [20736/54000 (38%)] Loss: -356294.000000\n",
      "Train Epoch: 119 [24832/54000 (46%)] Loss: -242383.343750\n",
      "Train Epoch: 119 [28928/54000 (54%)] Loss: -364730.093750\n",
      "Train Epoch: 119 [33024/54000 (61%)] Loss: -242670.515625\n",
      "Train Epoch: 119 [37120/54000 (69%)] Loss: -351126.062500\n",
      "Train Epoch: 119 [41216/54000 (76%)] Loss: -230436.671875\n",
      "Train Epoch: 119 [45312/54000 (84%)] Loss: -329796.562500\n",
      "Train Epoch: 119 [49408/54000 (91%)] Loss: -352828.031250\n",
      "    epoch          : 119\n",
      "    loss           : -281034.3186222957\n",
      "    val_loss       : -263463.37109375\n",
      "Train Epoch: 120 [256/54000 (0%)] Loss: -222815.828125\n",
      "Train Epoch: 120 [4352/54000 (8%)] Loss: -297219.562500\n",
      "Train Epoch: 120 [8448/54000 (16%)] Loss: -342278.812500\n",
      "Train Epoch: 120 [12544/54000 (23%)] Loss: -365211.593750\n",
      "Train Epoch: 120 [16640/54000 (31%)] Loss: -365583.406250\n",
      "Train Epoch: 120 [20736/54000 (38%)] Loss: -355837.843750\n",
      "Train Epoch: 120 [24832/54000 (46%)] Loss: -327287.500000\n",
      "Train Epoch: 120 [28928/54000 (54%)] Loss: -229301.703125\n",
      "Train Epoch: 120 [33024/54000 (61%)] Loss: -332648.343750\n",
      "Train Epoch: 120 [37120/54000 (69%)] Loss: -315234.593750\n",
      "Train Epoch: 120 [41216/54000 (76%)] Loss: -342537.000000\n",
      "Train Epoch: 120 [45312/54000 (84%)] Loss: -233758.890625\n",
      "Train Epoch: 120 [49408/54000 (91%)] Loss: -240178.265625\n",
      "    epoch          : 120\n",
      "    loss           : -282666.58225661056\n",
      "    val_loss       : -273345.51953125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0523_135517/checkpoint-epoch120.pth ...\n",
      "Train Epoch: 121 [256/54000 (0%)] Loss: -226160.796875\n",
      "Train Epoch: 121 [4352/54000 (8%)] Loss: -230498.953125\n",
      "Train Epoch: 121 [8448/54000 (16%)] Loss: -241220.125000\n",
      "Train Epoch: 121 [12544/54000 (23%)] Loss: -246104.015625\n",
      "Train Epoch: 121 [16640/54000 (31%)] Loss: -189251.234375\n",
      "Train Epoch: 121 [20736/54000 (38%)] Loss: -344711.218750\n",
      "Train Epoch: 121 [24832/54000 (46%)] Loss: -242122.140625\n",
      "Train Epoch: 121 [28928/54000 (54%)] Loss: -235592.750000\n",
      "Train Epoch: 121 [33024/54000 (61%)] Loss: -240147.796875\n",
      "Train Epoch: 121 [37120/54000 (69%)] Loss: -256002.843750\n",
      "Train Epoch: 121 [41216/54000 (76%)] Loss: -236737.750000\n",
      "Train Epoch: 121 [45312/54000 (84%)] Loss: -353051.156250\n",
      "Train Epoch: 121 [49408/54000 (91%)] Loss: -365927.093750\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 5.0000e-05.\n",
      "    epoch          : 121\n",
      "    loss           : -280248.4842623197\n",
      "    val_loss       : -275628.99296875\n",
      "Validation performance didn't improve for 20 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
