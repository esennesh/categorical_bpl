{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eli/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"cooldown\": 25,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 1710.798096\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: 1055.233398\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: 942.262756\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: 864.564697\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: 862.702576\n",
      "    epoch          : 1\n",
      "    loss           : 1013.6966534423829\n",
      "    val_loss       : 829.5912668250501\n",
      "    val_log_likelihood: -767.2054443359375\n",
      "    val_log_marginal: -808.9980917539448\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: 826.735352\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: 769.371704\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: 777.364624\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: 744.133667\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: 741.220520\n",
      "    epoch          : 2\n",
      "    loss           : 775.3894854736328\n",
      "    val_loss       : 738.3754828635604\n",
      "    val_log_likelihood: -703.4335144042968\n",
      "    val_log_marginal: -725.3245059620589\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: 728.419922\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: 738.570557\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: 703.386108\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: 705.125549\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: 685.441162\n",
      "    epoch          : 3\n",
      "    loss           : 708.5076641845703\n",
      "    val_loss       : 677.993728131242\n",
      "    val_log_likelihood: -660.5467041015625\n",
      "    val_log_marginal: -674.7099423896776\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: 674.640198\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: 665.619812\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: 671.305542\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: 663.513550\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: 644.750244\n",
      "    epoch          : 4\n",
      "    loss           : 662.8337060546875\n",
      "    val_loss       : 640.4555280960165\n",
      "    val_log_likelihood: -620.2486511230469\n",
      "    val_log_marginal: -636.3882956002565\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: 638.894592\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: 621.102539\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: 619.524292\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: 609.026367\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: 604.021240\n",
      "    epoch          : 5\n",
      "    loss           : 625.7763348388672\n",
      "    val_loss       : 608.5247149580158\n",
      "    val_log_likelihood: -589.257373046875\n",
      "    val_log_marginal: -607.1131974000484\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: 612.830139\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: 607.288208\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: 589.348877\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: 601.427124\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: 586.831665\n",
      "    epoch          : 6\n",
      "    loss           : 590.4214489746093\n",
      "    val_loss       : 569.3289204839617\n",
      "    val_log_likelihood: -551.6324035644532\n",
      "    val_log_marginal: -567.4579010106036\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: 559.358398\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: 560.703613\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: 560.001587\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: 530.845215\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: 554.490479\n",
      "    epoch          : 7\n",
      "    loss           : 551.8097357177735\n",
      "    val_loss       : 531.6741528782062\n",
      "    val_log_likelihood: -516.373159790039\n",
      "    val_log_marginal: -529.4784900568411\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: 538.444458\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: 538.756592\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: 513.763123\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: 520.019409\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: 497.180969\n",
      "    epoch          : 8\n",
      "    loss           : 523.7816915893554\n",
      "    val_loss       : 518.1753793503158\n",
      "    val_log_likelihood: -489.51490173339846\n",
      "    val_log_marginal: -514.1911027427763\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: 513.369751\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: 525.014038\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: 480.800293\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: 474.702454\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: 469.113800\n",
      "    epoch          : 9\n",
      "    loss           : 494.29827270507815\n",
      "    val_loss       : 475.87117206072435\n",
      "    val_log_likelihood: -450.8172912597656\n",
      "    val_log_marginal: -471.4897489952531\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: 481.564026\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: 454.467773\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: 452.598480\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: 456.351624\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: 459.931580\n",
      "    epoch          : 10\n",
      "    loss           : 467.3061254882812\n",
      "    val_loss       : 454.19749271804466\n",
      "    val_log_likelihood: -418.6364776611328\n",
      "    val_log_marginal: -449.4636652796771\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: 485.180115\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: 495.493591\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: 431.162872\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: 529.516479\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: 437.800354\n",
      "    epoch          : 11\n",
      "    loss           : 447.04051849365237\n",
      "    val_loss       : 423.85950946239757\n",
      "    val_log_likelihood: -395.8072479248047\n",
      "    val_log_marginal: -420.235342149809\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: 431.346802\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: 401.887695\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: 397.365997\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: 382.767761\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: 377.605164\n",
      "    epoch          : 12\n",
      "    loss           : 403.9117626953125\n",
      "    val_loss       : 391.1987750913948\n",
      "    val_log_likelihood: -353.2007537841797\n",
      "    val_log_marginal: -388.5015512361649\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: 386.119812\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: 434.691132\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: 382.282959\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: 366.416809\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: 327.544922\n",
      "    epoch          : 13\n",
      "    loss           : 367.57178283691405\n",
      "    val_loss       : 384.06783117260784\n",
      "    val_log_likelihood: -320.7166046142578\n",
      "    val_log_marginal: -374.95768722221254\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: 428.310150\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: 317.795074\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: 347.274902\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: 383.790375\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: 325.243317\n",
      "    epoch          : 14\n",
      "    loss           : 351.65294860839845\n",
      "    val_loss       : 319.2407349598594\n",
      "    val_log_likelihood: -296.03462982177734\n",
      "    val_log_marginal: -317.092277815938\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: 319.072876\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: 317.794678\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: 271.123352\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: 304.534058\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: 318.839600\n",
      "    epoch          : 15\n",
      "    loss           : 323.61328582763673\n",
      "    val_loss       : 317.1161181536503\n",
      "    val_log_likelihood: -273.3106094360352\n",
      "    val_log_marginal: -311.4409411728382\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: 302.924164\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: 292.951721\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: 297.720123\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: 277.670898\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: 278.493500\n",
      "    epoch          : 16\n",
      "    loss           : 303.19537979125977\n",
      "    val_loss       : 280.8485972118564\n",
      "    val_log_likelihood: -245.0719223022461\n",
      "    val_log_marginal: -276.9268047459424\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: 234.170166\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: 268.930542\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: 300.521179\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: 298.421356\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: 255.393875\n",
      "    epoch          : 17\n",
      "    loss           : 280.298113861084\n",
      "    val_loss       : 264.5459014844149\n",
      "    val_log_likelihood: -226.81736602783204\n",
      "    val_log_marginal: -258.09416881613424\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: 258.332092\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: 327.846741\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: 280.406006\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: 233.481430\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: 231.539307\n",
      "    epoch          : 18\n",
      "    loss           : 260.2912501525879\n",
      "    val_loss       : 258.43421829501165\n",
      "    val_log_likelihood: -193.9789260864258\n",
      "    val_log_marginal: -248.72862407267093\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: 288.961426\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: 187.733826\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: 355.346802\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: 230.563538\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: 285.204407\n",
      "    epoch          : 19\n",
      "    loss           : 255.44406646728515\n",
      "    val_loss       : 237.37306590396912\n",
      "    val_log_likelihood: -171.19847259521484\n",
      "    val_log_marginal: -232.74811064973474\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: 216.136932\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: 180.174194\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: 223.970001\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: 250.892700\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: 283.252319\n",
      "    epoch          : 20\n",
      "    loss           : 229.71818115234376\n",
      "    val_loss       : 209.7185275436379\n",
      "    val_log_likelihood: -143.85058364868163\n",
      "    val_log_marginal: -195.59974010661244\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch20.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: 193.972153\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: 211.527832\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: 179.852661\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: 166.683777\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: 205.419037\n",
      "    epoch          : 21\n",
      "    loss           : 208.18121215820312\n",
      "    val_loss       : 189.85775161599742\n",
      "    val_log_likelihood: -135.56180877685546\n",
      "    val_log_marginal: -180.84030885770918\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: 139.326096\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: 250.270096\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: 213.169098\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: 171.502243\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: 149.067307\n",
      "    epoch          : 22\n",
      "    loss           : 198.65710067749023\n",
      "    val_loss       : 168.97774654375388\n",
      "    val_log_likelihood: -108.82504119873047\n",
      "    val_log_marginal: -167.53401983343065\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: 215.816864\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: 154.165588\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: 238.322327\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: 260.775604\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: 180.921112\n",
      "    epoch          : 23\n",
      "    loss           : 194.09419189453126\n",
      "    val_loss       : 142.98403587061912\n",
      "    val_log_likelihood: -92.78223876953125\n",
      "    val_log_marginal: -134.7374953109771\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: 132.286453\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: 128.768646\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: 137.616745\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: 220.361618\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: 172.304810\n",
      "    epoch          : 24\n",
      "    loss           : 141.5380258178711\n",
      "    val_loss       : 141.03788972906767\n",
      "    val_log_likelihood: -74.85512886047363\n",
      "    val_log_marginal: -130.38288629086736\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: 131.823273\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: 105.658371\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: 129.478470\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: 166.425369\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: 180.066025\n",
      "    epoch          : 25\n",
      "    loss           : 144.90485488891602\n",
      "    val_loss       : 112.27996805533766\n",
      "    val_log_likelihood: -63.07348442077637\n",
      "    val_log_marginal: -105.81323978751898\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: 158.033478\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: 64.425957\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: 218.183273\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: 114.370430\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: 47.490791\n",
      "    epoch          : 26\n",
      "    loss           : 115.79223678588868\n",
      "    val_loss       : 125.51255981661379\n",
      "    val_log_likelihood: -46.530702114105225\n",
      "    val_log_marginal: -117.71553077772259\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: 103.664497\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: 170.163940\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: 138.430969\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: 129.118912\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: 94.159813\n",
      "    epoch          : 27\n",
      "    loss           : 164.01068225860595\n",
      "    val_loss       : 95.11002267338336\n",
      "    val_log_likelihood: -41.73010891675949\n",
      "    val_log_marginal: -88.77447300031766\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: 71.340942\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: 29.831774\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: 72.887314\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: 52.929462\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: 77.805695\n",
      "    epoch          : 28\n",
      "    loss           : 94.85397274017333\n",
      "    val_loss       : 79.07075012736023\n",
      "    val_log_likelihood: -19.91042137145996\n",
      "    val_log_marginal: -74.94267606288194\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: 58.265755\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: 102.355148\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: 195.608429\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: 142.568756\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: 132.487793\n",
      "    epoch          : 29\n",
      "    loss           : 106.37939659118652\n",
      "    val_loss       : 93.21007371554151\n",
      "    val_log_likelihood: -14.135541546344758\n",
      "    val_log_marginal: -87.02697126247308\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: 164.145386\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: 40.852745\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: 37.952240\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: 110.655556\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: 129.891113\n",
      "    epoch          : 30\n",
      "    loss           : 91.8208162021637\n",
      "    val_loss       : 81.20476158559323\n",
      "    val_log_likelihood: 1.1332736253738402\n",
      "    val_log_marginal: -70.40543121394062\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: 64.390991\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: 85.126289\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: 87.300392\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: 136.220322\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: 112.837952\n",
      "    epoch          : 31\n",
      "    loss           : 83.33881676197052\n",
      "    val_loss       : 63.563864738866684\n",
      "    val_log_likelihood: 10.036305618286132\n",
      "    val_log_marginal: -53.19115925356747\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: 181.096436\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: 171.114944\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: 74.259720\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: 64.884613\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: 114.225090\n",
      "    epoch          : 32\n",
      "    loss           : 79.120563788414\n",
      "    val_loss       : 58.320758180506516\n",
      "    val_log_likelihood: 7.381908094882965\n",
      "    val_log_marginal: -49.6148939806968\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: 48.053436\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: 99.843552\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: 85.834610\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: 48.031239\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: 14.525386\n",
      "    epoch          : 33\n",
      "    loss           : 63.447563710212705\n",
      "    val_loss       : 37.73020923314616\n",
      "    val_log_likelihood: 33.77597005367279\n",
      "    val_log_marginal: -31.86636030077935\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -30.870253\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -0.074277\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: 104.595222\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: 578.769714\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: 219.040833\n",
      "    epoch          : 34\n",
      "    loss           : 122.4124619436264\n",
      "    val_loss       : 108.83902506735176\n",
      "    val_log_likelihood: -24.594145488739013\n",
      "    val_log_marginal: -99.5205507997322\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: 254.517014\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: 67.336548\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: 50.625496\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: 82.414200\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: 51.687298\n",
      "    epoch          : 35\n",
      "    loss           : 66.04879723548889\n",
      "    val_loss       : 63.64331483794376\n",
      "    val_log_likelihood: 32.920625674724576\n",
      "    val_log_marginal: -57.53902262250782\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: 185.445023\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: 42.277599\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -35.789085\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: 79.465698\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: 121.921814\n",
      "    epoch          : 36\n",
      "    loss           : 73.32576870918274\n",
      "    val_loss       : 68.92422452224419\n",
      "    val_log_likelihood: 28.462532806396485\n",
      "    val_log_marginal: -57.96045362055302\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: 25.729286\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: 27.670116\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -40.525913\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: 55.964653\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: 68.824615\n",
      "    epoch          : 37\n",
      "    loss           : 35.89569962978363\n",
      "    val_loss       : 21.455366378463804\n",
      "    val_log_likelihood: 58.59071340560913\n",
      "    val_log_marginal: -12.188834215700638\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: 5.488905\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: 38.518654\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -53.063393\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: 69.179794\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: 36.195866\n",
      "    epoch          : 38\n",
      "    loss           : 31.46242112159729\n",
      "    val_loss       : 32.82150715561583\n",
      "    val_log_likelihood: 53.55428256988525\n",
      "    val_log_marginal: -27.837883209809668\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -29.272120\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: 1.860054\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -5.415513\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: 150.246902\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -1.630488\n",
      "    epoch          : 39\n",
      "    loss           : 18.953406591415405\n",
      "    val_loss       : 16.092006141413002\n",
      "    val_log_likelihood: 65.59422892332077\n",
      "    val_log_marginal: -8.24024176709354\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -42.177002\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -38.344627\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: 58.871540\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: 146.774521\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -2.746994\n",
      "    epoch          : 40\n",
      "    loss           : 14.470137102603912\n",
      "    val_loss       : 8.124128693621614\n",
      "    val_log_likelihood: 76.5798789024353\n",
      "    val_log_marginal: 4.239642908796663\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch40.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: 98.476868\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -5.074285\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: 130.047760\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: 35.430328\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: 99.025482\n",
      "    epoch          : 41\n",
      "    loss           : 50.95042509794235\n",
      "    val_loss       : 56.697797374334186\n",
      "    val_log_likelihood: 52.780025482177734\n",
      "    val_log_marginal: -41.65073328092694\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: 0.462524\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: 10.542565\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -30.337368\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -30.749346\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -8.782318\n",
      "    epoch          : 42\n",
      "    loss           : -2.713318130970001\n",
      "    val_loss       : -11.901862302981327\n",
      "    val_log_likelihood: 90.35441608428955\n",
      "    val_log_marginal: 18.424506055936213\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: 27.796387\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: 36.111847\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: 182.682007\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: 108.527313\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: 210.106964\n",
      "    epoch          : 43\n",
      "    loss           : 61.168284945487976\n",
      "    val_loss       : 55.199901662487534\n",
      "    val_log_likelihood: 48.198801565170285\n",
      "    val_log_marginal: -37.13415075540542\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: 10.341257\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: 0.052468\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -86.090958\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -97.104561\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: 45.988533\n",
      "    epoch          : 44\n",
      "    loss           : 10.915604228973388\n",
      "    val_loss       : -5.4722584029659584\n",
      "    val_log_likelihood: 93.68695325851441\n",
      "    val_log_marginal: 13.535851674526942\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -78.589363\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -20.161713\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -55.346256\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -50.930176\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: 35.556549\n",
      "    epoch          : 45\n",
      "    loss           : -14.55370111465454\n",
      "    val_loss       : -27.642222126852726\n",
      "    val_log_likelihood: 110.93384132385253\n",
      "    val_log_marginal: 38.99639316350222\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: 101.611221\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -57.418968\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -84.580841\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: 33.759506\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: 12.013241\n",
      "    epoch          : 46\n",
      "    loss           : -6.068709182739258\n",
      "    val_loss       : -31.26257135821506\n",
      "    val_log_likelihood: 112.4182481765747\n",
      "    val_log_marginal: 36.69802585989238\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: 6.525694\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -62.076180\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -39.465401\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: 39.746765\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -6.767569\n",
      "    epoch          : 47\n",
      "    loss           : 1.9525780439376832\n",
      "    val_loss       : 0.5849146119318874\n",
      "    val_log_likelihood: 107.44830169677735\n",
      "    val_log_marginal: 22.67844750421054\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: 92.474823\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: 81.719711\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -80.902809\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -8.293438\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -74.619461\n",
      "    epoch          : 48\n",
      "    loss           : -33.427463645935056\n",
      "    val_loss       : -44.48860791008919\n",
      "    val_log_likelihood: 132.25434494018555\n",
      "    val_log_marginal: 51.186070679873225\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -64.533638\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -82.107216\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: 21.601917\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -58.816345\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: 27.264656\n",
      "    epoch          : 49\n",
      "    loss           : -14.901781067848205\n",
      "    val_loss       : -25.732994044199586\n",
      "    val_log_likelihood: 127.77039022445679\n",
      "    val_log_marginal: 42.99492525570095\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -93.901871\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: 78.329086\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -13.542475\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -17.461037\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -33.677071\n",
      "    epoch          : 50\n",
      "    loss           : -37.22925888538361\n",
      "    val_loss       : 37.13113985918463\n",
      "    val_log_likelihood: 112.36273307800293\n",
      "    val_log_marginal: -24.67715010270478\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: 13.960224\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -38.059410\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: 21.605244\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -1.818621\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: 39.456066\n",
      "    epoch          : 51\n",
      "    loss           : 16.360529861450196\n",
      "    val_loss       : -34.71175596928224\n",
      "    val_log_likelihood: 124.71883773803711\n",
      "    val_log_marginal: 42.027119254435725\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -46.682518\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: 54.433266\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -73.718018\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -63.141529\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: 23.087387\n",
      "    epoch          : 52\n",
      "    loss           : -47.687222924232486\n",
      "    val_loss       : -42.09140952164307\n",
      "    val_log_likelihood: 144.32633514404296\n",
      "    val_log_marginal: 49.29120664818067\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -67.545647\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -65.402328\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -79.624840\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: 38.994320\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: 33.296356\n",
      "    epoch          : 53\n",
      "    loss           : -59.52932291984558\n",
      "    val_loss       : -63.03885819362476\n",
      "    val_log_likelihood: 145.36430397033692\n",
      "    val_log_marginal: 71.16582973524928\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -63.849728\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -70.427979\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: 63.010948\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -40.804234\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: 52.613907\n",
      "    epoch          : 54\n",
      "    loss           : -5.918771619796753\n",
      "    val_loss       : -26.444181801751274\n",
      "    val_log_likelihood: 138.9089740753174\n",
      "    val_log_marginal: 45.02857730835676\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -113.042664\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -140.751587\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -89.595383\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: 0.573925\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -83.323509\n",
      "    epoch          : 55\n",
      "    loss           : -61.662896542549134\n",
      "    val_loss       : -87.17029032977298\n",
      "    val_log_likelihood: 164.2268943786621\n",
      "    val_log_marginal: 95.89805956601016\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -110.754089\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -56.375656\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -156.166428\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -106.095345\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -9.801826\n",
      "    epoch          : 56\n",
      "    loss           : -72.54078796625137\n",
      "    val_loss       : -53.40001217853279\n",
      "    val_log_likelihood: 159.42342987060547\n",
      "    val_log_marginal: 62.16111809536818\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -58.009285\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -100.631653\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -66.112167\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: 12.303031\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -64.597549\n",
      "    epoch          : 57\n",
      "    loss           : -75.49664317131042\n",
      "    val_loss       : -65.23383376458659\n",
      "    val_log_likelihood: 160.3972427368164\n",
      "    val_log_marginal: 76.93362659476696\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -39.504704\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -128.589798\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -89.668732\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -90.384125\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: 26.706980\n",
      "    epoch          : 58\n",
      "    loss           : -3.3625817489624024\n",
      "    val_loss       : -5.718302963953457\n",
      "    val_log_likelihood: 137.657218170166\n",
      "    val_log_marginal: 26.214519341290007\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -19.632944\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: 125.186821\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -71.150223\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -128.826141\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -78.556946\n",
      "    epoch          : 59\n",
      "    loss           : -30.031551313400268\n",
      "    val_loss       : -67.19288710122927\n",
      "    val_log_likelihood: 158.41466636657714\n",
      "    val_log_marginal: 74.18075718546069\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -46.784519\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -66.626266\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -118.160042\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -55.762321\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -75.482468\n",
      "    epoch          : 60\n",
      "    loss           : -67.99128448963165\n",
      "    val_loss       : -78.0775045341812\n",
      "    val_log_likelihood: 168.87736587524415\n",
      "    val_log_marginal: 91.14627128429711\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch60.pth ...\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -97.514191\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -83.279953\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -41.936264\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -178.341492\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -109.757477\n",
      "    epoch          : 61\n",
      "    loss           : -100.78454652786255\n",
      "    val_loss       : -86.9537956529297\n",
      "    val_log_likelihood: 172.81084442138672\n",
      "    val_log_marginal: 94.92821909587826\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -34.918388\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: 27.358936\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -75.256714\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -86.002884\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -90.313034\n",
      "    epoch          : 62\n",
      "    loss           : -91.83977359771728\n",
      "    val_loss       : -88.50770960226654\n",
      "    val_log_likelihood: 180.6715850830078\n",
      "    val_log_marginal: 102.05173555538059\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -131.922424\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -119.952339\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -135.172089\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: 12.254293\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: 30.606159\n",
      "    epoch          : 63\n",
      "    loss           : -77.84093908548356\n",
      "    val_loss       : -5.430105197522785\n",
      "    val_log_likelihood: 159.51090240478516\n",
      "    val_log_marginal: 11.67497454546392\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -132.565674\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: 26.440077\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -57.233017\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -114.715286\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -151.500290\n",
      "    epoch          : 64\n",
      "    loss           : -38.0896098947525\n",
      "    val_loss       : -105.46596313957124\n",
      "    val_log_likelihood: 180.89061584472657\n",
      "    val_log_marginal: 114.74933082088828\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -90.622940\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -8.382449\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -2.278130\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -107.363846\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -107.255707\n",
      "    epoch          : 65\n",
      "    loss           : -102.4900680398941\n",
      "    val_loss       : -81.21596981156617\n",
      "    val_log_likelihood: 193.98472290039064\n",
      "    val_log_marginal: 94.0850280534476\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -154.629379\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -117.723679\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -74.164703\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -102.339256\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -43.853916\n",
      "    epoch          : 66\n",
      "    loss           : -87.90346403598785\n",
      "    val_loss       : -108.68317021885886\n",
      "    val_log_likelihood: 196.66002807617187\n",
      "    val_log_marginal: 119.2688049014658\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -168.427933\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -91.222076\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -66.881241\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -98.069305\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -184.035400\n",
      "    epoch          : 67\n",
      "    loss           : -79.29571778774262\n",
      "    val_loss       : -96.15122112389655\n",
      "    val_log_likelihood: 190.05479202270507\n",
      "    val_log_marginal: 107.58394200466573\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -108.255211\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -119.858902\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -126.614700\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -44.216541\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: 0.636071\n",
      "    epoch          : 68\n",
      "    loss           : -85.43475066661834\n",
      "    val_loss       : -102.03547411942854\n",
      "    val_log_likelihood: 194.11577301025392\n",
      "    val_log_marginal: 111.99267443493008\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: 24.611650\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -141.990173\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -112.554367\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -125.258369\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: 10.578561\n",
      "    epoch          : 69\n",
      "    loss           : -95.62816148757935\n",
      "    val_loss       : -110.64324046606198\n",
      "    val_log_likelihood: 199.61025390625\n",
      "    val_log_marginal: 122.67635233183947\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -142.170898\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -48.710979\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -158.030807\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -147.961731\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -52.567459\n",
      "    epoch          : 70\n",
      "    loss           : -124.6237127161026\n",
      "    val_loss       : -122.76616499759257\n",
      "    val_log_likelihood: 213.08113327026368\n",
      "    val_log_marginal: 131.30575546734036\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -66.235153\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -186.513184\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -134.387039\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -123.875679\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -58.294956\n",
      "    epoch          : 71\n",
      "    loss           : -98.29647338867187\n",
      "    val_loss       : -106.3580784291029\n",
      "    val_log_likelihood: 206.9736068725586\n",
      "    val_log_marginal: 112.81592333428561\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -134.471893\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -142.167328\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -134.917877\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -127.175812\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -93.258965\n",
      "    epoch          : 72\n",
      "    loss           : -95.38537015676498\n",
      "    val_loss       : -80.03330599013715\n",
      "    val_log_likelihood: 199.81544723510743\n",
      "    val_log_marginal: 90.76403636190079\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -14.895402\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -143.000259\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -203.977768\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -202.565475\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: 49.201458\n",
      "    epoch          : 73\n",
      "    loss           : -111.54983020305633\n",
      "    val_loss       : 22.016124506946653\n",
      "    val_log_likelihood: 205.94062042236328\n",
      "    val_log_marginal: -17.309920224174864\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -119.249527\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: 136.386047\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: 134.947861\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -98.216995\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -83.187370\n",
      "    epoch          : 74\n",
      "    loss           : 2.4020902729034423\n",
      "    val_loss       : -81.2219462393783\n",
      "    val_log_likelihood: 192.23041763305665\n",
      "    val_log_marginal: 101.15725342407822\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -123.973145\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -143.109741\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -135.257401\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -56.840836\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -145.641144\n",
      "    epoch          : 75\n",
      "    loss           : -103.3176497554779\n",
      "    val_loss       : -127.23803322482854\n",
      "    val_log_likelihood: 206.83145294189453\n",
      "    val_log_marginal: 137.42170297785432\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -150.196426\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -165.545303\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -158.963913\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -59.697517\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -145.200653\n",
      "    epoch          : 76\n",
      "    loss           : -147.3642342185974\n",
      "    val_loss       : -150.81703902138398\n",
      "    val_log_likelihood: 226.83403472900392\n",
      "    val_log_marginal: 160.08345649242398\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -189.750488\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -223.145630\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -39.474133\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -179.918335\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -162.652893\n",
      "    epoch          : 77\n",
      "    loss           : -151.92107746124267\n",
      "    val_loss       : -153.62654713802038\n",
      "    val_log_likelihood: 234.6285659790039\n",
      "    val_log_marginal: 163.82462989762425\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -150.018784\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -160.995728\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -151.450165\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: 40.104958\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: 60.364616\n",
      "    epoch          : 78\n",
      "    loss           : -56.5775237941742\n",
      "    val_loss       : -63.803840126469744\n",
      "    val_log_likelihood: 188.16951904296874\n",
      "    val_log_marginal: 79.82898423038695\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -75.173912\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -131.245148\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: 37.116203\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -176.181366\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -115.244293\n",
      "    epoch          : 79\n",
      "    loss           : -79.71760726451873\n",
      "    val_loss       : -102.96458366569132\n",
      "    val_log_likelihood: 202.87049102783203\n",
      "    val_log_marginal: 108.26680606491864\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -186.035110\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -94.470123\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -159.539062\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -124.838440\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -107.990936\n",
      "    epoch          : 80\n",
      "    loss           : -82.49189800262451\n",
      "    val_loss       : -132.8944572689943\n",
      "    val_log_likelihood: 216.7669532775879\n",
      "    val_log_marginal: 144.24944945387543\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -163.464676\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -212.890915\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -168.373932\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -129.583740\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -157.532516\n",
      "    epoch          : 81\n",
      "    loss           : -152.7408466720581\n",
      "    val_loss       : -157.8750501406379\n",
      "    val_log_likelihood: 235.28548736572264\n",
      "    val_log_marginal: 165.91596203111112\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -59.835514\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -221.208466\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -69.337997\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -157.268890\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -180.689087\n",
      "    epoch          : 82\n",
      "    loss           : -156.4224082183838\n",
      "    val_loss       : -158.48564380044118\n",
      "    val_log_likelihood: 241.4700134277344\n",
      "    val_log_marginal: 164.40265608389248\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -200.934692\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -164.358521\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -198.190918\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -170.718094\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -117.197289\n",
      "    epoch          : 83\n",
      "    loss           : -154.98368554115297\n",
      "    val_loss       : -143.22627260992303\n",
      "    val_log_likelihood: 239.91990127563477\n",
      "    val_log_marginal: 152.86862279996276\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -199.979446\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -137.731552\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -184.029358\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -192.005371\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -148.844452\n",
      "    epoch          : 84\n",
      "    loss           : -148.5210216999054\n",
      "    val_loss       : -163.02313564699142\n",
      "    val_log_likelihood: 244.97098541259766\n",
      "    val_log_marginal: 171.6943667266518\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -61.466507\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -237.013916\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -169.377213\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: 25.018845\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -23.590919\n",
      "    epoch          : 85\n",
      "    loss           : -71.59162596702576\n",
      "    val_loss       : -7.111621925514188\n",
      "    val_log_likelihood: 193.66891059875488\n",
      "    val_log_marginal: 45.24981542304159\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -141.108337\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -226.029816\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -11.324772\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -65.835564\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -120.274643\n",
      "    epoch          : 86\n",
      "    loss           : -108.27280935764313\n",
      "    val_loss       : -134.58656782684847\n",
      "    val_log_likelihood: 228.39298553466796\n",
      "    val_log_marginal: 146.38661774471402\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -96.338959\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -60.063171\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -126.892540\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: 16.459249\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -18.291204\n",
      "    epoch          : 87\n",
      "    loss           : -99.69628522872925\n",
      "    val_loss       : -141.58679748773574\n",
      "    val_log_likelihood: 232.62685012817383\n",
      "    val_log_marginal: 152.14224928767243\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -31.317799\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -146.492981\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -196.989563\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -75.542686\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -55.250816\n",
      "    epoch          : 88\n",
      "    loss           : -120.56404607772828\n",
      "    val_loss       : -147.04997221147642\n",
      "    val_log_likelihood: 240.32998809814453\n",
      "    val_log_marginal: 160.32530752457677\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -216.029434\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -109.426720\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -124.792923\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -155.018738\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -156.036575\n",
      "    epoch          : 89\n",
      "    loss           : -92.59244709014892\n",
      "    val_loss       : -124.0471485757269\n",
      "    val_log_likelihood: 229.57804794311522\n",
      "    val_log_marginal: 133.7165956590325\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -137.390762\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -10.111252\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -185.554382\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -132.485458\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -193.919769\n",
      "    epoch          : 90\n",
      "    loss           : -147.66728401184082\n",
      "    val_loss       : -167.23583899307997\n",
      "    val_log_likelihood: 249.30648345947264\n",
      "    val_log_marginal: 177.44907467067577\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -156.981842\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -198.853439\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -204.327805\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -155.990891\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -186.438156\n",
      "    epoch          : 91\n",
      "    loss           : -181.6059090423584\n",
      "    val_loss       : -166.2016949291341\n",
      "    val_log_likelihood: 257.27566986083986\n",
      "    val_log_marginal: 172.67885973340202\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -207.609100\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: 43.286545\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -228.272980\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -148.868973\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -175.513733\n",
      "    epoch          : 92\n",
      "    loss           : -149.4413523054123\n",
      "    val_loss       : -155.54738405877725\n",
      "    val_log_likelihood: 250.86937713623047\n",
      "    val_log_marginal: 163.68723356351256\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -197.623962\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -153.983047\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -197.956177\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -181.951981\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -190.637817\n",
      "    epoch          : 93\n",
      "    loss           : -159.25525384902954\n",
      "    val_loss       : -168.85330231226982\n",
      "    val_log_likelihood: 256.9271270751953\n",
      "    val_log_marginal: 176.33562255352734\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -188.788467\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -179.665344\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -183.514481\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -125.807869\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -64.671387\n",
      "    epoch          : 94\n",
      "    loss           : -152.26474540233613\n",
      "    val_loss       : -139.1675115255639\n",
      "    val_log_likelihood: 249.6787551879883\n",
      "    val_log_marginal: 150.71114794537425\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -233.239410\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -221.215866\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -96.613068\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -187.107819\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -203.014008\n",
      "    epoch          : 95\n",
      "    loss           : -154.31215900421142\n",
      "    val_loss       : -101.64816205035895\n",
      "    val_log_likelihood: 256.0655258178711\n",
      "    val_log_marginal: 113.80484667867422\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -110.812729\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -129.592529\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: 28.354612\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -169.362991\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -38.742039\n",
      "    epoch          : 96\n",
      "    loss           : -121.17915652751923\n",
      "    val_loss       : -157.9722747804597\n",
      "    val_log_likelihood: 257.59637756347655\n",
      "    val_log_marginal: 167.58824807674938\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -184.342621\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -171.302872\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -68.523834\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -186.435913\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -182.297501\n",
      "    epoch          : 97\n",
      "    loss           : -168.67022945404054\n",
      "    val_loss       : -181.63893141672014\n",
      "    val_log_likelihood: 269.4336227416992\n",
      "    val_log_marginal: 190.79004562608898\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -279.579803\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -226.498932\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -127.936073\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -159.734253\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -172.505402\n",
      "    epoch          : 98\n",
      "    loss           : -155.41085993766785\n",
      "    val_loss       : -142.73223978877067\n",
      "    val_log_likelihood: 253.65286712646486\n",
      "    val_log_marginal: 154.17408516034484\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -222.402924\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: 0.042395\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: 214.294037\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: 63.793861\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -168.339066\n",
      "    epoch          : 99\n",
      "    loss           : -33.78226883411408\n",
      "    val_loss       : -94.425518538896\n",
      "    val_log_likelihood: 231.73320236206055\n",
      "    val_log_marginal: 105.9540197096765\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -65.399300\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -98.687012\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -52.925270\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -187.587738\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -246.251770\n",
      "    epoch          : 100\n",
      "    loss           : -141.46631474971772\n",
      "    val_loss       : -176.6227996136062\n",
      "    val_log_likelihood: 263.8915084838867\n",
      "    val_log_marginal: 186.96956840008497\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -196.919678\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -203.556366\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -42.937576\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -170.376587\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -180.995880\n",
      "    epoch          : 101\n",
      "    loss           : -192.87463760375977\n",
      "    val_loss       : -190.42174632409586\n",
      "    val_log_likelihood: 277.1979949951172\n",
      "    val_log_marginal: 200.7869212389982\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -93.761963\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -210.564667\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -168.905106\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -166.978333\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -175.159119\n",
      "    epoch          : 102\n",
      "    loss           : -187.62912822723388\n",
      "    val_loss       : -176.14905471187086\n",
      "    val_log_likelihood: 277.2189010620117\n",
      "    val_log_marginal: 187.14812478303685\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -81.780685\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -54.483543\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -239.540344\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -139.991745\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -132.624954\n",
      "    epoch          : 103\n",
      "    loss           : -151.57129325389863\n",
      "    val_loss       : -110.86269664540887\n",
      "    val_log_likelihood: 261.9634475708008\n",
      "    val_log_marginal: 120.6355771407485\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -107.742599\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -99.513596\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -162.388351\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -238.307724\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -95.153999\n",
      "    epoch          : 104\n",
      "    loss           : -143.4240075969696\n",
      "    val_loss       : -183.1031764724292\n",
      "    val_log_likelihood: 275.5357864379883\n",
      "    val_log_marginal: 192.4453081548214\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -147.856613\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -259.711670\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -270.584808\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -208.789963\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -222.469818\n",
      "    epoch          : 105\n",
      "    loss           : -191.5005054473877\n",
      "    val_loss       : -188.12315435558557\n",
      "    val_log_likelihood: 285.345068359375\n",
      "    val_log_marginal: 199.3636776432395\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -278.134613\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -265.606995\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -222.670166\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -170.393005\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -110.145966\n",
      "    epoch          : 106\n",
      "    loss           : -172.26268620967866\n",
      "    val_loss       : -98.76627418650314\n",
      "    val_log_likelihood: 261.2804931640625\n",
      "    val_log_marginal: 116.47514760121703\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -221.327118\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -162.850250\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -136.805939\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -188.699066\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -180.169510\n",
      "    epoch          : 107\n",
      "    loss           : -155.71098170280456\n",
      "    val_loss       : -181.5024792522192\n",
      "    val_log_likelihood: 274.1347183227539\n",
      "    val_log_marginal: 188.9782659963127\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -219.317581\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -51.733475\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -176.646851\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -213.842941\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -201.704819\n",
      "    epoch          : 108\n",
      "    loss           : -172.91832931518556\n",
      "    val_loss       : -156.86037187874317\n",
      "    val_log_likelihood: 277.53443908691406\n",
      "    val_log_marginal: 173.39544420093299\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -22.525414\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -188.679001\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -42.785549\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -160.584930\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -100.968903\n",
      "    epoch          : 109\n",
      "    loss           : -178.7860944747925\n",
      "    val_loss       : -188.8701420409605\n",
      "    val_log_likelihood: 284.3149810791016\n",
      "    val_log_marginal: 199.2422239456326\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -293.493134\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -24.753513\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -21.024231\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -208.358337\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -148.415619\n",
      "    epoch          : 110\n",
      "    loss           : -175.00633823394776\n",
      "    val_loss       : -183.15020440611988\n",
      "    val_log_likelihood: 279.46986083984376\n",
      "    val_log_marginal: 185.54019312150777\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -274.331543\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -44.815285\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -64.797485\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -176.486343\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -128.696030\n",
      "    epoch          : 111\n",
      "    loss           : -156.91542632102966\n",
      "    val_loss       : -153.03149964520708\n",
      "    val_log_likelihood: 267.0933135986328\n",
      "    val_log_marginal: 162.23140615113078\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -147.896988\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -16.713518\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -65.938103\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -225.781845\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -90.497208\n",
      "    epoch          : 112\n",
      "    loss           : -186.0892468261719\n",
      "    val_loss       : -178.6047438840382\n",
      "    val_log_likelihood: 291.42008514404296\n",
      "    val_log_marginal: 189.0482132192701\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -210.054413\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -175.268982\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -154.433075\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -248.141296\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -236.538193\n",
      "    epoch          : 113\n",
      "    loss           : -193.052192363739\n",
      "    val_loss       : -167.75550906173885\n",
      "    val_log_likelihood: 289.6420074462891\n",
      "    val_log_marginal: 174.00987800508747\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -201.110886\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -219.667892\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -188.615707\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -65.256287\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -110.854645\n",
      "    epoch          : 114\n",
      "    loss           : -184.91120040893554\n",
      "    val_loss       : -197.50005937376994\n",
      "    val_log_likelihood: 289.52394409179686\n",
      "    val_log_marginal: 207.38178367353976\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -207.501953\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -208.338181\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -215.565643\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -207.684830\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -177.331345\n",
      "    epoch          : 115\n",
      "    loss           : -186.31312896728517\n",
      "    val_loss       : -188.60958137400448\n",
      "    val_log_likelihood: 297.30721893310545\n",
      "    val_log_marginal: 196.5963467967336\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -112.956665\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -263.866638\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -213.314667\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -69.818115\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -58.781136\n",
      "    epoch          : 116\n",
      "    loss           : -202.14557609558105\n",
      "    val_loss       : -185.63848578110338\n",
      "    val_log_likelihood: 295.53310241699216\n",
      "    val_log_marginal: 198.11169352307917\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -214.052094\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -51.144478\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -97.349396\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: 29.476629\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -88.503807\n",
      "    epoch          : 117\n",
      "    loss           : -179.9597317504883\n",
      "    val_loss       : -191.02115237936377\n",
      "    val_log_likelihood: 291.8672500610352\n",
      "    val_log_marginal: 200.2774793572724\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -227.185242\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -281.688354\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -171.505463\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -193.536652\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -216.319214\n",
      "    epoch          : 118\n",
      "    loss           : -205.29217155456544\n",
      "    val_loss       : -202.57723124474288\n",
      "    val_log_likelihood: 304.0462142944336\n",
      "    val_log_marginal: 215.85145907513797\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -282.444153\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -250.540283\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -265.359680\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -257.508575\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -259.887970\n",
      "    epoch          : 119\n",
      "    loss           : -218.03923316955567\n",
      "    val_loss       : -197.75935977157206\n",
      "    val_log_likelihood: 309.9378005981445\n",
      "    val_log_marginal: 206.04191582423306\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -314.327576\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -184.828552\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -299.549438\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -224.718567\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -76.985733\n",
      "    epoch          : 120\n",
      "    loss           : -194.74915522575378\n",
      "    val_loss       : -202.76429644990714\n",
      "    val_log_likelihood: 306.4851898193359\n",
      "    val_log_marginal: 208.6482712570578\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch120.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -282.533752\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -283.804626\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -244.794662\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -80.136253\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -160.362396\n",
      "    epoch          : 121\n",
      "    loss           : -207.90973110198973\n",
      "    val_loss       : -210.66187691166996\n",
      "    val_log_likelihood: 311.9533401489258\n",
      "    val_log_marginal: 217.5939374279231\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -94.623825\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -223.390289\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -217.460632\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -236.480591\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -62.538269\n",
      "    epoch          : 122\n",
      "    loss           : -144.4767065143585\n",
      "    val_loss       : -38.45580356037244\n",
      "    val_log_likelihood: 282.429182434082\n",
      "    val_log_marginal: 48.86817627623676\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -7.568690\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -169.462982\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -222.055069\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -246.711594\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -188.629364\n",
      "    epoch          : 123\n",
      "    loss           : -152.2370678138733\n",
      "    val_loss       : -174.31932913269847\n",
      "    val_log_likelihood: 300.51610107421874\n",
      "    val_log_marginal: 191.99952578014577\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -149.554092\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -152.761780\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -127.007690\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -214.719009\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -216.038971\n",
      "    epoch          : 124\n",
      "    loss           : -173.75139678001403\n",
      "    val_loss       : -165.7811475479044\n",
      "    val_log_likelihood: 281.00867156982423\n",
      "    val_log_marginal: 171.3169044710483\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -72.779190\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -181.860291\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -103.190468\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -215.391479\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -246.896484\n",
      "    epoch          : 125\n",
      "    loss           : -200.39893107891083\n",
      "    val_loss       : -208.68058487745003\n",
      "    val_log_likelihood: 310.0904937744141\n",
      "    val_log_marginal: 223.2320544719696\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -255.954346\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -326.908234\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -58.082027\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -58.709599\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -227.961685\n",
      "    epoch          : 126\n",
      "    loss           : -220.7922615814209\n",
      "    val_loss       : -214.75973513266072\n",
      "    val_log_likelihood: 313.7415283203125\n",
      "    val_log_marginal: 224.64135879107343\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -253.752640\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -81.738220\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -123.062714\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -303.341064\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -59.331623\n",
      "    epoch          : 127\n",
      "    loss           : -210.8456060028076\n",
      "    val_loss       : -174.66510536940768\n",
      "    val_log_likelihood: 295.1941619873047\n",
      "    val_log_marginal: 181.87382368072866\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -268.908630\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -107.278854\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -240.180511\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -180.338547\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -210.622314\n",
      "    epoch          : 128\n",
      "    loss           : -176.33788132667542\n",
      "    val_loss       : -162.7881294419989\n",
      "    val_log_likelihood: 308.5662414550781\n",
      "    val_log_marginal: 181.83233778774738\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -191.678070\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -154.305679\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -74.167046\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -157.662033\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -172.122879\n",
      "    epoch          : 129\n",
      "    loss           : -135.9237704372406\n",
      "    val_loss       : -157.09205016680062\n",
      "    val_log_likelihood: 286.0745254516602\n",
      "    val_log_marginal: 170.06059020087122\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -196.014420\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -215.132935\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -99.051056\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -292.118164\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -127.416138\n",
      "    epoch          : 130\n",
      "    loss           : -209.5676322746277\n",
      "    val_loss       : -217.8181900457479\n",
      "    val_log_likelihood: 313.33166046142577\n",
      "    val_log_marginal: 227.79851055833342\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -237.484879\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -149.027252\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -115.428490\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -222.256927\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -228.028580\n",
      "    epoch          : 131\n",
      "    loss           : -224.07154190063477\n",
      "    val_loss       : -194.34969368465244\n",
      "    val_log_likelihood: 314.3009887695313\n",
      "    val_log_marginal: 200.67439339384435\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -249.328430\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -87.235901\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -108.486298\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -231.598923\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -251.503769\n",
      "    epoch          : 132\n",
      "    loss           : -214.2401686859131\n",
      "    val_loss       : -213.92433637604117\n",
      "    val_log_likelihood: 319.0283401489258\n",
      "    val_log_marginal: 228.1903875512799\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -277.083344\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -137.260376\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -236.978638\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -219.093109\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -179.170441\n",
      "    epoch          : 133\n",
      "    loss           : -185.12701555252076\n",
      "    val_loss       : -90.27075842041522\n",
      "    val_log_likelihood: 283.3436309814453\n",
      "    val_log_marginal: 99.56114662513139\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -130.355392\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -170.971069\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: 62.355247\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -8.133793\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -193.378082\n",
      "    epoch          : 134\n",
      "    loss           : -161.5701574230194\n",
      "    val_loss       : -224.7113175155595\n",
      "    val_log_likelihood: 315.89271545410156\n",
      "    val_log_marginal: 234.34668669961394\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -146.111877\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -291.320007\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -301.151733\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -340.787720\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -105.250061\n",
      "    epoch          : 135\n",
      "    loss           : -232.5689207458496\n",
      "    val_loss       : -226.1905224015005\n",
      "    val_log_likelihood: 325.36241760253904\n",
      "    val_log_marginal: 237.63190286010504\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -302.792877\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -276.525024\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -264.473267\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -240.366776\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -151.983368\n",
      "    epoch          : 136\n",
      "    loss           : -225.34191394805907\n",
      "    val_loss       : -192.9986568475142\n",
      "    val_log_likelihood: 321.70385894775393\n",
      "    val_log_marginal: 209.51369940675795\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -223.604630\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: 50.904831\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -171.585236\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -91.483261\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -127.298630\n",
      "    epoch          : 137\n",
      "    loss           : -180.10918926239015\n",
      "    val_loss       : -176.8349401652813\n",
      "    val_log_likelihood: 312.3099334716797\n",
      "    val_log_marginal: 182.67452003471553\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -208.452347\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -324.490814\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -242.729279\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -246.326660\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -135.435898\n",
      "    epoch          : 138\n",
      "    loss           : -215.0838706588745\n",
      "    val_loss       : -216.94192293062807\n",
      "    val_log_likelihood: 324.4968200683594\n",
      "    val_log_marginal: 227.5985310895256\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -111.232101\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -201.908020\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -308.760803\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -308.402222\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -256.789062\n",
      "    epoch          : 139\n",
      "    loss           : -239.62058067321777\n",
      "    val_loss       : -240.77686812616884\n",
      "    val_log_likelihood: 333.6557144165039\n",
      "    val_log_marginal: 249.7111800506711\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -318.404724\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -300.839600\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -152.930847\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -111.861237\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -355.878510\n",
      "    epoch          : 140\n",
      "    loss           : -243.56918716430664\n",
      "    val_loss       : -221.83172322371973\n",
      "    val_log_likelihood: 334.8569366455078\n",
      "    val_log_marginal: 236.08409998230636\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch140.pth ...\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -255.567169\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -221.253601\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -239.243500\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: 15.837626\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: 842.706543\n",
      "    epoch          : 141\n",
      "    loss           : -99.84296303749085\n",
      "    val_loss       : -83.86312953056768\n",
      "    val_log_likelihood: 285.6338912963867\n",
      "    val_log_marginal: 110.2880808997899\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -240.803802\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -230.678329\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -75.432045\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -267.616394\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -237.092651\n",
      "    epoch          : 142\n",
      "    loss           : -182.63583387374877\n",
      "    val_loss       : -224.12239364450798\n",
      "    val_log_likelihood: 323.1656524658203\n",
      "    val_log_marginal: 235.77842699438332\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -272.901611\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -283.401550\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -341.905365\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -278.562836\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -146.527451\n",
      "    epoch          : 143\n",
      "    loss           : -248.85812507629396\n",
      "    val_loss       : -241.15132930008696\n",
      "    val_log_likelihood: 336.1267349243164\n",
      "    val_log_marginal: 252.31563478559255\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -277.160339\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -310.006927\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -273.427032\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -265.852600\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -299.358215\n",
      "    epoch          : 144\n",
      "    loss           : -228.67706069946288\n",
      "    val_loss       : -211.4912603179924\n",
      "    val_log_likelihood: 329.94359893798827\n",
      "    val_log_marginal: 218.00488643310965\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -290.784668\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -252.591873\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -260.964478\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -339.923279\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -207.602112\n",
      "    epoch          : 145\n",
      "    loss           : -236.18234592437744\n",
      "    val_loss       : -238.71233230289073\n",
      "    val_log_likelihood: 337.25501861572263\n",
      "    val_log_marginal: 245.41101148612796\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -282.204346\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -293.377228\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -275.144043\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -283.047577\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -269.907593\n",
      "    epoch          : 146\n",
      "    loss           : -253.36949569702148\n",
      "    val_loss       : -234.58651043241844\n",
      "    val_log_likelihood: 343.2222839355469\n",
      "    val_log_marginal: 241.67058593972857\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -357.947021\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -25.204903\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -260.776459\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -356.545502\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -256.465790\n",
      "    epoch          : 147\n",
      "    loss           : -234.7611231613159\n",
      "    val_loss       : -227.00975550767035\n",
      "    val_log_likelihood: 338.15454254150393\n",
      "    val_log_marginal: 243.9675312590204\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -250.312576\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -300.921417\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -34.376236\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -234.480957\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -231.076981\n",
      "    epoch          : 148\n",
      "    loss           : -195.55885529518127\n",
      "    val_loss       : -203.61432191822678\n",
      "    val_log_likelihood: 318.2026702880859\n",
      "    val_log_marginal: 212.18059569187463\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -326.605072\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -267.958282\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -257.576691\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -255.105469\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -191.164093\n",
      "    epoch          : 149\n",
      "    loss           : -222.0237664794922\n",
      "    val_loss       : -135.39112912975253\n",
      "    val_log_likelihood: 298.69163818359374\n",
      "    val_log_marginal: 143.91781004853547\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -102.600281\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -186.987854\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -278.417053\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -274.954376\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -260.203400\n",
      "    epoch          : 150\n",
      "    loss           : -210.24282326698304\n",
      "    val_loss       : -238.2170111477375\n",
      "    val_log_likelihood: 337.22505645751954\n",
      "    val_log_marginal: 251.6007074881345\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -115.116470\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -292.517670\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -215.365997\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -188.513046\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -109.432205\n",
      "    epoch          : 151\n",
      "    loss           : -223.68479775428773\n",
      "    val_loss       : -166.0721951304935\n",
      "    val_log_likelihood: 324.06060485839845\n",
      "    val_log_marginal: 182.65710988380016\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -309.718628\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -321.581299\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -270.259613\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -267.826080\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -251.477081\n",
      "    epoch          : 152\n",
      "    loss           : -234.0147985458374\n",
      "    val_loss       : -240.30314814811572\n",
      "    val_log_likelihood: 341.2437057495117\n",
      "    val_log_marginal: 247.83161704726518\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -303.207214\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -152.946136\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -283.524353\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -272.675476\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -129.377960\n",
      "    epoch          : 153\n",
      "    loss           : -255.09496726989747\n",
      "    val_loss       : -247.4273529981263\n",
      "    val_log_likelihood: 347.4139801025391\n",
      "    val_log_marginal: 254.33987322412432\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -289.129639\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -302.733765\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -347.672302\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -290.844757\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -290.535461\n",
      "    epoch          : 154\n",
      "    loss           : -263.28979179382327\n",
      "    val_loss       : -229.93152001714333\n",
      "    val_log_likelihood: 353.5890350341797\n",
      "    val_log_marginal: 250.86739277653396\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -292.038849\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -37.087242\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -173.611176\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -266.683167\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -250.437378\n",
      "    epoch          : 155\n",
      "    loss           : -224.7451579284668\n",
      "    val_loss       : -241.6974672054872\n",
      "    val_log_likelihood: 343.1510665893555\n",
      "    val_log_marginal: 249.3114844609052\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -273.276154\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -290.923950\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -367.520020\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -254.303055\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -121.345329\n",
      "    epoch          : 156\n",
      "    loss           : -227.59667125701904\n",
      "    val_loss       : -159.1807339082472\n",
      "    val_log_likelihood: 325.8027679443359\n",
      "    val_log_marginal: 170.86746105318306\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -217.734863\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -169.661606\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: 412.215271\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -1.677536\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -231.942703\n",
      "    epoch          : 157\n",
      "    loss           : -86.7266343832016\n",
      "    val_loss       : -167.06569218486547\n",
      "    val_log_likelihood: 314.46865997314455\n",
      "    val_log_marginal: 174.27290212772786\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -214.042404\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -165.439667\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -238.225586\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -113.148888\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -274.297363\n",
      "    epoch          : 158\n",
      "    loss           : -211.15498406410217\n",
      "    val_loss       : -244.55196203254164\n",
      "    val_log_likelihood: 343.6554885864258\n",
      "    val_log_marginal: 251.2488769158721\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -123.493095\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -133.100708\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -340.494202\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -338.903076\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -175.598343\n",
      "    epoch          : 159\n",
      "    loss           : -254.9550910949707\n",
      "    val_loss       : -231.69858615826814\n",
      "    val_log_likelihood: 351.26190185546875\n",
      "    val_log_marginal: 241.6359894014895\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -269.185181\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -284.310608\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -254.361374\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -266.390259\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: 46.406395\n",
      "    epoch          : 160\n",
      "    loss           : -222.39175830841066\n",
      "    val_loss       : -213.91145313866437\n",
      "    val_log_likelihood: 345.22679901123047\n",
      "    val_log_marginal: 222.93510995879768\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch160.pth ...\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -232.115448\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -97.169090\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -207.735016\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -229.941299\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -223.815155\n",
      "    epoch          : 161\n",
      "    loss           : -233.8232333755493\n",
      "    val_loss       : -241.47315925853326\n",
      "    val_log_likelihood: 343.3537857055664\n",
      "    val_log_marginal: 249.1762185632487\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -270.540039\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -307.219971\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -230.576202\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -309.119263\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -157.376892\n",
      "    epoch          : 162\n",
      "    loss           : -257.269467086792\n",
      "    val_loss       : -253.6659261607565\n",
      "    val_log_likelihood: 355.4326766967773\n",
      "    val_log_marginal: 261.9040962841362\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -142.087189\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -149.059326\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -378.367004\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -137.141861\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -109.005394\n",
      "    epoch          : 163\n",
      "    loss           : -254.70334426879882\n",
      "    val_loss       : -225.4081365610473\n",
      "    val_log_likelihood: 351.9501983642578\n",
      "    val_log_marginal: 238.30268914885818\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -273.653564\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -278.235687\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -258.499146\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -293.426605\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -234.436661\n",
      "    epoch          : 164\n",
      "    loss           : -226.3778675842285\n",
      "    val_loss       : -193.26182736540213\n",
      "    val_log_likelihood: 343.13907012939455\n",
      "    val_log_marginal: 202.13728847131134\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -357.873413\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -251.755997\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -74.354218\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -141.776428\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -277.955444\n",
      "    epoch          : 165\n",
      "    loss           : -235.0977052307129\n",
      "    val_loss       : -256.56742881909014\n",
      "    val_log_likelihood: 349.8615295410156\n",
      "    val_log_marginal: 262.0105624649674\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -314.910278\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -245.047089\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -298.043396\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -308.211426\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -163.049103\n",
      "    epoch          : 166\n",
      "    loss           : -268.63746810913085\n",
      "    val_loss       : -262.16330562327056\n",
      "    val_log_likelihood: 364.14923706054685\n",
      "    val_log_marginal: 268.88570344783363\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -383.782104\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -236.450684\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -305.330139\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -314.788025\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -381.408295\n",
      "    epoch          : 167\n",
      "    loss           : -279.75921989440917\n",
      "    val_loss       : -268.6577314304188\n",
      "    val_log_likelihood: 365.77931060791013\n",
      "    val_log_marginal: 276.12159468717874\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -276.365234\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -119.184456\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -352.066132\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -70.332634\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -253.685364\n",
      "    epoch          : 168\n",
      "    loss           : -244.5606739807129\n",
      "    val_loss       : -211.84611245663837\n",
      "    val_log_likelihood: 355.3181518554687\n",
      "    val_log_marginal: 222.34900926873087\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -265.137299\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -281.498779\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -54.701618\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -80.379486\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -218.163620\n",
      "    epoch          : 169\n",
      "    loss           : -167.60720112323762\n",
      "    val_loss       : -190.27324930829928\n",
      "    val_log_likelihood: 326.0792572021484\n",
      "    val_log_marginal: 199.1265424852273\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -214.638397\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -357.026764\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -251.281113\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -103.511452\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -211.539688\n",
      "    epoch          : 170\n",
      "    loss           : -192.21171800136565\n",
      "    val_loss       : -221.1327182411216\n",
      "    val_log_likelihood: 346.4719635009766\n",
      "    val_log_marginal: 236.02233189898615\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -266.279022\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -248.134277\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -286.150574\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -314.661438\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -311.173401\n",
      "    epoch          : 171\n",
      "    loss           : -272.5559527587891\n",
      "    val_loss       : -275.68756785169245\n",
      "    val_log_likelihood: 366.43016662597654\n",
      "    val_log_marginal: 281.9926285408475\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -310.618378\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -312.471375\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -306.701843\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -135.452148\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -264.124573\n",
      "    epoch          : 172\n",
      "    loss           : -271.9966714477539\n",
      "    val_loss       : -239.4770762286149\n",
      "    val_log_likelihood: 358.5565872192383\n",
      "    val_log_marginal: 252.66247158987161\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -151.544434\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -350.433289\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -45.640186\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -267.648376\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -71.176003\n",
      "    epoch          : 173\n",
      "    loss           : -237.07933758735658\n",
      "    val_loss       : -218.26136903790757\n",
      "    val_log_likelihood: 351.87533111572264\n",
      "    val_log_marginal: 228.94232561143212\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -125.255066\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -294.033539\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -268.210876\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -306.551025\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -285.912170\n",
      "    epoch          : 174\n",
      "    loss           : -245.28499969482422\n",
      "    val_loss       : -245.43100516051055\n",
      "    val_log_likelihood: 360.4955032348633\n",
      "    val_log_marginal: 258.2907298607222\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -277.356323\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -292.912354\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -302.003082\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -270.693237\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -188.232880\n",
      "    epoch          : 175\n",
      "    loss           : -263.2261266326904\n",
      "    val_loss       : -233.56720161456616\n",
      "    val_log_likelihood: 349.82373199462893\n",
      "    val_log_marginal: 241.16206592136842\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -271.431702\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -329.464783\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -343.099976\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -236.248398\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -153.364334\n",
      "    epoch          : 176\n",
      "    loss           : -217.7530963945389\n",
      "    val_loss       : -240.85887817787005\n",
      "    val_log_likelihood: 354.43850860595705\n",
      "    val_log_marginal: 248.28131801746787\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -286.704956\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -298.569092\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -299.921021\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -307.032471\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -223.811584\n",
      "    epoch          : 177\n",
      "    loss           : -263.2255937957764\n",
      "    val_loss       : -254.22834763238205\n",
      "    val_log_likelihood: 367.21392211914065\n",
      "    val_log_marginal: 262.0490127619356\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -330.635559\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -257.701294\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -295.254333\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -92.690582\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -135.050781\n",
      "    epoch          : 178\n",
      "    loss           : -240.3536851119995\n",
      "    val_loss       : -239.29888932639733\n",
      "    val_log_likelihood: 358.39375762939454\n",
      "    val_log_marginal: 241.3817292518914\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -285.299103\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -280.142822\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -297.879822\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -239.808014\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -95.924370\n",
      "    epoch          : 179\n",
      "    loss           : -235.1534247779846\n",
      "    val_loss       : -244.5206791772507\n",
      "    val_log_likelihood: 355.716943359375\n",
      "    val_log_marginal: 251.2927471387182\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -252.471039\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -289.346832\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -131.800369\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -200.253128\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -105.011932\n",
      "    epoch          : 180\n",
      "    loss           : -213.17562437057495\n",
      "    val_loss       : -96.84358131503686\n",
      "    val_log_likelihood: 321.3367034912109\n",
      "    val_log_marginal: 107.06175538040698\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch180.pth ...\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -196.467926\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -20.896658\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -22.603954\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -216.216858\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -171.770615\n",
      "    epoch          : 181\n",
      "    loss           : -208.47362365722657\n",
      "    val_loss       : -252.14452381767333\n",
      "    val_log_likelihood: 358.20586547851565\n",
      "    val_log_marginal: 262.68727920986714\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -367.212555\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -124.204254\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -308.937958\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -326.522400\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -107.584206\n",
      "    epoch          : 182\n",
      "    loss           : -269.37293922424317\n",
      "    val_loss       : -237.15448940070345\n",
      "    val_log_likelihood: 364.3608108520508\n",
      "    val_log_marginal: 248.3943482160568\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -277.841187\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -286.863403\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -290.730408\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -289.750458\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -299.314331\n",
      "    epoch          : 183\n",
      "    loss           : -262.84992378234864\n",
      "    val_loss       : -242.6479383567348\n",
      "    val_log_likelihood: 366.3442611694336\n",
      "    val_log_marginal: 256.17359657622876\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -328.098083\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -317.576660\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -313.584167\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -365.376709\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -243.537949\n",
      "    epoch          : 184\n",
      "    loss           : -269.2565376281738\n",
      "    val_loss       : -236.2957924734801\n",
      "    val_log_likelihood: 373.5677459716797\n",
      "    val_log_marginal: 244.61729186515558\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -334.990723\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -287.764221\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -298.329712\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -271.475464\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -196.700119\n",
      "    epoch          : 185\n",
      "    loss           : -250.3204526901245\n",
      "    val_loss       : -232.29405619893222\n",
      "    val_log_likelihood: 361.1722045898438\n",
      "    val_log_marginal: 239.7767334807664\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -282.643036\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -289.970673\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -262.370514\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -214.076447\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -181.637192\n",
      "    epoch          : 186\n",
      "    loss           : -192.13700794696808\n",
      "    val_loss       : -160.8594380294904\n",
      "    val_log_likelihood: 339.5426742553711\n",
      "    val_log_marginal: 178.40707244910303\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -40.166935\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -119.968033\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -327.259857\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -297.810699\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -346.677795\n",
      "    epoch          : 187\n",
      "    loss           : -258.2687919044495\n",
      "    val_loss       : -273.7253224104643\n",
      "    val_log_likelihood: 373.37658538818357\n",
      "    val_log_marginal: 281.04053316488864\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -323.442993\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -277.542175\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -330.796539\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -346.843170\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -330.423187\n",
      "    epoch          : 188\n",
      "    loss           : -298.3554739379883\n",
      "    val_loss       : -290.8128056212328\n",
      "    val_log_likelihood: 383.71619415283203\n",
      "    val_log_marginal: 295.70687716342593\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -319.942505\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -353.109467\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -413.946777\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -258.099792\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -310.135773\n",
      "    epoch          : 189\n",
      "    loss           : -287.8238006591797\n",
      "    val_loss       : -259.65903160348535\n",
      "    val_log_likelihood: 375.2727813720703\n",
      "    val_log_marginal: 269.96093433387676\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -144.390991\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -63.547787\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -361.774597\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -319.944458\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -318.561615\n",
      "    epoch          : 190\n",
      "    loss           : -260.61046981811523\n",
      "    val_loss       : -269.415110071376\n",
      "    val_log_likelihood: 374.88927154541017\n",
      "    val_log_marginal: 278.50699238259085\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -339.023193\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -318.881348\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -292.958618\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -164.111481\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -143.841415\n",
      "    epoch          : 191\n",
      "    loss           : -279.6503623962402\n",
      "    val_loss       : -267.63671292737126\n",
      "    val_log_likelihood: 380.918229675293\n",
      "    val_log_marginal: 274.95869142740867\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -122.213127\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -347.261871\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -300.701569\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -143.741653\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -304.145691\n",
      "    epoch          : 192\n",
      "    loss           : -275.5756699371338\n",
      "    val_loss       : -267.3099869838916\n",
      "    val_log_likelihood: 372.21770935058595\n",
      "    val_log_marginal: 274.72387738339603\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -273.850098\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -282.646118\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -301.884155\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -300.155853\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -131.796875\n",
      "    epoch          : 193\n",
      "    loss           : -243.63844856262207\n",
      "    val_loss       : -223.64063444454223\n",
      "    val_log_likelihood: 347.988525390625\n",
      "    val_log_marginal: 234.3787866424769\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -302.896729\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -260.906372\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -314.416992\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -288.291382\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -321.087921\n",
      "    epoch          : 194\n",
      "    loss           : -286.7967652893066\n",
      "    val_loss       : -280.11450946191326\n",
      "    val_log_likelihood: 388.24207611083983\n",
      "    val_log_marginal: 290.16338834427296\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -310.248505\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -321.812225\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -334.036438\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -364.276398\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -198.026489\n",
      "    epoch          : 195\n",
      "    loss           : -299.3030178070068\n",
      "    val_loss       : -287.0345867970958\n",
      "    val_log_likelihood: 391.2931625366211\n",
      "    val_log_marginal: 295.3873438920826\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -342.882385\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -157.992447\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -156.808319\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -169.827881\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -281.924713\n",
      "    epoch          : 196\n",
      "    loss           : -267.4080792999268\n",
      "    val_loss       : -212.81844041226432\n",
      "    val_log_likelihood: 383.01805572509767\n",
      "    val_log_marginal: 226.19725094027817\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -257.495636\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -265.408417\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -209.621124\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -248.274734\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -333.575500\n",
      "    epoch          : 197\n",
      "    loss           : -240.52725875377655\n",
      "    val_loss       : -274.7448059656657\n",
      "    val_log_likelihood: 383.22645111083983\n",
      "    val_log_marginal: 284.93503206558523\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -343.203125\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -319.256012\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -193.283340\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -316.737579\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -150.409088\n",
      "    epoch          : 198\n",
      "    loss           : -291.16454887390137\n",
      "    val_loss       : -275.86426119673996\n",
      "    val_log_likelihood: 384.576155090332\n",
      "    val_log_marginal: 283.9710654191673\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -415.439392\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -345.849762\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -287.859375\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -336.899292\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -170.364700\n",
      "    epoch          : 199\n",
      "    loss           : -304.140037612915\n",
      "    val_loss       : -279.0539736026898\n",
      "    val_log_likelihood: 390.0577926635742\n",
      "    val_log_marginal: 287.17586164698\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -193.120499\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -302.228943\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -303.949310\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -320.184784\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -111.658325\n",
      "    epoch          : 200\n",
      "    loss           : -272.12762283325196\n",
      "    val_loss       : -208.82829817160965\n",
      "    val_log_likelihood: 372.8230438232422\n",
      "    val_log_marginal: 219.39644935689867\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -79.314056\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -279.299774\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -113.578156\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: 105.811478\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -132.665649\n",
      "    epoch          : 201\n",
      "    loss           : -168.30481909751893\n",
      "    val_loss       : -98.84777028691023\n",
      "    val_log_likelihood: 342.64354553222654\n",
      "    val_log_marginal: 113.97582962684592\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -196.645737\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: 1750.135620\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: 323.076080\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -73.810890\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -231.724884\n",
      "    epoch          : 202\n",
      "    loss           : 53.702073669433595\n",
      "    val_loss       : -236.39615313578398\n",
      "    val_log_likelihood: 337.6107879638672\n",
      "    val_log_marginal: 248.5331314396113\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -262.545197\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -283.720673\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -310.909973\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -325.900726\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -164.466629\n",
      "    epoch          : 203\n",
      "    loss           : -278.39073989868166\n",
      "    val_loss       : -279.6924268687144\n",
      "    val_log_likelihood: 373.9324447631836\n",
      "    val_log_marginal: 287.1670281957705\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -323.332703\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -286.717865\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -409.678864\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -319.993042\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -191.648331\n",
      "    epoch          : 204\n",
      "    loss           : -300.7107563781738\n",
      "    val_loss       : -287.2089342840016\n",
      "    val_log_likelihood: 380.42923431396486\n",
      "    val_log_marginal: 293.511790939793\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -328.293518\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -426.777802\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -332.681671\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -322.465942\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -146.651749\n",
      "    epoch          : 205\n",
      "    loss           : -296.7671981048584\n",
      "    val_loss       : -271.919302736409\n",
      "    val_log_likelihood: 384.25372314453125\n",
      "    val_log_marginal: 283.8646369766444\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -165.167664\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -418.780457\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -326.491364\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -339.565674\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -322.409546\n",
      "    epoch          : 206\n",
      "    loss           : -294.9370077514648\n",
      "    val_loss       : -283.56063503464685\n",
      "    val_log_likelihood: 388.3406616210938\n",
      "    val_log_marginal: 292.25385113612424\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -416.375305\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -353.819336\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -327.902283\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -319.374390\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -178.126068\n",
      "    epoch          : 207\n",
      "    loss           : -300.9405992126465\n",
      "    val_loss       : -271.2607019079849\n",
      "    val_log_likelihood: 384.19232177734375\n",
      "    val_log_marginal: 279.19470598883925\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -309.351532\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -261.083191\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -351.589478\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -277.328796\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -330.211395\n",
      "    epoch          : 208\n",
      "    loss           : -280.58451782226564\n",
      "    val_loss       : -242.33183952877297\n",
      "    val_log_likelihood: 378.0752914428711\n",
      "    val_log_marginal: 249.18583686836064\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -120.976242\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -291.911743\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -171.828888\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -306.540070\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -190.519714\n",
      "    epoch          : 209\n",
      "    loss           : -293.2768306732178\n",
      "    val_loss       : -291.97528446540235\n",
      "    val_log_likelihood: 395.04351806640625\n",
      "    val_log_marginal: 298.21184701658785\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -410.065552\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -422.695862\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -350.805359\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -338.890808\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -356.862854\n",
      "    epoch          : 210\n",
      "    loss           : -318.19865707397463\n",
      "    val_loss       : -301.4566041857004\n",
      "    val_log_likelihood: 400.854264831543\n",
      "    val_log_marginal: 308.93879831396043\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -433.577240\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -285.520447\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -342.608887\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -342.901550\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -312.751160\n",
      "    epoch          : 211\n",
      "    loss           : -296.2855122375488\n",
      "    val_loss       : -267.29262872114776\n",
      "    val_log_likelihood: 393.22691802978517\n",
      "    val_log_marginal: 276.3685288351029\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -293.717957\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -145.712799\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -346.785980\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -170.147827\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -199.471924\n",
      "    epoch          : 212\n",
      "    loss           : -287.6179907989502\n",
      "    val_loss       : -286.08932420182975\n",
      "    val_log_likelihood: 395.7662658691406\n",
      "    val_log_marginal: 293.5625677485019\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -302.016022\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -418.023834\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -319.312927\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -282.396149\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -264.197998\n",
      "    epoch          : 213\n",
      "    loss           : -307.77921417236325\n",
      "    val_loss       : -292.6889758169651\n",
      "    val_log_likelihood: 398.23480529785155\n",
      "    val_log_marginal: 298.69979894036317\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -189.715027\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -155.322739\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -353.229492\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -316.654205\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -130.098724\n",
      "    epoch          : 214\n",
      "    loss           : -288.10692375183106\n",
      "    val_loss       : -231.7638549274765\n",
      "    val_log_likelihood: 389.8484115600586\n",
      "    val_log_marginal: 240.71856594495475\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -265.118225\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -412.363159\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -315.770203\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -244.292694\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -290.798401\n",
      "    epoch          : 215\n",
      "    loss           : -278.99604286193846\n",
      "    val_loss       : -270.3725083272904\n",
      "    val_log_likelihood: 394.36756134033203\n",
      "    val_log_marginal: 284.81547738648953\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -156.710800\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -420.497498\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -250.324219\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -328.954346\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -226.148407\n",
      "    epoch          : 216\n",
      "    loss           : -270.1013813400269\n",
      "    val_loss       : -277.79818382905796\n",
      "    val_log_likelihood: 391.17318267822264\n",
      "    val_log_marginal: 283.6067168157548\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -338.670746\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -335.436462\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -331.267242\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -318.567017\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -326.737488\n",
      "    epoch          : 217\n",
      "    loss           : -298.3114216613769\n",
      "    val_loss       : -290.2726733486168\n",
      "    val_log_likelihood: 400.4252227783203\n",
      "    val_log_marginal: 296.5919162672799\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -354.767731\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -405.095367\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -352.436462\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -245.450043\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -174.526352\n",
      "    epoch          : 218\n",
      "    loss           : -302.351184387207\n",
      "    val_loss       : -277.6738984720781\n",
      "    val_log_likelihood: 389.1948745727539\n",
      "    val_log_marginal: 292.3648337257938\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -181.152542\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -147.869949\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -280.442322\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -333.524841\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -187.027542\n",
      "    epoch          : 219\n",
      "    loss           : -308.2929806518555\n",
      "    val_loss       : -298.7587766611949\n",
      "    val_log_likelihood: 407.3514694213867\n",
      "    val_log_marginal: 308.08036975897846\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -353.930359\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -382.657471\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -433.982483\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -337.944183\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -252.749573\n",
      "    epoch          : 220\n",
      "    loss           : -308.98276763916016\n",
      "    val_loss       : -270.77391348006205\n",
      "    val_log_likelihood: 396.4467025756836\n",
      "    val_log_marginal: 279.54125922508535\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch220.pth ...\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -331.451263\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -335.542999\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -199.691437\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -433.671967\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -420.730896\n",
      "    epoch          : 221\n",
      "    loss           : -308.0328219604492\n",
      "    val_loss       : -278.16578785357996\n",
      "    val_log_likelihood: 404.72384490966795\n",
      "    val_log_marginal: 296.25211599841714\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -293.224060\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -153.639328\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -334.903442\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -167.271149\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -263.127838\n",
      "    epoch          : 222\n",
      "    loss           : -306.25191242218017\n",
      "    val_loss       : -279.33904354730623\n",
      "    val_log_likelihood: 402.9135025024414\n",
      "    val_log_marginal: 289.31118673570455\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -349.371338\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -338.306824\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -327.112976\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -187.697952\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -275.179565\n",
      "    epoch          : 223\n",
      "    loss           : -281.4520488739014\n",
      "    val_loss       : -268.99927802318706\n",
      "    val_log_likelihood: 388.3623672485352\n",
      "    val_log_marginal: 280.3912730123848\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -311.497711\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -426.368713\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -373.555969\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -435.205078\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -303.820496\n",
      "    epoch          : 224\n",
      "    loss           : -306.6887629699707\n",
      "    val_loss       : -306.3355508104898\n",
      "    val_log_likelihood: 408.5119903564453\n",
      "    val_log_marginal: 313.8889396674931\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -385.707947\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -357.637146\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -174.967834\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -360.505219\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -363.045349\n",
      "    epoch          : 225\n",
      "    loss           : -330.25855255126953\n",
      "    val_loss       : -313.55645594345407\n",
      "    val_log_likelihood: 416.6222869873047\n",
      "    val_log_marginal: 320.54577369131147\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -448.344543\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -194.018524\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -330.263855\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -303.782166\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -288.881622\n",
      "    epoch          : 226\n",
      "    loss           : -314.1576557159424\n",
      "    val_loss       : -288.0094817525707\n",
      "    val_log_likelihood: 407.9071014404297\n",
      "    val_log_marginal: 292.9101667199284\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -342.918732\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -373.630219\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -81.229416\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -341.898804\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -151.417709\n",
      "    epoch          : 227\n",
      "    loss           : -300.15508903503417\n",
      "    val_loss       : -286.29616151917725\n",
      "    val_log_likelihood: 406.60772399902345\n",
      "    val_log_marginal: 294.0348752271384\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -383.548279\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -158.161667\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -314.554962\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -116.601059\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -305.417236\n",
      "    epoch          : 228\n",
      "    loss           : -282.3253369140625\n",
      "    val_loss       : -253.7068049323745\n",
      "    val_log_likelihood: 394.94871673583987\n",
      "    val_log_marginal: 263.24651168324056\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -283.254883\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -346.936005\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -368.363708\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -311.480225\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -342.090607\n",
      "    epoch          : 229\n",
      "    loss           : -276.5283828353882\n",
      "    val_loss       : -258.54940812531856\n",
      "    val_log_likelihood: 398.22347869873045\n",
      "    val_log_marginal: 264.56413548476996\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -293.398315\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -334.532471\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -324.809631\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -385.060608\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -297.117371\n",
      "    epoch          : 230\n",
      "    loss           : -270.6301975917816\n",
      "    val_loss       : -235.09958761902527\n",
      "    val_log_likelihood: 397.4796676635742\n",
      "    val_log_marginal: 252.24157417528332\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -320.625488\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -310.903198\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -198.942932\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -9.618460\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -316.994446\n",
      "    epoch          : 231\n",
      "    loss           : -207.07665717601776\n",
      "    val_loss       : -271.06087988866494\n",
      "    val_log_likelihood: 398.11012268066406\n",
      "    val_log_marginal: 287.01743578203025\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -350.880371\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -377.543640\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -273.343414\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -237.873352\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -121.914543\n",
      "    epoch          : 232\n",
      "    loss           : -262.2705174064636\n",
      "    val_loss       : -243.64654501490296\n",
      "    val_log_likelihood: 386.8578811645508\n",
      "    val_log_marginal: 263.1140249725431\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -150.414246\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -306.903748\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -148.037048\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -321.899109\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -409.381561\n",
      "    epoch          : 233\n",
      "    loss           : -286.7281467437744\n",
      "    val_loss       : -299.0756356161088\n",
      "    val_log_likelihood: 409.6435913085937\n",
      "    val_log_marginal: 308.6818907111883\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -149.359573\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -312.183319\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -365.821838\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -373.450928\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -306.328247\n",
      "    epoch          : 234\n",
      "    loss           : -331.13671813964845\n",
      "    val_loss       : -315.75227171862497\n",
      "    val_log_likelihood: 417.32716674804686\n",
      "    val_log_marginal: 323.8032489309081\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -454.483704\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -324.109161\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -377.634094\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -316.136475\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -317.417755\n",
      "    epoch          : 235\n",
      "    loss           : -340.3902169799805\n",
      "    val_loss       : -314.34557741926983\n",
      "    val_log_likelihood: 417.147607421875\n",
      "    val_log_marginal: 321.2101627912372\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -374.293884\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -196.425858\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -374.958130\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -306.552368\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -298.098785\n",
      "    epoch          : 236\n",
      "    loss           : -332.3774626159668\n",
      "    val_loss       : -304.45370645821095\n",
      "    val_log_likelihood: 415.05352783203125\n",
      "    val_log_marginal: 308.4634853451036\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -338.572968\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -141.753784\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -366.197937\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -320.482056\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -299.637512\n",
      "    epoch          : 237\n",
      "    loss           : -286.3034460830688\n",
      "    val_loss       : -278.5324955434538\n",
      "    val_log_likelihood: 396.9717025756836\n",
      "    val_log_marginal: 282.3831640314311\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -339.412598\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -333.076660\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -173.636353\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -358.542664\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -109.286415\n",
      "    epoch          : 238\n",
      "    loss           : -308.36581451416015\n",
      "    val_loss       : -256.43870580755174\n",
      "    val_log_likelihood: 406.6769683837891\n",
      "    val_log_marginal: 287.3907519109547\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -111.279457\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -308.551422\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -188.997055\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -174.020615\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -336.738800\n",
      "    epoch          : 239\n",
      "    loss           : -291.9993963623047\n",
      "    val_loss       : -290.69849289162084\n",
      "    val_log_likelihood: 403.9702514648437\n",
      "    val_log_marginal: 295.9733540598303\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -321.063446\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -119.646568\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -322.488129\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -339.174744\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -323.190918\n",
      "    epoch          : 240\n",
      "    loss           : -278.8504076957703\n",
      "    val_loss       : -300.166522396449\n",
      "    val_log_likelihood: 415.2550872802734\n",
      "    val_log_marginal: 312.70609451726915\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch240.pth ...\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -370.323242\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -200.635895\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -442.565552\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -279.150146\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -286.219574\n",
      "    epoch          : 241\n",
      "    loss           : -323.84854400634765\n",
      "    val_loss       : -307.4303891163319\n",
      "    val_log_likelihood: 412.52906341552733\n",
      "    val_log_marginal: 313.31296238414944\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -400.733368\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -374.895935\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -355.776337\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -228.086700\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -337.884125\n",
      "    epoch          : 242\n",
      "    loss           : -333.3096466064453\n",
      "    val_loss       : -317.72738396851344\n",
      "    val_log_likelihood: 423.87840881347654\n",
      "    val_log_marginal: 325.8258672099561\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -462.202972\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -465.334595\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -351.999512\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -302.075806\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -308.910065\n",
      "    epoch          : 243\n",
      "    loss           : -326.43875793457033\n",
      "    val_loss       : -278.8287484242581\n",
      "    val_log_likelihood: 417.33771057128905\n",
      "    val_log_marginal: 296.651475988701\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -123.803780\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -272.171997\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -374.018005\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -291.907959\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -373.087402\n",
      "    epoch          : 244\n",
      "    loss           : -326.77853157043455\n",
      "    val_loss       : -317.67337115937846\n",
      "    val_log_likelihood: 425.1681243896484\n",
      "    val_log_marginal: 325.4101487923414\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -357.256042\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -368.178772\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -388.150421\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -149.444107\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -374.256592\n",
      "    epoch          : 245\n",
      "    loss           : -328.815389251709\n",
      "    val_loss       : -313.84034579768775\n",
      "    val_log_likelihood: 423.15459899902345\n",
      "    val_log_marginal: 319.520614945516\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -319.722870\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -208.670227\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -318.341858\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -329.429077\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -336.292725\n",
      "    epoch          : 246\n",
      "    loss           : -320.76279083251956\n",
      "    val_loss       : -290.19594685314223\n",
      "    val_log_likelihood: 416.8251724243164\n",
      "    val_log_marginal: 299.6456327671077\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -323.642456\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -428.633606\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -323.414612\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -133.515747\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -230.530289\n",
      "    epoch          : 247\n",
      "    loss           : -297.64719146728515\n",
      "    val_loss       : -290.3149799255654\n",
      "    val_log_likelihood: 411.83300628662107\n",
      "    val_log_marginal: 298.7072604823858\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -350.913574\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -197.077316\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -344.992889\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -124.755554\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -222.735138\n",
      "    epoch          : 248\n",
      "    loss           : -293.8030670166016\n",
      "    val_loss       : -267.48526491001246\n",
      "    val_log_likelihood: 406.3699157714844\n",
      "    val_log_marginal: 280.0288114059716\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -181.462799\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -335.454041\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -375.145966\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -384.078674\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -119.311996\n",
      "    epoch          : 249\n",
      "    loss           : -268.9520070648193\n",
      "    val_loss       : -177.20021522035822\n",
      "    val_log_likelihood: 396.6317672729492\n",
      "    val_log_marginal: 196.59138951934875\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -237.030670\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -25.117302\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -225.407684\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -130.235336\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -195.214188\n",
      "    epoch          : 250\n",
      "    loss           : -194.10560624122618\n",
      "    val_loss       : -228.9390741369687\n",
      "    val_log_likelihood: 389.21154022216797\n",
      "    val_log_marginal: 248.3592423927039\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -298.997864\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -292.375885\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -343.524475\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -435.417358\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -120.904564\n",
      "    epoch          : 251\n",
      "    loss           : -302.8172020721436\n",
      "    val_loss       : -295.19230405911804\n",
      "    val_log_likelihood: 413.9991851806641\n",
      "    val_log_marginal: 301.2876440223398\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -393.907349\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -349.403870\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -194.751495\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -363.278534\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -369.767700\n",
      "    epoch          : 252\n",
      "    loss           : -325.49649185180664\n",
      "    val_loss       : -319.68410378573464\n",
      "    val_log_likelihood: 420.01971130371095\n",
      "    val_log_marginal: 326.62390932701527\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -376.769562\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -367.332031\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -454.264374\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -372.753601\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -192.034592\n",
      "    epoch          : 253\n",
      "    loss           : -331.7097057342529\n",
      "    val_loss       : -294.75823099855336\n",
      "    val_log_likelihood: 410.00950622558594\n",
      "    val_log_marginal: 302.09604198001324\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -154.086060\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -187.128098\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -303.646179\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -341.005981\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -342.363159\n",
      "    epoch          : 254\n",
      "    loss           : -302.70183135986326\n",
      "    val_loss       : -311.2709001470357\n",
      "    val_log_likelihood: 417.7790924072266\n",
      "    val_log_marginal: 315.77442255057395\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -361.334381\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -338.702393\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -460.889709\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -362.060242\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -169.284897\n",
      "    epoch          : 255\n",
      "    loss           : -330.54333389282226\n",
      "    val_loss       : -307.7304262135178\n",
      "    val_log_likelihood: 423.5539489746094\n",
      "    val_log_marginal: 318.9495942786336\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -299.078979\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -361.426514\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -364.586792\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -361.375763\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -401.774841\n",
      "    epoch          : 256\n",
      "    loss           : -319.9393468475342\n",
      "    val_loss       : -206.48170200660826\n",
      "    val_log_likelihood: 406.66578369140626\n",
      "    val_log_marginal: 216.92225208096207\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -207.109268\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -205.983200\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -322.782990\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -291.309479\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -253.631607\n",
      "    epoch          : 257\n",
      "    loss           : -256.287016582489\n",
      "    val_loss       : -285.7037706114352\n",
      "    val_log_likelihood: 410.7120864868164\n",
      "    val_log_marginal: 293.97082414887853\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -385.283813\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -324.355042\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -347.905762\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -346.328644\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -152.525940\n",
      "    epoch          : 258\n",
      "    loss           : -305.200546875\n",
      "    val_loss       : -288.2644773214124\n",
      "    val_log_likelihood: 417.77098693847654\n",
      "    val_log_marginal: 296.3480555731803\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -314.214539\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -390.032013\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -345.289093\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -359.996704\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -176.650513\n",
      "    epoch          : 259\n",
      "    loss           : -329.81809875488284\n",
      "    val_loss       : -313.6926443839446\n",
      "    val_log_likelihood: 426.39707336425784\n",
      "    val_log_marginal: 321.1523429003354\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -373.561676\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -393.066925\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -410.254303\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -375.681335\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -382.144928\n",
      "    epoch          : 260\n",
      "    loss           : -344.4551014709473\n",
      "    val_loss       : -315.9316629175097\n",
      "    val_log_likelihood: 427.67164001464846\n",
      "    val_log_marginal: 324.6952661890537\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch260.pth ...\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -406.571075\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -186.957520\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -363.377472\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -301.840851\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -378.096191\n",
      "    epoch          : 261\n",
      "    loss           : -333.85014099121094\n",
      "    val_loss       : -308.00995383057744\n",
      "    val_log_likelihood: 427.3048370361328\n",
      "    val_log_marginal: 314.8048384524882\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -355.945587\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -473.109863\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -462.371490\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -288.277893\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -345.899719\n",
      "    epoch          : 262\n",
      "    loss           : -333.3804512023926\n",
      "    val_loss       : -308.1329875939526\n",
      "    val_log_likelihood: 427.767561340332\n",
      "    val_log_marginal: 320.5202302407473\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -353.362915\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -168.967255\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -335.382751\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -215.324585\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -361.561157\n",
      "    epoch          : 263\n",
      "    loss           : -330.08347610473635\n",
      "    val_loss       : -316.969917485863\n",
      "    val_log_likelihood: 432.2021026611328\n",
      "    val_log_marginal: 326.585264280811\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -153.041183\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -164.191360\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -480.843018\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -361.328857\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -206.109222\n",
      "    epoch          : 264\n",
      "    loss           : -312.796916217804\n",
      "    val_loss       : -270.1557810728438\n",
      "    val_log_likelihood: 417.86285705566405\n",
      "    val_log_marginal: 279.883394215256\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -342.189453\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -334.126343\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -360.481445\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -310.872223\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -316.199219\n",
      "    epoch          : 265\n",
      "    loss           : -294.0517427825928\n",
      "    val_loss       : -306.8411843980663\n",
      "    val_log_likelihood: 429.2927307128906\n",
      "    val_log_marginal: 314.03667612998345\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -145.806595\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -360.920563\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -330.464050\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -353.000671\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -151.129288\n",
      "    epoch          : 266\n",
      "    loss           : -315.71177589416504\n",
      "    val_loss       : -308.60817081555723\n",
      "    val_log_likelihood: 425.8284576416016\n",
      "    val_log_marginal: 321.5992434680462\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -345.281921\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -384.729248\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -420.900726\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -374.922363\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -387.701630\n",
      "    epoch          : 267\n",
      "    loss           : -348.36378173828126\n",
      "    val_loss       : -330.73166886176915\n",
      "    val_log_likelihood: 437.14781188964844\n",
      "    val_log_marginal: 339.10406618863345\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -350.659180\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -459.695801\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -379.149536\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -379.793152\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -339.576263\n",
      "    epoch          : 268\n",
      "    loss           : -336.74687225341796\n",
      "    val_loss       : -300.38382028611375\n",
      "    val_log_likelihood: 425.8452178955078\n",
      "    val_log_marginal: 307.4318513292819\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -370.585297\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -149.639969\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -191.906235\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -369.247772\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -299.649536\n",
      "    epoch          : 269\n",
      "    loss           : -321.2114320373535\n",
      "    val_loss       : -309.1450957725756\n",
      "    val_log_likelihood: 428.9717742919922\n",
      "    val_log_marginal: 315.69596115686\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -328.460815\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -451.613434\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -368.257355\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -372.905762\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -211.392792\n",
      "    epoch          : 270\n",
      "    loss           : -320.70564933776853\n",
      "    val_loss       : -281.7058348292485\n",
      "    val_log_likelihood: 426.9966552734375\n",
      "    val_log_marginal: 291.90047507844866\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -323.218597\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -175.054123\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -342.903015\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -420.734711\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -309.701172\n",
      "    epoch          : 271\n",
      "    loss           : -310.5953786468506\n",
      "    val_loss       : -295.20118576847017\n",
      "    val_log_likelihood: 427.2343292236328\n",
      "    val_log_marginal: 302.96486037410796\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -398.282532\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -305.557556\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -62.918198\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -324.471832\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -322.109222\n",
      "    epoch          : 272\n",
      "    loss           : -290.0292844390869\n",
      "    val_loss       : -299.26862914916126\n",
      "    val_log_likelihood: 426.6889984130859\n",
      "    val_log_marginal: 308.09711380936204\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -200.345917\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -182.800858\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -315.668610\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -250.571533\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -298.210602\n",
      "    epoch          : 273\n",
      "    loss           : -281.8877426815033\n",
      "    val_loss       : -288.5898992145434\n",
      "    val_log_likelihood: 421.6511962890625\n",
      "    val_log_marginal: 301.625944561242\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -321.884308\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -340.569641\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -169.643753\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -187.218658\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -222.662292\n",
      "    epoch          : 274\n",
      "    loss           : -304.0815112304688\n",
      "    val_loss       : -280.93475036974996\n",
      "    val_log_likelihood: 421.15075836181643\n",
      "    val_log_marginal: 287.49520719237626\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -296.314331\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -356.309357\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -426.354065\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -140.372345\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -275.638428\n",
      "    epoch          : 275\n",
      "    loss           : -289.44829299926755\n",
      "    val_loss       : -230.72853919444606\n",
      "    val_log_likelihood: 410.1291702270508\n",
      "    val_log_marginal: 256.7895741842687\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -257.035034\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -284.626251\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -343.528809\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -317.611359\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -352.463013\n",
      "    epoch          : 276\n",
      "    loss           : -303.76210578918455\n",
      "    val_loss       : -309.3886645378545\n",
      "    val_log_likelihood: 429.5929473876953\n",
      "    val_log_marginal: 316.69092668406665\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -194.301239\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -298.739197\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -292.632874\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -329.818481\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -137.698212\n",
      "    epoch          : 277\n",
      "    loss           : -280.4303643798828\n",
      "    val_loss       : -261.50061950515953\n",
      "    val_log_likelihood: 406.7665618896484\n",
      "    val_log_marginal: 270.3464218918234\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -326.484650\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -335.540161\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -295.263916\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -49.664803\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -338.363831\n",
      "    epoch          : 278\n",
      "    loss           : -291.2643125152588\n",
      "    val_loss       : -315.1368126695976\n",
      "    val_log_likelihood: 426.5507537841797\n",
      "    val_log_marginal: 321.9027878087014\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -360.098969\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -374.309723\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -366.728699\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -309.523926\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -185.400299\n",
      "    epoch          : 279\n",
      "    loss           : -346.77745193481445\n",
      "    val_loss       : -320.9773823120631\n",
      "    val_log_likelihood: 436.6574432373047\n",
      "    val_log_marginal: 325.7413067147843\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -367.717041\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -366.835876\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -460.205750\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -135.195984\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -291.834167\n",
      "    epoch          : 280\n",
      "    loss           : -330.44533081054686\n",
      "    val_loss       : -283.9923662379384\n",
      "    val_log_likelihood: 431.9499114990234\n",
      "    val_log_marginal: 292.31279686801133\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch280.pth ...\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -397.287933\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -365.661469\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -372.818787\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -205.548874\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -398.103912\n",
      "    epoch          : 281\n",
      "    loss           : -349.6455244445801\n",
      "    val_loss       : -336.9973083209246\n",
      "    val_log_likelihood: 440.0897583007812\n",
      "    val_log_marginal: 341.381917803362\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -397.899506\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -219.908768\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -395.108826\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -349.067993\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -351.835266\n",
      "    epoch          : 282\n",
      "    loss           : -342.23966751098635\n",
      "    val_loss       : -289.5686349993572\n",
      "    val_log_likelihood: 427.65754852294924\n",
      "    val_log_marginal: 294.89675450064243\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -213.202332\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -157.368408\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -451.499023\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -438.865082\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -114.342514\n",
      "    epoch          : 283\n",
      "    loss           : -282.6821598625183\n",
      "    val_loss       : -234.6430237814784\n",
      "    val_log_likelihood: 411.32435302734376\n",
      "    val_log_marginal: 244.8154918681831\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -273.549805\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -355.353760\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -379.928833\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -377.946106\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -374.304291\n",
      "    epoch          : 284\n",
      "    loss           : -340.5627997589111\n",
      "    val_loss       : -334.5303838448599\n",
      "    val_log_likelihood: 441.8102081298828\n",
      "    val_log_marginal: 340.76244620122014\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -232.274231\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -378.402740\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -389.260803\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -480.874695\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -228.833496\n",
      "    epoch          : 285\n",
      "    loss           : -360.06235473632813\n",
      "    val_loss       : -337.89124809298664\n",
      "    val_log_likelihood: 444.58876037597656\n",
      "    val_log_marginal: 343.74747682550026\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -403.614594\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -376.624939\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -399.517761\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -332.757874\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -352.367065\n",
      "    epoch          : 286\n",
      "    loss           : -305.3427691268921\n",
      "    val_loss       : -283.14535888750106\n",
      "    val_log_likelihood: 437.62941589355466\n",
      "    val_log_marginal: 288.658472975716\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -146.332581\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -352.974670\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -336.562653\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -369.415100\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -387.293640\n",
      "    epoch          : 287\n",
      "    loss           : -321.11409591674806\n",
      "    val_loss       : -314.09391537522896\n",
      "    val_log_likelihood: 441.93337097167966\n",
      "    val_log_marginal: 323.60467632226647\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -333.468811\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -375.744476\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -394.290222\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -331.085449\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -290.117767\n",
      "    epoch          : 288\n",
      "    loss           : -319.4437089538574\n",
      "    val_loss       : -271.1342182770371\n",
      "    val_log_likelihood: 411.71472320556643\n",
      "    val_log_marginal: 274.9655754264444\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -307.007751\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -135.020859\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -138.755920\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -377.923615\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -319.865387\n",
      "    epoch          : 289\n",
      "    loss           : -298.1536199951172\n",
      "    val_loss       : -330.82670600404964\n",
      "    val_log_likelihood: 435.35158386230466\n",
      "    val_log_marginal: 335.86890748851\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -331.297089\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -375.911285\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -401.811035\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -385.050781\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -222.239182\n",
      "    epoch          : 290\n",
      "    loss           : -356.1469316101074\n",
      "    val_loss       : -333.79299679659306\n",
      "    val_log_likelihood: 444.8301116943359\n",
      "    val_log_marginal: 339.68715939931576\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -491.700073\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -485.421936\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -416.728699\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -380.429321\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -389.786591\n",
      "    epoch          : 291\n",
      "    loss           : -358.59328567504883\n",
      "    val_loss       : -323.2960457839072\n",
      "    val_log_likelihood: 448.1861511230469\n",
      "    val_log_marginal: 334.307247357443\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -401.747742\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -184.651581\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -115.973091\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -205.845276\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -277.973694\n",
      "    epoch          : 292\n",
      "    loss           : -307.72293144226074\n",
      "    val_loss       : -274.2765835043043\n",
      "    val_log_likelihood: 433.50828857421874\n",
      "    val_log_marginal: 290.1617488120901\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -316.122101\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -334.651642\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -342.525909\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -275.139191\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -273.652832\n",
      "    epoch          : 293\n",
      "    loss           : -293.63801133155823\n",
      "    val_loss       : -289.0875984046608\n",
      "    val_log_likelihood: 431.8736511230469\n",
      "    val_log_marginal: 301.06783417500554\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -213.654541\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -323.318970\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -408.597534\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -346.330444\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -286.019897\n",
      "    epoch          : 294\n",
      "    loss           : -321.8799537658691\n",
      "    val_loss       : -298.9532111885026\n",
      "    val_log_likelihood: 434.84735107421875\n",
      "    val_log_marginal: 310.76284612454475\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -407.110443\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -390.140747\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -352.642487\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -405.722839\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -396.578064\n",
      "    epoch          : 295\n",
      "    loss           : -334.42374015808105\n",
      "    val_loss       : -343.67404015343635\n",
      "    val_log_likelihood: 449.27631225585935\n",
      "    val_log_marginal: 349.8256766784944\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -392.217102\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -476.733795\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -331.175293\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -417.625610\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -355.351562\n",
      "    epoch          : 296\n",
      "    loss           : -359.8269970703125\n",
      "    val_loss       : -334.51454611662774\n",
      "    val_log_likelihood: 451.09498901367186\n",
      "    val_log_marginal: 345.23190876208247\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -391.901398\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -492.960999\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -314.293579\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -100.232361\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -323.865540\n",
      "    epoch          : 297\n",
      "    loss           : -319.09051498413083\n",
      "    val_loss       : -302.38086548112335\n",
      "    val_log_likelihood: 431.02543182373046\n",
      "    val_log_marginal: 313.3168686500781\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -191.181427\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -402.464905\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -381.115448\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -341.829742\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -343.267090\n",
      "    epoch          : 298\n",
      "    loss           : -314.4004787445068\n",
      "    val_loss       : -293.6079357530922\n",
      "    val_log_likelihood: 440.9475463867187\n",
      "    val_log_marginal: 318.00707039572296\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -396.379395\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -384.993439\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -308.841919\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -396.928162\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -342.928894\n",
      "    epoch          : 299\n",
      "    loss           : -303.72967311859134\n",
      "    val_loss       : -293.18222131729124\n",
      "    val_log_likelihood: 434.21217346191406\n",
      "    val_log_marginal: 302.4595353666693\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -331.710388\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -452.253510\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -123.965111\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -469.198059\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -392.175171\n",
      "    epoch          : 300\n",
      "    loss           : -308.89805849075316\n",
      "    val_loss       : -335.2518842831254\n",
      "    val_log_likelihood: 444.7235382080078\n",
      "    val_log_marginal: 341.23003461547194\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -193.515152\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -186.430084\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -473.579681\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -415.219482\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -382.828125\n",
      "    epoch          : 301\n",
      "    loss           : -361.40459854125976\n",
      "    val_loss       : -339.8800862001255\n",
      "    val_log_likelihood: 448.8266235351563\n",
      "    val_log_marginal: 344.10249414332213\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -430.068420\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -203.241180\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -384.604797\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -395.671661\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -312.896179\n",
      "    epoch          : 302\n",
      "    loss           : -359.5872850036621\n",
      "    val_loss       : -333.6329466000199\n",
      "    val_log_likelihood: 445.064892578125\n",
      "    val_log_marginal: 339.51952614597974\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -372.164917\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -200.206711\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -215.156693\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -219.378922\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -494.489227\n",
      "    epoch          : 303\n",
      "    loss           : -367.3107473754883\n",
      "    val_loss       : -339.3020493265241\n",
      "    val_log_likelihood: 454.1820861816406\n",
      "    val_log_marginal: 345.2179365135729\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -221.804718\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -371.265137\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -347.830322\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -342.760681\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -231.494354\n",
      "    epoch          : 304\n",
      "    loss           : -356.4167263793945\n",
      "    val_loss       : -325.5410801828839\n",
      "    val_log_likelihood: 449.0569732666016\n",
      "    val_log_marginal: 335.073603690401\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -207.900269\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -359.126099\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -387.979950\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -332.378906\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -155.061401\n",
      "    epoch          : 305\n",
      "    loss           : -305.99135239601134\n",
      "    val_loss       : -256.0417462969199\n",
      "    val_log_likelihood: 427.70052337646484\n",
      "    val_log_marginal: 271.231688535586\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -366.273071\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -215.502502\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -283.699799\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -299.322296\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -123.713158\n",
      "    epoch          : 306\n",
      "    loss           : -228.4110468673706\n",
      "    val_loss       : -241.11233360450714\n",
      "    val_log_likelihood: 419.37696838378906\n",
      "    val_log_marginal: 260.04247098825874\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -237.858612\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -151.582809\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -357.348480\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -365.306152\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -317.998718\n",
      "    epoch          : 307\n",
      "    loss           : -309.29993932724\n",
      "    val_loss       : -311.1832094239071\n",
      "    val_log_likelihood: 440.87046203613284\n",
      "    val_log_marginal: 324.1100985508412\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -373.864441\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -323.459717\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -400.085083\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -395.045166\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -184.231705\n",
      "    epoch          : 308\n",
      "    loss           : -354.7857264709473\n",
      "    val_loss       : -335.179255114682\n",
      "    val_log_likelihood: 446.35007629394534\n",
      "    val_log_marginal: 342.7539613265544\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -385.120544\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -390.352936\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -471.529114\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -213.380096\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -171.111740\n",
      "    epoch          : 309\n",
      "    loss           : -345.45209091186524\n",
      "    val_loss       : -298.6967026268132\n",
      "    val_log_likelihood: 444.5225555419922\n",
      "    val_log_marginal: 310.828652362426\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -376.982239\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -334.428955\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -265.772125\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: 24.383430\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -123.039833\n",
      "    epoch          : 310\n",
      "    loss           : -251.70116722106934\n",
      "    val_loss       : -265.7729926784523\n",
      "    val_log_likelihood: 414.46617126464844\n",
      "    val_log_marginal: 273.7533977594227\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -163.507477\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -380.315186\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -176.477753\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -400.757751\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -392.515930\n",
      "    epoch          : 311\n",
      "    loss           : -347.24111892700193\n",
      "    val_loss       : -347.71662490349263\n",
      "    val_log_likelihood: 453.66290893554685\n",
      "    val_log_marginal: 354.5018281485885\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -412.279053\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -352.253082\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -401.694550\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -356.826630\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -359.263000\n",
      "    epoch          : 312\n",
      "    loss           : -374.81491821289063\n",
      "    val_loss       : -353.34013384915886\n",
      "    val_log_likelihood: 457.2903228759766\n",
      "    val_log_marginal: 358.84234930835663\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -368.660950\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -387.887817\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -423.610992\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -412.020721\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -413.472534\n",
      "    epoch          : 313\n",
      "    loss           : -373.96739288330076\n",
      "    val_loss       : -342.55068083088844\n",
      "    val_log_likelihood: 459.8766632080078\n",
      "    val_log_marginal: 350.8974454831332\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -335.483276\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -207.125961\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -466.952515\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -382.463196\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -394.316589\n",
      "    epoch          : 314\n",
      "    loss           : -360.35844268798826\n",
      "    val_loss       : -327.05968853663654\n",
      "    val_log_likelihood: 447.95396118164064\n",
      "    val_log_marginal: 333.52658172883093\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -391.936401\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -200.864426\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -473.792816\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -291.890656\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -322.488953\n",
      "    epoch          : 315\n",
      "    loss           : -335.08110694885255\n",
      "    val_loss       : -330.552723518759\n",
      "    val_log_likelihood: 448.8765075683594\n",
      "    val_log_marginal: 342.5110721826553\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -436.570953\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -199.831116\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -498.847961\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -489.801727\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -224.953766\n",
      "    epoch          : 316\n",
      "    loss           : -363.96525604248046\n",
      "    val_loss       : -335.48133968319746\n",
      "    val_log_likelihood: 455.2915496826172\n",
      "    val_log_marginal: 344.05931442193685\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -193.360748\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -412.937775\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -348.708618\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -310.630432\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -368.557465\n",
      "    epoch          : 317\n",
      "    loss           : -317.8092890930176\n",
      "    val_loss       : -283.2211619989015\n",
      "    val_log_likelihood: 439.73070068359374\n",
      "    val_log_marginal: 293.47519898561035\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -308.821167\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -315.136108\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -326.656311\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -325.810120\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -306.586975\n",
      "    epoch          : 318\n",
      "    loss           : -311.94689079284666\n",
      "    val_loss       : -312.1734527510591\n",
      "    val_log_likelihood: 447.235107421875\n",
      "    val_log_marginal: 320.4154167573899\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -356.895905\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -311.638763\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -353.006165\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -149.826035\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -365.984131\n",
      "    epoch          : 319\n",
      "    loss           : -292.0371449661255\n",
      "    val_loss       : -218.48134256638588\n",
      "    val_log_likelihood: 439.43702697753906\n",
      "    val_log_marginal: 224.27651917003163\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -146.013260\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -354.998901\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -305.158936\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -256.446320\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -304.885376\n",
      "    epoch          : 320\n",
      "    loss           : -299.8023023223877\n",
      "    val_loss       : -304.84813959412276\n",
      "    val_log_likelihood: 437.9245361328125\n",
      "    val_log_marginal: 316.3161317985505\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch320.pth ...\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -392.062408\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -182.070770\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -390.529602\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -374.549591\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -495.309235\n",
      "    epoch          : 321\n",
      "    loss           : -345.5495878601074\n",
      "    val_loss       : -331.99139764662834\n",
      "    val_log_likelihood: 445.62465515136716\n",
      "    val_log_marginal: 340.21581836007533\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -370.605133\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -424.721558\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -386.339294\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -215.879379\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -238.603424\n",
      "    epoch          : 322\n",
      "    loss           : -358.7144338989258\n",
      "    val_loss       : -344.947778715007\n",
      "    val_log_likelihood: 453.02805786132814\n",
      "    val_log_marginal: 353.3390182826668\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -212.925629\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -400.874634\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -418.562347\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -318.609375\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -204.750534\n",
      "    epoch          : 323\n",
      "    loss           : -368.797568359375\n",
      "    val_loss       : -344.50298213399947\n",
      "    val_log_likelihood: 454.3937683105469\n",
      "    val_log_marginal: 350.7776066813618\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -193.888885\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -232.020355\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -418.716949\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -398.379669\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -406.287262\n",
      "    epoch          : 324\n",
      "    loss           : -370.57392593383787\n",
      "    val_loss       : -340.43163325302305\n",
      "    val_log_likelihood: 460.24708557128906\n",
      "    val_log_marginal: 351.17251986004413\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -412.096436\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -189.354401\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -226.105789\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -419.164154\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -230.098694\n",
      "    epoch          : 325\n",
      "    loss           : -371.85378997802735\n",
      "    val_loss       : -352.3904832523316\n",
      "    val_log_likelihood: 465.142919921875\n",
      "    val_log_marginal: 357.9922641742975\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -201.091034\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -423.874817\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -401.440460\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -360.785858\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -376.271393\n",
      "    epoch          : 326\n",
      "    loss           : -372.9588160705566\n",
      "    val_loss       : -335.95739711169153\n",
      "    val_log_likelihood: 464.87159729003906\n",
      "    val_log_marginal: 341.2823226328939\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -370.486267\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -488.900909\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -365.633453\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -322.121552\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -336.670807\n",
      "    epoch          : 327\n",
      "    loss           : -324.3942126083374\n",
      "    val_loss       : -275.8527774235234\n",
      "    val_log_likelihood: 442.5248092651367\n",
      "    val_log_marginal: 302.7087498583129\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -127.190308\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -277.318787\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -329.294312\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -301.705383\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -314.928406\n",
      "    epoch          : 328\n",
      "    loss           : -282.2375874042511\n",
      "    val_loss       : -286.5526752926409\n",
      "    val_log_likelihood: 445.59539184570315\n",
      "    val_log_marginal: 297.61379142291844\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -296.191223\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -198.522461\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -148.345398\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -363.998077\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -316.547546\n",
      "    epoch          : 329\n",
      "    loss           : -323.989854888916\n",
      "    val_loss       : -304.26244032867254\n",
      "    val_log_likelihood: 445.0359832763672\n",
      "    val_log_marginal: 311.4615315999836\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -409.123413\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -260.688538\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -309.613037\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -406.515259\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -360.566406\n",
      "    epoch          : 330\n",
      "    loss           : -283.9673160171509\n",
      "    val_loss       : -282.80129711329937\n",
      "    val_log_likelihood: 435.5249816894531\n",
      "    val_log_marginal: 289.7255940269679\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -324.434723\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -334.399414\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -394.990723\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -291.083099\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -390.552124\n",
      "    epoch          : 331\n",
      "    loss           : -338.8854621887207\n",
      "    val_loss       : -343.85415276139975\n",
      "    val_log_likelihood: 452.826220703125\n",
      "    val_log_marginal: 351.2818218071014\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -187.695679\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -389.717468\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -501.975555\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -322.276154\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -192.924652\n",
      "    epoch          : 332\n",
      "    loss           : -358.1558576965332\n",
      "    val_loss       : -324.24732811208816\n",
      "    val_log_likelihood: 450.3335906982422\n",
      "    val_log_marginal: 329.63553247116505\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -443.843506\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -482.639954\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -394.282227\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -357.200134\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -373.355011\n",
      "    epoch          : 333\n",
      "    loss           : -352.9538801574707\n",
      "    val_loss       : -311.2259872244671\n",
      "    val_log_likelihood: 456.5138793945313\n",
      "    val_log_marginal: 322.01928654573857\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -442.320709\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -413.396973\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -197.741348\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -304.731873\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -363.194366\n",
      "    epoch          : 334\n",
      "    loss           : -336.9416015625\n",
      "    val_loss       : -317.1505173983052\n",
      "    val_log_likelihood: 454.0000701904297\n",
      "    val_log_marginal: 325.55956920050085\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -478.747162\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -127.412552\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -371.449585\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -399.640961\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -266.759277\n",
      "    epoch          : 335\n",
      "    loss           : -307.75296920776367\n",
      "    val_loss       : -332.8688359981403\n",
      "    val_log_likelihood: 457.6799621582031\n",
      "    val_log_marginal: 338.3812833552842\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -391.861389\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -390.486328\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -405.746826\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -420.889771\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -376.434753\n",
      "    epoch          : 336\n",
      "    loss           : -371.5688674926758\n",
      "    val_loss       : -350.34912607651205\n",
      "    val_log_likelihood: 465.94643249511716\n",
      "    val_log_marginal: 359.71039803884923\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -409.548035\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -413.979309\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -431.479065\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -382.632019\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -403.553101\n",
      "    epoch          : 337\n",
      "    loss           : -367.359246673584\n",
      "    val_loss       : -343.27384081874044\n",
      "    val_log_likelihood: 463.4849792480469\n",
      "    val_log_marginal: 348.39089458547534\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -196.053864\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -392.095459\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -478.844910\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -357.728302\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -115.275597\n",
      "    epoch          : 338\n",
      "    loss           : -345.43085525512697\n",
      "    val_loss       : -189.49133657570928\n",
      "    val_log_likelihood: 432.69885711669923\n",
      "    val_log_marginal: 204.64657133854925\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -192.541412\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -189.034119\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: 283.560638\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: 72.870140\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -199.454163\n",
      "    epoch          : 339\n",
      "    loss           : -128.30061103820802\n",
      "    val_loss       : -180.58286112230272\n",
      "    val_log_likelihood: 413.79087982177737\n",
      "    val_log_marginal: 206.7218407701701\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -60.761452\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -228.877045\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -336.765930\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -192.899414\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -216.138824\n",
      "    epoch          : 340\n",
      "    loss           : -316.46372497558593\n",
      "    val_loss       : -333.42173161888496\n",
      "    val_log_likelihood: 451.60234375\n",
      "    val_log_marginal: 351.13121184879344\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch340.pth ...\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -493.041229\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -398.393494\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -369.036255\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -216.024338\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -408.477356\n",
      "    epoch          : 341\n",
      "    loss           : -373.05311309814454\n",
      "    val_loss       : -357.2148212512955\n",
      "    val_log_likelihood: 459.7321411132813\n",
      "    val_log_marginal: 362.7673734690994\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -399.676056\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -246.367264\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -215.041122\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -386.539490\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -388.719940\n",
      "    epoch          : 342\n",
      "    loss           : -379.12123931884764\n",
      "    val_loss       : -346.8952772837132\n",
      "    val_log_likelihood: 459.6522674560547\n",
      "    val_log_marginal: 356.7922698136419\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -422.955353\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -404.629211\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -402.563873\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -402.851288\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -366.260986\n",
      "    epoch          : 343\n",
      "    loss           : -381.78331741333005\n",
      "    val_loss       : -358.25890881177037\n",
      "    val_log_likelihood: 462.5364715576172\n",
      "    val_log_marginal: 362.6772495750338\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -400.667938\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -365.473572\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -417.584839\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -334.793091\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -407.400116\n",
      "    epoch          : 344\n",
      "    loss           : -375.4173513793945\n",
      "    val_loss       : -315.97654959019275\n",
      "    val_log_likelihood: 465.998193359375\n",
      "    val_log_marginal: 322.6207989472896\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -348.579468\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -319.520691\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -198.477234\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -337.084198\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -272.858490\n",
      "    epoch          : 345\n",
      "    loss           : -311.98935150146485\n",
      "    val_loss       : -290.4981286395341\n",
      "    val_log_likelihood: 448.96228942871096\n",
      "    val_log_marginal: 298.3600617666636\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -325.909485\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -393.743652\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -470.339355\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -359.269226\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -282.508514\n",
      "    epoch          : 346\n",
      "    loss           : -336.1697372436523\n",
      "    val_loss       : -306.2450816631317\n",
      "    val_log_likelihood: 438.24620513916017\n",
      "    val_log_marginal: 316.4175935272127\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -211.183945\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -339.159973\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: 44.833641\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -362.673859\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -327.540283\n",
      "    epoch          : 347\n",
      "    loss           : -268.4383960771561\n",
      "    val_loss       : -306.21882200278344\n",
      "    val_log_likelihood: 444.1509384155273\n",
      "    val_log_marginal: 312.89906773159817\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -376.767700\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -408.105927\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -428.380127\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -417.815063\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -178.181122\n",
      "    epoch          : 348\n",
      "    loss           : -339.3755068206787\n",
      "    val_loss       : -209.7817867686972\n",
      "    val_log_likelihood: 453.84983215332034\n",
      "    val_log_marginal: 222.57577046640714\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -226.917999\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -97.427696\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -117.811043\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -261.689697\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -371.540710\n",
      "    epoch          : 349\n",
      "    loss           : -277.2391807937622\n",
      "    val_loss       : -341.10375257879497\n",
      "    val_log_likelihood: 453.37030029296875\n",
      "    val_log_marginal: 349.9817948948592\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -436.635498\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -215.017166\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -392.344299\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -411.823853\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -255.241043\n",
      "    epoch          : 350\n",
      "    loss           : -382.3062399291992\n",
      "    val_loss       : -360.4766283125617\n",
      "    val_log_likelihood: 466.6200714111328\n",
      "    val_log_marginal: 367.58764873705866\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -256.534973\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -409.878601\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -500.171631\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -392.779266\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -476.302490\n",
      "    epoch          : 351\n",
      "    loss           : -374.8296943664551\n",
      "    val_loss       : -330.94484974015506\n",
      "    val_log_likelihood: 460.35374450683594\n",
      "    val_log_marginal: 338.74543511979283\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -307.627533\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -209.656326\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -439.101044\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -511.468353\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -356.487366\n",
      "    epoch          : 352\n",
      "    loss           : -366.0381170654297\n",
      "    val_loss       : -345.36265397435056\n",
      "    val_log_likelihood: 457.60093994140624\n",
      "    val_log_marginal: 350.2493598219007\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -262.291809\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -426.922485\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -409.311005\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -452.253052\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -396.867737\n",
      "    epoch          : 353\n",
      "    loss           : -359.65000579833986\n",
      "    val_loss       : -336.1783868556842\n",
      "    val_log_likelihood: 459.84019165039064\n",
      "    val_log_marginal: 344.48343970812857\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -245.847641\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -250.987305\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -426.853180\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -402.425446\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -422.444763\n",
      "    epoch          : 354\n",
      "    loss           : -380.5696659851074\n",
      "    val_loss       : -349.8889155793935\n",
      "    val_log_likelihood: 468.5785247802734\n",
      "    val_log_marginal: 358.4250302437693\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -432.869019\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -520.894958\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -441.797119\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -352.771820\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -253.260773\n",
      "    epoch          : 355\n",
      "    loss           : -382.78182342529294\n",
      "    val_loss       : -353.71363852880893\n",
      "    val_log_likelihood: 473.3677917480469\n",
      "    val_log_marginal: 360.4752863626927\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -376.575104\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -205.611191\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -408.174438\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -425.747375\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -391.547852\n",
      "    epoch          : 356\n",
      "    loss           : -381.30295715332034\n",
      "    val_loss       : -317.08548491392287\n",
      "    val_log_likelihood: 460.8209930419922\n",
      "    val_log_marginal: 324.1244595613342\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -240.545105\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -305.549164\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -353.529449\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -403.479736\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -402.269592\n",
      "    epoch          : 357\n",
      "    loss           : -371.40644134521483\n",
      "    val_loss       : -349.53081525862217\n",
      "    val_log_likelihood: 473.35504760742185\n",
      "    val_log_marginal: 358.6089787017554\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -394.450928\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -364.325775\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -227.293152\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -442.834229\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -350.679626\n",
      "    epoch          : 358\n",
      "    loss           : -380.52060363769533\n",
      "    val_loss       : -350.6730802208185\n",
      "    val_log_likelihood: 476.3472381591797\n",
      "    val_log_marginal: 360.33804753758017\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -220.722031\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -187.614471\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -248.218628\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -385.494995\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -224.338348\n",
      "    epoch          : 359\n",
      "    loss           : -362.20632598876955\n",
      "    val_loss       : -299.27439728826283\n",
      "    val_log_likelihood: 464.8131439208984\n",
      "    val_log_marginal: 311.11263230480256\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -319.928589\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -481.544434\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -287.214722\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -185.450897\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -104.101387\n",
      "    epoch          : 360\n",
      "    loss           : -271.6417377948761\n",
      "    val_loss       : -238.01967706298456\n",
      "    val_log_likelihood: 445.6119018554688\n",
      "    val_log_marginal: 251.69671063013374\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch360.pth ...\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -26.039749\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -262.726654\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -258.557129\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -478.803650\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -216.850021\n",
      "    epoch          : 361\n",
      "    loss           : -272.94466280937195\n",
      "    val_loss       : -331.69624347593634\n",
      "    val_log_likelihood: 453.17035522460935\n",
      "    val_log_marginal: 342.05537591464815\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -407.699036\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -207.730362\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -174.881607\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -79.559807\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -305.676666\n",
      "    epoch          : 362\n",
      "    loss           : -334.6535132598877\n",
      "    val_loss       : -277.274327016063\n",
      "    val_log_likelihood: 459.5471649169922\n",
      "    val_log_marginal: 296.6672272864729\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -263.810211\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -359.553833\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -477.295715\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -396.462830\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -413.652924\n",
      "    epoch          : 363\n",
      "    loss           : -339.36724113464356\n",
      "    val_loss       : -354.93532935660335\n",
      "    val_log_likelihood: 465.71405334472655\n",
      "    val_log_marginal: 360.7720015805215\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -436.349304\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -416.633972\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -438.017426\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -408.685486\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -376.558289\n",
      "    epoch          : 364\n",
      "    loss           : -385.92774124145507\n",
      "    val_loss       : -360.8393188992515\n",
      "    val_log_likelihood: 474.59132080078126\n",
      "    val_log_marginal: 365.80275286994873\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -398.449799\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -439.984192\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -397.798157\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -230.949814\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -374.589233\n",
      "    epoch          : 365\n",
      "    loss           : -360.3654273223877\n",
      "    val_loss       : -331.1353200495243\n",
      "    val_log_likelihood: 462.7241180419922\n",
      "    val_log_marginal: 336.8161670062691\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -139.697449\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -242.415482\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -453.102783\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -521.230957\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -397.133209\n",
      "    epoch          : 366\n",
      "    loss           : -371.69468811035154\n",
      "    val_loss       : -360.5115097271279\n",
      "    val_log_likelihood: 470.66404418945314\n",
      "    val_log_marginal: 364.80663566030563\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -434.706451\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -525.866150\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -416.215820\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -452.447876\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -437.365021\n",
      "    epoch          : 367\n",
      "    loss           : -390.17109085083007\n",
      "    val_loss       : -361.8459601473063\n",
      "    val_log_likelihood: 478.0054656982422\n",
      "    val_log_marginal: 369.8230639929129\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -409.772064\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -450.474762\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -394.822723\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -393.789917\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -426.180023\n",
      "    epoch          : 368\n",
      "    loss           : -373.79511825561525\n",
      "    val_loss       : -319.7861553983763\n",
      "    val_log_likelihood: 470.36452026367186\n",
      "    val_log_marginal: 329.3438976768404\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -376.474548\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -388.503540\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -426.419739\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -176.438232\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -384.618774\n",
      "    epoch          : 369\n",
      "    loss           : -361.8905355834961\n",
      "    val_loss       : -335.28094254843893\n",
      "    val_log_likelihood: 473.0681396484375\n",
      "    val_log_marginal: 341.45478640682995\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -349.146912\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -395.920380\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -377.282013\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -310.317261\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -395.786591\n",
      "    epoch          : 370\n",
      "    loss           : -345.4527498626709\n",
      "    val_loss       : -335.31048804130404\n",
      "    val_log_likelihood: 467.06267700195315\n",
      "    val_log_marginal: 346.30980372168125\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -413.344879\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -258.767334\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -203.789368\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -353.235992\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -254.567169\n",
      "    epoch          : 371\n",
      "    loss           : -371.8500225830078\n",
      "    val_loss       : -342.9225595664233\n",
      "    val_log_likelihood: 467.76654663085935\n",
      "    val_log_marginal: 350.49992733187975\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -438.479797\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -401.946960\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -386.892365\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -427.809967\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -217.213547\n",
      "    epoch          : 372\n",
      "    loss           : -365.43382720947267\n",
      "    val_loss       : -288.66404560636727\n",
      "    val_log_likelihood: 464.47998962402346\n",
      "    val_log_marginal: 299.88554005511105\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -371.579163\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -89.740677\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -469.919342\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -352.701782\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -376.952820\n",
      "    epoch          : 373\n",
      "    loss           : -312.8821000289917\n",
      "    val_loss       : -297.1403590627946\n",
      "    val_log_likelihood: 463.8806488037109\n",
      "    val_log_marginal: 307.2829866427928\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -350.149536\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -377.867554\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -272.969025\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -292.574799\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -96.441666\n",
      "    epoch          : 374\n",
      "    loss           : -229.21502594947816\n",
      "    val_loss       : -208.16843568254262\n",
      "    val_log_likelihood: 429.02759246826173\n",
      "    val_log_marginal: 224.65774301849305\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -348.269897\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -351.949371\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -153.920853\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -285.819366\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -394.155090\n",
      "    epoch          : 375\n",
      "    loss           : -291.6649118041992\n",
      "    val_loss       : -315.06918379273264\n",
      "    val_log_likelihood: 463.77940063476564\n",
      "    val_log_marginal: 328.940215006946\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -397.583374\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -409.535156\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -508.382843\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -249.591446\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -405.704529\n",
      "    epoch          : 376\n",
      "    loss           : -367.08890365600587\n",
      "    val_loss       : -362.5051517095417\n",
      "    val_log_likelihood: 470.8165771484375\n",
      "    val_log_marginal: 367.5084996975178\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -420.947571\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -411.563751\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -413.216248\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -412.598053\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -231.695923\n",
      "    epoch          : 377\n",
      "    loss           : -384.5924688720703\n",
      "    val_loss       : -351.1885738806799\n",
      "    val_log_likelihood: 468.89149780273436\n",
      "    val_log_marginal: 360.2240425851196\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -429.576569\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -254.944168\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -243.566330\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -242.241776\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -425.787354\n",
      "    epoch          : 378\n",
      "    loss           : -386.71564071655274\n",
      "    val_loss       : -358.7447264321148\n",
      "    val_log_likelihood: 470.59523315429686\n",
      "    val_log_marginal: 367.3509726794717\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -420.798370\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -399.773010\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -203.897095\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -369.145630\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -437.758698\n",
      "    epoch          : 379\n",
      "    loss           : -384.53692138671875\n",
      "    val_loss       : -364.97217050995675\n",
      "    val_log_likelihood: 476.7497039794922\n",
      "    val_log_marginal: 370.0536601581397\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -225.636932\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -411.616577\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -374.513611\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -411.170776\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -384.264374\n",
      "    epoch          : 380\n",
      "    loss           : -385.15743209838865\n",
      "    val_loss       : -353.62813398391006\n",
      "    val_log_likelihood: 480.93397521972656\n",
      "    val_log_marginal: 361.64013202004134\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch380.pth ...\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -424.146484\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -222.495972\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -423.149689\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -417.319702\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -412.308075\n",
      "    epoch          : 381\n",
      "    loss           : -374.4740844726563\n",
      "    val_loss       : -342.50442300159483\n",
      "    val_log_likelihood: 473.26910095214845\n",
      "    val_log_marginal: 348.62502457387757\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -428.006317\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -193.888779\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -405.759460\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -465.110687\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -316.862396\n",
      "    epoch          : 382\n",
      "    loss           : -355.0510356140137\n",
      "    val_loss       : -342.72028675619515\n",
      "    val_log_likelihood: 471.5768585205078\n",
      "    val_log_marginal: 354.8877397125545\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -193.123444\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -424.733337\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -377.568756\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -397.160645\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -372.724518\n",
      "    epoch          : 383\n",
      "    loss           : -364.98427841186526\n",
      "    val_loss       : -311.6204350573942\n",
      "    val_log_likelihood: 467.64147033691404\n",
      "    val_log_marginal: 321.50363177470865\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -398.442017\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -335.955200\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -312.694153\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -410.675201\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -409.643585\n",
      "    epoch          : 384\n",
      "    loss           : -339.17911575317385\n",
      "    val_loss       : -354.6104613877833\n",
      "    val_log_likelihood: 467.44078674316404\n",
      "    val_log_marginal: 359.5529870849103\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -426.200897\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -525.668823\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -355.294678\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -251.140198\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -359.788025\n",
      "    epoch          : 385\n",
      "    loss           : -387.90899780273435\n",
      "    val_loss       : -362.58494306877253\n",
      "    val_log_likelihood: 477.15406188964846\n",
      "    val_log_marginal: 368.2902099746599\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -208.665726\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -459.959961\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -407.541565\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -361.305603\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -418.484283\n",
      "    epoch          : 386\n",
      "    loss           : -397.5772399902344\n",
      "    val_loss       : -365.16458906643095\n",
      "    val_log_likelihood: 486.08209838867185\n",
      "    val_log_marginal: 370.5750860210508\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -464.248566\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -430.828613\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -428.787720\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -483.489197\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -338.204376\n",
      "    epoch          : 387\n",
      "    loss           : -357.29010139465333\n",
      "    val_loss       : -268.87133304029703\n",
      "    val_log_likelihood: 465.78879699707034\n",
      "    val_log_marginal: 276.2767032954842\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -359.315430\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -184.635223\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -512.747803\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -241.800446\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -235.980133\n",
      "    epoch          : 388\n",
      "    loss           : -330.4765855407715\n",
      "    val_loss       : -348.79424558822063\n",
      "    val_log_likelihood: 473.0386169433594\n",
      "    val_log_marginal: 357.00607348047197\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -513.110840\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -527.665588\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -516.401855\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -288.703735\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -407.243805\n",
      "    epoch          : 389\n",
      "    loss           : -370.06396892547605\n",
      "    val_loss       : -307.79176698215304\n",
      "    val_log_likelihood: 461.5262084960938\n",
      "    val_log_marginal: 323.5770514998585\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -404.278381\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -421.315826\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -404.127136\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -421.776062\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -428.443848\n",
      "    epoch          : 390\n",
      "    loss           : -358.97476303100586\n",
      "    val_loss       : -355.5957397771999\n",
      "    val_log_likelihood: 475.4249969482422\n",
      "    val_log_marginal: 362.09750056412093\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -233.644623\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -215.852341\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -177.348282\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -374.224670\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -390.791321\n",
      "    epoch          : 391\n",
      "    loss           : -325.8004274368286\n",
      "    val_loss       : -355.0728704461828\n",
      "    val_log_likelihood: 478.78233032226564\n",
      "    val_log_marginal: 362.22282268963863\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -406.205963\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -398.662842\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -403.668030\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -416.264923\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -405.361633\n",
      "    epoch          : 392\n",
      "    loss           : -361.6458525085449\n",
      "    val_loss       : -333.7957945022732\n",
      "    val_log_likelihood: 469.6750152587891\n",
      "    val_log_marginal: 348.3598122551794\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -389.711517\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -491.328217\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -182.417450\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -438.069214\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -445.427338\n",
      "    epoch          : 393\n",
      "    loss           : -368.8454563140869\n",
      "    val_loss       : -344.72212360296396\n",
      "    val_log_likelihood: 478.6429748535156\n",
      "    val_log_marginal: 356.06711278080337\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -421.063782\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -378.946259\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -239.064240\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -228.375641\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -358.662537\n",
      "    epoch          : 394\n",
      "    loss           : -370.0167208862305\n",
      "    val_loss       : -361.2895025789738\n",
      "    val_log_likelihood: 478.13067321777345\n",
      "    val_log_marginal: 366.1105338849414\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -434.844238\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -222.432220\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -212.144897\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -429.457062\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -370.926147\n",
      "    epoch          : 395\n",
      "    loss           : -394.74818634033204\n",
      "    val_loss       : -364.5936048872769\n",
      "    val_log_likelihood: 484.0331970214844\n",
      "    val_log_marginal: 372.05564922206105\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -428.440735\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -415.434265\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -444.722321\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -437.219818\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -363.952881\n",
      "    epoch          : 396\n",
      "    loss           : -394.24349685668943\n",
      "    val_loss       : -371.7734174348414\n",
      "    val_log_likelihood: 487.1066619873047\n",
      "    val_log_marginal: 379.2840682711452\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -452.933167\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -533.735596\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -433.429596\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -436.548981\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -172.946136\n",
      "    epoch          : 397\n",
      "    loss           : -382.3060179138184\n",
      "    val_loss       : -310.10581378489735\n",
      "    val_log_likelihood: 472.7635864257812\n",
      "    val_log_marginal: 316.72220221944156\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -459.446716\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -389.642761\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -495.973450\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -392.444275\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -282.241699\n",
      "    epoch          : 398\n",
      "    loss           : -351.8603621673584\n",
      "    val_loss       : -333.1672957254574\n",
      "    val_log_likelihood: 474.65503845214846\n",
      "    val_log_marginal: 341.63964544311006\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -360.440552\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -396.151459\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -393.919678\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -421.474304\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -396.556946\n",
      "    epoch          : 399\n",
      "    loss           : -373.56046600341796\n",
      "    val_loss       : -312.2663506168872\n",
      "    val_log_likelihood: 479.14036865234374\n",
      "    val_log_marginal: 319.56556930579245\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -438.214783\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -363.421265\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -354.163879\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -390.022522\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -338.767090\n",
      "    epoch          : 400\n",
      "    loss           : -331.1347576904297\n",
      "    val_loss       : -329.5895999716595\n",
      "    val_log_likelihood: 469.7788543701172\n",
      "    val_log_marginal: 337.255914510414\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -405.768280\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -506.600067\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -305.323792\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -462.109924\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -280.147308\n",
      "    epoch          : 401\n",
      "    loss           : -342.1483471679687\n",
      "    val_loss       : -337.64374180473385\n",
      "    val_log_likelihood: 469.90894775390626\n",
      "    val_log_marginal: 346.04025188721715\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -223.837341\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -364.421295\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -481.733551\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -388.632477\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -424.522614\n",
      "    epoch          : 402\n",
      "    loss           : -372.20453018188476\n",
      "    val_loss       : -355.1170248910785\n",
      "    val_log_likelihood: 477.7215118408203\n",
      "    val_log_marginal: 362.79944162406025\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -431.793182\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -414.878113\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -440.084167\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -407.385376\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -316.919098\n",
      "    epoch          : 403\n",
      "    loss           : -376.8393977355957\n",
      "    val_loss       : -357.74687614887955\n",
      "    val_log_likelihood: 478.90252075195315\n",
      "    val_log_marginal: 369.7001728374511\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -422.808411\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -352.641785\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -201.373596\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -155.457062\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -396.670990\n",
      "    epoch          : 404\n",
      "    loss           : -362.92391250610353\n",
      "    val_loss       : -338.53575384095313\n",
      "    val_log_likelihood: 476.6073791503906\n",
      "    val_log_marginal: 351.9624654773623\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -504.301239\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -392.576996\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -501.230438\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -162.483124\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -326.293732\n",
      "    epoch          : 405\n",
      "    loss           : -321.98844307899475\n",
      "    val_loss       : -301.6813381560147\n",
      "    val_log_likelihood: 460.7688842773438\n",
      "    val_log_marginal: 311.9820063803345\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -340.974274\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -181.518829\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -407.121796\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -323.515442\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -393.799561\n",
      "    epoch          : 406\n",
      "    loss           : -358.64151359558105\n",
      "    val_loss       : -360.5623044144362\n",
      "    val_log_likelihood: 479.8435974121094\n",
      "    val_log_marginal: 368.75719619505105\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -434.033356\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -372.422852\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -426.478882\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -431.599365\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -211.236603\n",
      "    epoch          : 407\n",
      "    loss           : -390.0164967346191\n",
      "    val_loss       : -368.3981165358797\n",
      "    val_log_likelihood: 486.0148071289062\n",
      "    val_log_marginal: 375.73729484416543\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -440.458649\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -442.040466\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -365.453918\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -442.891235\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -388.712769\n",
      "    epoch          : 408\n",
      "    loss           : -401.62225738525393\n",
      "    val_loss       : -379.1861074401997\n",
      "    val_log_likelihood: 487.3638458251953\n",
      "    val_log_marginal: 383.56396071946585\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -417.833313\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -455.391907\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -238.076263\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -453.536102\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -440.404633\n",
      "    epoch          : 409\n",
      "    loss           : -409.0899691772461\n",
      "    val_loss       : -372.53669576346874\n",
      "    val_log_likelihood: 492.64574279785154\n",
      "    val_log_marginal: 379.3916811225652\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -406.064697\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -448.592285\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -429.371307\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -445.464233\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -74.922623\n",
      "    epoch          : 410\n",
      "    loss           : -345.4456422615051\n",
      "    val_loss       : -308.8141804041341\n",
      "    val_log_likelihood: 458.24488220214846\n",
      "    val_log_marginal: 317.9165758747607\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -375.198059\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -330.254883\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -157.496460\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -278.256897\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -325.052734\n",
      "    epoch          : 411\n",
      "    loss           : -331.56010234832763\n",
      "    val_loss       : -329.6514193570241\n",
      "    val_log_likelihood: 470.07945251464844\n",
      "    val_log_marginal: 337.8749536152929\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -388.582336\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -526.899292\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -335.257477\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -442.467072\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -259.659058\n",
      "    epoch          : 412\n",
      "    loss           : -384.0217007446289\n",
      "    val_loss       : -372.99419038332996\n",
      "    val_log_likelihood: 489.4849914550781\n",
      "    val_log_marginal: 382.26301435634787\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -432.690582\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -469.816193\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -423.899078\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -430.049103\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -428.749512\n",
      "    epoch          : 413\n",
      "    loss           : -406.2245393371582\n",
      "    val_loss       : -378.4346100423485\n",
      "    val_log_likelihood: 490.11798706054685\n",
      "    val_log_marginal: 385.50120881862944\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -470.764709\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -408.516113\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -420.310059\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -545.900269\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -421.417389\n",
      "    epoch          : 414\n",
      "    loss           : -400.5057293701172\n",
      "    val_loss       : -360.1302027492784\n",
      "    val_log_likelihood: 487.2023193359375\n",
      "    val_log_marginal: 364.7146032106132\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -401.239838\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -401.408875\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -405.350830\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -233.710587\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -413.377563\n",
      "    epoch          : 415\n",
      "    loss           : -385.0824269104004\n",
      "    val_loss       : -371.9481729809195\n",
      "    val_log_likelihood: 486.9642272949219\n",
      "    val_log_marginal: 377.11178641878064\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -464.110199\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -400.770691\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -434.578674\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -451.975342\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -241.072296\n",
      "    epoch          : 416\n",
      "    loss           : -389.6138255310059\n",
      "    val_loss       : -333.67104710508136\n",
      "    val_log_likelihood: 469.2743835449219\n",
      "    val_log_marginal: 342.4125389780849\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -442.241180\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -323.092773\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -373.315216\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -366.041046\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -404.059479\n",
      "    epoch          : 417\n",
      "    loss           : -341.2496783447266\n",
      "    val_loss       : -326.6379261761904\n",
      "    val_log_likelihood: 477.67649536132814\n",
      "    val_log_marginal: 335.02325564596345\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -414.603271\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -409.795929\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -351.169800\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -204.930099\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -188.483704\n",
      "    epoch          : 418\n",
      "    loss           : -349.5800679779053\n",
      "    val_loss       : -280.86483156662433\n",
      "    val_log_likelihood: 472.1248779296875\n",
      "    val_log_marginal: 289.4518593620442\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -390.517700\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -497.469055\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -359.255920\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -28.128136\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -277.888794\n",
      "    epoch          : 419\n",
      "    loss           : -288.35193806648255\n",
      "    val_loss       : -245.75350582748652\n",
      "    val_log_likelihood: 452.8820495605469\n",
      "    val_log_marginal: 263.06680363363057\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -185.201431\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -276.100403\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -294.034607\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -383.027802\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -395.504974\n",
      "    epoch          : 420\n",
      "    loss           : -293.1656495285034\n",
      "    val_loss       : -354.3140068491921\n",
      "    val_log_likelihood: 473.8353515625\n",
      "    val_log_marginal: 361.7936305638403\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch420.pth ...\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -426.191956\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -430.476105\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -427.392334\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -401.191742\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -290.667145\n",
      "    epoch          : 421\n",
      "    loss           : -376.4657286071777\n",
      "    val_loss       : -339.86546457391233\n",
      "    val_log_likelihood: 475.29469299316406\n",
      "    val_log_marginal: 349.0109904598445\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -422.209991\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -427.177216\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -371.958984\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -200.356903\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -368.134033\n",
      "    epoch          : 422\n",
      "    loss           : -385.5880209350586\n",
      "    val_loss       : -361.82149882335216\n",
      "    val_log_likelihood: 485.2546447753906\n",
      "    val_log_marginal: 370.4779331173748\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -423.358032\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -371.408325\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -425.090393\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -445.937683\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -445.179626\n",
      "    epoch          : 423\n",
      "    loss           : -396.89433685302737\n",
      "    val_loss       : -373.82934333141895\n",
      "    val_log_likelihood: 484.3228393554688\n",
      "    val_log_marginal: 380.0859913270921\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -248.151718\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -467.010406\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -456.589294\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -547.415894\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -400.204102\n",
      "    epoch          : 424\n",
      "    loss           : -405.9237249755859\n",
      "    val_loss       : -354.49214390125127\n",
      "    val_log_likelihood: 492.90711364746096\n",
      "    val_log_marginal: 361.2962308369844\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -407.575684\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -443.714966\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -539.788818\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -550.864319\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -436.436584\n",
      "    epoch          : 425\n",
      "    loss           : -403.70761322021485\n",
      "    val_loss       : -372.4836918056011\n",
      "    val_log_likelihood: 490.79987182617185\n",
      "    val_log_marginal: 379.266892214492\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -200.827606\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -547.773682\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -367.268555\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -245.985199\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -242.719330\n",
      "    epoch          : 426\n",
      "    loss           : -383.67045501708986\n",
      "    val_loss       : -269.2726459806785\n",
      "    val_log_likelihood: 472.51015014648436\n",
      "    val_log_marginal: 273.7105470333532\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -380.823181\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -217.240189\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -247.901489\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -252.343506\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: 42.413010\n",
      "    epoch          : 427\n",
      "    loss           : -122.5852423286438\n",
      "    val_loss       : -287.9742404924706\n",
      "    val_log_likelihood: 442.8850341796875\n",
      "    val_log_marginal: 311.74631793908776\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -375.948669\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -500.599670\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -404.370453\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -437.250916\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -402.838928\n",
      "    epoch          : 428\n",
      "    loss           : -362.87235023498533\n",
      "    val_loss       : -371.248234307766\n",
      "    val_log_likelihood: 483.14525146484374\n",
      "    val_log_marginal: 377.3478825423866\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -380.808472\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -389.649902\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -436.191528\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -452.627686\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -251.764542\n",
      "    epoch          : 429\n",
      "    loss           : -382.85623840332033\n",
      "    val_loss       : -350.6731903709471\n",
      "    val_log_likelihood: 480.5189239501953\n",
      "    val_log_marginal: 356.1364188443869\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -409.361816\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -440.238373\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -301.207336\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -141.882141\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -422.644501\n",
      "    epoch          : 430\n",
      "    loss           : -331.7753909301758\n",
      "    val_loss       : -363.1208633089438\n",
      "    val_log_likelihood: 481.63890686035154\n",
      "    val_log_marginal: 373.25435663083476\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -536.378174\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -535.872192\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -422.447052\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -547.422668\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -458.486633\n",
      "    epoch          : 431\n",
      "    loss           : -406.6303221130371\n",
      "    val_loss       : -372.976517983526\n",
      "    val_log_likelihood: 492.0923095703125\n",
      "    val_log_marginal: 381.931914056465\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -394.254364\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -445.468201\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -406.400116\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -443.747864\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -429.512268\n",
      "    epoch          : 432\n",
      "    loss           : -405.914783782959\n",
      "    val_loss       : -378.47611108962445\n",
      "    val_log_likelihood: 491.24559631347654\n",
      "    val_log_marginal: 383.02963436730204\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -416.236969\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -415.999207\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -434.887726\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -406.901550\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -518.543945\n",
      "    epoch          : 433\n",
      "    loss           : -394.6256999206543\n",
      "    val_loss       : -368.1422784358263\n",
      "    val_log_likelihood: 486.8271026611328\n",
      "    val_log_marginal: 373.92320203073325\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -387.515625\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -527.880676\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -419.852295\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -221.909500\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -446.990662\n",
      "    epoch          : 434\n",
      "    loss           : -405.5773127746582\n",
      "    val_loss       : -381.1150743685663\n",
      "    val_log_likelihood: 495.1947326660156\n",
      "    val_log_marginal: 387.2379998296765\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -429.733582\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -378.373108\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -419.922211\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -149.869568\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -247.335663\n",
      "    epoch          : 435\n",
      "    loss           : -388.4636027526856\n",
      "    val_loss       : -361.5216671830043\n",
      "    val_log_likelihood: 488.0139526367187\n",
      "    val_log_marginal: 369.0937125246972\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -459.893341\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -438.632202\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -503.337860\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -319.536835\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -433.881348\n",
      "    epoch          : 436\n",
      "    loss           : -381.1523838806152\n",
      "    val_loss       : -372.3777899725363\n",
      "    val_log_likelihood: 491.178466796875\n",
      "    val_log_marginal: 379.7684740629047\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -410.066498\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -367.791443\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -372.283630\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -483.899780\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -210.736908\n",
      "    epoch          : 437\n",
      "    loss           : -297.13924921035766\n",
      "    val_loss       : -240.36946213953198\n",
      "    val_log_likelihood: 450.9754638671875\n",
      "    val_log_marginal: 274.74433143623173\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -372.595367\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: 127.099976\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -456.469391\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -325.674500\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -413.064453\n",
      "    epoch          : 438\n",
      "    loss           : -305.523966255188\n",
      "    val_loss       : -366.4317173793912\n",
      "    val_log_likelihood: 486.2902526855469\n",
      "    val_log_marginal: 376.318803133443\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -429.440247\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -558.441284\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -431.272308\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -466.403320\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -455.119629\n",
      "    epoch          : 439\n",
      "    loss           : -407.9223637390137\n",
      "    val_loss       : -381.3483005464077\n",
      "    val_log_likelihood: 494.8528167724609\n",
      "    val_log_marginal: 388.6874244425447\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -441.854706\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -446.327332\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -394.067566\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -455.787872\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -263.955017\n",
      "    epoch          : 440\n",
      "    loss           : -415.54483978271486\n",
      "    val_loss       : -379.64909697119145\n",
      "    val_log_likelihood: 496.3946594238281\n",
      "    val_log_marginal: 387.2160174075514\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch440.pth ...\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -444.704346\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -275.976074\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -282.294159\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -378.029724\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -445.404968\n",
      "    epoch          : 441\n",
      "    loss           : -412.8485751342773\n",
      "    val_loss       : -372.75306482389567\n",
      "    val_log_likelihood: 492.47071228027346\n",
      "    val_log_marginal: 378.44106145426633\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -425.911377\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -446.500183\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -425.215271\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -426.095764\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -344.198120\n",
      "    epoch          : 442\n",
      "    loss           : -391.95286041259766\n",
      "    val_loss       : -345.27323610503225\n",
      "    val_log_likelihood: 483.67548217773435\n",
      "    val_log_marginal: 350.35377870164814\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -431.354828\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -423.320465\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -423.017181\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -304.529114\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -355.754150\n",
      "    epoch          : 443\n",
      "    loss           : -374.6089488983154\n",
      "    val_loss       : -350.2147272804752\n",
      "    val_log_likelihood: 480.5351501464844\n",
      "    val_log_marginal: 353.003696507588\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -429.976471\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -415.902740\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -211.484497\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -198.707779\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -432.533600\n",
      "    epoch          : 444\n",
      "    loss           : -366.7719348144531\n",
      "    val_loss       : -304.78216640539466\n",
      "    val_log_likelihood: 485.2407928466797\n",
      "    val_log_marginal: 320.0489243756987\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -189.515411\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -345.663818\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -233.954330\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -442.885925\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -405.286072\n",
      "    epoch          : 445\n",
      "    loss           : -382.0996499633789\n",
      "    val_loss       : -364.5871961880475\n",
      "    val_log_likelihood: 495.93202819824216\n",
      "    val_log_marginal: 368.8442367296666\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -447.372101\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -362.093262\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -418.293488\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -386.833649\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -357.271637\n",
      "    epoch          : 446\n",
      "    loss           : -389.5247926330566\n",
      "    val_loss       : -365.66997142359617\n",
      "    val_log_likelihood: 493.33852233886716\n",
      "    val_log_marginal: 369.7709497321397\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -359.690857\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -468.939758\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -422.716858\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -462.506195\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -248.031067\n",
      "    epoch          : 447\n",
      "    loss           : -387.1516290283203\n",
      "    val_loss       : -374.11863752957436\n",
      "    val_log_likelihood: 495.1012786865234\n",
      "    val_log_marginal: 379.6703862179071\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -432.431549\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -508.692627\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -233.674042\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -443.001404\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -437.747314\n",
      "    epoch          : 448\n",
      "    loss           : -396.8929197692871\n",
      "    val_loss       : -370.69610329475256\n",
      "    val_log_likelihood: 497.21515502929685\n",
      "    val_log_marginal: 378.8658256464769\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -408.571838\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -425.455078\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -553.832764\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -466.687744\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -457.981415\n",
      "    epoch          : 449\n",
      "    loss           : -408.3604640197754\n",
      "    val_loss       : -384.8072113445029\n",
      "    val_log_likelihood: 497.7218902587891\n",
      "    val_log_marginal: 389.4076952729374\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -479.760986\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -565.386902\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -448.220886\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -466.264862\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -318.450623\n",
      "    epoch          : 450\n",
      "    loss           : -407.3214242553711\n",
      "    val_loss       : -370.427337423712\n",
      "    val_log_likelihood: 499.57607421875\n",
      "    val_log_marginal: 379.7490120496601\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -544.327515\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -482.258820\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -263.185669\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -389.334991\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -171.635559\n",
      "    epoch          : 451\n",
      "    loss           : -388.6828511047363\n",
      "    val_loss       : -338.5783704120666\n",
      "    val_log_likelihood: 485.7620147705078\n",
      "    val_log_marginal: 350.67045009869037\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -397.257568\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -533.998291\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -443.804810\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -265.782654\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -225.890381\n",
      "    epoch          : 452\n",
      "    loss           : -351.1233170318603\n",
      "    val_loss       : -344.6437753593549\n",
      "    val_log_likelihood: 480.88152770996095\n",
      "    val_log_marginal: 356.2533910434693\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -533.272827\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -409.320007\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -391.521118\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -390.186493\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -443.683777\n",
      "    epoch          : 453\n",
      "    loss           : -389.4057215881348\n",
      "    val_loss       : -373.87880316022785\n",
      "    val_log_likelihood: 495.47472229003904\n",
      "    val_log_marginal: 380.3014485258609\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -537.976135\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -436.919189\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -362.174652\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -359.170227\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -96.275452\n",
      "    epoch          : 454\n",
      "    loss           : -344.0843249893189\n",
      "    val_loss       : -283.1891236998141\n",
      "    val_log_likelihood: 446.87411346435545\n",
      "    val_log_marginal: 296.9605945866555\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -380.130249\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -336.896118\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -323.715271\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -392.391296\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -413.562866\n",
      "    epoch          : 455\n",
      "    loss           : -346.78216033935547\n",
      "    val_loss       : -337.32760419882834\n",
      "    val_log_likelihood: 487.9684814453125\n",
      "    val_log_marginal: 349.7855483081192\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -378.344727\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -455.164795\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -413.304504\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -368.540131\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -363.657959\n",
      "    epoch          : 456\n",
      "    loss           : -378.0571615600586\n",
      "    val_loss       : -320.953726734221\n",
      "    val_log_likelihood: 488.745458984375\n",
      "    val_log_marginal: 329.46385889314115\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -348.509033\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -416.551270\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -405.647461\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -377.969238\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -428.825806\n",
      "    epoch          : 457\n",
      "    loss           : -351.13532890319823\n",
      "    val_loss       : -358.0340288951993\n",
      "    val_log_likelihood: 490.1667755126953\n",
      "    val_log_marginal: 365.66377498733743\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -414.141357\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -470.079315\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -414.514374\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -415.648926\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -206.676102\n",
      "    epoch          : 458\n",
      "    loss           : -392.0191870880127\n",
      "    val_loss       : -351.3607599614188\n",
      "    val_log_likelihood: 491.93805541992185\n",
      "    val_log_marginal: 366.4825384665281\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -439.133698\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -555.577881\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -399.723755\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -223.328629\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -377.710205\n",
      "    epoch          : 459\n",
      "    loss           : -393.25542739868166\n",
      "    val_loss       : -352.600980084762\n",
      "    val_log_likelihood: 482.34046936035156\n",
      "    val_log_marginal: 358.593896375224\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -436.460144\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -413.608337\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -379.094971\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -410.107056\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -460.661407\n",
      "    epoch          : 460\n",
      "    loss           : -366.00012268066405\n",
      "    val_loss       : -372.26487487275153\n",
      "    val_log_likelihood: 495.2473937988281\n",
      "    val_log_marginal: 380.3022717572573\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch460.pth ...\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -457.921875\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -449.788086\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -473.487427\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -409.015015\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -434.064972\n",
      "    epoch          : 461\n",
      "    loss           : -407.47486343383787\n",
      "    val_loss       : -378.65186715647576\n",
      "    val_log_likelihood: 499.22621459960936\n",
      "    val_log_marginal: 385.5247563932091\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -240.382996\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -437.075317\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -427.218933\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -315.338013\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -365.786346\n",
      "    epoch          : 462\n",
      "    loss           : -405.59142684936523\n",
      "    val_loss       : -378.9091272443533\n",
      "    val_log_likelihood: 498.6336303710938\n",
      "    val_log_marginal: 385.4565499647553\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -456.384094\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -214.482559\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -360.416107\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -441.429352\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -438.362274\n",
      "    epoch          : 463\n",
      "    loss           : -402.6071141052246\n",
      "    val_loss       : -357.8984518598765\n",
      "    val_log_likelihood: 499.1840850830078\n",
      "    val_log_marginal: 364.470770246163\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -356.077698\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -358.875885\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -443.584778\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -382.701385\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -332.038757\n",
      "    epoch          : 464\n",
      "    loss           : -356.6384359741211\n",
      "    val_loss       : -338.78751395307484\n",
      "    val_log_likelihood: 483.26084289550784\n",
      "    val_log_marginal: 344.8034175541782\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -515.153870\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -414.008728\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -194.159210\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -382.089996\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: 235.469299\n",
      "    epoch          : 465\n",
      "    loss           : -282.72945274353026\n",
      "    val_loss       : -287.6661044219509\n",
      "    val_log_likelihood: 474.27093811035155\n",
      "    val_log_marginal: 300.9510378431529\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -381.345886\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -366.347748\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -187.700317\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -361.651154\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -257.776733\n",
      "    epoch          : 466\n",
      "    loss           : -276.46607914924624\n",
      "    val_loss       : -376.11451895516365\n",
      "    val_log_likelihood: 488.3093688964844\n",
      "    val_log_marginal: 383.56670841984453\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -390.787720\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -459.362671\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -443.514465\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -379.746460\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -260.556213\n",
      "    epoch          : 467\n",
      "    loss           : -402.9035656738281\n",
      "    val_loss       : -377.0458523524925\n",
      "    val_log_likelihood: 489.63999938964844\n",
      "    val_log_marginal: 381.8543172163712\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -444.672028\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -409.301025\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -417.646790\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -445.651764\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -267.199341\n",
      "    epoch          : 468\n",
      "    loss           : -415.365841217041\n",
      "    val_loss       : -384.13318302240225\n",
      "    val_log_likelihood: 504.4931640625\n",
      "    val_log_marginal: 392.10883986540136\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -461.675049\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -467.042786\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -473.456604\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -459.701477\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -258.558594\n",
      "    epoch          : 469\n",
      "    loss           : -412.5657243347168\n",
      "    val_loss       : -384.64362461399287\n",
      "    val_log_likelihood: 495.76206970214844\n",
      "    val_log_marginal: 389.48937269635496\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -242.635117\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -444.908722\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -245.787781\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -482.189423\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -453.347626\n",
      "    epoch          : 470\n",
      "    loss           : -418.2896084594727\n",
      "    val_loss       : -391.2370127823204\n",
      "    val_log_likelihood: 504.74668884277344\n",
      "    val_log_marginal: 394.8374692242593\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -478.776306\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -502.139771\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -570.608704\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -432.060425\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -440.488586\n",
      "    epoch          : 471\n",
      "    loss           : -417.5464372253418\n",
      "    val_loss       : -366.0090039532632\n",
      "    val_log_likelihood: 501.7196472167969\n",
      "    val_log_marginal: 372.0079022932815\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -417.306427\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -228.893402\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -428.798401\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -319.574036\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -231.593185\n",
      "    epoch          : 472\n",
      "    loss           : -322.99960884094236\n",
      "    val_loss       : -324.81834022924306\n",
      "    val_log_likelihood: 467.3954223632812\n",
      "    val_log_marginal: 332.7759578306228\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -359.645203\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -461.768036\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -436.391785\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -433.664093\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -462.686951\n",
      "    epoch          : 473\n",
      "    loss           : -396.76933319091796\n",
      "    val_loss       : -382.1840028114617\n",
      "    val_log_likelihood: 502.3294311523438\n",
      "    val_log_marginal: 389.7039925393057\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -238.511505\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -449.081787\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -260.757355\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -434.497192\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -437.394043\n",
      "    epoch          : 474\n",
      "    loss           : -403.6204524230957\n",
      "    val_loss       : -365.23052995316687\n",
      "    val_log_likelihood: 500.0554534912109\n",
      "    val_log_marginal: 371.6722672361881\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -375.992096\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -260.664490\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -266.226501\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -375.915527\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -363.813293\n",
      "    epoch          : 475\n",
      "    loss           : -394.1955676269531\n",
      "    val_loss       : -358.0880966518074\n",
      "    val_log_likelihood: 496.1756225585938\n",
      "    val_log_marginal: 364.9729764703661\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -431.394897\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -472.278748\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -394.873413\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -439.196289\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -255.194626\n",
      "    epoch          : 476\n",
      "    loss           : -395.4227101135254\n",
      "    val_loss       : -385.21818677932026\n",
      "    val_log_likelihood: 499.9466278076172\n",
      "    val_log_marginal: 388.0870908234268\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -557.064697\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -430.511475\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -453.755371\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -182.005768\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -510.581055\n",
      "    epoch          : 477\n",
      "    loss           : -380.03847702026366\n",
      "    val_loss       : -285.1881541768089\n",
      "    val_log_likelihood: 471.8365844726562\n",
      "    val_log_marginal: 300.39823704697193\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -368.312195\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -396.498871\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -417.758270\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -397.635437\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -452.321838\n",
      "    epoch          : 478\n",
      "    loss           : -395.8929557800293\n",
      "    val_loss       : -383.2534172900021\n",
      "    val_log_likelihood: 502.66814880371095\n",
      "    val_log_marginal: 390.04974905364213\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -402.599609\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -550.255859\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -549.824707\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -530.596985\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -394.204498\n",
      "    epoch          : 479\n",
      "    loss           : -415.9252185058594\n",
      "    val_loss       : -379.9376023218036\n",
      "    val_log_likelihood: 506.5052001953125\n",
      "    val_log_marginal: 388.3666263263673\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -398.109680\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -567.903564\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -448.748993\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -458.258911\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -458.359467\n",
      "    epoch          : 480\n",
      "    loss           : -410.70212356567384\n",
      "    val_loss       : -378.6426728567109\n",
      "    val_log_likelihood: 505.8176971435547\n",
      "    val_log_marginal: 386.2360403429717\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch480.pth ...\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -454.603668\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -428.034790\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -557.730591\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -427.324158\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -102.272499\n",
      "    epoch          : 481\n",
      "    loss           : -403.1145107269287\n",
      "    val_loss       : -354.3892379526049\n",
      "    val_log_likelihood: 495.31409606933596\n",
      "    val_log_marginal: 359.38856893023046\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -384.963837\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -418.003113\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -418.561615\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -399.465271\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -455.129028\n",
      "    epoch          : 482\n",
      "    loss           : -380.29452682495116\n",
      "    val_loss       : -367.59585966039447\n",
      "    val_log_likelihood: 498.26256103515624\n",
      "    val_log_marginal: 377.48264857791366\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -413.310333\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -453.188416\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -398.192383\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -417.448761\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -60.069832\n",
      "    epoch          : 483\n",
      "    loss           : -351.565129032135\n",
      "    val_loss       : -250.13036694787442\n",
      "    val_log_likelihood: 462.1297576904297\n",
      "    val_log_marginal: 256.8354941766709\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -316.932648\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -323.111389\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -384.008850\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -438.277252\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -413.425751\n",
      "    epoch          : 484\n",
      "    loss           : -343.07159324645994\n",
      "    val_loss       : -376.23032881636175\n",
      "    val_log_likelihood: 493.93274841308596\n",
      "    val_log_marginal: 382.77752031348643\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -442.832245\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -427.150330\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -441.992645\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -268.563904\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -456.726501\n",
      "    epoch          : 485\n",
      "    loss           : -418.2254811096191\n",
      "    val_loss       : -388.6288355935365\n",
      "    val_log_likelihood: 504.8904296875\n",
      "    val_log_marginal: 394.18802617900076\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -224.221756\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -486.045349\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -441.815613\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -481.889709\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -443.173096\n",
      "    epoch          : 486\n",
      "    loss           : -416.05220657348633\n",
      "    val_loss       : -382.8918094964698\n",
      "    val_log_likelihood: 506.23728332519534\n",
      "    val_log_marginal: 390.4518469304143\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -218.771652\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -542.067322\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -239.768555\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -423.128723\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -391.390259\n",
      "    epoch          : 487\n",
      "    loss           : -389.53960021972654\n",
      "    val_loss       : -349.22084755506364\n",
      "    val_log_likelihood: 498.8129425048828\n",
      "    val_log_marginal: 355.88769305534663\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -494.885406\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -490.680817\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -435.248962\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -435.122101\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -430.362000\n",
      "    epoch          : 488\n",
      "    loss           : -391.76191772460936\n",
      "    val_loss       : -361.76881521865727\n",
      "    val_log_likelihood: 497.4294738769531\n",
      "    val_log_marginal: 372.7670604277402\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -539.416809\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -446.160950\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -382.548737\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -540.811951\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -450.276001\n",
      "    epoch          : 489\n",
      "    loss           : -404.20116287231446\n",
      "    val_loss       : -367.9603366222233\n",
      "    val_log_likelihood: 500.3131134033203\n",
      "    val_log_marginal: 375.12380851469936\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -546.357910\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -234.505600\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -366.551697\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -258.236328\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -412.922272\n",
      "    epoch          : 490\n",
      "    loss           : -394.62117111206055\n",
      "    val_loss       : -369.5444000590593\n",
      "    val_log_likelihood: 501.71295776367185\n",
      "    val_log_marginal: 377.4490852136165\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -553.588928\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -450.218811\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -380.904083\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -448.784576\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -240.733459\n",
      "    epoch          : 491\n",
      "    loss           : -376.21510711669924\n",
      "    val_loss       : -359.70961710345\n",
      "    val_log_likelihood: 494.2344696044922\n",
      "    val_log_marginal: 365.3510869428331\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -434.274994\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -453.223328\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -555.126465\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -549.599365\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -379.761719\n",
      "    epoch          : 492\n",
      "    loss           : -407.66374282836915\n",
      "    val_loss       : -385.0034754930064\n",
      "    val_log_likelihood: 507.7774169921875\n",
      "    val_log_marginal: 390.7069354828447\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -232.008209\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -406.412933\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -441.014221\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -331.170715\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -176.242828\n",
      "    epoch          : 493\n",
      "    loss           : -385.9695230102539\n",
      "    val_loss       : -314.5038981765509\n",
      "    val_log_likelihood: 491.50063171386716\n",
      "    val_log_marginal: 321.4097523424774\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -498.273621\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -263.815247\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -363.042267\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -338.586823\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -300.191467\n",
      "    epoch          : 494\n",
      "    loss           : -369.6774708557129\n",
      "    val_loss       : -317.8732717173174\n",
      "    val_log_likelihood: 494.76063537597656\n",
      "    val_log_marginal: 323.18903242610395\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -340.295837\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -454.052826\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -535.441406\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -166.292145\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -375.578796\n",
      "    epoch          : 495\n",
      "    loss           : -364.0488000488281\n",
      "    val_loss       : -356.1368186056614\n",
      "    val_log_likelihood: 495.237451171875\n",
      "    val_log_marginal: 362.6670261967999\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -227.385696\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -235.903030\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -398.041168\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -397.668030\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -223.145874\n",
      "    epoch          : 496\n",
      "    loss           : -382.09339935302734\n",
      "    val_loss       : -365.43291150182483\n",
      "    val_log_likelihood: 497.6043212890625\n",
      "    val_log_marginal: 373.0520492013544\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -424.129028\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -429.793549\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -425.626221\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -434.297058\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -365.108154\n",
      "    epoch          : 497\n",
      "    loss           : -407.3350561523437\n",
      "    val_loss       : -366.64119466152044\n",
      "    val_log_likelihood: 486.1598663330078\n",
      "    val_log_marginal: 372.5753701288253\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -430.605316\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -487.384460\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -414.127136\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -549.225098\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -428.529236\n",
      "    epoch          : 498\n",
      "    loss           : -404.7355017089844\n",
      "    val_loss       : -385.6995591642335\n",
      "    val_log_likelihood: 508.2256561279297\n",
      "    val_log_marginal: 393.97497303150595\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -463.526886\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -571.861450\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -384.445618\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -387.042145\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -362.521271\n",
      "    epoch          : 499\n",
      "    loss           : -411.9966537475586\n",
      "    val_loss       : -365.7889988349751\n",
      "    val_log_likelihood: 501.2635955810547\n",
      "    val_log_marginal: 370.58846640922127\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -420.786926\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -429.585663\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -427.714874\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -415.683685\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -440.371857\n",
      "    epoch          : 500\n",
      "    loss           : -381.3424856567383\n",
      "    val_loss       : -341.9660966884345\n",
      "    val_log_likelihood: 491.7466583251953\n",
      "    val_log_marginal: 348.44706947095693\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch500.pth ...\n",
      "Train Epoch: 501 [512/54000 (1%)] Loss: -504.193237\n",
      "Train Epoch: 501 [11776/54000 (22%)] Loss: -239.122971\n",
      "Train Epoch: 501 [23040/54000 (43%)] Loss: -382.465454\n",
      "Train Epoch: 501 [34304/54000 (64%)] Loss: -371.045349\n",
      "Train Epoch: 501 [45568/54000 (84%)] Loss: -265.753204\n",
      "    epoch          : 501\n",
      "    loss           : -345.62603401184083\n",
      "    val_loss       : -339.29039283283055\n",
      "    val_log_likelihood: 493.7360107421875\n",
      "    val_log_marginal: 344.07555123455825\n",
      "Train Epoch: 502 [512/54000 (1%)] Loss: -545.511963\n",
      "Train Epoch: 502 [11776/54000 (22%)] Loss: -395.311432\n",
      "Train Epoch: 502 [23040/54000 (43%)] Loss: -401.691956\n",
      "Train Epoch: 502 [34304/54000 (64%)] Loss: -408.138458\n",
      "Train Epoch: 502 [45568/54000 (84%)] Loss: -423.186951\n",
      "    epoch          : 502\n",
      "    loss           : -369.4575790405273\n",
      "    val_loss       : -380.32820596396925\n",
      "    val_log_likelihood: 505.0639953613281\n",
      "    val_log_marginal: 386.2312067311257\n",
      "Train Epoch: 503 [512/54000 (1%)] Loss: -481.441803\n",
      "Train Epoch: 503 [11776/54000 (22%)] Loss: -230.935242\n",
      "Train Epoch: 503 [23040/54000 (43%)] Loss: -422.775970\n",
      "Train Epoch: 503 [34304/54000 (64%)] Loss: -230.922211\n",
      "Train Epoch: 503 [45568/54000 (84%)] Loss: -375.840607\n",
      "    epoch          : 503\n",
      "    loss           : -400.9160874938965\n",
      "    val_loss       : -355.1725766606629\n",
      "    val_log_likelihood: 496.24465026855466\n",
      "    val_log_marginal: 368.73357370057124\n",
      "Train Epoch: 504 [512/54000 (1%)] Loss: -432.754425\n",
      "Train Epoch: 504 [11776/54000 (22%)] Loss: -421.188416\n",
      "Train Epoch: 504 [23040/54000 (43%)] Loss: -205.368042\n",
      "Train Epoch: 504 [34304/54000 (64%)] Loss: -435.358307\n",
      "Train Epoch: 504 [45568/54000 (84%)] Loss: -439.917053\n",
      "    epoch          : 504\n",
      "    loss           : -394.3923515319824\n",
      "    val_loss       : -378.6914301596582\n",
      "    val_log_likelihood: 497.44012451171875\n",
      "    val_log_marginal: 384.88319030962884\n",
      "Train Epoch: 505 [512/54000 (1%)] Loss: -486.781158\n",
      "Train Epoch: 505 [11776/54000 (22%)] Loss: -201.494034\n",
      "Train Epoch: 505 [23040/54000 (43%)] Loss: -392.086700\n",
      "Train Epoch: 505 [34304/54000 (64%)] Loss: -443.972473\n",
      "Train Epoch: 505 [45568/54000 (84%)] Loss: -439.523102\n",
      "    epoch          : 505\n",
      "    loss           : -398.3081605529785\n",
      "    val_loss       : -371.78155020549895\n",
      "    val_log_likelihood: 501.7146270751953\n",
      "    val_log_marginal: 376.26644643805923\n",
      "Train Epoch: 506 [512/54000 (1%)] Loss: -412.682068\n",
      "Train Epoch: 506 [11776/54000 (22%)] Loss: -418.388672\n",
      "Train Epoch: 506 [23040/54000 (43%)] Loss: -228.273468\n",
      "Train Epoch: 506 [34304/54000 (64%)] Loss: -138.903870\n",
      "Train Epoch: 506 [45568/54000 (84%)] Loss: -270.342010\n",
      "    epoch          : 506\n",
      "    loss           : -349.81578102111814\n",
      "    val_loss       : -268.7240937259048\n",
      "    val_log_likelihood: 484.9994873046875\n",
      "    val_log_marginal: 280.0410023909062\n",
      "Train Epoch: 507 [512/54000 (1%)] Loss: -354.551086\n",
      "Train Epoch: 507 [11776/54000 (22%)] Loss: -242.587860\n",
      "Train Epoch: 507 [23040/54000 (43%)] Loss: -228.094208\n",
      "Train Epoch: 507 [34304/54000 (64%)] Loss: -424.601288\n",
      "Train Epoch: 507 [45568/54000 (84%)] Loss: -462.648743\n",
      "    epoch          : 507\n",
      "    loss           : -391.8346965789795\n",
      "    val_loss       : -392.55683208089323\n",
      "    val_log_likelihood: 507.93377380371095\n",
      "    val_log_marginal: 398.51049123711584\n",
      "Train Epoch: 508 [512/54000 (1%)] Loss: -565.597351\n",
      "Train Epoch: 508 [11776/54000 (22%)] Loss: -557.082886\n",
      "Train Epoch: 508 [23040/54000 (43%)] Loss: -458.381592\n",
      "Train Epoch: 508 [34304/54000 (64%)] Loss: -222.871521\n",
      "Train Epoch: 508 [45568/54000 (84%)] Loss: -449.231384\n",
      "    epoch          : 508\n",
      "    loss           : -425.3508430480957\n",
      "    val_loss       : -392.56606809310614\n",
      "    val_log_likelihood: 512.3406829833984\n",
      "    val_log_marginal: 401.705723111704\n",
      "Train Epoch: 509 [512/54000 (1%)] Loss: -561.354492\n",
      "Train Epoch: 509 [11776/54000 (22%)] Loss: -432.176758\n",
      "Train Epoch: 509 [23040/54000 (43%)] Loss: -482.490112\n",
      "Train Epoch: 509 [34304/54000 (64%)] Loss: -248.148224\n",
      "Train Epoch: 509 [45568/54000 (84%)] Loss: -398.956757\n",
      "    epoch          : 509\n",
      "    loss           : -420.8987808227539\n",
      "    val_loss       : -385.5483391834423\n",
      "    val_log_likelihood: 512.0370391845703\n",
      "    val_log_marginal: 394.1712152000517\n",
      "Train Epoch: 510 [512/54000 (1%)] Loss: -419.093262\n",
      "Train Epoch: 510 [11776/54000 (22%)] Loss: -397.812347\n",
      "Train Epoch: 510 [23040/54000 (43%)] Loss: -200.372131\n",
      "Train Epoch: 510 [34304/54000 (64%)] Loss: -440.799805\n",
      "Train Epoch: 510 [45568/54000 (84%)] Loss: -164.896133\n",
      "    epoch          : 510\n",
      "    loss           : -386.21916446685793\n",
      "    val_loss       : -249.60666899345816\n",
      "    val_log_likelihood: 466.3120620727539\n",
      "    val_log_marginal: 270.33496427498756\n",
      "Train Epoch: 511 [512/54000 (1%)] Loss: -222.385483\n",
      "Train Epoch: 511 [11776/54000 (22%)] Loss: -480.258362\n",
      "Train Epoch: 511 [23040/54000 (43%)] Loss: -383.813477\n",
      "Train Epoch: 511 [34304/54000 (64%)] Loss: -483.966919\n",
      "Train Epoch: 511 [45568/54000 (84%)] Loss: -390.457886\n",
      "    epoch          : 511\n",
      "    loss           : -332.15461448669436\n",
      "    val_loss       : -358.5399787709117\n",
      "    val_log_likelihood: 493.5148101806641\n",
      "    val_log_marginal: 365.17907150915596\n",
      "Train Epoch: 512 [512/54000 (1%)] Loss: -412.187225\n",
      "Train Epoch: 512 [11776/54000 (22%)] Loss: -403.052002\n",
      "Train Epoch: 512 [23040/54000 (43%)] Loss: -340.534943\n",
      "Train Epoch: 512 [34304/54000 (64%)] Loss: -243.503113\n",
      "Train Epoch: 512 [45568/54000 (84%)] Loss: -180.639267\n",
      "    epoch          : 512\n",
      "    loss           : -299.4857526397705\n",
      "    val_loss       : -336.57178522404286\n",
      "    val_log_likelihood: 489.9176910400391\n",
      "    val_log_marginal: 343.209861649473\n",
      "Train Epoch: 513 [512/54000 (1%)] Loss: -406.584991\n",
      "Train Epoch: 513 [11776/54000 (22%)] Loss: -201.476410\n",
      "Train Epoch: 513 [23040/54000 (43%)] Loss: -442.633759\n",
      "Train Epoch: 513 [34304/54000 (64%)] Loss: -445.496033\n",
      "Train Epoch: 513 [45568/54000 (84%)] Loss: -468.436615\n",
      "    epoch          : 513\n",
      "    loss           : -406.90344009399416\n",
      "    val_loss       : -381.43009862154724\n",
      "    val_log_likelihood: 506.0030120849609\n",
      "    val_log_marginal: 388.6271313820063\n",
      "Train Epoch: 514 [512/54000 (1%)] Loss: -428.047852\n",
      "Train Epoch: 514 [11776/54000 (22%)] Loss: -415.247864\n",
      "Train Epoch: 514 [23040/54000 (43%)] Loss: -455.309875\n",
      "Train Epoch: 514 [34304/54000 (64%)] Loss: -465.033813\n",
      "Train Epoch: 514 [45568/54000 (84%)] Loss: -473.279114\n",
      "    epoch          : 514\n",
      "    loss           : -422.32844772338865\n",
      "    val_loss       : -391.9657954670489\n",
      "    val_log_likelihood: 508.1433502197266\n",
      "    val_log_marginal: 395.60087660665056\n",
      "Train Epoch: 515 [512/54000 (1%)] Loss: -287.634521\n",
      "Train Epoch: 515 [11776/54000 (22%)] Loss: -262.454346\n",
      "Train Epoch: 515 [23040/54000 (43%)] Loss: -560.841553\n",
      "Train Epoch: 515 [34304/54000 (64%)] Loss: -583.212646\n",
      "Train Epoch: 515 [45568/54000 (84%)] Loss: -404.487854\n",
      "    epoch          : 515\n",
      "    loss           : -422.0311067199707\n",
      "    val_loss       : -385.62025960851463\n",
      "    val_log_likelihood: 512.4466064453125\n",
      "    val_log_marginal: 392.4178389724344\n",
      "Train Epoch: 516 [512/54000 (1%)] Loss: -472.455475\n",
      "Train Epoch: 516 [11776/54000 (22%)] Loss: -474.007019\n",
      "Train Epoch: 516 [23040/54000 (43%)] Loss: -528.101379\n",
      "Train Epoch: 516 [34304/54000 (64%)] Loss: -386.770233\n",
      "Train Epoch: 516 [45568/54000 (84%)] Loss: -451.273682\n",
      "    epoch          : 516\n",
      "    loss           : -398.7730815124512\n",
      "    val_loss       : -364.3423769371584\n",
      "    val_log_likelihood: 505.59253845214846\n",
      "    val_log_marginal: 377.5096828643234\n",
      "Train Epoch: 517 [512/54000 (1%)] Loss: -396.876617\n",
      "Train Epoch: 517 [11776/54000 (22%)] Loss: -184.847260\n",
      "Train Epoch: 517 [23040/54000 (43%)] Loss: -524.255432\n",
      "Train Epoch: 517 [34304/54000 (64%)] Loss: -510.071045\n",
      "Train Epoch: 517 [45568/54000 (84%)] Loss: -446.812592\n",
      "    epoch          : 517\n",
      "    loss           : -388.9002911376953\n",
      "    val_loss       : -372.26507248021665\n",
      "    val_log_likelihood: 505.2135986328125\n",
      "    val_log_marginal: 382.11883093975484\n",
      "Train Epoch: 518 [512/54000 (1%)] Loss: -443.935760\n",
      "Train Epoch: 518 [11776/54000 (22%)] Loss: -267.815155\n",
      "Train Epoch: 518 [23040/54000 (43%)] Loss: -447.554657\n",
      "Train Epoch: 518 [34304/54000 (64%)] Loss: -469.141235\n",
      "Train Epoch: 518 [45568/54000 (84%)] Loss: -473.125946\n",
      "    epoch          : 518\n",
      "    loss           : -416.2129133605957\n",
      "    val_loss       : -389.15987003967166\n",
      "    val_log_likelihood: 510.5961212158203\n",
      "    val_log_marginal: 393.13516649343075\n",
      "Train Epoch: 519 [512/54000 (1%)] Loss: -474.092743\n",
      "Train Epoch: 519 [11776/54000 (22%)] Loss: -245.050659\n",
      "Train Epoch: 519 [23040/54000 (43%)] Loss: -273.360962\n",
      "Train Epoch: 519 [34304/54000 (64%)] Loss: -482.983337\n",
      "Train Epoch: 519 [45568/54000 (84%)] Loss: -464.534485\n",
      "    epoch          : 519\n",
      "    loss           : -430.5147103881836\n",
      "    val_loss       : -400.4057161698118\n",
      "    val_log_likelihood: 516.0840362548828\n",
      "    val_log_marginal: 404.29396459951903\n",
      "Train Epoch: 520 [512/54000 (1%)] Loss: -498.279907\n",
      "Train Epoch: 520 [11776/54000 (22%)] Loss: -452.611694\n",
      "Train Epoch: 520 [23040/54000 (43%)] Loss: -450.209503\n",
      "Train Epoch: 520 [34304/54000 (64%)] Loss: -256.161285\n",
      "Train Epoch: 520 [45568/54000 (84%)] Loss: -428.529846\n",
      "    epoch          : 520\n",
      "    loss           : -427.4791326904297\n",
      "    val_loss       : -364.96481290291996\n",
      "    val_log_likelihood: 514.4947448730469\n",
      "    val_log_marginal: 371.80563428364695\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch520.pth ...\n",
      "Train Epoch: 521 [512/54000 (1%)] Loss: -431.940369\n",
      "Train Epoch: 521 [11776/54000 (22%)] Loss: -163.607635\n",
      "Train Epoch: 521 [23040/54000 (43%)] Loss: -190.322037\n",
      "Train Epoch: 521 [34304/54000 (64%)] Loss: -340.517151\n",
      "Train Epoch: 521 [45568/54000 (84%)] Loss: -397.287659\n",
      "    epoch          : 521\n",
      "    loss           : -283.32986280918124\n",
      "    val_loss       : -346.1399662371725\n",
      "    val_log_likelihood: 475.18964233398435\n",
      "    val_log_marginal: 355.09756343401966\n",
      "Train Epoch: 522 [512/54000 (1%)] Loss: -467.999786\n",
      "Train Epoch: 522 [11776/54000 (22%)] Loss: -481.086121\n",
      "Train Epoch: 522 [23040/54000 (43%)] Loss: -279.725342\n",
      "Train Epoch: 522 [34304/54000 (64%)] Loss: -221.718292\n",
      "Train Epoch: 522 [45568/54000 (84%)] Loss: -357.485565\n",
      "    epoch          : 522\n",
      "    loss           : -375.00484939575193\n",
      "    val_loss       : -328.3932351775467\n",
      "    val_log_likelihood: 495.9377197265625\n",
      "    val_log_marginal: 341.18058285526934\n",
      "Train Epoch: 523 [512/54000 (1%)] Loss: -403.098816\n",
      "Train Epoch: 523 [11776/54000 (22%)] Loss: -448.921753\n",
      "Train Epoch: 523 [23040/54000 (43%)] Loss: -437.616272\n",
      "Train Epoch: 523 [34304/54000 (64%)] Loss: -479.797577\n",
      "Train Epoch: 523 [45568/54000 (84%)] Loss: -389.671204\n",
      "    epoch          : 523\n",
      "    loss           : -414.8183689880371\n",
      "    val_loss       : -391.9311527652666\n",
      "    val_log_likelihood: 508.1038787841797\n",
      "    val_log_marginal: 396.68324027322234\n",
      "Train Epoch: 524 [512/54000 (1%)] Loss: -395.671631\n",
      "Train Epoch: 524 [11776/54000 (22%)] Loss: -463.801270\n",
      "Train Epoch: 524 [23040/54000 (43%)] Loss: -575.843506\n",
      "Train Epoch: 524 [34304/54000 (64%)] Loss: -444.552582\n",
      "Train Epoch: 524 [45568/54000 (84%)] Loss: -443.053101\n",
      "    epoch          : 524\n",
      "    loss           : -421.38877166748046\n",
      "    val_loss       : -394.772912626341\n",
      "    val_log_likelihood: 515.5576019287109\n",
      "    val_log_marginal: 400.69723751769106\n",
      "Train Epoch: 525 [512/54000 (1%)] Loss: -254.105225\n",
      "Train Epoch: 525 [11776/54000 (22%)] Loss: -441.055756\n",
      "Train Epoch: 525 [23040/54000 (43%)] Loss: -188.669891\n",
      "Train Epoch: 525 [34304/54000 (64%)] Loss: -394.317444\n",
      "Train Epoch: 525 [45568/54000 (84%)] Loss: -381.521423\n",
      "    epoch          : 525\n",
      "    loss           : -404.8621337890625\n",
      "    val_loss       : -359.43600435424594\n",
      "    val_log_likelihood: 497.8335479736328\n",
      "    val_log_marginal: 364.85214648656546\n",
      "Train Epoch: 526 [512/54000 (1%)] Loss: -422.083832\n",
      "Train Epoch: 526 [11776/54000 (22%)] Loss: -451.840271\n",
      "Train Epoch: 526 [23040/54000 (43%)] Loss: -429.098877\n",
      "Train Epoch: 526 [34304/54000 (64%)] Loss: -210.263733\n",
      "Train Epoch: 526 [45568/54000 (84%)] Loss: -342.799103\n",
      "    epoch          : 526\n",
      "    loss           : -387.0690182495117\n",
      "    val_loss       : -355.3565409235656\n",
      "    val_log_likelihood: 504.06940002441405\n",
      "    val_log_marginal: 367.5313864056979\n",
      "Train Epoch: 527 [512/54000 (1%)] Loss: -402.049683\n",
      "Train Epoch: 527 [11776/54000 (22%)] Loss: -379.458771\n",
      "Train Epoch: 527 [23040/54000 (43%)] Loss: -270.154846\n",
      "Train Epoch: 527 [34304/54000 (64%)] Loss: -429.326294\n",
      "Train Epoch: 527 [45568/54000 (84%)] Loss: -186.745621\n",
      "    epoch          : 527\n",
      "    loss           : -384.7882424926758\n",
      "    val_loss       : -341.0292356085032\n",
      "    val_log_likelihood: 497.6296752929687\n",
      "    val_log_marginal: 352.38772987984123\n",
      "Train Epoch: 528 [512/54000 (1%)] Loss: -379.019165\n",
      "Train Epoch: 528 [11776/54000 (22%)] Loss: -396.654327\n",
      "Train Epoch: 528 [23040/54000 (43%)] Loss: -424.306061\n",
      "Train Epoch: 528 [34304/54000 (64%)] Loss: -435.999207\n",
      "Train Epoch: 528 [45568/54000 (84%)] Loss: -235.854919\n",
      "    epoch          : 528\n",
      "    loss           : -386.08212966918944\n",
      "    val_loss       : -375.3859931062907\n",
      "    val_log_likelihood: 503.6650817871094\n",
      "    val_log_marginal: 382.9583207231297\n",
      "Train Epoch: 529 [512/54000 (1%)] Loss: -429.324066\n",
      "Train Epoch: 529 [11776/54000 (22%)] Loss: -470.336792\n",
      "Train Epoch: 529 [23040/54000 (43%)] Loss: -462.605713\n",
      "Train Epoch: 529 [34304/54000 (64%)] Loss: -235.729553\n",
      "Train Epoch: 529 [45568/54000 (84%)] Loss: -451.151031\n",
      "    epoch          : 529\n",
      "    loss           : -424.03885528564456\n",
      "    val_loss       : -393.93386545758693\n",
      "    val_log_likelihood: 506.15854797363284\n",
      "    val_log_marginal: 395.65585417710247\n",
      "Train Epoch: 530 [512/54000 (1%)] Loss: -464.535248\n",
      "Train Epoch: 530 [11776/54000 (22%)] Loss: -576.191711\n",
      "Train Epoch: 530 [23040/54000 (43%)] Loss: -302.306244\n",
      "Train Epoch: 530 [34304/54000 (64%)] Loss: -457.220367\n",
      "Train Epoch: 530 [45568/54000 (84%)] Loss: -251.298950\n",
      "    epoch          : 530\n",
      "    loss           : -422.8426625061035\n",
      "    val_loss       : -365.0289512054995\n",
      "    val_log_likelihood: 509.3683227539062\n",
      "    val_log_marginal: 373.23196718880143\n",
      "Train Epoch: 531 [512/54000 (1%)] Loss: -411.484985\n",
      "Train Epoch: 531 [11776/54000 (22%)] Loss: -421.056641\n",
      "Train Epoch: 531 [23040/54000 (43%)] Loss: -257.207520\n",
      "Train Epoch: 531 [34304/54000 (64%)] Loss: -202.049438\n",
      "Train Epoch: 531 [45568/54000 (84%)] Loss: -472.260956\n",
      "    epoch          : 531\n",
      "    loss           : -416.82182861328124\n",
      "    val_loss       : -382.92823556251824\n",
      "    val_log_likelihood: 512.5071990966796\n",
      "    val_log_marginal: 391.67682109735904\n",
      "Train Epoch: 532 [512/54000 (1%)] Loss: -219.896835\n",
      "Train Epoch: 532 [11776/54000 (22%)] Loss: -234.724564\n",
      "Train Epoch: 532 [23040/54000 (43%)] Loss: -385.651917\n",
      "Train Epoch: 532 [34304/54000 (64%)] Loss: -525.877136\n",
      "Train Epoch: 532 [45568/54000 (84%)] Loss: -396.036469\n",
      "    epoch          : 532\n",
      "    loss           : -406.24744110107423\n",
      "    val_loss       : -384.8439065299928\n",
      "    val_log_likelihood: 507.84482727050784\n",
      "    val_log_marginal: 389.0686809945852\n",
      "Train Epoch: 533 [512/54000 (1%)] Loss: -476.648163\n",
      "Train Epoch: 533 [11776/54000 (22%)] Loss: -406.254242\n",
      "Train Epoch: 533 [23040/54000 (43%)] Loss: -575.847961\n",
      "Train Epoch: 533 [34304/54000 (64%)] Loss: -243.615356\n",
      "Train Epoch: 533 [45568/54000 (84%)] Loss: -455.235535\n",
      "    epoch          : 533\n",
      "    loss           : -423.21168914794924\n",
      "    val_loss       : -388.22942626997826\n",
      "    val_log_likelihood: 513.3344909667969\n",
      "    val_log_marginal: 395.0308832574636\n",
      "Train Epoch: 534 [512/54000 (1%)] Loss: -489.442474\n",
      "Train Epoch: 534 [11776/54000 (22%)] Loss: -248.585297\n",
      "Train Epoch: 534 [23040/54000 (43%)] Loss: -491.835358\n",
      "Train Epoch: 534 [34304/54000 (64%)] Loss: -451.430786\n",
      "Train Epoch: 534 [45568/54000 (84%)] Loss: -491.796112\n",
      "    epoch          : 534\n",
      "    loss           : -419.16729782104494\n",
      "    val_loss       : -358.4950236586854\n",
      "    val_log_likelihood: 506.06601867675784\n",
      "    val_log_marginal: 365.66815274916587\n",
      "Train Epoch: 535 [512/54000 (1%)] Loss: -183.848724\n",
      "Train Epoch: 535 [11776/54000 (22%)] Loss: -396.782196\n",
      "Train Epoch: 535 [23040/54000 (43%)] Loss: -390.527344\n",
      "Train Epoch: 535 [34304/54000 (64%)] Loss: -195.414200\n",
      "Train Epoch: 535 [45568/54000 (84%)] Loss: -341.401733\n",
      "    epoch          : 535\n",
      "    loss           : -340.8834911346436\n",
      "    val_loss       : -328.4020259704441\n",
      "    val_log_likelihood: 490.31328735351565\n",
      "    val_log_marginal: 338.53263122849165\n",
      "Train Epoch: 536 [512/54000 (1%)] Loss: -399.244293\n",
      "Train Epoch: 536 [11776/54000 (22%)] Loss: -436.344086\n",
      "Train Epoch: 536 [23040/54000 (43%)] Loss: -429.191345\n",
      "Train Epoch: 536 [34304/54000 (64%)] Loss: -388.418396\n",
      "Train Epoch: 536 [45568/54000 (84%)] Loss: -392.100891\n",
      "    epoch          : 536\n",
      "    loss           : -397.42366958618163\n",
      "    val_loss       : -362.0063966937363\n",
      "    val_log_likelihood: 508.8916259765625\n",
      "    val_log_marginal: 375.1109025842338\n",
      "Train Epoch: 537 [512/54000 (1%)] Loss: -372.032837\n",
      "Train Epoch: 537 [11776/54000 (22%)] Loss: -279.559082\n",
      "Train Epoch: 537 [23040/54000 (43%)] Loss: -417.841888\n",
      "Train Epoch: 537 [34304/54000 (64%)] Loss: -404.870178\n",
      "Train Epoch: 537 [45568/54000 (84%)] Loss: -314.257263\n",
      "    epoch          : 537\n",
      "    loss           : -389.097890625\n",
      "    val_loss       : -323.74063720032575\n",
      "    val_log_likelihood: 499.1231719970703\n",
      "    val_log_marginal: 338.4927786577493\n",
      "Train Epoch: 538 [512/54000 (1%)] Loss: -408.438293\n",
      "Train Epoch: 538 [11776/54000 (22%)] Loss: -165.510361\n",
      "Train Epoch: 538 [23040/54000 (43%)] Loss: -370.326843\n",
      "Train Epoch: 538 [34304/54000 (64%)] Loss: -187.901520\n",
      "Train Epoch: 538 [45568/54000 (84%)] Loss: -372.248291\n",
      "    epoch          : 538\n",
      "    loss           : -364.1949498748779\n",
      "    val_loss       : -372.5244229333475\n",
      "    val_log_likelihood: 505.66575317382814\n",
      "    val_log_marginal: 377.23002958707514\n",
      "Train Epoch: 539 [512/54000 (1%)] Loss: -549.819336\n",
      "Train Epoch: 539 [11776/54000 (22%)] Loss: -419.699921\n",
      "Train Epoch: 539 [23040/54000 (43%)] Loss: -401.518677\n",
      "Train Epoch: 539 [34304/54000 (64%)] Loss: -427.461975\n",
      "Train Epoch: 539 [45568/54000 (84%)] Loss: -425.645752\n",
      "    epoch          : 539\n",
      "    loss           : -412.98415756225586\n",
      "    val_loss       : -379.86543879043313\n",
      "    val_log_likelihood: 510.86927185058596\n",
      "    val_log_marginal: 386.415546654006\n",
      "Train Epoch: 540 [512/54000 (1%)] Loss: -443.374542\n",
      "Train Epoch: 540 [11776/54000 (22%)] Loss: -574.849731\n",
      "Train Epoch: 540 [23040/54000 (43%)] Loss: -397.561279\n",
      "Train Epoch: 540 [34304/54000 (64%)] Loss: -470.725494\n",
      "Train Epoch: 540 [45568/54000 (84%)] Loss: -255.229797\n",
      "    epoch          : 540\n",
      "    loss           : -420.39032119750976\n",
      "    val_loss       : -386.94166640490295\n",
      "    val_log_likelihood: 516.001318359375\n",
      "    val_log_marginal: 395.1462929379195\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch540.pth ...\n",
      "Train Epoch: 541 [512/54000 (1%)] Loss: -405.379456\n",
      "Train Epoch: 541 [11776/54000 (22%)] Loss: -386.929108\n",
      "Train Epoch: 541 [23040/54000 (43%)] Loss: -460.226562\n",
      "Train Epoch: 541 [34304/54000 (64%)] Loss: -435.241821\n",
      "Train Epoch: 541 [45568/54000 (84%)] Loss: -256.716827\n",
      "    epoch          : 541\n",
      "    loss           : -416.40767715454103\n",
      "    val_loss       : -378.692688761279\n",
      "    val_log_likelihood: 508.55064086914064\n",
      "    val_log_marginal: 382.6354780200869\n",
      "Train Epoch: 542 [512/54000 (1%)] Loss: -466.574890\n",
      "Train Epoch: 542 [11776/54000 (22%)] Loss: -211.058502\n",
      "Train Epoch: 542 [23040/54000 (43%)] Loss: -467.825745\n",
      "Train Epoch: 542 [34304/54000 (64%)] Loss: -546.760193\n",
      "Train Epoch: 542 [45568/54000 (84%)] Loss: -433.904724\n",
      "    epoch          : 542\n",
      "    loss           : -410.90228790283203\n",
      "    val_loss       : -324.1014884294942\n",
      "    val_log_likelihood: 493.1570709228516\n",
      "    val_log_marginal: 336.62831336446106\n",
      "Train Epoch: 543 [512/54000 (1%)] Loss: -388.394043\n",
      "Train Epoch: 543 [11776/54000 (22%)] Loss: -505.996490\n",
      "Train Epoch: 543 [23040/54000 (43%)] Loss: -418.153320\n",
      "Train Epoch: 543 [34304/54000 (64%)] Loss: -218.856186\n",
      "Train Epoch: 543 [45568/54000 (84%)] Loss: -426.030853\n",
      "    epoch          : 543\n",
      "    loss           : -369.58666000366213\n",
      "    val_loss       : -359.05915247537195\n",
      "    val_log_likelihood: 506.79100036621094\n",
      "    val_log_marginal: 367.7483664806932\n",
      "Train Epoch: 544 [512/54000 (1%)] Loss: -454.170807\n",
      "Train Epoch: 544 [11776/54000 (22%)] Loss: -421.292328\n",
      "Train Epoch: 544 [23040/54000 (43%)] Loss: -439.400208\n",
      "Train Epoch: 544 [34304/54000 (64%)] Loss: -402.295959\n",
      "Train Epoch: 544 [45568/54000 (84%)] Loss: -289.508270\n",
      "    epoch          : 544\n",
      "    loss           : -426.0079847717285\n",
      "    val_loss       : -392.42472558412703\n",
      "    val_log_likelihood: 515.9980773925781\n",
      "    val_log_marginal: 400.4645562002596\n",
      "Train Epoch: 545 [512/54000 (1%)] Loss: -252.700806\n",
      "Train Epoch: 545 [11776/54000 (22%)] Loss: -456.402771\n",
      "Train Epoch: 545 [23040/54000 (43%)] Loss: -458.060577\n",
      "Train Epoch: 545 [34304/54000 (64%)] Loss: -266.133667\n",
      "Train Epoch: 545 [45568/54000 (84%)] Loss: -275.867188\n",
      "    epoch          : 545\n",
      "    loss           : -402.2743426513672\n",
      "    val_loss       : -313.37186537291853\n",
      "    val_log_likelihood: 498.0305236816406\n",
      "    val_log_marginal: 327.6119766060263\n",
      "Train Epoch: 546 [512/54000 (1%)] Loss: -70.809669\n",
      "Train Epoch: 546 [11776/54000 (22%)] Loss: -425.765472\n",
      "Train Epoch: 546 [23040/54000 (43%)] Loss: -514.546814\n",
      "Train Epoch: 546 [34304/54000 (64%)] Loss: -401.669312\n",
      "Train Epoch: 546 [45568/54000 (84%)] Loss: -419.912842\n",
      "    epoch          : 546\n",
      "    loss           : -353.39394660949705\n",
      "    val_loss       : -297.94537899978457\n",
      "    val_log_likelihood: 502.996923828125\n",
      "    val_log_marginal: 307.84253313057144\n",
      "Train Epoch: 547 [512/54000 (1%)] Loss: -386.253632\n",
      "Train Epoch: 547 [11776/54000 (22%)] Loss: -178.655533\n",
      "Train Epoch: 547 [23040/54000 (43%)] Loss: -171.036407\n",
      "Train Epoch: 547 [34304/54000 (64%)] Loss: -397.508789\n",
      "Train Epoch: 547 [45568/54000 (84%)] Loss: -453.494507\n",
      "    epoch          : 547\n",
      "    loss           : -335.5237009048462\n",
      "    val_loss       : -372.7476380778477\n",
      "    val_log_likelihood: 501.7381134033203\n",
      "    val_log_marginal: 378.7749308347121\n",
      "Train Epoch: 548 [512/54000 (1%)] Loss: -470.669983\n",
      "Train Epoch: 548 [11776/54000 (22%)] Loss: -431.270020\n",
      "Train Epoch: 548 [23040/54000 (43%)] Loss: -197.234802\n",
      "Train Epoch: 548 [34304/54000 (64%)] Loss: -325.236755\n",
      "Train Epoch: 548 [45568/54000 (84%)] Loss: -179.532013\n",
      "    epoch          : 548\n",
      "    loss           : -311.03629341125486\n",
      "    val_loss       : -239.38472690116615\n",
      "    val_log_likelihood: 485.59601440429685\n",
      "    val_log_marginal: 247.58391362843113\n",
      "Train Epoch: 549 [512/54000 (1%)] Loss: -417.097809\n",
      "Train Epoch: 549 [11776/54000 (22%)] Loss: -352.056946\n",
      "Train Epoch: 549 [23040/54000 (43%)] Loss: -401.947205\n",
      "Train Epoch: 549 [34304/54000 (64%)] Loss: -268.525879\n",
      "Train Epoch: 549 [45568/54000 (84%)] Loss: -462.482361\n",
      "    epoch          : 549\n",
      "    loss           : -355.27935527801515\n",
      "    val_loss       : -391.4326290048659\n",
      "    val_log_likelihood: 507.27337951660155\n",
      "    val_log_marginal: 398.048814837262\n",
      "Train Epoch: 550 [512/54000 (1%)] Loss: -280.540985\n",
      "Train Epoch: 550 [11776/54000 (22%)] Loss: -448.959045\n",
      "Train Epoch: 550 [23040/54000 (43%)] Loss: -453.630829\n",
      "Train Epoch: 550 [34304/54000 (64%)] Loss: -229.999130\n",
      "Train Epoch: 550 [45568/54000 (84%)] Loss: -565.763794\n",
      "    epoch          : 550\n",
      "    loss           : -428.0646240234375\n",
      "    val_loss       : -397.863451591786\n",
      "    val_log_likelihood: 513.8994323730469\n",
      "    val_log_marginal: 403.1369444038719\n",
      "Train Epoch: 551 [512/54000 (1%)] Loss: -455.535034\n",
      "Train Epoch: 551 [11776/54000 (22%)] Loss: -445.878632\n",
      "Train Epoch: 551 [23040/54000 (43%)] Loss: -474.288147\n",
      "Train Epoch: 551 [34304/54000 (64%)] Loss: -236.462173\n",
      "Train Epoch: 551 [45568/54000 (84%)] Loss: -409.285095\n",
      "    epoch          : 551\n",
      "    loss           : -436.020333404541\n",
      "    val_loss       : -403.6453609565273\n",
      "    val_log_likelihood: 518.5831573486328\n",
      "    val_log_marginal: 408.9661776449531\n",
      "Train Epoch: 552 [512/54000 (1%)] Loss: -579.545105\n",
      "Train Epoch: 552 [11776/54000 (22%)] Loss: -498.136688\n",
      "Train Epoch: 552 [23040/54000 (43%)] Loss: -458.052429\n",
      "Train Epoch: 552 [34304/54000 (64%)] Loss: -587.152710\n",
      "Train Epoch: 552 [45568/54000 (84%)] Loss: -464.877777\n",
      "    epoch          : 552\n",
      "    loss           : -439.54169631958007\n",
      "    val_loss       : -403.22632044665517\n",
      "    val_log_likelihood: 518.8723266601562\n",
      "    val_log_marginal: 408.31712177433076\n",
      "Train Epoch: 553 [512/54000 (1%)] Loss: -493.811951\n",
      "Train Epoch: 553 [11776/54000 (22%)] Loss: -502.033661\n",
      "Train Epoch: 553 [23040/54000 (43%)] Loss: -503.783844\n",
      "Train Epoch: 553 [34304/54000 (64%)] Loss: -286.154144\n",
      "Train Epoch: 553 [45568/54000 (84%)] Loss: -451.687592\n",
      "    epoch          : 553\n",
      "    loss           : -441.00019317626953\n",
      "    val_loss       : -404.47869685553013\n",
      "    val_log_likelihood: 523.1425720214844\n",
      "    val_log_marginal: 409.3685881499248\n",
      "Train Epoch: 554 [512/54000 (1%)] Loss: -498.270630\n",
      "Train Epoch: 554 [11776/54000 (22%)] Loss: -493.670746\n",
      "Train Epoch: 554 [23040/54000 (43%)] Loss: -467.580750\n",
      "Train Epoch: 554 [34304/54000 (64%)] Loss: -418.700470\n",
      "Train Epoch: 554 [45568/54000 (84%)] Loss: -400.930115\n",
      "    epoch          : 554\n",
      "    loss           : -435.9565249633789\n",
      "    val_loss       : -383.0030345834792\n",
      "    val_log_likelihood: 513.6406921386719\n",
      "    val_log_marginal: 386.9422762725502\n",
      "Train Epoch: 555 [512/54000 (1%)] Loss: -566.628296\n",
      "Train Epoch: 555 [11776/54000 (22%)] Loss: -209.294342\n",
      "Train Epoch: 555 [23040/54000 (43%)] Loss: -498.151489\n",
      "Train Epoch: 555 [34304/54000 (64%)] Loss: -491.260529\n",
      "Train Epoch: 555 [45568/54000 (84%)] Loss: -464.529266\n",
      "    epoch          : 555\n",
      "    loss           : -423.3635479736328\n",
      "    val_loss       : -387.5191932437941\n",
      "    val_log_likelihood: 517.6732238769531\n",
      "    val_log_marginal: 394.45767566002934\n",
      "Train Epoch: 556 [512/54000 (1%)] Loss: -241.666367\n",
      "Train Epoch: 556 [11776/54000 (22%)] Loss: -411.945618\n",
      "Train Epoch: 556 [23040/54000 (43%)] Loss: -454.934174\n",
      "Train Epoch: 556 [34304/54000 (64%)] Loss: -436.393921\n",
      "Train Epoch: 556 [45568/54000 (84%)] Loss: -288.948242\n",
      "    epoch          : 556\n",
      "    loss           : -412.9188458251953\n",
      "    val_loss       : -331.8552108043805\n",
      "    val_log_likelihood: 505.0945648193359\n",
      "    val_log_marginal: 340.6168691490203\n",
      "Train Epoch: 557 [512/54000 (1%)] Loss: -379.361023\n",
      "Train Epoch: 557 [11776/54000 (22%)] Loss: -198.704498\n",
      "Train Epoch: 557 [23040/54000 (43%)] Loss: -509.002319\n",
      "Train Epoch: 557 [34304/54000 (64%)] Loss: -197.230545\n",
      "Train Epoch: 557 [45568/54000 (84%)] Loss: -463.742798\n",
      "    epoch          : 557\n",
      "    loss           : -400.0472102355957\n",
      "    val_loss       : -385.81202559769156\n",
      "    val_log_likelihood: 513.2261047363281\n",
      "    val_log_marginal: 391.390967838885\n",
      "Train Epoch: 558 [512/54000 (1%)] Loss: -451.125122\n",
      "Train Epoch: 558 [11776/54000 (22%)] Loss: -499.465881\n",
      "Train Epoch: 558 [23040/54000 (43%)] Loss: -477.441193\n",
      "Train Epoch: 558 [34304/54000 (64%)] Loss: -402.264893\n",
      "Train Epoch: 558 [45568/54000 (84%)] Loss: -253.764786\n",
      "    epoch          : 558\n",
      "    loss           : -423.7561485290527\n",
      "    val_loss       : -387.7844161417335\n",
      "    val_log_likelihood: 517.3815612792969\n",
      "    val_log_marginal: 392.88913607560096\n",
      "Train Epoch: 559 [512/54000 (1%)] Loss: -437.561157\n",
      "Train Epoch: 559 [11776/54000 (22%)] Loss: -494.279175\n",
      "Train Epoch: 559 [23040/54000 (43%)] Loss: -406.540344\n",
      "Train Epoch: 559 [34304/54000 (64%)] Loss: -392.319092\n",
      "Train Epoch: 559 [45568/54000 (84%)] Loss: -240.480011\n",
      "    epoch          : 559\n",
      "    loss           : -380.05500633239745\n",
      "    val_loss       : -329.93621129095555\n",
      "    val_log_likelihood: 500.8613525390625\n",
      "    val_log_marginal: 338.53770204894244\n",
      "Train Epoch: 560 [512/54000 (1%)] Loss: -407.986877\n",
      "Train Epoch: 560 [11776/54000 (22%)] Loss: -449.225586\n",
      "Train Epoch: 560 [23040/54000 (43%)] Loss: -465.609528\n",
      "Train Epoch: 560 [34304/54000 (64%)] Loss: -417.914368\n",
      "Train Epoch: 560 [45568/54000 (84%)] Loss: -400.452209\n",
      "    epoch          : 560\n",
      "    loss           : -402.9274446105957\n",
      "    val_loss       : -374.51442506928\n",
      "    val_log_likelihood: 511.8499328613281\n",
      "    val_log_marginal: 382.0224656742066\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch560.pth ...\n",
      "Train Epoch: 561 [512/54000 (1%)] Loss: -280.568420\n",
      "Train Epoch: 561 [11776/54000 (22%)] Loss: -443.808289\n",
      "Train Epoch: 561 [23040/54000 (43%)] Loss: -429.432617\n",
      "Train Epoch: 561 [34304/54000 (64%)] Loss: -417.276611\n",
      "Train Epoch: 561 [45568/54000 (84%)] Loss: -247.864410\n",
      "    epoch          : 561\n",
      "    loss           : -389.38920623779296\n",
      "    val_loss       : -332.6171011090279\n",
      "    val_log_likelihood: 502.12915954589846\n",
      "    val_log_marginal: 347.10267959497867\n",
      "Train Epoch: 562 [512/54000 (1%)] Loss: -354.749451\n",
      "Train Epoch: 562 [11776/54000 (22%)] Loss: -411.189423\n",
      "Train Epoch: 562 [23040/54000 (43%)] Loss: -533.033936\n",
      "Train Epoch: 562 [34304/54000 (64%)] Loss: -440.150543\n",
      "Train Epoch: 562 [45568/54000 (84%)] Loss: -217.433807\n",
      "    epoch          : 562\n",
      "    loss           : -388.92102813720703\n",
      "    val_loss       : -366.8026644619182\n",
      "    val_log_likelihood: 510.4889343261719\n",
      "    val_log_marginal: 373.25010663082287\n",
      "Train Epoch: 563 [512/54000 (1%)] Loss: -445.259155\n",
      "Train Epoch: 563 [11776/54000 (22%)] Loss: -465.499390\n",
      "Train Epoch: 563 [23040/54000 (43%)] Loss: -462.285767\n",
      "Train Epoch: 563 [34304/54000 (64%)] Loss: -321.771057\n",
      "Train Epoch: 563 [45568/54000 (84%)] Loss: -400.976959\n",
      "    epoch          : 563\n",
      "    loss           : -371.6908306121826\n",
      "    val_loss       : -353.03263216037305\n",
      "    val_log_likelihood: 502.8112426757813\n",
      "    val_log_marginal: 365.17764764536867\n",
      "Train Epoch: 564 [512/54000 (1%)] Loss: -200.917694\n",
      "Train Epoch: 564 [11776/54000 (22%)] Loss: -450.438019\n",
      "Train Epoch: 564 [23040/54000 (43%)] Loss: -462.900696\n",
      "Train Epoch: 564 [34304/54000 (64%)] Loss: -488.785370\n",
      "Train Epoch: 564 [45568/54000 (84%)] Loss: -471.361237\n",
      "    epoch          : 564\n",
      "    loss           : -418.5400033569336\n",
      "    val_loss       : -388.82462150454523\n",
      "    val_log_likelihood: 511.5484985351562\n",
      "    val_log_marginal: 394.85909441821275\n",
      "Train Epoch: 565 [512/54000 (1%)] Loss: -484.402344\n",
      "Train Epoch: 565 [11776/54000 (22%)] Loss: -483.021454\n",
      "Train Epoch: 565 [23040/54000 (43%)] Loss: -442.931671\n",
      "Train Epoch: 565 [34304/54000 (64%)] Loss: -285.319641\n",
      "Train Epoch: 565 [45568/54000 (84%)] Loss: -473.707306\n",
      "    epoch          : 565\n",
      "    loss           : -430.8962405395508\n",
      "    val_loss       : -389.24631666988137\n",
      "    val_log_likelihood: 518.4455810546875\n",
      "    val_log_marginal: 396.48162636719655\n",
      "Train Epoch: 566 [512/54000 (1%)] Loss: -227.859756\n",
      "Train Epoch: 566 [11776/54000 (22%)] Loss: -482.528503\n",
      "Train Epoch: 566 [23040/54000 (43%)] Loss: -437.492676\n",
      "Train Epoch: 566 [34304/54000 (64%)] Loss: -248.934814\n",
      "Train Epoch: 566 [45568/54000 (84%)] Loss: -457.227264\n",
      "    epoch          : 566\n",
      "    loss           : -436.54081359863284\n",
      "    val_loss       : -394.0523144066334\n",
      "    val_log_likelihood: 519.7516204833985\n",
      "    val_log_marginal: 400.41478053519376\n",
      "Train Epoch: 567 [512/54000 (1%)] Loss: -460.060242\n",
      "Train Epoch: 567 [11776/54000 (22%)] Loss: -459.038483\n",
      "Train Epoch: 567 [23040/54000 (43%)] Loss: -470.368713\n",
      "Train Epoch: 567 [34304/54000 (64%)] Loss: -487.616547\n",
      "Train Epoch: 567 [45568/54000 (84%)] Loss: -248.533203\n",
      "    epoch          : 567\n",
      "    loss           : -429.440059967041\n",
      "    val_loss       : -377.6047337191179\n",
      "    val_log_likelihood: 514.0964599609375\n",
      "    val_log_marginal: 382.7957677062601\n",
      "Train Epoch: 568 [512/54000 (1%)] Loss: -264.488251\n",
      "Train Epoch: 568 [11776/54000 (22%)] Loss: -463.992615\n",
      "Train Epoch: 568 [23040/54000 (43%)] Loss: -513.405273\n",
      "Train Epoch: 568 [34304/54000 (64%)] Loss: -187.484680\n",
      "Train Epoch: 568 [45568/54000 (84%)] Loss: -353.263214\n",
      "    epoch          : 568\n",
      "    loss           : -366.4475437164307\n",
      "    val_loss       : -289.67116240300237\n",
      "    val_log_likelihood: 493.4852294921875\n",
      "    val_log_marginal: 311.9865562241524\n",
      "Train Epoch: 569 [512/54000 (1%)] Loss: -345.853394\n",
      "Train Epoch: 569 [11776/54000 (22%)] Loss: -387.470978\n",
      "Train Epoch: 569 [23040/54000 (43%)] Loss: -191.215027\n",
      "Train Epoch: 569 [34304/54000 (64%)] Loss: -377.900024\n",
      "Train Epoch: 569 [45568/54000 (84%)] Loss: -218.592789\n",
      "    epoch          : 569\n",
      "    loss           : -361.94273330688475\n",
      "    val_loss       : -316.98836570382116\n",
      "    val_log_likelihood: 499.0728332519531\n",
      "    val_log_marginal: 337.19908073879776\n",
      "Train Epoch: 570 [512/54000 (1%)] Loss: -351.978210\n",
      "Train Epoch: 570 [11776/54000 (22%)] Loss: -359.014282\n",
      "Train Epoch: 570 [23040/54000 (43%)] Loss: -227.064743\n",
      "Train Epoch: 570 [34304/54000 (64%)] Loss: -308.309021\n",
      "Train Epoch: 570 [45568/54000 (84%)] Loss: -265.030762\n",
      "    epoch          : 570\n",
      "    loss           : -274.49350927352907\n",
      "    val_loss       : -268.12457856610416\n",
      "    val_log_likelihood: 477.3124633789063\n",
      "    val_log_marginal: 280.71025667453796\n",
      "Train Epoch: 571 [512/54000 (1%)] Loss: -317.751801\n",
      "Train Epoch: 571 [11776/54000 (22%)] Loss: -252.020035\n",
      "Train Epoch: 571 [23040/54000 (43%)] Loss: -163.502136\n",
      "Train Epoch: 571 [34304/54000 (64%)] Loss: -355.685791\n",
      "Train Epoch: 571 [45568/54000 (84%)] Loss: -355.091888\n",
      "    epoch          : 571\n",
      "    loss           : -332.8364561462402\n",
      "    val_loss       : -332.58071719892325\n",
      "    val_log_likelihood: 495.64366149902344\n",
      "    val_log_marginal: 336.8234389882535\n",
      "Train Epoch: 572 [512/54000 (1%)] Loss: -391.138947\n",
      "Train Epoch: 572 [11776/54000 (22%)] Loss: -405.959717\n",
      "Train Epoch: 572 [23040/54000 (43%)] Loss: -476.272400\n",
      "Train Epoch: 572 [34304/54000 (64%)] Loss: -443.905060\n",
      "Train Epoch: 572 [45568/54000 (84%)] Loss: -181.661545\n",
      "    epoch          : 572\n",
      "    loss           : -385.6986669921875\n",
      "    val_loss       : -365.9717593818903\n",
      "    val_log_likelihood: 506.6252899169922\n",
      "    val_log_marginal: 370.52424597479404\n",
      "Train Epoch: 573 [512/54000 (1%)] Loss: -529.304138\n",
      "Train Epoch: 573 [11776/54000 (22%)] Loss: -480.395569\n",
      "Train Epoch: 573 [23040/54000 (43%)] Loss: -281.922150\n",
      "Train Epoch: 573 [34304/54000 (64%)] Loss: -445.134399\n",
      "Train Epoch: 573 [45568/54000 (84%)] Loss: -463.672089\n",
      "    epoch          : 573\n",
      "    loss           : -428.8793782043457\n",
      "    val_loss       : -401.9259622849524\n",
      "    val_log_likelihood: 519.4288391113281\n",
      "    val_log_marginal: 407.52521544657645\n",
      "Train Epoch: 574 [512/54000 (1%)] Loss: -464.265778\n",
      "Train Epoch: 574 [11776/54000 (22%)] Loss: -256.644592\n",
      "Train Epoch: 574 [23040/54000 (43%)] Loss: -440.734680\n",
      "Train Epoch: 574 [34304/54000 (64%)] Loss: -446.644714\n",
      "Train Epoch: 574 [45568/54000 (84%)] Loss: -418.740692\n",
      "    epoch          : 574\n",
      "    loss           : -440.5695967102051\n",
      "    val_loss       : -403.7772676911205\n",
      "    val_log_likelihood: 520.8462951660156\n",
      "    val_log_marginal: 410.0240527939051\n",
      "Train Epoch: 575 [512/54000 (1%)] Loss: -481.202393\n",
      "Train Epoch: 575 [11776/54000 (22%)] Loss: -582.769409\n",
      "Train Epoch: 575 [23040/54000 (43%)] Loss: -458.821228\n",
      "Train Epoch: 575 [34304/54000 (64%)] Loss: -494.193298\n",
      "Train Epoch: 575 [45568/54000 (84%)] Loss: -493.886810\n",
      "    epoch          : 575\n",
      "    loss           : -438.4298042297363\n",
      "    val_loss       : -393.2239875953645\n",
      "    val_log_likelihood: 518.4445068359375\n",
      "    val_log_marginal: 399.07915860898794\n",
      "Train Epoch: 576 [512/54000 (1%)] Loss: -487.599335\n",
      "Train Epoch: 576 [11776/54000 (22%)] Loss: -474.022095\n",
      "Train Epoch: 576 [23040/54000 (43%)] Loss: -466.328918\n",
      "Train Epoch: 576 [34304/54000 (64%)] Loss: -451.242493\n",
      "Train Epoch: 576 [45568/54000 (84%)] Loss: -237.847580\n",
      "    epoch          : 576\n",
      "    loss           : -434.2463656616211\n",
      "    val_loss       : -393.80274436995387\n",
      "    val_log_likelihood: 520.2846252441407\n",
      "    val_log_marginal: 401.91200731135905\n",
      "Train Epoch: 577 [512/54000 (1%)] Loss: -570.410767\n",
      "Train Epoch: 577 [11776/54000 (22%)] Loss: -414.969788\n",
      "Train Epoch: 577 [23040/54000 (43%)] Loss: -218.658905\n",
      "Train Epoch: 577 [34304/54000 (64%)] Loss: -494.615845\n",
      "Train Epoch: 577 [45568/54000 (84%)] Loss: -475.787720\n",
      "    epoch          : 577\n",
      "    loss           : -414.12407775878904\n",
      "    val_loss       : -396.8147521702573\n",
      "    val_log_likelihood: 520.4467529296875\n",
      "    val_log_marginal: 403.71522366187736\n",
      "Train Epoch: 578 [512/54000 (1%)] Loss: -292.713196\n",
      "Train Epoch: 578 [11776/54000 (22%)] Loss: -495.833435\n",
      "Train Epoch: 578 [23040/54000 (43%)] Loss: -469.120789\n",
      "Train Epoch: 578 [34304/54000 (64%)] Loss: -450.151489\n",
      "Train Epoch: 578 [45568/54000 (84%)] Loss: -472.184723\n",
      "    epoch          : 578\n",
      "    loss           : -434.7690306091309\n",
      "    val_loss       : -394.7263535981998\n",
      "    val_log_likelihood: 523.2233428955078\n",
      "    val_log_marginal: 403.076767816022\n",
      "Train Epoch: 579 [512/54000 (1%)] Loss: -452.681396\n",
      "Train Epoch: 579 [11776/54000 (22%)] Loss: -466.890747\n",
      "Train Epoch: 579 [23040/54000 (43%)] Loss: -171.297592\n",
      "Train Epoch: 579 [34304/54000 (64%)] Loss: -423.083893\n",
      "Train Epoch: 579 [45568/54000 (84%)] Loss: -198.558258\n",
      "    epoch          : 579\n",
      "    loss           : -388.77263793945315\n",
      "    val_loss       : -323.2922426626086\n",
      "    val_log_likelihood: 492.0371887207031\n",
      "    val_log_marginal: 333.83659388013183\n",
      "Train Epoch: 580 [512/54000 (1%)] Loss: -500.016449\n",
      "Train Epoch: 580 [11776/54000 (22%)] Loss: -337.579712\n",
      "Train Epoch: 580 [23040/54000 (43%)] Loss: -405.281494\n",
      "Train Epoch: 580 [34304/54000 (64%)] Loss: -191.335175\n",
      "Train Epoch: 580 [45568/54000 (84%)] Loss: -359.133331\n",
      "    epoch          : 580\n",
      "    loss           : -366.47159606933593\n",
      "    val_loss       : -369.55070383772255\n",
      "    val_log_likelihood: 507.63605041503905\n",
      "    val_log_marginal: 378.0097781706601\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch580.pth ...\n",
      "Train Epoch: 581 [512/54000 (1%)] Loss: -431.468811\n",
      "Train Epoch: 581 [11776/54000 (22%)] Loss: -465.760406\n",
      "Train Epoch: 581 [23040/54000 (43%)] Loss: -466.732178\n",
      "Train Epoch: 581 [34304/54000 (64%)] Loss: -384.157745\n",
      "Train Epoch: 581 [45568/54000 (84%)] Loss: -240.304230\n",
      "    epoch          : 581\n",
      "    loss           : -401.11559158325196\n",
      "    val_loss       : -331.9425599841401\n",
      "    val_log_likelihood: 512.1734497070313\n",
      "    val_log_marginal: 337.1269192185253\n",
      "Train Epoch: 582 [512/54000 (1%)] Loss: -412.095520\n",
      "Train Epoch: 582 [11776/54000 (22%)] Loss: -421.419159\n",
      "Train Epoch: 582 [23040/54000 (43%)] Loss: -437.271240\n",
      "Train Epoch: 582 [34304/54000 (64%)] Loss: -476.445435\n",
      "Train Epoch: 582 [45568/54000 (84%)] Loss: -268.176453\n",
      "    epoch          : 582\n",
      "    loss           : -387.83385635375976\n",
      "    val_loss       : -356.81317285317925\n",
      "    val_log_likelihood: 495.1400451660156\n",
      "    val_log_marginal: 364.33425102047624\n",
      "Train Epoch: 583 [512/54000 (1%)] Loss: -408.722260\n",
      "Train Epoch: 583 [11776/54000 (22%)] Loss: -583.377686\n",
      "Train Epoch: 583 [23040/54000 (43%)] Loss: -452.185913\n",
      "Train Epoch: 583 [34304/54000 (64%)] Loss: -443.809692\n",
      "Train Epoch: 583 [45568/54000 (84%)] Loss: -236.649551\n",
      "    epoch          : 583\n",
      "    loss           : -385.5069650268555\n",
      "    val_loss       : -379.81357064433394\n",
      "    val_log_likelihood: 514.0475372314453\n",
      "    val_log_marginal: 386.73847953855136\n",
      "Train Epoch: 584 [512/54000 (1%)] Loss: -419.155762\n",
      "Train Epoch: 584 [11776/54000 (22%)] Loss: -199.665131\n",
      "Train Epoch: 584 [23040/54000 (43%)] Loss: -200.488464\n",
      "Train Epoch: 584 [34304/54000 (64%)] Loss: -405.877350\n",
      "Train Epoch: 584 [45568/54000 (84%)] Loss: -477.925049\n",
      "    epoch          : 584\n",
      "    loss           : -422.22053817749025\n",
      "    val_loss       : -398.00963895916937\n",
      "    val_log_likelihood: 519.9609161376953\n",
      "    val_log_marginal: 403.3973961148545\n",
      "Train Epoch: 585 [512/54000 (1%)] Loss: -579.177002\n",
      "Train Epoch: 585 [11776/54000 (22%)] Loss: -273.603882\n",
      "Train Epoch: 585 [23040/54000 (43%)] Loss: -416.533386\n",
      "Train Epoch: 585 [34304/54000 (64%)] Loss: -446.327179\n",
      "Train Epoch: 585 [45568/54000 (84%)] Loss: -415.744629\n",
      "    epoch          : 585\n",
      "    loss           : -441.1295623779297\n",
      "    val_loss       : -404.6111825725064\n",
      "    val_log_likelihood: 525.8245727539063\n",
      "    val_log_marginal: 409.69220510311425\n",
      "Train Epoch: 586 [512/54000 (1%)] Loss: -466.973877\n",
      "Train Epoch: 586 [11776/54000 (22%)] Loss: -496.344818\n",
      "Train Epoch: 586 [23040/54000 (43%)] Loss: -250.757462\n",
      "Train Epoch: 586 [34304/54000 (64%)] Loss: -296.453430\n",
      "Train Epoch: 586 [45568/54000 (84%)] Loss: -463.093170\n",
      "    epoch          : 586\n",
      "    loss           : -442.9276802062988\n",
      "    val_loss       : -408.0884022096172\n",
      "    val_log_likelihood: 528.6243713378906\n",
      "    val_log_marginal: 414.3625494774436\n",
      "Train Epoch: 587 [512/54000 (1%)] Loss: -474.783356\n",
      "Train Epoch: 587 [11776/54000 (22%)] Loss: -502.981567\n",
      "Train Epoch: 587 [23040/54000 (43%)] Loss: -476.364563\n",
      "Train Epoch: 587 [34304/54000 (64%)] Loss: -454.309937\n",
      "Train Epoch: 587 [45568/54000 (84%)] Loss: -588.950134\n",
      "    epoch          : 587\n",
      "    loss           : -440.2538635253906\n",
      "    val_loss       : -401.52599660698326\n",
      "    val_log_likelihood: 525.6835235595703\n",
      "    val_log_marginal: 405.94180606119335\n",
      "Train Epoch: 588 [512/54000 (1%)] Loss: -500.760681\n",
      "Train Epoch: 588 [11776/54000 (22%)] Loss: -463.102905\n",
      "Train Epoch: 588 [23040/54000 (43%)] Loss: -484.588013\n",
      "Train Epoch: 588 [34304/54000 (64%)] Loss: -484.439697\n",
      "Train Epoch: 588 [45568/54000 (84%)] Loss: -264.012878\n",
      "    epoch          : 588\n",
      "    loss           : -431.55449417114255\n",
      "    val_loss       : -395.15054344013333\n",
      "    val_log_likelihood: 523.45263671875\n",
      "    val_log_marginal: 402.46462821550665\n",
      "Train Epoch: 589 [512/54000 (1%)] Loss: -247.867706\n",
      "Train Epoch: 589 [11776/54000 (22%)] Loss: -236.666168\n",
      "Train Epoch: 589 [23040/54000 (43%)] Loss: -589.080200\n",
      "Train Epoch: 589 [34304/54000 (64%)] Loss: -470.888550\n",
      "Train Epoch: 589 [45568/54000 (84%)] Loss: -447.537903\n",
      "    epoch          : 589\n",
      "    loss           : -433.0694338989258\n",
      "    val_loss       : -390.12130922302606\n",
      "    val_log_likelihood: 520.4105407714844\n",
      "    val_log_marginal: 395.7981742512435\n",
      "Train Epoch: 590 [512/54000 (1%)] Loss: -420.093018\n",
      "Train Epoch: 590 [11776/54000 (22%)] Loss: -580.480164\n",
      "Train Epoch: 590 [23040/54000 (43%)] Loss: -451.597351\n",
      "Train Epoch: 590 [34304/54000 (64%)] Loss: -434.510742\n",
      "Train Epoch: 590 [45568/54000 (84%)] Loss: -243.602371\n",
      "    epoch          : 590\n",
      "    loss           : -384.441897277832\n",
      "    val_loss       : -286.4257905524224\n",
      "    val_log_likelihood: 488.92054443359376\n",
      "    val_log_marginal: 289.5836593668908\n",
      "Train Epoch: 591 [512/54000 (1%)] Loss: -312.853271\n",
      "Train Epoch: 591 [11776/54000 (22%)] Loss: -142.719391\n",
      "Train Epoch: 591 [23040/54000 (43%)] Loss: -406.862061\n",
      "Train Epoch: 591 [34304/54000 (64%)] Loss: -524.891235\n",
      "Train Epoch: 591 [45568/54000 (84%)] Loss: -225.131668\n",
      "    epoch          : 591\n",
      "    loss           : -355.8086682891846\n",
      "    val_loss       : -333.61669383049014\n",
      "    val_log_likelihood: 511.20497436523436\n",
      "    val_log_marginal: 346.5937090266496\n",
      "Train Epoch: 592 [512/54000 (1%)] Loss: -556.850769\n",
      "Train Epoch: 592 [11776/54000 (22%)] Loss: -166.294144\n",
      "Train Epoch: 592 [23040/54000 (43%)] Loss: -375.155334\n",
      "Train Epoch: 592 [34304/54000 (64%)] Loss: -242.516296\n",
      "Train Epoch: 592 [45568/54000 (84%)] Loss: -457.787109\n",
      "    epoch          : 592\n",
      "    loss           : -383.92250549316407\n",
      "    val_loss       : -381.71256883647294\n",
      "    val_log_likelihood: 513.5270904541015\n",
      "    val_log_marginal: 388.940871879831\n",
      "Train Epoch: 593 [512/54000 (1%)] Loss: -372.817230\n",
      "Train Epoch: 593 [11776/54000 (22%)] Loss: -406.455994\n",
      "Train Epoch: 593 [23040/54000 (43%)] Loss: -485.891571\n",
      "Train Epoch: 593 [34304/54000 (64%)] Loss: -469.815186\n",
      "Train Epoch: 593 [45568/54000 (84%)] Loss: -471.235321\n",
      "    epoch          : 593\n",
      "    loss           : -427.9309646606445\n",
      "    val_loss       : -386.90904806312176\n",
      "    val_log_likelihood: 523.4247406005859\n",
      "    val_log_marginal: 392.8783315773323\n",
      "Train Epoch: 594 [512/54000 (1%)] Loss: -430.681335\n",
      "Train Epoch: 594 [11776/54000 (22%)] Loss: -476.737427\n",
      "Train Epoch: 594 [23040/54000 (43%)] Loss: -546.377014\n",
      "Train Epoch: 594 [34304/54000 (64%)] Loss: -272.573761\n",
      "Train Epoch: 594 [45568/54000 (84%)] Loss: -341.491547\n",
      "    epoch          : 594\n",
      "    loss           : -383.4584154510498\n",
      "    val_loss       : -351.5884560087696\n",
      "    val_log_likelihood: 491.8272399902344\n",
      "    val_log_marginal: 356.67713107283777\n",
      "Train Epoch: 595 [512/54000 (1%)] Loss: -406.745544\n",
      "Train Epoch: 595 [11776/54000 (22%)] Loss: -330.364075\n",
      "Train Epoch: 595 [23040/54000 (43%)] Loss: -429.888641\n",
      "Train Epoch: 595 [34304/54000 (64%)] Loss: -410.917419\n",
      "Train Epoch: 595 [45568/54000 (84%)] Loss: -249.070328\n",
      "    epoch          : 595\n",
      "    loss           : -375.25065238952635\n",
      "    val_loss       : -386.30878759678455\n",
      "    val_log_likelihood: 510.9963775634766\n",
      "    val_log_marginal: 391.45804847441616\n",
      "Train Epoch: 596 [512/54000 (1%)] Loss: -401.285583\n",
      "Train Epoch: 596 [11776/54000 (22%)] Loss: -469.757416\n",
      "Train Epoch: 596 [23040/54000 (43%)] Loss: -271.204102\n",
      "Train Epoch: 596 [34304/54000 (64%)] Loss: -225.528900\n",
      "Train Epoch: 596 [45568/54000 (84%)] Loss: -472.691772\n",
      "    epoch          : 596\n",
      "    loss           : -422.1470651245117\n",
      "    val_loss       : -398.1977451460436\n",
      "    val_log_likelihood: 518.8809234619141\n",
      "    val_log_marginal: 404.031541640684\n",
      "Train Epoch: 597 [512/54000 (1%)] Loss: -448.754608\n",
      "Train Epoch: 597 [11776/54000 (22%)] Loss: -451.314575\n",
      "Train Epoch: 597 [23040/54000 (43%)] Loss: -415.465302\n",
      "Train Epoch: 597 [34304/54000 (64%)] Loss: -369.737823\n",
      "Train Epoch: 597 [45568/54000 (84%)] Loss: -190.673859\n",
      "    epoch          : 597\n",
      "    loss           : -411.9651530456543\n",
      "    val_loss       : -352.4045849973336\n",
      "    val_log_likelihood: 516.7459869384766\n",
      "    val_log_marginal: 361.3115677010268\n",
      "Train Epoch: 598 [512/54000 (1%)] Loss: -258.031311\n",
      "Train Epoch: 598 [11776/54000 (22%)] Loss: -260.693970\n",
      "Train Epoch: 598 [23040/54000 (43%)] Loss: -429.194061\n",
      "Train Epoch: 598 [34304/54000 (64%)] Loss: -413.556946\n",
      "Train Epoch: 598 [45568/54000 (84%)] Loss: -255.280609\n",
      "    epoch          : 598\n",
      "    loss           : -393.75420043945314\n",
      "    val_loss       : -382.100420053117\n",
      "    val_log_likelihood: 511.09209594726565\n",
      "    val_log_marginal: 389.6517518799752\n",
      "Train Epoch: 599 [512/54000 (1%)] Loss: -399.641388\n",
      "Train Epoch: 599 [11776/54000 (22%)] Loss: -437.917450\n",
      "Train Epoch: 599 [23040/54000 (43%)] Loss: -406.469574\n",
      "Train Epoch: 599 [34304/54000 (64%)] Loss: -422.706848\n",
      "Train Epoch: 599 [45568/54000 (84%)] Loss: -192.001068\n",
      "    epoch          : 599\n",
      "    loss           : -360.6125270843506\n",
      "    val_loss       : -369.1774201504886\n",
      "    val_log_likelihood: 505.01287231445315\n",
      "    val_log_marginal: 379.77042650319635\n",
      "Train Epoch: 600 [512/54000 (1%)] Loss: -171.631088\n",
      "Train Epoch: 600 [11776/54000 (22%)] Loss: -237.366241\n",
      "Train Epoch: 600 [23040/54000 (43%)] Loss: -236.654968\n",
      "Train Epoch: 600 [34304/54000 (64%)] Loss: -388.609314\n",
      "Train Epoch: 600 [45568/54000 (84%)] Loss: -400.612274\n",
      "    epoch          : 600\n",
      "    loss           : -404.8111148071289\n",
      "    val_loss       : -381.15644183587284\n",
      "    val_log_likelihood: 505.8973022460938\n",
      "    val_log_marginal: 385.9654832649976\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch600.pth ...\n",
      "Train Epoch: 601 [512/54000 (1%)] Loss: -448.795288\n",
      "Train Epoch: 601 [11776/54000 (22%)] Loss: -457.340973\n",
      "Train Epoch: 601 [23040/54000 (43%)] Loss: -281.355377\n",
      "Train Epoch: 601 [34304/54000 (64%)] Loss: -262.025330\n",
      "Train Epoch: 601 [45568/54000 (84%)] Loss: -476.136932\n",
      "    epoch          : 601\n",
      "    loss           : -431.4727589416504\n",
      "    val_loss       : -402.8500588497147\n",
      "    val_log_likelihood: 520.1447113037109\n",
      "    val_log_marginal: 406.56972081388824\n",
      "Train Epoch: 602 [512/54000 (1%)] Loss: -451.097260\n",
      "Train Epoch: 602 [11776/54000 (22%)] Loss: -506.445953\n",
      "Train Epoch: 602 [23040/54000 (43%)] Loss: -495.499115\n",
      "Train Epoch: 602 [34304/54000 (64%)] Loss: -480.040161\n",
      "Train Epoch: 602 [45568/54000 (84%)] Loss: -401.093140\n",
      "    epoch          : 602\n",
      "    loss           : -434.2543948364258\n",
      "    val_loss       : -378.3063041752204\n",
      "    val_log_likelihood: 520.7932647705078\n",
      "    val_log_marginal: 387.99793533124034\n",
      "Train Epoch: 603 [512/54000 (1%)] Loss: -545.526733\n",
      "Train Epoch: 603 [11776/54000 (22%)] Loss: -480.543274\n",
      "Train Epoch: 603 [23040/54000 (43%)] Loss: -234.025055\n",
      "Train Epoch: 603 [34304/54000 (64%)] Loss: -472.625275\n",
      "Train Epoch: 603 [45568/54000 (84%)] Loss: -437.769531\n",
      "    epoch          : 603\n",
      "    loss           : -421.5145594787598\n",
      "    val_loss       : -396.82081709355117\n",
      "    val_log_likelihood: 519.9149200439454\n",
      "    val_log_marginal: 399.3863141334965\n",
      "Train Epoch: 604 [512/54000 (1%)] Loss: -583.958984\n",
      "Train Epoch: 604 [11776/54000 (22%)] Loss: -285.796509\n",
      "Train Epoch: 604 [23040/54000 (43%)] Loss: -586.317261\n",
      "Train Epoch: 604 [34304/54000 (64%)] Loss: -499.421204\n",
      "Train Epoch: 604 [45568/54000 (84%)] Loss: -474.672546\n",
      "    epoch          : 604\n",
      "    loss           : -433.94161758422854\n",
      "    val_loss       : -392.976181672886\n",
      "    val_log_likelihood: 525.0427673339843\n",
      "    val_log_marginal: 397.5551985356957\n",
      "Train Epoch: 605 [512/54000 (1%)] Loss: -421.828156\n",
      "Train Epoch: 605 [11776/54000 (22%)] Loss: -556.533691\n",
      "Train Epoch: 605 [23040/54000 (43%)] Loss: -466.058167\n",
      "Train Epoch: 605 [34304/54000 (64%)] Loss: -475.154419\n",
      "Train Epoch: 605 [45568/54000 (84%)] Loss: -460.042480\n",
      "    epoch          : 605\n",
      "    loss           : -430.5085969543457\n",
      "    val_loss       : -387.2560701880604\n",
      "    val_log_likelihood: 523.1168640136718\n",
      "    val_log_marginal: 393.7635484612558\n",
      "Train Epoch: 606 [512/54000 (1%)] Loss: -477.795807\n",
      "Train Epoch: 606 [11776/54000 (22%)] Loss: -222.992218\n",
      "Train Epoch: 606 [23040/54000 (43%)] Loss: -558.866211\n",
      "Train Epoch: 606 [34304/54000 (64%)] Loss: -478.218353\n",
      "Train Epoch: 606 [45568/54000 (84%)] Loss: -466.709290\n",
      "    epoch          : 606\n",
      "    loss           : -426.1210342407227\n",
      "    val_loss       : -383.76432918347416\n",
      "    val_log_likelihood: 522.4775482177735\n",
      "    val_log_marginal: 393.0816378865391\n",
      "Train Epoch: 607 [512/54000 (1%)] Loss: -260.204712\n",
      "Train Epoch: 607 [11776/54000 (22%)] Loss: -454.894226\n",
      "Train Epoch: 607 [23040/54000 (43%)] Loss: -471.719299\n",
      "Train Epoch: 607 [34304/54000 (64%)] Loss: -347.442413\n",
      "Train Epoch: 607 [45568/54000 (84%)] Loss: -214.461456\n",
      "    epoch          : 607\n",
      "    loss           : -378.99042892456055\n",
      "    val_loss       : -188.72819812707604\n",
      "    val_log_likelihood: 494.815234375\n",
      "    val_log_marginal: 214.246690237903\n",
      "Train Epoch: 608 [512/54000 (1%)] Loss: -405.612091\n",
      "Train Epoch: 608 [11776/54000 (22%)] Loss: -267.012512\n",
      "Train Epoch: 608 [23040/54000 (43%)] Loss: -445.989288\n",
      "Train Epoch: 608 [34304/54000 (64%)] Loss: -358.186707\n",
      "Train Epoch: 608 [45568/54000 (84%)] Loss: -284.915710\n",
      "    epoch          : 608\n",
      "    loss           : -375.59858253479\n",
      "    val_loss       : -396.6087546462193\n",
      "    val_log_likelihood: 517.5457824707031\n",
      "    val_log_marginal: 402.1560010287914\n",
      "Train Epoch: 609 [512/54000 (1%)] Loss: -424.266174\n",
      "Train Epoch: 609 [11776/54000 (22%)] Loss: -411.984558\n",
      "Train Epoch: 609 [23040/54000 (43%)] Loss: -498.640198\n",
      "Train Epoch: 609 [34304/54000 (64%)] Loss: -247.397736\n",
      "Train Epoch: 609 [45568/54000 (84%)] Loss: -545.687195\n",
      "    epoch          : 609\n",
      "    loss           : -405.74472381591795\n",
      "    val_loss       : -329.5044917456806\n",
      "    val_log_likelihood: 512.7700042724609\n",
      "    val_log_marginal: 335.0649999287354\n",
      "Train Epoch: 610 [512/54000 (1%)] Loss: -365.191620\n",
      "Train Epoch: 610 [11776/54000 (22%)] Loss: 159.389069\n",
      "Train Epoch: 610 [23040/54000 (43%)] Loss: -175.946091\n",
      "Train Epoch: 610 [34304/54000 (64%)] Loss: -455.374756\n",
      "Train Epoch: 610 [45568/54000 (84%)] Loss: -364.747375\n",
      "    epoch          : 610\n",
      "    loss           : -337.19710209369657\n",
      "    val_loss       : -372.9528858894482\n",
      "    val_log_likelihood: 517.3987823486328\n",
      "    val_log_marginal: 382.929443686828\n",
      "Train Epoch: 611 [512/54000 (1%)] Loss: -267.145630\n",
      "Train Epoch: 611 [11776/54000 (22%)] Loss: -490.875488\n",
      "Train Epoch: 611 [23040/54000 (43%)] Loss: -446.069336\n",
      "Train Epoch: 611 [34304/54000 (64%)] Loss: -213.684357\n",
      "Train Epoch: 611 [45568/54000 (84%)] Loss: -362.177460\n",
      "    epoch          : 611\n",
      "    loss           : -417.25718566894534\n",
      "    val_loss       : -367.2281350184232\n",
      "    val_log_likelihood: 506.56395263671874\n",
      "    val_log_marginal: 381.1538917560138\n",
      "Train Epoch: 612 [512/54000 (1%)] Loss: -422.828308\n",
      "Train Epoch: 612 [11776/54000 (22%)] Loss: -462.464935\n",
      "Train Epoch: 612 [23040/54000 (43%)] Loss: -475.580872\n",
      "Train Epoch: 612 [34304/54000 (64%)] Loss: -513.220032\n",
      "Train Epoch: 612 [45568/54000 (84%)] Loss: -252.489288\n",
      "    epoch          : 612\n",
      "    loss           : -426.23552795410154\n",
      "    val_loss       : -400.21120969057085\n",
      "    val_log_likelihood: 524.59609375\n",
      "    val_log_marginal: 405.62354473136367\n",
      "Train Epoch: 613 [512/54000 (1%)] Loss: -254.009979\n",
      "Train Epoch: 613 [11776/54000 (22%)] Loss: -587.496765\n",
      "Train Epoch: 613 [23040/54000 (43%)] Loss: -586.948364\n",
      "Train Epoch: 613 [34304/54000 (64%)] Loss: -446.535034\n",
      "Train Epoch: 613 [45568/54000 (84%)] Loss: -234.449097\n",
      "    epoch          : 613\n",
      "    loss           : -430.0136238098145\n",
      "    val_loss       : -396.03669934421777\n",
      "    val_log_likelihood: 525.0817047119141\n",
      "    val_log_marginal: 403.49972053430974\n",
      "Train Epoch: 614 [512/54000 (1%)] Loss: -481.099945\n",
      "Train Epoch: 614 [11776/54000 (22%)] Loss: -238.036179\n",
      "Train Epoch: 614 [23040/54000 (43%)] Loss: -242.443008\n",
      "Train Epoch: 614 [34304/54000 (64%)] Loss: -580.473083\n",
      "Train Epoch: 614 [45568/54000 (84%)] Loss: -411.727997\n",
      "    epoch          : 614\n",
      "    loss           : -433.34820068359375\n",
      "    val_loss       : -403.05996177718043\n",
      "    val_log_likelihood: 527.6625183105468\n",
      "    val_log_marginal: 410.4351694982499\n",
      "Train Epoch: 615 [512/54000 (1%)] Loss: -602.614868\n",
      "Train Epoch: 615 [11776/54000 (22%)] Loss: -450.439636\n",
      "Train Epoch: 615 [23040/54000 (43%)] Loss: -424.630493\n",
      "Train Epoch: 615 [34304/54000 (64%)] Loss: -485.505768\n",
      "Train Epoch: 615 [45568/54000 (84%)] Loss: -423.602020\n",
      "    epoch          : 615\n",
      "    loss           : -445.69316009521486\n",
      "    val_loss       : -403.34073092956095\n",
      "    val_log_likelihood: 526.0857299804687\n",
      "    val_log_marginal: 407.00445221848787\n",
      "Train Epoch: 616 [512/54000 (1%)] Loss: -248.729050\n",
      "Train Epoch: 616 [11776/54000 (22%)] Loss: -592.717163\n",
      "Train Epoch: 616 [23040/54000 (43%)] Loss: -420.479797\n",
      "Train Epoch: 616 [34304/54000 (64%)] Loss: -452.532135\n",
      "Train Epoch: 616 [45568/54000 (84%)] Loss: -385.176300\n",
      "    epoch          : 616\n",
      "    loss           : -424.6163775634766\n",
      "    val_loss       : -347.9290171455592\n",
      "    val_log_likelihood: 513.9637084960938\n",
      "    val_log_marginal: 361.1514798227803\n",
      "Train Epoch: 617 [512/54000 (1%)] Loss: -399.790192\n",
      "Train Epoch: 617 [11776/54000 (22%)] Loss: -370.045227\n",
      "Train Epoch: 617 [23040/54000 (43%)] Loss: -249.907883\n",
      "Train Epoch: 617 [34304/54000 (64%)] Loss: -271.766541\n",
      "Train Epoch: 617 [45568/54000 (84%)] Loss: -213.928192\n",
      "    epoch          : 617\n",
      "    loss           : -377.17240898132326\n",
      "    val_loss       : -373.76671102046964\n",
      "    val_log_likelihood: 513.6954986572266\n",
      "    val_log_marginal: 382.45734147168696\n",
      "Train Epoch: 618 [512/54000 (1%)] Loss: -485.216736\n",
      "Train Epoch: 618 [11776/54000 (22%)] Loss: -370.847290\n",
      "Train Epoch: 618 [23040/54000 (43%)] Loss: -355.751709\n",
      "Train Epoch: 618 [34304/54000 (64%)] Loss: -163.566299\n",
      "Train Epoch: 618 [45568/54000 (84%)] Loss: -222.166122\n",
      "    epoch          : 618\n",
      "    loss           : -297.187053732872\n",
      "    val_loss       : -274.48913812600074\n",
      "    val_log_likelihood: 492.98834228515625\n",
      "    val_log_marginal: 292.9435594269478\n",
      "Train Epoch: 619 [512/54000 (1%)] Loss: -431.514038\n",
      "Train Epoch: 619 [11776/54000 (22%)] Loss: -362.465424\n",
      "Train Epoch: 619 [23040/54000 (43%)] Loss: -370.261963\n",
      "Train Epoch: 619 [34304/54000 (64%)] Loss: -355.602142\n",
      "Train Epoch: 619 [45568/54000 (84%)] Loss: -389.579041\n",
      "    epoch          : 619\n",
      "    loss           : -360.81434715270996\n",
      "    val_loss       : -378.7425785029307\n",
      "    val_log_likelihood: 511.55372619628906\n",
      "    val_log_marginal: 388.29699747655883\n",
      "Train Epoch: 620 [512/54000 (1%)] Loss: -399.920837\n",
      "Train Epoch: 620 [11776/54000 (22%)] Loss: -486.717407\n",
      "Train Epoch: 620 [23040/54000 (43%)] Loss: -469.757782\n",
      "Train Epoch: 620 [34304/54000 (64%)] Loss: -484.086517\n",
      "Train Epoch: 620 [45568/54000 (84%)] Loss: -479.537598\n",
      "    epoch          : 620\n",
      "    loss           : -438.42057037353516\n",
      "    val_loss       : -408.5983184138313\n",
      "    val_log_likelihood: 526.2165832519531\n",
      "    val_log_marginal: 413.32887940598283\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch620.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 621 [512/54000 (1%)] Loss: -271.695190\n",
      "Train Epoch: 621 [11776/54000 (22%)] Loss: -480.843689\n",
      "Train Epoch: 621 [23040/54000 (43%)] Loss: -602.235474\n",
      "Train Epoch: 621 [34304/54000 (64%)] Loss: -462.834015\n",
      "Train Epoch: 621 [45568/54000 (84%)] Loss: -318.484802\n",
      "    epoch          : 621\n",
      "    loss           : -448.91582763671875\n",
      "    val_loss       : -410.8579454109073\n",
      "    val_log_likelihood: 532.4726257324219\n",
      "    val_log_marginal: 416.3017981965095\n",
      "Train Epoch: 622 [512/54000 (1%)] Loss: -455.515381\n",
      "Train Epoch: 622 [11776/54000 (22%)] Loss: -485.521637\n",
      "Train Epoch: 622 [23040/54000 (43%)] Loss: -479.951935\n",
      "Train Epoch: 622 [34304/54000 (64%)] Loss: -266.647369\n",
      "Train Epoch: 622 [45568/54000 (84%)] Loss: -299.928558\n",
      "    epoch          : 622\n",
      "    loss           : -450.2892611694336\n",
      "    val_loss       : -413.56939617898314\n",
      "    val_log_likelihood: 533.1126678466796\n",
      "    val_log_marginal: 418.1865211326629\n",
      "Train Epoch: 623 [512/54000 (1%)] Loss: -454.415497\n",
      "Train Epoch: 623 [11776/54000 (22%)] Loss: -266.130157\n",
      "Train Epoch: 623 [23040/54000 (43%)] Loss: -605.307312\n",
      "Train Epoch: 623 [34304/54000 (64%)] Loss: -598.703186\n",
      "Train Epoch: 623 [45568/54000 (84%)] Loss: -479.588074\n",
      "    epoch          : 623\n",
      "    loss           : -451.6047950744629\n",
      "    val_loss       : -408.3308154013008\n",
      "    val_log_likelihood: 531.0439270019531\n",
      "    val_log_marginal: 416.3429774459452\n",
      "Train Epoch: 624 [512/54000 (1%)] Loss: -456.992218\n",
      "Train Epoch: 624 [11776/54000 (22%)] Loss: -464.554077\n",
      "Train Epoch: 624 [23040/54000 (43%)] Loss: -455.818970\n",
      "Train Epoch: 624 [34304/54000 (64%)] Loss: -310.861053\n",
      "Train Epoch: 624 [45568/54000 (84%)] Loss: -234.428192\n",
      "    epoch          : 624\n",
      "    loss           : -447.9083169555664\n",
      "    val_loss       : -406.82056697532533\n",
      "    val_log_likelihood: 530.9526672363281\n",
      "    val_log_marginal: 413.9711879637092\n",
      "Train Epoch: 625 [512/54000 (1%)] Loss: -474.574646\n",
      "Train Epoch: 625 [11776/54000 (22%)] Loss: -425.902710\n",
      "Train Epoch: 625 [23040/54000 (43%)] Loss: -503.035004\n",
      "Train Epoch: 625 [34304/54000 (64%)] Loss: -288.075592\n",
      "Train Epoch: 625 [45568/54000 (84%)] Loss: -284.053406\n",
      "    epoch          : 625\n",
      "    loss           : -448.5695413208008\n",
      "    val_loss       : -404.3250771369785\n",
      "    val_log_likelihood: 531.871694946289\n",
      "    val_log_marginal: 411.23923341669143\n",
      "Train Epoch: 626 [512/54000 (1%)] Loss: -594.229126\n",
      "Train Epoch: 626 [11776/54000 (22%)] Loss: -497.461365\n",
      "Train Epoch: 626 [23040/54000 (43%)] Loss: -263.125610\n",
      "Train Epoch: 626 [34304/54000 (64%)] Loss: -432.764374\n",
      "Train Epoch: 626 [45568/54000 (84%)] Loss: -308.081726\n",
      "    epoch          : 626\n",
      "    loss           : -445.6173539733887\n",
      "    val_loss       : -403.7558459499851\n",
      "    val_log_likelihood: 531.9568817138672\n",
      "    val_log_marginal: 410.1056094247848\n",
      "Train Epoch: 627 [512/54000 (1%)] Loss: -482.231323\n",
      "Train Epoch: 627 [11776/54000 (22%)] Loss: -229.891998\n",
      "Train Epoch: 627 [23040/54000 (43%)] Loss: -216.653839\n",
      "Train Epoch: 627 [34304/54000 (64%)] Loss: -480.993652\n",
      "Train Epoch: 627 [45568/54000 (84%)] Loss: -411.396729\n",
      "    epoch          : 627\n",
      "    loss           : -428.89468872070313\n",
      "    val_loss       : -375.437255198881\n",
      "    val_log_likelihood: 526.7821380615235\n",
      "    val_log_marginal: 383.4985999920872\n",
      "Train Epoch: 628 [512/54000 (1%)] Loss: -283.198090\n",
      "Train Epoch: 628 [11776/54000 (22%)] Loss: -155.011673\n",
      "Train Epoch: 628 [23040/54000 (43%)] Loss: -478.396423\n",
      "Train Epoch: 628 [34304/54000 (64%)] Loss: -285.232941\n",
      "Train Epoch: 628 [45568/54000 (84%)] Loss: -363.103302\n",
      "    epoch          : 628\n",
      "    loss           : -418.74235900878904\n",
      "    val_loss       : -392.86407701615246\n",
      "    val_log_likelihood: 522.6098449707031\n",
      "    val_log_marginal: 398.3164595331997\n",
      "Train Epoch: 629 [512/54000 (1%)] Loss: -505.225006\n",
      "Train Epoch: 629 [11776/54000 (22%)] Loss: -233.744263\n",
      "Train Epoch: 629 [23040/54000 (43%)] Loss: -231.065506\n",
      "Train Epoch: 629 [34304/54000 (64%)] Loss: -264.873474\n",
      "Train Epoch: 629 [45568/54000 (84%)] Loss: -565.712769\n",
      "    epoch          : 629\n",
      "    loss           : -425.73708724975586\n",
      "    val_loss       : -399.2135554205626\n",
      "    val_log_likelihood: 526.9179016113281\n",
      "    val_log_marginal: 407.0887691888958\n",
      "Train Epoch: 630 [512/54000 (1%)] Loss: -291.376038\n",
      "Train Epoch: 630 [11776/54000 (22%)] Loss: -576.555725\n",
      "Train Epoch: 630 [23040/54000 (43%)] Loss: -498.536011\n",
      "Train Epoch: 630 [34304/54000 (64%)] Loss: -338.033691\n",
      "Train Epoch: 630 [45568/54000 (84%)] Loss: -430.213440\n",
      "    epoch          : 630\n",
      "    loss           : -419.9205744934082\n",
      "    val_loss       : -365.2281560396776\n",
      "    val_log_likelihood: 519.9686798095703\n",
      "    val_log_marginal: 375.58675303624085\n",
      "Train Epoch: 631 [512/54000 (1%)] Loss: -444.891479\n",
      "Train Epoch: 631 [11776/54000 (22%)] Loss: -487.183746\n",
      "Train Epoch: 631 [23040/54000 (43%)] Loss: -433.669861\n",
      "Train Epoch: 631 [34304/54000 (64%)] Loss: -580.292175\n",
      "Train Epoch: 631 [45568/54000 (84%)] Loss: -479.456726\n",
      "    epoch          : 631\n",
      "    loss           : -426.6810299682617\n",
      "    val_loss       : -397.75202739778905\n",
      "    val_log_likelihood: 529.5357421875\n",
      "    val_log_marginal: 404.45842672996224\n",
      "Train Epoch: 632 [512/54000 (1%)] Loss: -484.277771\n",
      "Train Epoch: 632 [11776/54000 (22%)] Loss: -434.555786\n",
      "Train Epoch: 632 [23040/54000 (43%)] Loss: -413.679810\n",
      "Train Epoch: 632 [34304/54000 (64%)] Loss: -236.528229\n",
      "Train Epoch: 632 [45568/54000 (84%)] Loss: -213.420578\n",
      "    epoch          : 632\n",
      "    loss           : -391.6538022994995\n",
      "    val_loss       : -357.3213776675984\n",
      "    val_log_likelihood: 494.17710876464844\n",
      "    val_log_marginal: 364.5998906698078\n",
      "Train Epoch: 633 [512/54000 (1%)] Loss: -439.740234\n",
      "Train Epoch: 633 [11776/54000 (22%)] Loss: -466.778778\n",
      "Train Epoch: 633 [23040/54000 (43%)] Loss: -344.741272\n",
      "Train Epoch: 633 [34304/54000 (64%)] Loss: -198.999527\n",
      "Train Epoch: 633 [45568/54000 (84%)] Loss: -359.099579\n",
      "    epoch          : 633\n",
      "    loss           : -387.4687447357178\n",
      "    val_loss       : -333.92940713688733\n",
      "    val_log_likelihood: 508.8259033203125\n",
      "    val_log_marginal: 344.2035110663623\n",
      "Train Epoch: 634 [512/54000 (1%)] Loss: -248.857452\n",
      "Train Epoch: 634 [11776/54000 (22%)] Loss: -340.791748\n",
      "Train Epoch: 634 [23040/54000 (43%)] Loss: -462.035553\n",
      "Train Epoch: 634 [34304/54000 (64%)] Loss: -261.781555\n",
      "Train Epoch: 634 [45568/54000 (84%)] Loss: -395.668427\n",
      "    epoch          : 634\n",
      "    loss           : -360.7931583404541\n",
      "    val_loss       : -366.3742848256603\n",
      "    val_log_likelihood: 516.0061920166015\n",
      "    val_log_marginal: 375.2459408778697\n",
      "Train Epoch: 635 [512/54000 (1%)] Loss: -446.948029\n",
      "Train Epoch: 635 [11776/54000 (22%)] Loss: -447.224243\n",
      "Train Epoch: 635 [23040/54000 (43%)] Loss: -464.658569\n",
      "Train Epoch: 635 [34304/54000 (64%)] Loss: -337.600403\n",
      "Train Epoch: 635 [45568/54000 (84%)] Loss: -436.613159\n",
      "    epoch          : 635\n",
      "    loss           : -401.2669915771484\n",
      "    val_loss       : -382.51625357940793\n",
      "    val_log_likelihood: 516.1360717773438\n",
      "    val_log_marginal: 388.46570187695323\n",
      "Train Epoch: 636 [512/54000 (1%)] Loss: -450.115540\n",
      "Train Epoch: 636 [11776/54000 (22%)] Loss: -392.149536\n",
      "Train Epoch: 636 [23040/54000 (43%)] Loss: -570.067627\n",
      "Train Epoch: 636 [34304/54000 (64%)] Loss: -468.141571\n",
      "Train Epoch: 636 [45568/54000 (84%)] Loss: -464.319763\n",
      "    epoch          : 636\n",
      "    loss           : -417.58327514648437\n",
      "    val_loss       : -402.7210799375549\n",
      "    val_log_likelihood: 529.365771484375\n",
      "    val_log_marginal: 408.40789456106734\n",
      "Train Epoch: 637 [512/54000 (1%)] Loss: -438.468384\n",
      "Train Epoch: 637 [11776/54000 (22%)] Loss: -476.264099\n",
      "Train Epoch: 637 [23040/54000 (43%)] Loss: -248.460205\n",
      "Train Epoch: 637 [34304/54000 (64%)] Loss: -607.248901\n",
      "Train Epoch: 637 [45568/54000 (84%)] Loss: -282.096680\n",
      "    epoch          : 637\n",
      "    loss           : -443.9142448425293\n",
      "    val_loss       : -407.46439374815674\n",
      "    val_log_likelihood: 531.5667205810547\n",
      "    val_log_marginal: 414.0408858876675\n",
      "Train Epoch: 638 [512/54000 (1%)] Loss: -500.183167\n",
      "Train Epoch: 638 [11776/54000 (22%)] Loss: -464.768860\n",
      "Train Epoch: 638 [23040/54000 (43%)] Loss: -464.870605\n",
      "Train Epoch: 638 [34304/54000 (64%)] Loss: -608.104492\n",
      "Train Epoch: 638 [45568/54000 (84%)] Loss: -424.194519\n",
      "    epoch          : 638\n",
      "    loss           : -449.56218734741213\n",
      "    val_loss       : -406.52977519743143\n",
      "    val_log_likelihood: 530.39072265625\n",
      "    val_log_marginal: 409.8148221556097\n",
      "Train Epoch: 639 [512/54000 (1%)] Loss: -503.427124\n",
      "Train Epoch: 639 [11776/54000 (22%)] Loss: -463.289185\n",
      "Train Epoch: 639 [23040/54000 (43%)] Loss: -234.394547\n",
      "Train Epoch: 639 [34304/54000 (64%)] Loss: -388.511139\n",
      "Train Epoch: 639 [45568/54000 (84%)] Loss: -428.283051\n",
      "    epoch          : 639\n",
      "    loss           : -410.4343603515625\n",
      "    val_loss       : -349.011028753221\n",
      "    val_log_likelihood: 517.6651611328125\n",
      "    val_log_marginal: 361.7324945431203\n",
      "Train Epoch: 640 [512/54000 (1%)] Loss: -323.647034\n",
      "Train Epoch: 640 [11776/54000 (22%)] Loss: -160.659943\n",
      "Train Epoch: 640 [23040/54000 (43%)] Loss: -476.284973\n",
      "Train Epoch: 640 [34304/54000 (64%)] Loss: -458.888611\n",
      "Train Epoch: 640 [45568/54000 (84%)] Loss: -416.384979\n",
      "    epoch          : 640\n",
      "    loss           : -386.65662963867186\n",
      "    val_loss       : -344.8559629613534\n",
      "    val_log_likelihood: 521.156201171875\n",
      "    val_log_marginal: 354.59184433482585\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch640.pth ...\n",
      "Train Epoch: 641 [512/54000 (1%)] Loss: -575.442749\n",
      "Train Epoch: 641 [11776/54000 (22%)] Loss: -423.391296\n",
      "Train Epoch: 641 [23040/54000 (43%)] Loss: -419.910156\n",
      "Train Epoch: 641 [34304/54000 (64%)] Loss: -407.143494\n",
      "Train Epoch: 641 [45568/54000 (84%)] Loss: -379.955566\n",
      "    epoch          : 641\n",
      "    loss           : -383.36926773071286\n",
      "    val_loss       : -386.00567170325667\n",
      "    val_log_likelihood: 523.1758880615234\n",
      "    val_log_marginal: 393.9661056239158\n",
      "Train Epoch: 642 [512/54000 (1%)] Loss: -430.037384\n",
      "Train Epoch: 642 [11776/54000 (22%)] Loss: -233.210114\n",
      "Train Epoch: 642 [23040/54000 (43%)] Loss: -406.231354\n",
      "Train Epoch: 642 [34304/54000 (64%)] Loss: -437.020355\n",
      "Train Epoch: 642 [45568/54000 (84%)] Loss: -372.162537\n",
      "    epoch          : 642\n",
      "    loss           : -409.94473281860354\n",
      "    val_loss       : -335.7189713329077\n",
      "    val_log_likelihood: 524.8954803466797\n",
      "    val_log_marginal: 341.0208977933973\n",
      "Train Epoch: 643 [512/54000 (1%)] Loss: -402.319427\n",
      "Train Epoch: 643 [11776/54000 (22%)] Loss: -545.110229\n",
      "Train Epoch: 643 [23040/54000 (43%)] Loss: -226.415009\n",
      "Train Epoch: 643 [34304/54000 (64%)] Loss: -425.549011\n",
      "Train Epoch: 643 [45568/54000 (84%)] Loss: -287.705750\n",
      "    epoch          : 643\n",
      "    loss           : -382.62560836791994\n",
      "    val_loss       : -317.31989986058323\n",
      "    val_log_likelihood: 510.1774871826172\n",
      "    val_log_marginal: 336.6755905721337\n",
      "Train Epoch: 644 [512/54000 (1%)] Loss: -419.057861\n",
      "Train Epoch: 644 [11776/54000 (22%)] Loss: -430.448517\n",
      "Train Epoch: 644 [23040/54000 (43%)] Loss: -572.648132\n",
      "Train Epoch: 644 [34304/54000 (64%)] Loss: -549.246399\n",
      "Train Epoch: 644 [45568/54000 (84%)] Loss: -377.951447\n",
      "    epoch          : 644\n",
      "    loss           : -359.59106788635256\n",
      "    val_loss       : -269.58330096304417\n",
      "    val_log_likelihood: 468.7717788696289\n",
      "    val_log_marginal: 276.37276663593946\n",
      "Train Epoch: 645 [512/54000 (1%)] Loss: -176.930008\n",
      "Train Epoch: 645 [11776/54000 (22%)] Loss: -368.619568\n",
      "Train Epoch: 645 [23040/54000 (43%)] Loss: -262.297516\n",
      "Train Epoch: 645 [34304/54000 (64%)] Loss: -392.459900\n",
      "Train Epoch: 645 [45568/54000 (84%)] Loss: -473.823334\n",
      "    epoch          : 645\n",
      "    loss           : -391.7977053833008\n",
      "    val_loss       : -377.2807316323742\n",
      "    val_log_likelihood: 524.8444854736329\n",
      "    val_log_marginal: 385.5408415149897\n",
      "Train Epoch: 646 [512/54000 (1%)] Loss: -475.496185\n",
      "Train Epoch: 646 [11776/54000 (22%)] Loss: -574.188843\n",
      "Train Epoch: 646 [23040/54000 (43%)] Loss: -468.747314\n",
      "Train Epoch: 646 [34304/54000 (64%)] Loss: -430.753418\n",
      "Train Epoch: 646 [45568/54000 (84%)] Loss: -511.359619\n",
      "    epoch          : 646\n",
      "    loss           : -441.88643203735353\n",
      "    val_loss       : -414.8881435781717\n",
      "    val_log_likelihood: 531.305874633789\n",
      "    val_log_marginal: 418.0801598031074\n",
      "Train Epoch: 647 [512/54000 (1%)] Loss: -464.916290\n",
      "Train Epoch: 647 [11776/54000 (22%)] Loss: -608.113220\n",
      "Train Epoch: 647 [23040/54000 (43%)] Loss: -476.782166\n",
      "Train Epoch: 647 [34304/54000 (64%)] Loss: -463.448547\n",
      "Train Epoch: 647 [45568/54000 (84%)] Loss: -464.893188\n",
      "    epoch          : 647\n",
      "    loss           : -453.07752365112304\n",
      "    val_loss       : -412.8686784436926\n",
      "    val_log_likelihood: 531.7417724609375\n",
      "    val_log_marginal: 416.69550344608723\n",
      "Train Epoch: 648 [512/54000 (1%)] Loss: -514.213440\n",
      "Train Epoch: 648 [11776/54000 (22%)] Loss: -275.845428\n",
      "Train Epoch: 648 [23040/54000 (43%)] Loss: -608.740417\n",
      "Train Epoch: 648 [34304/54000 (64%)] Loss: -309.591034\n",
      "Train Epoch: 648 [45568/54000 (84%)] Loss: -480.488617\n",
      "    epoch          : 648\n",
      "    loss           : -450.93825103759764\n",
      "    val_loss       : -410.32807270027695\n",
      "    val_log_likelihood: 535.694223022461\n",
      "    val_log_marginal: 416.9880228158087\n",
      "Train Epoch: 649 [512/54000 (1%)] Loss: -448.196533\n",
      "Train Epoch: 649 [11776/54000 (22%)] Loss: -298.305786\n",
      "Train Epoch: 649 [23040/54000 (43%)] Loss: -449.561462\n",
      "Train Epoch: 649 [34304/54000 (64%)] Loss: -461.834076\n",
      "Train Epoch: 649 [45568/54000 (84%)] Loss: -586.751587\n",
      "    epoch          : 649\n",
      "    loss           : -431.4366128540039\n",
      "    val_loss       : -381.2898950461298\n",
      "    val_log_likelihood: 526.7838623046875\n",
      "    val_log_marginal: 387.8311911489815\n",
      "Train Epoch: 650 [512/54000 (1%)] Loss: -555.830688\n",
      "Train Epoch: 650 [11776/54000 (22%)] Loss: -406.682526\n",
      "Train Epoch: 650 [23040/54000 (43%)] Loss: -478.188263\n",
      "Train Epoch: 650 [34304/54000 (64%)] Loss: -445.504669\n",
      "Train Epoch: 650 [45568/54000 (84%)] Loss: -378.853424\n",
      "    epoch          : 650\n",
      "    loss           : -393.9253150177002\n",
      "    val_loss       : -322.9922643151134\n",
      "    val_log_likelihood: 504.2501159667969\n",
      "    val_log_marginal: 329.0140904787928\n",
      "Train Epoch: 651 [512/54000 (1%)] Loss: -389.734070\n",
      "Train Epoch: 651 [11776/54000 (22%)] Loss: -434.913574\n",
      "Train Epoch: 651 [23040/54000 (43%)] Loss: -420.123627\n",
      "Train Epoch: 651 [34304/54000 (64%)] Loss: -463.300140\n",
      "Train Epoch: 651 [45568/54000 (84%)] Loss: -470.210510\n",
      "    epoch          : 651\n",
      "    loss           : -418.2006610107422\n",
      "    val_loss       : -401.1590820161626\n",
      "    val_log_likelihood: 528.0495361328125\n",
      "    val_log_marginal: 408.8765755813673\n",
      "Train Epoch: 652 [512/54000 (1%)] Loss: -433.180176\n",
      "Train Epoch: 652 [11776/54000 (22%)] Loss: -418.662781\n",
      "Train Epoch: 652 [23040/54000 (43%)] Loss: -434.214386\n",
      "Train Epoch: 652 [34304/54000 (64%)] Loss: -444.256897\n",
      "Train Epoch: 652 [45568/54000 (84%)] Loss: -461.920715\n",
      "    epoch          : 652\n",
      "    loss           : -417.9765972900391\n",
      "    val_loss       : -398.81668230928483\n",
      "    val_log_likelihood: 525.2910827636719\n",
      "    val_log_marginal: 405.80341936014594\n",
      "Train Epoch: 653 [512/54000 (1%)] Loss: -296.099121\n",
      "Train Epoch: 653 [11776/54000 (22%)] Loss: -600.074341\n",
      "Train Epoch: 653 [23040/54000 (43%)] Loss: -458.521301\n",
      "Train Epoch: 653 [34304/54000 (64%)] Loss: -247.267914\n",
      "Train Epoch: 653 [45568/54000 (84%)] Loss: -449.897339\n",
      "    epoch          : 653\n",
      "    loss           : -395.51506561279297\n",
      "    val_loss       : -266.604774851352\n",
      "    val_log_likelihood: 517.9359375\n",
      "    val_log_marginal: 281.95029362253933\n",
      "Train Epoch: 654 [512/54000 (1%)] Loss: -404.290161\n",
      "Train Epoch: 654 [11776/54000 (22%)] Loss: -409.030701\n",
      "Train Epoch: 654 [23040/54000 (43%)] Loss: -165.239990\n",
      "Train Epoch: 654 [34304/54000 (64%)] Loss: -192.744843\n",
      "Train Epoch: 654 [45568/54000 (84%)] Loss: -438.259308\n",
      "    epoch          : 654\n",
      "    loss           : -285.8150602293015\n",
      "    val_loss       : -342.71006530243903\n",
      "    val_log_likelihood: 518.476107788086\n",
      "    val_log_marginal: 354.94330273605885\n",
      "Train Epoch: 655 [512/54000 (1%)] Loss: -429.318756\n",
      "Train Epoch: 655 [11776/54000 (22%)] Loss: -530.452637\n",
      "Train Epoch: 655 [23040/54000 (43%)] Loss: -591.666687\n",
      "Train Epoch: 655 [34304/54000 (64%)] Loss: -455.787781\n",
      "Train Epoch: 655 [45568/54000 (84%)] Loss: -244.838333\n",
      "    epoch          : 655\n",
      "    loss           : -421.1975230407715\n",
      "    val_loss       : -402.2812608078122\n",
      "    val_log_likelihood: 530.678857421875\n",
      "    val_log_marginal: 409.78618776398525\n",
      "Train Epoch: 656 [512/54000 (1%)] Loss: -458.820221\n",
      "Train Epoch: 656 [11776/54000 (22%)] Loss: -412.121094\n",
      "Train Epoch: 656 [23040/54000 (43%)] Loss: -509.825073\n",
      "Train Epoch: 656 [34304/54000 (64%)] Loss: -481.043457\n",
      "Train Epoch: 656 [45568/54000 (84%)] Loss: -478.041473\n",
      "    epoch          : 656\n",
      "    loss           : -445.6645045471191\n",
      "    val_loss       : -405.56638996712866\n",
      "    val_log_likelihood: 532.7304656982421\n",
      "    val_log_marginal: 412.0997264210135\n",
      "Train Epoch: 657 [512/54000 (1%)] Loss: -252.579361\n",
      "Train Epoch: 657 [11776/54000 (22%)] Loss: -455.398438\n",
      "Train Epoch: 657 [23040/54000 (43%)] Loss: -270.392303\n",
      "Train Epoch: 657 [34304/54000 (64%)] Loss: -498.119934\n",
      "Train Epoch: 657 [45568/54000 (84%)] Loss: -316.270813\n",
      "    epoch          : 657\n",
      "    loss           : -451.3853355407715\n",
      "    val_loss       : -411.10028810668734\n",
      "    val_log_likelihood: 534.0789978027344\n",
      "    val_log_marginal: 416.44491614438596\n",
      "Train Epoch: 658 [512/54000 (1%)] Loss: -444.830505\n",
      "Train Epoch: 658 [11776/54000 (22%)] Loss: -478.823181\n",
      "Train Epoch: 658 [23040/54000 (43%)] Loss: -478.699280\n",
      "Train Epoch: 658 [34304/54000 (64%)] Loss: -444.044373\n",
      "Train Epoch: 658 [45568/54000 (84%)] Loss: 27.345047\n",
      "    epoch          : 658\n",
      "    loss           : -395.6491089820862\n",
      "    val_loss       : -289.4549474136904\n",
      "    val_log_likelihood: 495.1148651123047\n",
      "    val_log_marginal: 315.6943999234587\n",
      "Train Epoch: 659 [512/54000 (1%)] Loss: -162.804337\n",
      "Train Epoch: 659 [11776/54000 (22%)] Loss: -202.146973\n",
      "Train Epoch: 659 [23040/54000 (43%)] Loss: -592.629028\n",
      "Train Epoch: 659 [34304/54000 (64%)] Loss: -469.641785\n",
      "Train Epoch: 659 [45568/54000 (84%)] Loss: -472.798035\n",
      "    epoch          : 659\n",
      "    loss           : -403.221697769165\n",
      "    val_loss       : -392.4366918541491\n",
      "    val_log_likelihood: 522.7706359863281\n",
      "    val_log_marginal: 401.5542080622623\n",
      "Train Epoch: 660 [512/54000 (1%)] Loss: -278.913208\n",
      "Train Epoch: 660 [11776/54000 (22%)] Loss: -473.767822\n",
      "Train Epoch: 660 [23040/54000 (43%)] Loss: -413.587646\n",
      "Train Epoch: 660 [34304/54000 (64%)] Loss: -581.807312\n",
      "Train Epoch: 660 [45568/54000 (84%)] Loss: -405.713684\n",
      "    epoch          : 660\n",
      "    loss           : -428.2059770202637\n",
      "    val_loss       : -360.4602147469297\n",
      "    val_log_likelihood: 528.894091796875\n",
      "    val_log_marginal: 369.0125712756068\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch660.pth ...\n",
      "Train Epoch: 661 [512/54000 (1%)] Loss: -438.816193\n",
      "Train Epoch: 661 [11776/54000 (22%)] Loss: -311.733246\n",
      "Train Epoch: 661 [23040/54000 (43%)] Loss: -248.834015\n",
      "Train Epoch: 661 [34304/54000 (64%)] Loss: -415.325073\n",
      "Train Epoch: 661 [45568/54000 (84%)] Loss: -189.547546\n",
      "    epoch          : 661\n",
      "    loss           : -288.9538425254822\n",
      "    val_loss       : -212.31435974482446\n",
      "    val_log_likelihood: 503.3146606445313\n",
      "    val_log_marginal: 222.15005350448192\n",
      "Train Epoch: 662 [512/54000 (1%)] Loss: -350.258301\n",
      "Train Epoch: 662 [11776/54000 (22%)] Loss: -176.424698\n",
      "Train Epoch: 662 [23040/54000 (43%)] Loss: -409.191589\n",
      "Train Epoch: 662 [34304/54000 (64%)] Loss: -389.323975\n",
      "Train Epoch: 662 [45568/54000 (84%)] Loss: -441.126038\n",
      "    epoch          : 662\n",
      "    loss           : -336.51558460235594\n",
      "    val_loss       : -394.02501127682626\n",
      "    val_log_likelihood: 525.0036651611329\n",
      "    val_log_marginal: 400.1790584053844\n",
      "Train Epoch: 663 [512/54000 (1%)] Loss: -444.152924\n",
      "Train Epoch: 663 [11776/54000 (22%)] Loss: -480.785828\n",
      "Train Epoch: 663 [23040/54000 (43%)] Loss: -476.483276\n",
      "Train Epoch: 663 [34304/54000 (64%)] Loss: -601.120422\n",
      "Train Epoch: 663 [45568/54000 (84%)] Loss: -489.190094\n",
      "    epoch          : 663\n",
      "    loss           : -447.334228515625\n",
      "    val_loss       : -411.1556579111144\n",
      "    val_log_likelihood: 533.9752960205078\n",
      "    val_log_marginal: 419.90688392557206\n",
      "Train Epoch: 664 [512/54000 (1%)] Loss: -514.273193\n",
      "Train Epoch: 664 [11776/54000 (22%)] Loss: -474.821411\n",
      "Train Epoch: 664 [23040/54000 (43%)] Loss: -266.974731\n",
      "Train Epoch: 664 [34304/54000 (64%)] Loss: -275.267548\n",
      "Train Epoch: 664 [45568/54000 (84%)] Loss: -484.402100\n",
      "    epoch          : 664\n",
      "    loss           : -450.68974563598636\n",
      "    val_loss       : -411.79306890033183\n",
      "    val_log_likelihood: 528.1566223144531\n",
      "    val_log_marginal: 415.4162824336439\n",
      "Train Epoch: 665 [512/54000 (1%)] Loss: -493.193054\n",
      "Train Epoch: 665 [11776/54000 (22%)] Loss: -463.671326\n",
      "Train Epoch: 665 [23040/54000 (43%)] Loss: -503.089417\n",
      "Train Epoch: 665 [34304/54000 (64%)] Loss: -501.724365\n",
      "Train Epoch: 665 [45568/54000 (84%)] Loss: -471.746216\n",
      "    epoch          : 665\n",
      "    loss           : -449.5495040893555\n",
      "    val_loss       : -408.7167014237493\n",
      "    val_log_likelihood: 535.201058959961\n",
      "    val_log_marginal: 415.1284665580839\n",
      "Train Epoch: 666 [512/54000 (1%)] Loss: -412.830750\n",
      "Train Epoch: 666 [11776/54000 (22%)] Loss: -585.925537\n",
      "Train Epoch: 666 [23040/54000 (43%)] Loss: -412.987854\n",
      "Train Epoch: 666 [34304/54000 (64%)] Loss: -502.497559\n",
      "Train Epoch: 666 [45568/54000 (84%)] Loss: -448.511902\n",
      "    epoch          : 666\n",
      "    loss           : -444.41958404541015\n",
      "    val_loss       : -399.09912731051446\n",
      "    val_log_likelihood: 530.5157318115234\n",
      "    val_log_marginal: 406.7952338922769\n",
      "Train Epoch: 667 [512/54000 (1%)] Loss: -479.694275\n",
      "Train Epoch: 667 [11776/54000 (22%)] Loss: -263.560333\n",
      "Train Epoch: 667 [23040/54000 (43%)] Loss: -575.880615\n",
      "Train Epoch: 667 [34304/54000 (64%)] Loss: -486.148071\n",
      "Train Epoch: 667 [45568/54000 (84%)] Loss: -611.679688\n",
      "    epoch          : 667\n",
      "    loss           : -442.86126525878905\n",
      "    val_loss       : -405.69638426154853\n",
      "    val_log_likelihood: 533.7694519042968\n",
      "    val_log_marginal: 411.78401185637233\n",
      "Train Epoch: 668 [512/54000 (1%)] Loss: -520.792908\n",
      "Train Epoch: 668 [11776/54000 (22%)] Loss: -417.583954\n",
      "Train Epoch: 668 [23040/54000 (43%)] Loss: -429.769318\n",
      "Train Epoch: 668 [34304/54000 (64%)] Loss: -278.853149\n",
      "Train Epoch: 668 [45568/54000 (84%)] Loss: -198.593933\n",
      "    epoch          : 668\n",
      "    loss           : -399.3807406616211\n",
      "    val_loss       : -330.83647700063887\n",
      "    val_log_likelihood: 513.7764862060546\n",
      "    val_log_marginal: 346.0013183388859\n",
      "Train Epoch: 669 [512/54000 (1%)] Loss: -412.349976\n",
      "Train Epoch: 669 [11776/54000 (22%)] Loss: -536.841675\n",
      "Train Epoch: 669 [23040/54000 (43%)] Loss: -553.211548\n",
      "Train Epoch: 669 [34304/54000 (64%)] Loss: -408.902252\n",
      "Train Epoch: 669 [45568/54000 (84%)] Loss: -467.730591\n",
      "    epoch          : 669\n",
      "    loss           : -397.3788469982147\n",
      "    val_loss       : -398.60462532471865\n",
      "    val_log_likelihood: 524.5584869384766\n",
      "    val_log_marginal: 405.18532415293157\n",
      "Train Epoch: 670 [512/54000 (1%)] Loss: -451.332825\n",
      "Train Epoch: 670 [11776/54000 (22%)] Loss: -486.240234\n",
      "Train Epoch: 670 [23040/54000 (43%)] Loss: -490.004578\n",
      "Train Epoch: 670 [34304/54000 (64%)] Loss: -446.592529\n",
      "Train Epoch: 670 [45568/54000 (84%)] Loss: -494.875488\n",
      "    epoch          : 670\n",
      "    loss           : -446.3311587524414\n",
      "    val_loss       : -411.47520903963596\n",
      "    val_log_likelihood: 534.8825866699219\n",
      "    val_log_marginal: 415.7320395389305\n",
      "Train Epoch: 671 [512/54000 (1%)] Loss: -465.314575\n",
      "Train Epoch: 671 [11776/54000 (22%)] Loss: -503.237122\n",
      "Train Epoch: 671 [23040/54000 (43%)] Loss: -367.538147\n",
      "Train Epoch: 671 [34304/54000 (64%)] Loss: -484.087555\n",
      "Train Epoch: 671 [45568/54000 (84%)] Loss: -494.299591\n",
      "    epoch          : 671\n",
      "    loss           : -437.90820770263673\n",
      "    val_loss       : -391.1472364300862\n",
      "    val_log_likelihood: 528.6591857910156\n",
      "    val_log_marginal: 399.7328707050532\n",
      "Train Epoch: 672 [512/54000 (1%)] Loss: -242.814606\n",
      "Train Epoch: 672 [11776/54000 (22%)] Loss: -461.565063\n",
      "Train Epoch: 672 [23040/54000 (43%)] Loss: -453.237152\n",
      "Train Epoch: 672 [34304/54000 (64%)] Loss: -490.743958\n",
      "Train Epoch: 672 [45568/54000 (84%)] Loss: -429.771057\n",
      "    epoch          : 672\n",
      "    loss           : -436.9992919921875\n",
      "    val_loss       : -408.93066025730224\n",
      "    val_log_likelihood: 534.0682312011719\n",
      "    val_log_marginal: 416.54854369740303\n",
      "Train Epoch: 673 [512/54000 (1%)] Loss: -515.131409\n",
      "Train Epoch: 673 [11776/54000 (22%)] Loss: -518.624390\n",
      "Train Epoch: 673 [23040/54000 (43%)] Loss: -240.968353\n",
      "Train Epoch: 673 [34304/54000 (64%)] Loss: -465.303253\n",
      "Train Epoch: 673 [45568/54000 (84%)] Loss: -396.434631\n",
      "    epoch          : 673\n",
      "    loss           : -440.9559027099609\n",
      "    val_loss       : -397.46548175476494\n",
      "    val_log_likelihood: 527.8806396484375\n",
      "    val_log_marginal: 407.2604651180008\n",
      "Train Epoch: 674 [512/54000 (1%)] Loss: -292.267761\n",
      "Train Epoch: 674 [11776/54000 (22%)] Loss: -565.683838\n",
      "Train Epoch: 674 [23040/54000 (43%)] Loss: -459.977417\n",
      "Train Epoch: 674 [34304/54000 (64%)] Loss: -505.198547\n",
      "Train Epoch: 674 [45568/54000 (84%)] Loss: -466.058411\n",
      "    epoch          : 674\n",
      "    loss           : -437.6495832824707\n",
      "    val_loss       : -403.84105241820214\n",
      "    val_log_likelihood: 534.9084930419922\n",
      "    val_log_marginal: 410.5192420933396\n",
      "Train Epoch: 675 [512/54000 (1%)] Loss: -269.503906\n",
      "Train Epoch: 675 [11776/54000 (22%)] Loss: -484.149078\n",
      "Train Epoch: 675 [23040/54000 (43%)] Loss: -428.640076\n",
      "Train Epoch: 675 [34304/54000 (64%)] Loss: -490.126770\n",
      "Train Epoch: 675 [45568/54000 (84%)] Loss: -271.623962\n",
      "    epoch          : 675\n",
      "    loss           : -430.9136032104492\n",
      "    val_loss       : -351.15007441882045\n",
      "    val_log_likelihood: 508.7020568847656\n",
      "    val_log_marginal: 357.2987493958333\n",
      "Train Epoch: 676 [512/54000 (1%)] Loss: -507.669006\n",
      "Train Epoch: 676 [11776/54000 (22%)] Loss: -448.346710\n",
      "Train Epoch: 676 [23040/54000 (43%)] Loss: -470.374329\n",
      "Train Epoch: 676 [34304/54000 (64%)] Loss: -430.020020\n",
      "Train Epoch: 676 [45568/54000 (84%)] Loss: -300.702148\n",
      "    epoch          : 676\n",
      "    loss           : -411.19493103027344\n",
      "    val_loss       : -406.49842899832873\n",
      "    val_log_likelihood: 526.8393707275391\n",
      "    val_log_marginal: 409.8805403377861\n",
      "Train Epoch: 677 [512/54000 (1%)] Loss: -599.051270\n",
      "Train Epoch: 677 [11776/54000 (22%)] Loss: -459.523071\n",
      "Train Epoch: 677 [23040/54000 (43%)] Loss: -485.020203\n",
      "Train Epoch: 677 [34304/54000 (64%)] Loss: -246.198486\n",
      "Train Epoch: 677 [45568/54000 (84%)] Loss: -476.367035\n",
      "    epoch          : 677\n",
      "    loss           : -444.28018508911134\n",
      "    val_loss       : -400.54030700381844\n",
      "    val_log_likelihood: 535.1579681396485\n",
      "    val_log_marginal: 405.27403753735126\n",
      "Train Epoch: 678 [512/54000 (1%)] Loss: -518.599121\n",
      "Train Epoch: 678 [11776/54000 (22%)] Loss: -424.169952\n",
      "Train Epoch: 678 [23040/54000 (43%)] Loss: -461.059937\n",
      "Train Epoch: 678 [34304/54000 (64%)] Loss: -302.589386\n",
      "Train Epoch: 678 [45568/54000 (84%)] Loss: -422.135529\n",
      "    epoch          : 678\n",
      "    loss           : -447.12884078979494\n",
      "    val_loss       : -411.29172280635686\n",
      "    val_log_likelihood: 535.8968872070312\n",
      "    val_log_marginal: 416.6038848945892\n",
      "Train Epoch: 679 [512/54000 (1%)] Loss: -490.750763\n",
      "Train Epoch: 679 [11776/54000 (22%)] Loss: -517.655457\n",
      "Train Epoch: 679 [23040/54000 (43%)] Loss: -462.231995\n",
      "Train Epoch: 679 [34304/54000 (64%)] Loss: -286.684753\n",
      "Train Epoch: 679 [45568/54000 (84%)] Loss: -494.456207\n",
      "    epoch          : 679\n",
      "    loss           : -452.2826559448242\n",
      "    val_loss       : -411.5305532516912\n",
      "    val_log_likelihood: 541.0987518310546\n",
      "    val_log_marginal: 419.5367116201669\n",
      "Train Epoch: 680 [512/54000 (1%)] Loss: -508.627869\n",
      "Train Epoch: 680 [11776/54000 (22%)] Loss: -482.817444\n",
      "Train Epoch: 680 [23040/54000 (43%)] Loss: -437.868256\n",
      "Train Epoch: 680 [34304/54000 (64%)] Loss: -503.864288\n",
      "Train Epoch: 680 [45568/54000 (84%)] Loss: -493.291229\n",
      "    epoch          : 680\n",
      "    loss           : -455.82409515380857\n",
      "    val_loss       : -412.7743993781507\n",
      "    val_log_likelihood: 540.2269622802735\n",
      "    val_log_marginal: 419.6063073221594\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch680.pth ...\n",
      "Train Epoch: 681 [512/54000 (1%)] Loss: -475.068085\n",
      "Train Epoch: 681 [11776/54000 (22%)] Loss: -459.322601\n",
      "Train Epoch: 681 [23040/54000 (43%)] Loss: -517.987488\n",
      "Train Epoch: 681 [34304/54000 (64%)] Loss: -605.375244\n",
      "Train Epoch: 681 [45568/54000 (84%)] Loss: -479.536133\n",
      "    epoch          : 681\n",
      "    loss           : -454.7947747802734\n",
      "    val_loss       : -405.1220408327878\n",
      "    val_log_likelihood: 537.8416137695312\n",
      "    val_log_marginal: 409.08306047961776\n",
      "Train Epoch: 682 [512/54000 (1%)] Loss: -610.901001\n",
      "Train Epoch: 682 [11776/54000 (22%)] Loss: -505.157959\n",
      "Train Epoch: 682 [23040/54000 (43%)] Loss: -486.384766\n",
      "Train Epoch: 682 [34304/54000 (64%)] Loss: -458.084076\n",
      "Train Epoch: 682 [45568/54000 (84%)] Loss: -395.035522\n",
      "    epoch          : 682\n",
      "    loss           : -439.91395294189454\n",
      "    val_loss       : -396.83525927010925\n",
      "    val_log_likelihood: 536.725210571289\n",
      "    val_log_marginal: 401.9965298872441\n",
      "Train Epoch: 683 [512/54000 (1%)] Loss: -447.093903\n",
      "Train Epoch: 683 [11776/54000 (22%)] Loss: -505.169556\n",
      "Train Epoch: 683 [23040/54000 (43%)] Loss: -462.516846\n",
      "Train Epoch: 683 [34304/54000 (64%)] Loss: -495.942993\n",
      "Train Epoch: 683 [45568/54000 (84%)] Loss: -479.657684\n",
      "    epoch          : 683\n",
      "    loss           : -438.39208877563476\n",
      "    val_loss       : -403.80484902244064\n",
      "    val_log_likelihood: 535.4660064697266\n",
      "    val_log_marginal: 410.6249970506877\n",
      "Train Epoch: 684 [512/54000 (1%)] Loss: -492.481537\n",
      "Train Epoch: 684 [11776/54000 (22%)] Loss: -503.417419\n",
      "Train Epoch: 684 [23040/54000 (43%)] Loss: -464.345886\n",
      "Train Epoch: 684 [34304/54000 (64%)] Loss: -507.199829\n",
      "Train Epoch: 684 [45568/54000 (84%)] Loss: -409.862213\n",
      "    epoch          : 684\n",
      "    loss           : -448.01713333129885\n",
      "    val_loss       : -407.3359061134979\n",
      "    val_log_likelihood: 540.6415588378907\n",
      "    val_log_marginal: 413.70662553162117\n",
      "Train Epoch: 685 [512/54000 (1%)] Loss: -509.949463\n",
      "Train Epoch: 685 [11776/54000 (22%)] Loss: -497.125244\n",
      "Train Epoch: 685 [23040/54000 (43%)] Loss: -579.300293\n",
      "Train Epoch: 685 [34304/54000 (64%)] Loss: -472.110046\n",
      "Train Epoch: 685 [45568/54000 (84%)] Loss: -449.352814\n",
      "    epoch          : 685\n",
      "    loss           : -430.97098266601563\n",
      "    val_loss       : -396.32238396592436\n",
      "    val_log_likelihood: 535.4533996582031\n",
      "    val_log_marginal: 405.0668697119662\n",
      "Train Epoch: 686 [512/54000 (1%)] Loss: -443.092133\n",
      "Train Epoch: 686 [11776/54000 (22%)] Loss: -505.724701\n",
      "Train Epoch: 686 [23040/54000 (43%)] Loss: -489.332458\n",
      "Train Epoch: 686 [34304/54000 (64%)] Loss: -440.006165\n",
      "Train Epoch: 686 [45568/54000 (84%)] Loss: -438.915710\n",
      "    epoch          : 686\n",
      "    loss           : -418.99633544921875\n",
      "    val_loss       : -363.8110174067318\n",
      "    val_log_likelihood: 521.5516052246094\n",
      "    val_log_marginal: 372.47626680620016\n",
      "Train Epoch: 687 [512/54000 (1%)] Loss: -490.537750\n",
      "Train Epoch: 687 [11776/54000 (22%)] Loss: -560.794189\n",
      "Train Epoch: 687 [23040/54000 (43%)] Loss: -438.735535\n",
      "Train Epoch: 687 [34304/54000 (64%)] Loss: -461.042603\n",
      "Train Epoch: 687 [45568/54000 (84%)] Loss: -389.321381\n",
      "    epoch          : 687\n",
      "    loss           : -422.07946975708006\n",
      "    val_loss       : -392.3578460821882\n",
      "    val_log_likelihood: 537.7569732666016\n",
      "    val_log_marginal: 403.30850887037815\n",
      "Train Epoch: 688 [512/54000 (1%)] Loss: -226.834579\n",
      "Train Epoch: 688 [11776/54000 (22%)] Loss: -611.380981\n",
      "Train Epoch: 688 [23040/54000 (43%)] Loss: -227.862732\n",
      "Train Epoch: 688 [34304/54000 (64%)] Loss: -472.723389\n",
      "Train Epoch: 688 [45568/54000 (84%)] Loss: -417.295715\n",
      "    epoch          : 688\n",
      "    loss           : -442.85138687133787\n",
      "    val_loss       : -413.0095744751394\n",
      "    val_log_likelihood: 539.7293914794922\n",
      "    val_log_marginal: 418.4327626157552\n",
      "Train Epoch: 689 [512/54000 (1%)] Loss: -316.659149\n",
      "Train Epoch: 689 [11776/54000 (22%)] Loss: -428.716919\n",
      "Train Epoch: 689 [23040/54000 (43%)] Loss: -514.856934\n",
      "Train Epoch: 689 [34304/54000 (64%)] Loss: -478.416412\n",
      "Train Epoch: 689 [45568/54000 (84%)] Loss: -603.254211\n",
      "    epoch          : 689\n",
      "    loss           : -447.8187614440918\n",
      "    val_loss       : -395.2117162741721\n",
      "    val_log_likelihood: 537.5871002197266\n",
      "    val_log_marginal: 406.4586018171161\n",
      "Train Epoch: 690 [512/54000 (1%)] Loss: -500.414337\n",
      "Train Epoch: 690 [11776/54000 (22%)] Loss: -496.893036\n",
      "Train Epoch: 690 [23040/54000 (43%)] Loss: -514.018372\n",
      "Train Epoch: 690 [34304/54000 (64%)] Loss: -457.390778\n",
      "Train Epoch: 690 [45568/54000 (84%)] Loss: -288.873077\n",
      "    epoch          : 690\n",
      "    loss           : -432.9470916748047\n",
      "    val_loss       : -354.63203631825746\n",
      "    val_log_likelihood: 527.2974273681641\n",
      "    val_log_marginal: 361.32419113777576\n",
      "Train Epoch: 691 [512/54000 (1%)] Loss: -177.194351\n",
      "Train Epoch: 691 [11776/54000 (22%)] Loss: -430.778625\n",
      "Train Epoch: 691 [23040/54000 (43%)] Loss: -457.724152\n",
      "Train Epoch: 691 [34304/54000 (64%)] Loss: -443.933868\n",
      "Train Epoch: 691 [45568/54000 (84%)] Loss: -528.359558\n",
      "    epoch          : 691\n",
      "    loss           : -369.92959663391116\n",
      "    val_loss       : -370.6163329118863\n",
      "    val_log_likelihood: 528.0570678710938\n",
      "    val_log_marginal: 385.4227019082755\n",
      "Train Epoch: 692 [512/54000 (1%)] Loss: -471.180573\n",
      "Train Epoch: 692 [11776/54000 (22%)] Loss: -432.831909\n",
      "Train Epoch: 692 [23040/54000 (43%)] Loss: -506.775269\n",
      "Train Epoch: 692 [34304/54000 (64%)] Loss: -457.080627\n",
      "Train Epoch: 692 [45568/54000 (84%)] Loss: -487.810852\n",
      "    epoch          : 692\n",
      "    loss           : -441.352414855957\n",
      "    val_loss       : -401.00204736050216\n",
      "    val_log_likelihood: 537.41455078125\n",
      "    val_log_marginal: 412.36080332286656\n",
      "Train Epoch: 693 [512/54000 (1%)] Loss: -588.665161\n",
      "Train Epoch: 693 [11776/54000 (22%)] Loss: -507.027344\n",
      "Train Epoch: 693 [23040/54000 (43%)] Loss: -500.066467\n",
      "Train Epoch: 693 [34304/54000 (64%)] Loss: -265.552368\n",
      "Train Epoch: 693 [45568/54000 (84%)] Loss: -204.349991\n",
      "    epoch          : 693\n",
      "    loss           : -428.9223565673828\n",
      "    val_loss       : -370.80807510986926\n",
      "    val_log_likelihood: 518.3400665283203\n",
      "    val_log_marginal: 375.7041034260781\n",
      "Train Epoch: 694 [512/54000 (1%)] Loss: -422.912720\n",
      "Train Epoch: 694 [11776/54000 (22%)] Loss: -428.382690\n",
      "Train Epoch: 694 [23040/54000 (43%)] Loss: -193.867783\n",
      "Train Epoch: 694 [34304/54000 (64%)] Loss: -412.276611\n",
      "Train Epoch: 694 [45568/54000 (84%)] Loss: -187.397751\n",
      "    epoch          : 694\n",
      "    loss           : -356.042017288208\n",
      "    val_loss       : -264.8882044155151\n",
      "    val_log_likelihood: 514.4457611083984\n",
      "    val_log_marginal: 279.6262696627527\n",
      "Train Epoch: 695 [512/54000 (1%)] Loss: -314.724915\n",
      "Train Epoch: 695 [11776/54000 (22%)] Loss: -235.977020\n",
      "Train Epoch: 695 [23040/54000 (43%)] Loss: -461.459137\n",
      "Train Epoch: 695 [34304/54000 (64%)] Loss: -370.277283\n",
      "Train Epoch: 695 [45568/54000 (84%)] Loss: -262.085449\n",
      "    epoch          : 695\n",
      "    loss           : -388.6177467346191\n",
      "    val_loss       : -362.30662198681387\n",
      "    val_log_likelihood: 522.291455078125\n",
      "    val_log_marginal: 368.95012192986906\n",
      "Train Epoch: 696 [512/54000 (1%)] Loss: -192.591339\n",
      "Train Epoch: 696 [11776/54000 (22%)] Loss: -455.656281\n",
      "Train Epoch: 696 [23040/54000 (43%)] Loss: -519.634644\n",
      "Train Epoch: 696 [34304/54000 (64%)] Loss: -434.709961\n",
      "Train Epoch: 696 [45568/54000 (84%)] Loss: -427.045959\n",
      "    epoch          : 696\n",
      "    loss           : -402.3918978881836\n",
      "    val_loss       : -403.50316247567537\n",
      "    val_log_likelihood: 535.1673614501954\n",
      "    val_log_marginal: 412.9200177554041\n",
      "Train Epoch: 697 [512/54000 (1%)] Loss: -467.314087\n",
      "Train Epoch: 697 [11776/54000 (22%)] Loss: -455.205231\n",
      "Train Epoch: 697 [23040/54000 (43%)] Loss: -262.522583\n",
      "Train Epoch: 697 [34304/54000 (64%)] Loss: -483.413208\n",
      "Train Epoch: 697 [45568/54000 (84%)] Loss: -477.201691\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "    epoch          : 697\n",
      "    loss           : -455.2494532775879\n",
      "    val_loss       : -412.8344129053876\n",
      "    val_log_likelihood: 537.7548034667968\n",
      "    val_log_marginal: 420.43833859600124\n",
      "Train Epoch: 698 [512/54000 (1%)] Loss: -319.960632\n",
      "Train Epoch: 698 [11776/54000 (22%)] Loss: -518.298096\n",
      "Train Epoch: 698 [23040/54000 (43%)] Loss: -618.265503\n",
      "Train Epoch: 698 [34304/54000 (64%)] Loss: -524.109497\n",
      "Train Epoch: 698 [45568/54000 (84%)] Loss: -622.556030\n",
      "    epoch          : 698\n",
      "    loss           : -465.04575927734373\n",
      "    val_loss       : -422.99769359771165\n",
      "    val_log_likelihood: 541.8832427978516\n",
      "    val_log_marginal: 427.5714887212962\n",
      "Train Epoch: 699 [512/54000 (1%)] Loss: -443.608337\n",
      "Train Epoch: 699 [11776/54000 (22%)] Loss: -627.828369\n",
      "Train Epoch: 699 [23040/54000 (43%)] Loss: -527.984070\n",
      "Train Epoch: 699 [34304/54000 (64%)] Loss: -501.463226\n",
      "Train Epoch: 699 [45568/54000 (84%)] Loss: -310.806366\n",
      "    epoch          : 699\n",
      "    loss           : -465.55878479003906\n",
      "    val_loss       : -422.17386163342746\n",
      "    val_log_likelihood: 542.8137298583985\n",
      "    val_log_marginal: 427.87928497232497\n",
      "Train Epoch: 700 [512/54000 (1%)] Loss: -518.244202\n",
      "Train Epoch: 700 [11776/54000 (22%)] Loss: -492.859344\n",
      "Train Epoch: 700 [23040/54000 (43%)] Loss: -447.379883\n",
      "Train Epoch: 700 [34304/54000 (64%)] Loss: -477.887482\n",
      "Train Epoch: 700 [45568/54000 (84%)] Loss: -497.113953\n",
      "    epoch          : 700\n",
      "    loss           : -465.8659417724609\n",
      "    val_loss       : -426.78152245078235\n",
      "    val_log_likelihood: 543.7578948974609\n",
      "    val_log_marginal: 429.6428780850024\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch700.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 701 [512/54000 (1%)] Loss: -619.610535\n",
      "Train Epoch: 701 [11776/54000 (22%)] Loss: -474.924927\n",
      "Train Epoch: 701 [23040/54000 (43%)] Loss: -482.455048\n",
      "Train Epoch: 701 [34304/54000 (64%)] Loss: -498.562622\n",
      "Train Epoch: 701 [45568/54000 (84%)] Loss: -619.500732\n",
      "    epoch          : 701\n",
      "    loss           : -467.7279708862305\n",
      "    val_loss       : -425.09278444293886\n",
      "    val_log_likelihood: 542.6557067871094\n",
      "    val_log_marginal: 428.53502776660025\n",
      "Train Epoch: 702 [512/54000 (1%)] Loss: -621.329590\n",
      "Train Epoch: 702 [11776/54000 (22%)] Loss: -268.890289\n",
      "Train Epoch: 702 [23040/54000 (43%)] Loss: -621.289062\n",
      "Train Epoch: 702 [34304/54000 (64%)] Loss: -520.129028\n",
      "Train Epoch: 702 [45568/54000 (84%)] Loss: -446.851837\n",
      "    epoch          : 702\n",
      "    loss           : -467.0546792602539\n",
      "    val_loss       : -425.1181230291724\n",
      "    val_log_likelihood: 545.3656005859375\n",
      "    val_log_marginal: 430.2907889474183\n",
      "Train Epoch: 703 [512/54000 (1%)] Loss: -539.007996\n",
      "Train Epoch: 703 [11776/54000 (22%)] Loss: -506.277161\n",
      "Train Epoch: 703 [23040/54000 (43%)] Loss: -470.053680\n",
      "Train Epoch: 703 [34304/54000 (64%)] Loss: -452.941254\n",
      "Train Epoch: 703 [45568/54000 (84%)] Loss: -508.278625\n",
      "    epoch          : 703\n",
      "    loss           : -467.72695068359377\n",
      "    val_loss       : -426.2949670199305\n",
      "    val_log_likelihood: 545.8363372802735\n",
      "    val_log_marginal: 431.26472229622306\n",
      "Train Epoch: 704 [512/54000 (1%)] Loss: -514.374878\n",
      "Train Epoch: 704 [11776/54000 (22%)] Loss: -492.188660\n",
      "Train Epoch: 704 [23040/54000 (43%)] Loss: -500.329834\n",
      "Train Epoch: 704 [34304/54000 (64%)] Loss: -509.362457\n",
      "Train Epoch: 704 [45568/54000 (84%)] Loss: -500.015259\n",
      "    epoch          : 704\n",
      "    loss           : -467.92119964599607\n",
      "    val_loss       : -423.3191999189556\n",
      "    val_log_likelihood: 543.8691314697265\n",
      "    val_log_marginal: 429.2251441415903\n",
      "Train Epoch: 705 [512/54000 (1%)] Loss: -493.582733\n",
      "Train Epoch: 705 [11776/54000 (22%)] Loss: -527.953613\n",
      "Train Epoch: 705 [23040/54000 (43%)] Loss: -451.738525\n",
      "Train Epoch: 705 [34304/54000 (64%)] Loss: -446.106689\n",
      "Train Epoch: 705 [45568/54000 (84%)] Loss: -319.571625\n",
      "    epoch          : 705\n",
      "    loss           : -467.25447540283204\n",
      "    val_loss       : -426.51001714132724\n",
      "    val_log_likelihood: 545.1423950195312\n",
      "    val_log_marginal: 430.08252009488643\n",
      "Train Epoch: 706 [512/54000 (1%)] Loss: -610.859619\n",
      "Train Epoch: 706 [11776/54000 (22%)] Loss: -305.975006\n",
      "Train Epoch: 706 [23040/54000 (43%)] Loss: -511.368561\n",
      "Train Epoch: 706 [34304/54000 (64%)] Loss: -528.927551\n",
      "Train Epoch: 706 [45568/54000 (84%)] Loss: -459.277802\n",
      "    epoch          : 706\n",
      "    loss           : -467.3238397216797\n",
      "    val_loss       : -423.77905090898275\n",
      "    val_log_likelihood: 543.0157592773437\n",
      "    val_log_marginal: 428.3579313207418\n",
      "Train Epoch: 707 [512/54000 (1%)] Loss: -320.092194\n",
      "Train Epoch: 707 [11776/54000 (22%)] Loss: -619.917725\n",
      "Train Epoch: 707 [23040/54000 (43%)] Loss: -471.891357\n",
      "Train Epoch: 707 [34304/54000 (64%)] Loss: -501.997192\n",
      "Train Epoch: 707 [45568/54000 (84%)] Loss: -440.431824\n",
      "    epoch          : 707\n",
      "    loss           : -468.31871490478517\n",
      "    val_loss       : -426.9711004352197\n",
      "    val_log_likelihood: 546.3647735595703\n",
      "    val_log_marginal: 431.47025644890965\n",
      "Train Epoch: 708 [512/54000 (1%)] Loss: -453.887817\n",
      "Train Epoch: 708 [11776/54000 (22%)] Loss: -614.969910\n",
      "Train Epoch: 708 [23040/54000 (43%)] Loss: -477.073334\n",
      "Train Epoch: 708 [34304/54000 (64%)] Loss: -621.672119\n",
      "Train Epoch: 708 [45568/54000 (84%)] Loss: -519.716736\n",
      "    epoch          : 708\n",
      "    loss           : -468.12032409667967\n",
      "    val_loss       : -423.64912532735616\n",
      "    val_log_likelihood: 545.566763305664\n",
      "    val_log_marginal: 430.5696382824332\n",
      "Train Epoch: 709 [512/54000 (1%)] Loss: -486.958130\n",
      "Train Epoch: 709 [11776/54000 (22%)] Loss: -533.023865\n",
      "Train Epoch: 709 [23040/54000 (43%)] Loss: -290.927429\n",
      "Train Epoch: 709 [34304/54000 (64%)] Loss: -525.815979\n",
      "Train Epoch: 709 [45568/54000 (84%)] Loss: -485.687073\n",
      "    epoch          : 709\n",
      "    loss           : -467.9701922607422\n",
      "    val_loss       : -420.21699777692555\n",
      "    val_log_likelihood: 543.9463043212891\n",
      "    val_log_marginal: 427.76250517629086\n",
      "Train Epoch: 710 [512/54000 (1%)] Loss: -522.314514\n",
      "Train Epoch: 710 [11776/54000 (22%)] Loss: -515.905273\n",
      "Train Epoch: 710 [23040/54000 (43%)] Loss: -517.308472\n",
      "Train Epoch: 710 [34304/54000 (64%)] Loss: -537.016357\n",
      "Train Epoch: 710 [45568/54000 (84%)] Loss: -451.896729\n",
      "    epoch          : 710\n",
      "    loss           : -468.60997161865237\n",
      "    val_loss       : -425.11018207464366\n",
      "    val_log_likelihood: 541.2480010986328\n",
      "    val_log_marginal: 427.5594118397683\n",
      "Train Epoch: 711 [512/54000 (1%)] Loss: -444.696838\n",
      "Train Epoch: 711 [11776/54000 (22%)] Loss: -283.119995\n",
      "Train Epoch: 711 [23040/54000 (43%)] Loss: -479.692596\n",
      "Train Epoch: 711 [34304/54000 (64%)] Loss: -327.635315\n",
      "Train Epoch: 711 [45568/54000 (84%)] Loss: -450.958801\n",
      "    epoch          : 711\n",
      "    loss           : -468.6703677368164\n",
      "    val_loss       : -424.47328815665094\n",
      "    val_log_likelihood: 546.4808349609375\n",
      "    val_log_marginal: 430.57254946641626\n",
      "Train Epoch: 712 [512/54000 (1%)] Loss: -278.154602\n",
      "Train Epoch: 712 [11776/54000 (22%)] Loss: -487.045868\n",
      "Train Epoch: 712 [23040/54000 (43%)] Loss: -261.001007\n",
      "Train Epoch: 712 [34304/54000 (64%)] Loss: -291.468140\n",
      "Train Epoch: 712 [45568/54000 (84%)] Loss: -627.307739\n",
      "    epoch          : 712\n",
      "    loss           : -468.0767645263672\n",
      "    val_loss       : -425.06343558877705\n",
      "    val_log_likelihood: 546.77587890625\n",
      "    val_log_marginal: 430.65487954430284\n",
      "Train Epoch: 713 [512/54000 (1%)] Loss: -626.755005\n",
      "Train Epoch: 713 [11776/54000 (22%)] Loss: -531.933960\n",
      "Train Epoch: 713 [23040/54000 (43%)] Loss: -486.501831\n",
      "Train Epoch: 713 [34304/54000 (64%)] Loss: -304.141541\n",
      "Train Epoch: 713 [45568/54000 (84%)] Loss: -494.453339\n",
      "    epoch          : 713\n",
      "    loss           : -469.16218048095703\n",
      "    val_loss       : -423.3253885010257\n",
      "    val_log_likelihood: 544.931787109375\n",
      "    val_log_marginal: 428.6692704169911\n",
      "Train Epoch: 714 [512/54000 (1%)] Loss: -534.445984\n",
      "Train Epoch: 714 [11776/54000 (22%)] Loss: -478.314514\n",
      "Train Epoch: 714 [23040/54000 (43%)] Loss: -478.352844\n",
      "Train Epoch: 714 [34304/54000 (64%)] Loss: -443.429688\n",
      "Train Epoch: 714 [45568/54000 (84%)] Loss: -524.049988\n",
      "    epoch          : 714\n",
      "    loss           : -469.0023126220703\n",
      "    val_loss       : -423.2120186382905\n",
      "    val_log_likelihood: 544.4943542480469\n",
      "    val_log_marginal: 428.7396017957479\n",
      "Train Epoch: 715 [512/54000 (1%)] Loss: -492.189514\n",
      "Train Epoch: 715 [11776/54000 (22%)] Loss: -621.753113\n",
      "Train Epoch: 715 [23040/54000 (43%)] Loss: -503.948639\n",
      "Train Epoch: 715 [34304/54000 (64%)] Loss: -471.895355\n",
      "Train Epoch: 715 [45568/54000 (84%)] Loss: -516.341370\n",
      "    epoch          : 715\n",
      "    loss           : -468.9193811035156\n",
      "    val_loss       : -426.49330400060865\n",
      "    val_log_likelihood: 547.0788208007813\n",
      "    val_log_marginal: 430.40253136734674\n",
      "Train Epoch: 716 [512/54000 (1%)] Loss: -621.591064\n",
      "Train Epoch: 716 [11776/54000 (22%)] Loss: -511.349365\n",
      "Train Epoch: 716 [23040/54000 (43%)] Loss: -632.316223\n",
      "Train Epoch: 716 [34304/54000 (64%)] Loss: -493.957184\n",
      "Train Epoch: 716 [45568/54000 (84%)] Loss: -458.120270\n",
      "    epoch          : 716\n",
      "    loss           : -468.39156829833985\n",
      "    val_loss       : -426.46994957178833\n",
      "    val_log_likelihood: 547.2815063476562\n",
      "    val_log_marginal: 431.32858132885366\n",
      "Train Epoch: 717 [512/54000 (1%)] Loss: -238.544907\n",
      "Train Epoch: 717 [11776/54000 (22%)] Loss: -488.338074\n",
      "Train Epoch: 717 [23040/54000 (43%)] Loss: -476.349854\n",
      "Train Epoch: 717 [34304/54000 (64%)] Loss: -497.444977\n",
      "Train Epoch: 717 [45568/54000 (84%)] Loss: -624.918823\n",
      "    epoch          : 717\n",
      "    loss           : -468.3113249206543\n",
      "    val_loss       : -423.6960949987173\n",
      "    val_log_likelihood: 544.375244140625\n",
      "    val_log_marginal: 428.06921321861444\n",
      "Train Epoch: 718 [512/54000 (1%)] Loss: -527.234680\n",
      "Train Epoch: 718 [11776/54000 (22%)] Loss: -433.544250\n",
      "Train Epoch: 718 [23040/54000 (43%)] Loss: -602.109741\n",
      "Train Epoch: 718 [34304/54000 (64%)] Loss: -449.912201\n",
      "Train Epoch: 718 [45568/54000 (84%)] Loss: -504.860443\n",
      "    epoch          : 718\n",
      "    loss           : -468.2771546936035\n",
      "    val_loss       : -424.22139227688314\n",
      "    val_log_likelihood: 544.0489654541016\n",
      "    val_log_marginal: 427.3315687920898\n",
      "Train Epoch: 719 [512/54000 (1%)] Loss: -502.295929\n",
      "Train Epoch: 719 [11776/54000 (22%)] Loss: -511.632507\n",
      "Train Epoch: 719 [23040/54000 (43%)] Loss: -509.832458\n",
      "Train Epoch: 719 [34304/54000 (64%)] Loss: -522.474854\n",
      "Train Epoch: 719 [45568/54000 (84%)] Loss: -489.974091\n",
      "    epoch          : 719\n",
      "    loss           : -469.2923345947266\n",
      "    val_loss       : -423.1423532249406\n",
      "    val_log_likelihood: 543.9513885498047\n",
      "    val_log_marginal: 427.51561407783794\n",
      "Train Epoch: 720 [512/54000 (1%)] Loss: -523.216064\n",
      "Train Epoch: 720 [11776/54000 (22%)] Loss: -439.478790\n",
      "Train Epoch: 720 [23040/54000 (43%)] Loss: -514.202820\n",
      "Train Epoch: 720 [34304/54000 (64%)] Loss: -509.701385\n",
      "Train Epoch: 720 [45568/54000 (84%)] Loss: -322.521057\n",
      "    epoch          : 720\n",
      "    loss           : -469.7563543701172\n",
      "    val_loss       : -425.2455097978935\n",
      "    val_log_likelihood: 546.2340423583985\n",
      "    val_log_marginal: 430.29871840663907\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch720.pth ...\n",
      "Train Epoch: 721 [512/54000 (1%)] Loss: -263.867706\n",
      "Train Epoch: 721 [11776/54000 (22%)] Loss: -520.798767\n",
      "Train Epoch: 721 [23040/54000 (43%)] Loss: -521.071472\n",
      "Train Epoch: 721 [34304/54000 (64%)] Loss: -524.380249\n",
      "Train Epoch: 721 [45568/54000 (84%)] Loss: -504.537964\n",
      "    epoch          : 721\n",
      "    loss           : -468.7972589111328\n",
      "    val_loss       : -425.84842707496136\n",
      "    val_log_likelihood: 547.0699829101562\n",
      "    val_log_marginal: 430.67261006181616\n",
      "Train Epoch: 722 [512/54000 (1%)] Loss: -529.037964\n",
      "Train Epoch: 722 [11776/54000 (22%)] Loss: -496.891907\n",
      "Train Epoch: 722 [23040/54000 (43%)] Loss: -619.161316\n",
      "Train Epoch: 722 [34304/54000 (64%)] Loss: -321.809875\n",
      "Train Epoch: 722 [45568/54000 (84%)] Loss: -450.089294\n",
      "    epoch          : 722\n",
      "    loss           : -468.52099456787107\n",
      "    val_loss       : -422.3040363200009\n",
      "    val_log_likelihood: 543.8094665527344\n",
      "    val_log_marginal: 427.2539868589491\n",
      "Train Epoch: 723 [512/54000 (1%)] Loss: -532.890015\n",
      "Train Epoch: 723 [11776/54000 (22%)] Loss: -481.606049\n",
      "Train Epoch: 723 [23040/54000 (43%)] Loss: -474.067322\n",
      "Train Epoch: 723 [34304/54000 (64%)] Loss: -478.877350\n",
      "Train Epoch: 723 [45568/54000 (84%)] Loss: -448.837280\n",
      "    epoch          : 723\n",
      "    loss           : -469.7508724975586\n",
      "    val_loss       : -421.0040364647284\n",
      "    val_log_likelihood: 544.6726806640625\n",
      "    val_log_marginal: 427.5684799220413\n",
      "Train Epoch: 724 [512/54000 (1%)] Loss: -528.383057\n",
      "Train Epoch: 724 [11776/54000 (22%)] Loss: -519.751038\n",
      "Train Epoch: 724 [23040/54000 (43%)] Loss: -515.099792\n",
      "Train Epoch: 724 [34304/54000 (64%)] Loss: -450.795441\n",
      "Train Epoch: 724 [45568/54000 (84%)] Loss: -443.758209\n",
      "    epoch          : 724\n",
      "    loss           : -468.99242309570315\n",
      "    val_loss       : -421.3897153554484\n",
      "    val_log_likelihood: 542.9585571289062\n",
      "    val_log_marginal: 426.7800059093168\n",
      "Train Epoch: 725 [512/54000 (1%)] Loss: -516.934021\n",
      "Train Epoch: 725 [11776/54000 (22%)] Loss: -626.316895\n",
      "Train Epoch: 725 [23040/54000 (43%)] Loss: -622.597961\n",
      "Train Epoch: 725 [34304/54000 (64%)] Loss: -529.838379\n",
      "Train Epoch: 725 [45568/54000 (84%)] Loss: -618.909973\n",
      "    epoch          : 725\n",
      "    loss           : -469.8690521240234\n",
      "    val_loss       : -426.0123175468296\n",
      "    val_log_likelihood: 547.7376525878906\n",
      "    val_log_marginal: 430.2832306306809\n",
      "Train Epoch: 726 [512/54000 (1%)] Loss: -516.586426\n",
      "Train Epoch: 726 [11776/54000 (22%)] Loss: -533.388367\n",
      "Train Epoch: 726 [23040/54000 (43%)] Loss: -522.186279\n",
      "Train Epoch: 726 [34304/54000 (64%)] Loss: -535.584229\n",
      "Train Epoch: 726 [45568/54000 (84%)] Loss: -460.948761\n",
      "    epoch          : 726\n",
      "    loss           : -469.68691055297853\n",
      "    val_loss       : -425.4018328756094\n",
      "    val_log_likelihood: 547.511508178711\n",
      "    val_log_marginal: 430.1362627067415\n",
      "Train Epoch: 727 [512/54000 (1%)] Loss: -624.512329\n",
      "Train Epoch: 727 [11776/54000 (22%)] Loss: -623.919434\n",
      "Train Epoch: 727 [23040/54000 (43%)] Loss: -293.484009\n",
      "Train Epoch: 727 [34304/54000 (64%)] Loss: -316.940857\n",
      "Train Epoch: 727 [45568/54000 (84%)] Loss: -449.464539\n",
      "    epoch          : 727\n",
      "    loss           : -469.06700668334963\n",
      "    val_loss       : -425.72628090418874\n",
      "    val_log_likelihood: 548.2018676757813\n",
      "    val_log_marginal: 431.48408535309136\n",
      "Train Epoch: 728 [512/54000 (1%)] Loss: -523.255127\n",
      "Train Epoch: 728 [11776/54000 (22%)] Loss: -624.370361\n",
      "Train Epoch: 728 [23040/54000 (43%)] Loss: -272.604797\n",
      "Train Epoch: 728 [34304/54000 (64%)] Loss: -625.318420\n",
      "Train Epoch: 728 [45568/54000 (84%)] Loss: -492.307983\n",
      "    epoch          : 728\n",
      "    loss           : -469.6839486694336\n",
      "    val_loss       : -423.3204769704491\n",
      "    val_log_likelihood: 545.8689331054687\n",
      "    val_log_marginal: 429.3721004921943\n",
      "Train Epoch: 729 [512/54000 (1%)] Loss: -262.326050\n",
      "Train Epoch: 729 [11776/54000 (22%)] Loss: -488.703613\n",
      "Train Epoch: 729 [23040/54000 (43%)] Loss: -483.528564\n",
      "Train Epoch: 729 [34304/54000 (64%)] Loss: -513.663574\n",
      "Train Epoch: 729 [45568/54000 (84%)] Loss: -511.860016\n",
      "    epoch          : 729\n",
      "    loss           : -469.54613830566404\n",
      "    val_loss       : -423.6536828489974\n",
      "    val_log_likelihood: 546.7809753417969\n",
      "    val_log_marginal: 429.1059395071119\n",
      "Train Epoch: 730 [512/54000 (1%)] Loss: -269.563629\n",
      "Train Epoch: 730 [11776/54000 (22%)] Loss: -499.740601\n",
      "Train Epoch: 730 [23040/54000 (43%)] Loss: -526.011475\n",
      "Train Epoch: 730 [34304/54000 (64%)] Loss: -491.219391\n",
      "Train Epoch: 730 [45568/54000 (84%)] Loss: -310.094788\n",
      "    epoch          : 730\n",
      "    loss           : -469.83582977294924\n",
      "    val_loss       : -424.0045910291374\n",
      "    val_log_likelihood: 548.1824523925782\n",
      "    val_log_marginal: 430.679588669166\n",
      "Train Epoch: 731 [512/54000 (1%)] Loss: -500.562683\n",
      "Train Epoch: 731 [11776/54000 (22%)] Loss: -533.466003\n",
      "Train Epoch: 731 [23040/54000 (43%)] Loss: -530.204956\n",
      "Train Epoch: 731 [34304/54000 (64%)] Loss: -309.441956\n",
      "Train Epoch: 731 [45568/54000 (84%)] Loss: -499.622314\n",
      "    epoch          : 731\n",
      "    loss           : -469.843928527832\n",
      "    val_loss       : -424.03697639405726\n",
      "    val_log_likelihood: 546.7319610595703\n",
      "    val_log_marginal: 428.9439142469317\n",
      "Train Epoch: 732 [512/54000 (1%)] Loss: -462.704346\n",
      "Train Epoch: 732 [11776/54000 (22%)] Loss: -482.904053\n",
      "Train Epoch: 732 [23040/54000 (43%)] Loss: -520.260620\n",
      "Train Epoch: 732 [34304/54000 (64%)] Loss: -517.717346\n",
      "Train Epoch: 732 [45568/54000 (84%)] Loss: -519.190674\n",
      "    epoch          : 732\n",
      "    loss           : -470.3037303161621\n",
      "    val_loss       : -426.0047138091177\n",
      "    val_log_likelihood: 545.8934265136719\n",
      "    val_log_marginal: 429.75492259152236\n",
      "Train Epoch: 733 [512/54000 (1%)] Loss: -443.936401\n",
      "Train Epoch: 733 [11776/54000 (22%)] Loss: -322.062927\n",
      "Train Epoch: 733 [23040/54000 (43%)] Loss: -513.231689\n",
      "Train Epoch: 733 [34304/54000 (64%)] Loss: -312.236206\n",
      "Train Epoch: 733 [45568/54000 (84%)] Loss: -525.682739\n",
      "    epoch          : 733\n",
      "    loss           : -470.1208638000488\n",
      "    val_loss       : -426.42806738168\n",
      "    val_log_likelihood: 550.5160491943359\n",
      "    val_log_marginal: 433.1741879489273\n",
      "Train Epoch: 734 [512/54000 (1%)] Loss: -521.344543\n",
      "Train Epoch: 734 [11776/54000 (22%)] Loss: -487.718323\n",
      "Train Epoch: 734 [23040/54000 (43%)] Loss: -289.098694\n",
      "Train Epoch: 734 [34304/54000 (64%)] Loss: -445.549866\n",
      "Train Epoch: 734 [45568/54000 (84%)] Loss: -320.304688\n",
      "    epoch          : 734\n",
      "    loss           : -469.8959616088867\n",
      "    val_loss       : -424.5265754973516\n",
      "    val_log_likelihood: 547.9517761230469\n",
      "    val_log_marginal: 429.96916421018557\n",
      "Train Epoch: 735 [512/54000 (1%)] Loss: -313.829346\n",
      "Train Epoch: 735 [11776/54000 (22%)] Loss: -327.449554\n",
      "Train Epoch: 735 [23040/54000 (43%)] Loss: -300.443420\n",
      "Train Epoch: 735 [34304/54000 (64%)] Loss: -630.302307\n",
      "Train Epoch: 735 [45568/54000 (84%)] Loss: -499.047058\n",
      "    epoch          : 735\n",
      "    loss           : -470.89905242919923\n",
      "    val_loss       : -424.1060542190447\n",
      "    val_log_likelihood: 546.2639129638671\n",
      "    val_log_marginal: 429.2945128109509\n",
      "Train Epoch: 736 [512/54000 (1%)] Loss: -526.850220\n",
      "Train Epoch: 736 [11776/54000 (22%)] Loss: -515.951538\n",
      "Train Epoch: 736 [23040/54000 (43%)] Loss: -531.955200\n",
      "Train Epoch: 736 [34304/54000 (64%)] Loss: -516.843140\n",
      "Train Epoch: 736 [45568/54000 (84%)] Loss: -532.090820\n",
      "    epoch          : 736\n",
      "    loss           : -470.5654934692383\n",
      "    val_loss       : -425.49314362034204\n",
      "    val_log_likelihood: 549.3325744628906\n",
      "    val_log_marginal: 431.449831886217\n",
      "Train Epoch: 737 [512/54000 (1%)] Loss: -537.070007\n",
      "Train Epoch: 737 [11776/54000 (22%)] Loss: -485.783112\n",
      "Train Epoch: 737 [23040/54000 (43%)] Loss: -312.670105\n",
      "Train Epoch: 737 [34304/54000 (64%)] Loss: -307.082458\n",
      "Train Epoch: 737 [45568/54000 (84%)] Loss: -442.305511\n",
      "    epoch          : 737\n",
      "    loss           : -469.95287536621095\n",
      "    val_loss       : -423.6497100971639\n",
      "    val_log_likelihood: 547.240380859375\n",
      "    val_log_marginal: 429.5227834311303\n",
      "Train Epoch: 738 [512/54000 (1%)] Loss: -533.454407\n",
      "Train Epoch: 738 [11776/54000 (22%)] Loss: -515.624084\n",
      "Train Epoch: 738 [23040/54000 (43%)] Loss: -500.778503\n",
      "Train Epoch: 738 [34304/54000 (64%)] Loss: -529.982178\n",
      "Train Epoch: 738 [45568/54000 (84%)] Loss: -296.858826\n",
      "    epoch          : 738\n",
      "    loss           : -470.6273483276367\n",
      "    val_loss       : -426.9991654198617\n",
      "    val_log_likelihood: 547.4711456298828\n",
      "    val_log_marginal: 430.95021198354664\n",
      "Train Epoch: 739 [512/54000 (1%)] Loss: -532.639526\n",
      "Train Epoch: 739 [11776/54000 (22%)] Loss: -518.781738\n",
      "Train Epoch: 739 [23040/54000 (43%)] Loss: -518.646790\n",
      "Train Epoch: 739 [34304/54000 (64%)] Loss: -484.302002\n",
      "Train Epoch: 739 [45568/54000 (84%)] Loss: -448.401794\n",
      "    epoch          : 739\n",
      "    loss           : -470.64297805786134\n",
      "    val_loss       : -428.8668985299766\n",
      "    val_log_likelihood: 548.3488952636719\n",
      "    val_log_marginal: 432.8069481757014\n",
      "Train Epoch: 740 [512/54000 (1%)] Loss: -493.252350\n",
      "Train Epoch: 740 [11776/54000 (22%)] Loss: -512.005554\n",
      "Train Epoch: 740 [23040/54000 (43%)] Loss: -511.709473\n",
      "Train Epoch: 740 [34304/54000 (64%)] Loss: -510.789368\n",
      "Train Epoch: 740 [45568/54000 (84%)] Loss: -516.927673\n",
      "    epoch          : 740\n",
      "    loss           : -471.3103356933594\n",
      "    val_loss       : -424.03233593087646\n",
      "    val_log_likelihood: 547.3024047851562\n",
      "    val_log_marginal: 429.76824961192904\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch740.pth ...\n",
      "Train Epoch: 741 [512/54000 (1%)] Loss: -539.030457\n",
      "Train Epoch: 741 [11776/54000 (22%)] Loss: -626.698303\n",
      "Train Epoch: 741 [23040/54000 (43%)] Loss: -507.108582\n",
      "Train Epoch: 741 [34304/54000 (64%)] Loss: -534.982056\n",
      "Train Epoch: 741 [45568/54000 (84%)] Loss: -289.176453\n",
      "    epoch          : 741\n",
      "    loss           : -471.1968292236328\n",
      "    val_loss       : -424.85955651924013\n",
      "    val_log_likelihood: 546.7415740966796\n",
      "    val_log_marginal: 429.41012439850965\n",
      "Train Epoch: 742 [512/54000 (1%)] Loss: -464.746674\n",
      "Train Epoch: 742 [11776/54000 (22%)] Loss: -505.354065\n",
      "Train Epoch: 742 [23040/54000 (43%)] Loss: -476.809143\n",
      "Train Epoch: 742 [34304/54000 (64%)] Loss: -447.482269\n",
      "Train Epoch: 742 [45568/54000 (84%)] Loss: -517.234253\n",
      "    epoch          : 742\n",
      "    loss           : -470.51386169433596\n",
      "    val_loss       : -425.38083103057\n",
      "    val_log_likelihood: 549.4205993652344\n",
      "    val_log_marginal: 431.3939957205206\n",
      "Train Epoch: 743 [512/54000 (1%)] Loss: -481.622101\n",
      "Train Epoch: 743 [11776/54000 (22%)] Loss: -507.367401\n",
      "Train Epoch: 743 [23040/54000 (43%)] Loss: -523.197510\n",
      "Train Epoch: 743 [34304/54000 (64%)] Loss: -513.461914\n",
      "Train Epoch: 743 [45568/54000 (84%)] Loss: -510.928223\n",
      "    epoch          : 743\n",
      "    loss           : -470.81578643798827\n",
      "    val_loss       : -426.7061108076945\n",
      "    val_log_likelihood: 549.7838317871094\n",
      "    val_log_marginal: 431.81386135704815\n",
      "Train Epoch: 744 [512/54000 (1%)] Loss: -517.940247\n",
      "Train Epoch: 744 [11776/54000 (22%)] Loss: -628.012451\n",
      "Train Epoch: 744 [23040/54000 (43%)] Loss: -527.511719\n",
      "Train Epoch: 744 [34304/54000 (64%)] Loss: -268.994751\n",
      "Train Epoch: 744 [45568/54000 (84%)] Loss: -322.535675\n",
      "    epoch          : 744\n",
      "    loss           : -471.1733395385742\n",
      "    val_loss       : -422.7170462710783\n",
      "    val_log_likelihood: 546.2161376953125\n",
      "    val_log_marginal: 428.27606571667945\n",
      "Train Epoch: 745 [512/54000 (1%)] Loss: -273.320404\n",
      "Train Epoch: 745 [11776/54000 (22%)] Loss: -519.264954\n",
      "Train Epoch: 745 [23040/54000 (43%)] Loss: -304.898193\n",
      "Train Epoch: 745 [34304/54000 (64%)] Loss: -429.666534\n",
      "Train Epoch: 745 [45568/54000 (84%)] Loss: -515.248108\n",
      "    epoch          : 745\n",
      "    loss           : -471.14154571533203\n",
      "    val_loss       : -425.755635818094\n",
      "    val_log_likelihood: 550.2602111816407\n",
      "    val_log_marginal: 431.76739462814777\n",
      "Train Epoch: 746 [512/54000 (1%)] Loss: -520.000732\n",
      "Train Epoch: 746 [11776/54000 (22%)] Loss: -505.915802\n",
      "Train Epoch: 746 [23040/54000 (43%)] Loss: -460.331177\n",
      "Train Epoch: 746 [34304/54000 (64%)] Loss: -501.766907\n",
      "Train Epoch: 746 [45568/54000 (84%)] Loss: -525.893799\n",
      "    epoch          : 746\n",
      "    loss           : -471.7334217834473\n",
      "    val_loss       : -425.0268585400656\n",
      "    val_log_likelihood: 548.7293304443359\n",
      "    val_log_marginal: 430.17189700193705\n",
      "Train Epoch: 747 [512/54000 (1%)] Loss: -627.076416\n",
      "Train Epoch: 747 [11776/54000 (22%)] Loss: -506.724609\n",
      "Train Epoch: 747 [23040/54000 (43%)] Loss: -625.321106\n",
      "Train Epoch: 747 [34304/54000 (64%)] Loss: -453.567474\n",
      "Train Epoch: 747 [45568/54000 (84%)] Loss: -461.605408\n",
      "    epoch          : 747\n",
      "    loss           : -471.37602935791017\n",
      "    val_loss       : -424.21869691219183\n",
      "    val_log_likelihood: 546.7437530517578\n",
      "    val_log_marginal: 428.84552720673383\n",
      "Train Epoch: 748 [512/54000 (1%)] Loss: -636.376831\n",
      "Train Epoch: 748 [11776/54000 (22%)] Loss: -481.030029\n",
      "Train Epoch: 748 [23040/54000 (43%)] Loss: -521.338196\n",
      "Train Epoch: 748 [34304/54000 (64%)] Loss: -498.570587\n",
      "Train Epoch: 748 [45568/54000 (84%)] Loss: -525.341919\n",
      "    epoch          : 748\n",
      "    loss           : -471.19468353271486\n",
      "    val_loss       : -423.7457533173263\n",
      "    val_log_likelihood: 548.6229614257812\n",
      "    val_log_marginal: 430.1455750625727\n",
      "Train Epoch: 749 [512/54000 (1%)] Loss: -445.144592\n",
      "Train Epoch: 749 [11776/54000 (22%)] Loss: -499.141846\n",
      "Train Epoch: 749 [23040/54000 (43%)] Loss: -522.951904\n",
      "Train Epoch: 749 [34304/54000 (64%)] Loss: -531.255981\n",
      "Train Epoch: 749 [45568/54000 (84%)] Loss: -452.755066\n",
      "    epoch          : 749\n",
      "    loss           : -470.7536334228516\n",
      "    val_loss       : -426.63924014642834\n",
      "    val_log_likelihood: 549.8178771972656\n",
      "    val_log_marginal: 431.5340397871454\n",
      "Train Epoch: 750 [512/54000 (1%)] Loss: -545.602173\n",
      "Train Epoch: 750 [11776/54000 (22%)] Loss: -299.095520\n",
      "Train Epoch: 750 [23040/54000 (43%)] Loss: -464.216095\n",
      "Train Epoch: 750 [34304/54000 (64%)] Loss: -517.485229\n",
      "Train Epoch: 750 [45568/54000 (84%)] Loss: -436.181366\n",
      "    epoch          : 750\n",
      "    loss           : -471.6916424560547\n",
      "    val_loss       : -427.09294645804914\n",
      "    val_log_likelihood: 549.8311553955078\n",
      "    val_log_marginal: 431.52938999080004\n",
      "Train Epoch: 751 [512/54000 (1%)] Loss: -489.925659\n",
      "Train Epoch: 751 [11776/54000 (22%)] Loss: -629.072388\n",
      "Train Epoch: 751 [23040/54000 (43%)] Loss: -272.356506\n",
      "Train Epoch: 751 [34304/54000 (64%)] Loss: -628.246460\n",
      "Train Epoch: 751 [45568/54000 (84%)] Loss: -449.906036\n",
      "    epoch          : 751\n",
      "    loss           : -471.83502380371095\n",
      "    val_loss       : -425.77729761116206\n",
      "    val_log_likelihood: 548.7066223144532\n",
      "    val_log_marginal: 430.6047703694552\n",
      "Train Epoch: 752 [512/54000 (1%)] Loss: -501.843750\n",
      "Train Epoch: 752 [11776/54000 (22%)] Loss: -525.806274\n",
      "Train Epoch: 752 [23040/54000 (43%)] Loss: -498.344971\n",
      "Train Epoch: 752 [34304/54000 (64%)] Loss: -498.353790\n",
      "Train Epoch: 752 [45568/54000 (84%)] Loss: -524.376709\n",
      "    epoch          : 752\n",
      "    loss           : -471.9618145751953\n",
      "    val_loss       : -424.93162197172643\n",
      "    val_log_likelihood: 548.8627075195312\n",
      "    val_log_marginal: 429.93050944544376\n",
      "Train Epoch: 753 [512/54000 (1%)] Loss: -482.228210\n",
      "Train Epoch: 753 [11776/54000 (22%)] Loss: -513.272949\n",
      "Train Epoch: 753 [23040/54000 (43%)] Loss: -514.342224\n",
      "Train Epoch: 753 [34304/54000 (64%)] Loss: -506.705170\n",
      "Train Epoch: 753 [45568/54000 (84%)] Loss: -441.009888\n",
      "    epoch          : 753\n",
      "    loss           : -471.6226550292969\n",
      "    val_loss       : -425.96347110755744\n",
      "    val_log_likelihood: 548.7860626220703\n",
      "    val_log_marginal: 430.3127598684281\n",
      "Train Epoch: 754 [512/54000 (1%)] Loss: -513.113831\n",
      "Train Epoch: 754 [11776/54000 (22%)] Loss: -448.699707\n",
      "Train Epoch: 754 [23040/54000 (43%)] Loss: -310.294556\n",
      "Train Epoch: 754 [34304/54000 (64%)] Loss: -491.502808\n",
      "Train Epoch: 754 [45568/54000 (84%)] Loss: -521.878052\n",
      "    epoch          : 754\n",
      "    loss           : -472.1110842895508\n",
      "    val_loss       : -427.1165721943602\n",
      "    val_log_likelihood: 549.3016143798828\n",
      "    val_log_marginal: 431.1642237302273\n",
      "Train Epoch: 755 [512/54000 (1%)] Loss: -628.191284\n",
      "Train Epoch: 755 [11776/54000 (22%)] Loss: -633.613403\n",
      "Train Epoch: 755 [23040/54000 (43%)] Loss: -644.117004\n",
      "Train Epoch: 755 [34304/54000 (64%)] Loss: -274.721985\n",
      "Train Epoch: 755 [45568/54000 (84%)] Loss: -511.176117\n",
      "    epoch          : 755\n",
      "    loss           : -471.89722412109376\n",
      "    val_loss       : -427.8852587405592\n",
      "    val_log_likelihood: 551.9174774169921\n",
      "    val_log_marginal: 433.3221386287361\n",
      "Train Epoch: 756 [512/54000 (1%)] Loss: -626.498230\n",
      "Train Epoch: 756 [11776/54000 (22%)] Loss: -528.774292\n",
      "Train Epoch: 756 [23040/54000 (43%)] Loss: -495.851807\n",
      "Train Epoch: 756 [34304/54000 (64%)] Loss: -305.628723\n",
      "Train Epoch: 756 [45568/54000 (84%)] Loss: -506.861053\n",
      "    epoch          : 756\n",
      "    loss           : -471.73362548828123\n",
      "    val_loss       : -426.52720915358515\n",
      "    val_log_likelihood: 548.7142517089844\n",
      "    val_log_marginal: 430.1406277548522\n",
      "Train Epoch: 757 [512/54000 (1%)] Loss: -459.290070\n",
      "Train Epoch: 757 [11776/54000 (22%)] Loss: -528.326843\n",
      "Train Epoch: 757 [23040/54000 (43%)] Loss: -293.582245\n",
      "Train Epoch: 757 [34304/54000 (64%)] Loss: -637.362549\n",
      "Train Epoch: 757 [45568/54000 (84%)] Loss: -541.050354\n",
      "    epoch          : 757\n",
      "    loss           : -471.81470642089846\n",
      "    val_loss       : -424.2403393559158\n",
      "    val_log_likelihood: 549.4463531494141\n",
      "    val_log_marginal: 430.118082212405\n",
      "Train Epoch: 758 [512/54000 (1%)] Loss: -249.839539\n",
      "Train Epoch: 758 [11776/54000 (22%)] Loss: -525.454590\n",
      "Train Epoch: 758 [23040/54000 (43%)] Loss: -540.804016\n",
      "Train Epoch: 758 [34304/54000 (64%)] Loss: -500.777527\n",
      "Train Epoch: 758 [45568/54000 (84%)] Loss: -454.336761\n",
      "    epoch          : 758\n",
      "    loss           : -472.371572265625\n",
      "    val_loss       : -425.94608246050774\n",
      "    val_log_likelihood: 551.3096618652344\n",
      "    val_log_marginal: 432.50721506662666\n",
      "Train Epoch: 759 [512/54000 (1%)] Loss: -492.021271\n",
      "Train Epoch: 759 [11776/54000 (22%)] Loss: -530.561096\n",
      "Train Epoch: 759 [23040/54000 (43%)] Loss: -261.738586\n",
      "Train Epoch: 759 [34304/54000 (64%)] Loss: -277.981140\n",
      "Train Epoch: 759 [45568/54000 (84%)] Loss: -519.023315\n",
      "    epoch          : 759\n",
      "    loss           : -470.9170631408691\n",
      "    val_loss       : -426.50323348231615\n",
      "    val_log_likelihood: 550.7335418701172\n",
      "    val_log_marginal: 431.7720833303375\n",
      "Train Epoch: 760 [512/54000 (1%)] Loss: -511.408386\n",
      "Train Epoch: 760 [11776/54000 (22%)] Loss: -277.996521\n",
      "Train Epoch: 760 [23040/54000 (43%)] Loss: -637.376221\n",
      "Train Epoch: 760 [34304/54000 (64%)] Loss: -631.972290\n",
      "Train Epoch: 760 [45568/54000 (84%)] Loss: -498.220215\n",
      "    epoch          : 760\n",
      "    loss           : -471.5233595275879\n",
      "    val_loss       : -427.4428254798055\n",
      "    val_log_likelihood: 548.1681518554688\n",
      "    val_log_marginal: 429.9540727663786\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch760.pth ...\n",
      "Train Epoch: 761 [512/54000 (1%)] Loss: -519.711731\n",
      "Train Epoch: 761 [11776/54000 (22%)] Loss: -508.072083\n",
      "Train Epoch: 761 [23040/54000 (43%)] Loss: -497.530701\n",
      "Train Epoch: 761 [34304/54000 (64%)] Loss: -453.631195\n",
      "Train Epoch: 761 [45568/54000 (84%)] Loss: -316.728302\n",
      "    epoch          : 761\n",
      "    loss           : -472.4046887207031\n",
      "    val_loss       : -424.4209911108017\n",
      "    val_log_likelihood: 549.6830993652344\n",
      "    val_log_marginal: 430.87942698120577\n",
      "Train Epoch: 762 [512/54000 (1%)] Loss: -523.058960\n",
      "Train Epoch: 762 [11776/54000 (22%)] Loss: -305.558563\n",
      "Train Epoch: 762 [23040/54000 (43%)] Loss: -513.059143\n",
      "Train Epoch: 762 [34304/54000 (64%)] Loss: -534.393188\n",
      "Train Epoch: 762 [45568/54000 (84%)] Loss: -448.358215\n",
      "    epoch          : 762\n",
      "    loss           : -471.5797198486328\n",
      "    val_loss       : -428.01681240480394\n",
      "    val_log_likelihood: 551.4047821044921\n",
      "    val_log_marginal: 431.98357807286084\n",
      "Train Epoch: 763 [512/54000 (1%)] Loss: -281.476990\n",
      "Train Epoch: 763 [11776/54000 (22%)] Loss: -493.685547\n",
      "Train Epoch: 763 [23040/54000 (43%)] Loss: -310.332184\n",
      "Train Epoch: 763 [34304/54000 (64%)] Loss: -512.121216\n",
      "Train Epoch: 763 [45568/54000 (84%)] Loss: -300.210632\n",
      "    epoch          : 763\n",
      "    loss           : -472.50715301513674\n",
      "    val_loss       : -427.5546123163775\n",
      "    val_log_likelihood: 551.7196044921875\n",
      "    val_log_marginal: 432.36578086204827\n",
      "Train Epoch: 764 [512/54000 (1%)] Loss: -538.949829\n",
      "Train Epoch: 764 [11776/54000 (22%)] Loss: -530.129211\n",
      "Train Epoch: 764 [23040/54000 (43%)] Loss: -510.320282\n",
      "Train Epoch: 764 [34304/54000 (64%)] Loss: -282.096954\n",
      "Train Epoch: 764 [45568/54000 (84%)] Loss: -502.336517\n",
      "    epoch          : 764\n",
      "    loss           : -472.27794952392577\n",
      "    val_loss       : -426.69165574647485\n",
      "    val_log_likelihood: 550.509619140625\n",
      "    val_log_marginal: 431.8396782729775\n",
      "Train Epoch: 765 [512/54000 (1%)] Loss: -505.976440\n",
      "Train Epoch: 765 [11776/54000 (22%)] Loss: -262.661530\n",
      "Train Epoch: 765 [23040/54000 (43%)] Loss: -510.082336\n",
      "Train Epoch: 765 [34304/54000 (64%)] Loss: -323.242249\n",
      "Train Epoch: 765 [45568/54000 (84%)] Loss: -312.585876\n",
      "    epoch          : 765\n",
      "    loss           : -472.4483465576172\n",
      "    val_loss       : -422.92800308726726\n",
      "    val_log_likelihood: 548.3431518554687\n",
      "    val_log_marginal: 428.76697361655533\n",
      "Train Epoch: 766 [512/54000 (1%)] Loss: -623.617798\n",
      "Train Epoch: 766 [11776/54000 (22%)] Loss: -490.807831\n",
      "Train Epoch: 766 [23040/54000 (43%)] Loss: -484.577606\n",
      "Train Epoch: 766 [34304/54000 (64%)] Loss: -485.081726\n",
      "Train Epoch: 766 [45568/54000 (84%)] Loss: -525.425293\n",
      "    epoch          : 766\n",
      "    loss           : -471.82309829711915\n",
      "    val_loss       : -426.48611996714027\n",
      "    val_log_likelihood: 546.6548767089844\n",
      "    val_log_marginal: 428.93308301839335\n",
      "Train Epoch: 767 [512/54000 (1%)] Loss: -291.967773\n",
      "Train Epoch: 767 [11776/54000 (22%)] Loss: -632.653381\n",
      "Train Epoch: 767 [23040/54000 (43%)] Loss: -549.765198\n",
      "Train Epoch: 767 [34304/54000 (64%)] Loss: -535.481689\n",
      "Train Epoch: 767 [45568/54000 (84%)] Loss: -513.628235\n",
      "    epoch          : 767\n",
      "    loss           : -472.85471099853515\n",
      "    val_loss       : -428.41742705833167\n",
      "    val_log_likelihood: 551.1059326171875\n",
      "    val_log_marginal: 432.11643980704247\n",
      "Train Epoch: 768 [512/54000 (1%)] Loss: -525.866394\n",
      "Train Epoch: 768 [11776/54000 (22%)] Loss: -538.041565\n",
      "Train Epoch: 768 [23040/54000 (43%)] Loss: -513.359375\n",
      "Train Epoch: 768 [34304/54000 (64%)] Loss: -324.213531\n",
      "Train Epoch: 768 [45568/54000 (84%)] Loss: -519.821899\n",
      "    epoch          : 768\n",
      "    loss           : -472.1565924072266\n",
      "    val_loss       : -426.14663422387093\n",
      "    val_log_likelihood: 551.0170440673828\n",
      "    val_log_marginal: 431.9028762016445\n",
      "Train Epoch: 769 [512/54000 (1%)] Loss: -537.954102\n",
      "Train Epoch: 769 [11776/54000 (22%)] Loss: -319.683777\n",
      "Train Epoch: 769 [23040/54000 (43%)] Loss: -634.580933\n",
      "Train Epoch: 769 [34304/54000 (64%)] Loss: -441.561829\n",
      "Train Epoch: 769 [45568/54000 (84%)] Loss: -505.741394\n",
      "    epoch          : 769\n",
      "    loss           : -472.17510375976565\n",
      "    val_loss       : -426.9100219272077\n",
      "    val_log_likelihood: 546.848779296875\n",
      "    val_log_marginal: 429.4337297296639\n",
      "Train Epoch: 770 [512/54000 (1%)] Loss: -526.430054\n",
      "Train Epoch: 770 [11776/54000 (22%)] Loss: -480.407593\n",
      "Train Epoch: 770 [23040/54000 (43%)] Loss: -528.126831\n",
      "Train Epoch: 770 [34304/54000 (64%)] Loss: -628.576477\n",
      "Train Epoch: 770 [45568/54000 (84%)] Loss: -313.303223\n",
      "    epoch          : 770\n",
      "    loss           : -472.82071990966796\n",
      "    val_loss       : -426.47477453164754\n",
      "    val_log_likelihood: 546.880502319336\n",
      "    val_log_marginal: 429.2009401094168\n",
      "Train Epoch: 771 [512/54000 (1%)] Loss: -457.139435\n",
      "Train Epoch: 771 [11776/54000 (22%)] Loss: -522.405579\n",
      "Train Epoch: 771 [23040/54000 (43%)] Loss: -448.075134\n",
      "Train Epoch: 771 [34304/54000 (64%)] Loss: -329.849213\n",
      "Train Epoch: 771 [45568/54000 (84%)] Loss: -313.647003\n",
      "    epoch          : 771\n",
      "    loss           : -472.911242980957\n",
      "    val_loss       : -422.9731494845822\n",
      "    val_log_likelihood: 548.2772918701172\n",
      "    val_log_marginal: 429.21149362586436\n",
      "Train Epoch: 772 [512/54000 (1%)] Loss: -641.053955\n",
      "Train Epoch: 772 [11776/54000 (22%)] Loss: -286.173065\n",
      "Train Epoch: 772 [23040/54000 (43%)] Loss: -312.569916\n",
      "Train Epoch: 772 [34304/54000 (64%)] Loss: -506.463379\n",
      "Train Epoch: 772 [45568/54000 (84%)] Loss: -443.093414\n",
      "    epoch          : 772\n",
      "    loss           : -473.1956051635742\n",
      "    val_loss       : -430.1302178053185\n",
      "    val_log_likelihood: 554.0176696777344\n",
      "    val_log_marginal: 434.9318657744676\n",
      "Train Epoch: 773 [512/54000 (1%)] Loss: -275.938629\n",
      "Train Epoch: 773 [11776/54000 (22%)] Loss: -486.879700\n",
      "Train Epoch: 773 [23040/54000 (43%)] Loss: -507.979553\n",
      "Train Epoch: 773 [34304/54000 (64%)] Loss: -542.522705\n",
      "Train Epoch: 773 [45568/54000 (84%)] Loss: -323.119904\n",
      "    epoch          : 773\n",
      "    loss           : -472.6562319946289\n",
      "    val_loss       : -426.7251533137634\n",
      "    val_log_likelihood: 547.3348999023438\n",
      "    val_log_marginal: 430.284272220492\n",
      "Train Epoch: 774 [512/54000 (1%)] Loss: -528.116943\n",
      "Train Epoch: 774 [11776/54000 (22%)] Loss: -646.213989\n",
      "Train Epoch: 774 [23040/54000 (43%)] Loss: -630.916748\n",
      "Train Epoch: 774 [34304/54000 (64%)] Loss: -452.207642\n",
      "Train Epoch: 774 [45568/54000 (84%)] Loss: -517.657776\n",
      "    epoch          : 774\n",
      "    loss           : -472.926583404541\n",
      "    val_loss       : -426.88423511814324\n",
      "    val_log_likelihood: 550.7529754638672\n",
      "    val_log_marginal: 431.2591213084015\n",
      "Train Epoch: 775 [512/54000 (1%)] Loss: -624.734375\n",
      "Train Epoch: 775 [11776/54000 (22%)] Loss: -507.808746\n",
      "Train Epoch: 775 [23040/54000 (43%)] Loss: -507.900330\n",
      "Train Epoch: 775 [34304/54000 (64%)] Loss: -530.187012\n",
      "Train Epoch: 775 [45568/54000 (84%)] Loss: -311.946655\n",
      "    epoch          : 775\n",
      "    loss           : -472.9484872436523\n",
      "    val_loss       : -427.4638712272048\n",
      "    val_log_likelihood: 548.8430877685547\n",
      "    val_log_marginal: 431.2823247563095\n",
      "Train Epoch: 776 [512/54000 (1%)] Loss: -494.283783\n",
      "Train Epoch: 776 [11776/54000 (22%)] Loss: -538.197266\n",
      "Train Epoch: 776 [23040/54000 (43%)] Loss: -507.975433\n",
      "Train Epoch: 776 [34304/54000 (64%)] Loss: -636.979675\n",
      "Train Epoch: 776 [45568/54000 (84%)] Loss: -630.990112\n",
      "    epoch          : 776\n",
      "    loss           : -473.31229888916016\n",
      "    val_loss       : -428.9373714195564\n",
      "    val_log_likelihood: 550.2849853515625\n",
      "    val_log_marginal: 431.7212912093848\n",
      "Train Epoch: 777 [512/54000 (1%)] Loss: -491.642090\n",
      "Train Epoch: 777 [11776/54000 (22%)] Loss: -517.612183\n",
      "Train Epoch: 777 [23040/54000 (43%)] Loss: -503.553223\n",
      "Train Epoch: 777 [34304/54000 (64%)] Loss: -507.206207\n",
      "Train Epoch: 777 [45568/54000 (84%)] Loss: -456.717957\n",
      "    epoch          : 777\n",
      "    loss           : -472.85709777832034\n",
      "    val_loss       : -427.09824211206285\n",
      "    val_log_likelihood: 551.784228515625\n",
      "    val_log_marginal: 432.08017394756735\n",
      "Train Epoch: 778 [512/54000 (1%)] Loss: -526.926514\n",
      "Train Epoch: 778 [11776/54000 (22%)] Loss: -311.190674\n",
      "Train Epoch: 778 [23040/54000 (43%)] Loss: -489.636627\n",
      "Train Epoch: 778 [34304/54000 (64%)] Loss: -502.580292\n",
      "Train Epoch: 778 [45568/54000 (84%)] Loss: -441.977783\n",
      "    epoch          : 778\n",
      "    loss           : -472.62427520751953\n",
      "    val_loss       : -426.8424203051254\n",
      "    val_log_likelihood: 551.2716064453125\n",
      "    val_log_marginal: 431.5600061941892\n",
      "Train Epoch: 779 [512/54000 (1%)] Loss: -501.922119\n",
      "Train Epoch: 779 [11776/54000 (22%)] Loss: -334.114899\n",
      "Train Epoch: 779 [23040/54000 (43%)] Loss: -502.245697\n",
      "Train Epoch: 779 [34304/54000 (64%)] Loss: -513.578918\n",
      "Train Epoch: 779 [45568/54000 (84%)] Loss: -328.839630\n",
      "    epoch          : 779\n",
      "    loss           : -473.67897857666014\n",
      "    val_loss       : -428.94071430787443\n",
      "    val_log_likelihood: 552.4254089355469\n",
      "    val_log_marginal: 432.8810952808708\n",
      "Train Epoch: 780 [512/54000 (1%)] Loss: -494.448425\n",
      "Train Epoch: 780 [11776/54000 (22%)] Loss: -541.093994\n",
      "Train Epoch: 780 [23040/54000 (43%)] Loss: -523.354553\n",
      "Train Epoch: 780 [34304/54000 (64%)] Loss: -261.129425\n",
      "Train Epoch: 780 [45568/54000 (84%)] Loss: -486.082031\n",
      "    epoch          : 780\n",
      "    loss           : -473.22459014892576\n",
      "    val_loss       : -426.8948409639299\n",
      "    val_log_likelihood: 550.3635131835938\n",
      "    val_log_marginal: 431.4462855089523\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch780.pth ...\n",
      "Train Epoch: 781 [512/54000 (1%)] Loss: -443.518738\n",
      "Train Epoch: 781 [11776/54000 (22%)] Loss: -446.740143\n",
      "Train Epoch: 781 [23040/54000 (43%)] Loss: -296.383789\n",
      "Train Epoch: 781 [34304/54000 (64%)] Loss: -499.215454\n",
      "Train Epoch: 781 [45568/54000 (84%)] Loss: -515.087769\n",
      "    epoch          : 781\n",
      "    loss           : -473.31993408203124\n",
      "    val_loss       : -426.3282182807103\n",
      "    val_log_likelihood: 551.67490234375\n",
      "    val_log_marginal: 432.02001480422916\n",
      "Train Epoch: 782 [512/54000 (1%)] Loss: -494.976196\n",
      "Train Epoch: 782 [11776/54000 (22%)] Loss: -501.729553\n",
      "Train Epoch: 782 [23040/54000 (43%)] Loss: -500.082397\n",
      "Train Epoch: 782 [34304/54000 (64%)] Loss: -468.421631\n",
      "Train Epoch: 782 [45568/54000 (84%)] Loss: -507.969543\n",
      "    epoch          : 782\n",
      "    loss           : -473.1588146972656\n",
      "    val_loss       : -425.13386114835737\n",
      "    val_log_likelihood: 553.0585113525391\n",
      "    val_log_marginal: 433.33072082810105\n",
      "Train Epoch: 783 [512/54000 (1%)] Loss: -267.089508\n",
      "Train Epoch: 783 [11776/54000 (22%)] Loss: -645.032104\n",
      "Train Epoch: 783 [23040/54000 (43%)] Loss: -255.328857\n",
      "Train Epoch: 783 [34304/54000 (64%)] Loss: -634.340027\n",
      "Train Epoch: 783 [45568/54000 (84%)] Loss: -486.273682\n",
      "    epoch          : 783\n",
      "    loss           : -473.92923431396486\n",
      "    val_loss       : -425.2709341507405\n",
      "    val_log_likelihood: 549.6769775390625\n",
      "    val_log_marginal: 430.0952074829489\n",
      "Train Epoch: 784 [512/54000 (1%)] Loss: -523.202637\n",
      "Train Epoch: 784 [11776/54000 (22%)] Loss: -533.285034\n",
      "Train Epoch: 784 [23040/54000 (43%)] Loss: -272.615021\n",
      "Train Epoch: 784 [34304/54000 (64%)] Loss: -499.129242\n",
      "Train Epoch: 784 [45568/54000 (84%)] Loss: -314.819000\n",
      "    epoch          : 784\n",
      "    loss           : -473.9406719970703\n",
      "    val_loss       : -425.75173584725707\n",
      "    val_log_likelihood: 551.8529083251954\n",
      "    val_log_marginal: 433.00692386515436\n",
      "Train Epoch: 785 [512/54000 (1%)] Loss: -515.770264\n",
      "Train Epoch: 785 [11776/54000 (22%)] Loss: -320.405426\n",
      "Train Epoch: 785 [23040/54000 (43%)] Loss: -482.184692\n",
      "Train Epoch: 785 [34304/54000 (64%)] Loss: -501.691345\n",
      "Train Epoch: 785 [45568/54000 (84%)] Loss: -510.110535\n",
      "    epoch          : 785\n",
      "    loss           : -474.08233520507815\n",
      "    val_loss       : -426.5636332187802\n",
      "    val_log_likelihood: 550.4210723876953\n",
      "    val_log_marginal: 431.63956040628256\n",
      "Train Epoch: 786 [512/54000 (1%)] Loss: -512.521118\n",
      "Train Epoch: 786 [11776/54000 (22%)] Loss: -283.488556\n",
      "Train Epoch: 786 [23040/54000 (43%)] Loss: -292.235229\n",
      "Train Epoch: 786 [34304/54000 (64%)] Loss: -497.807465\n",
      "Train Epoch: 786 [45568/54000 (84%)] Loss: -483.676941\n",
      "    epoch          : 786\n",
      "    loss           : -473.89988037109373\n",
      "    val_loss       : -430.68209039829674\n",
      "    val_log_likelihood: 552.1937866210938\n",
      "    val_log_marginal: 433.3321472425214\n",
      "Train Epoch: 787 [512/54000 (1%)] Loss: -514.430603\n",
      "Train Epoch: 787 [11776/54000 (22%)] Loss: -492.520660\n",
      "Train Epoch: 787 [23040/54000 (43%)] Loss: -300.828369\n",
      "Train Epoch: 787 [34304/54000 (64%)] Loss: -634.547852\n",
      "Train Epoch: 787 [45568/54000 (84%)] Loss: -311.632202\n",
      "    epoch          : 787\n",
      "    loss           : -473.8303332519531\n",
      "    val_loss       : -426.5968382121995\n",
      "    val_log_likelihood: 554.2027374267578\n",
      "    val_log_marginal: 434.27874701581896\n",
      "Train Epoch: 788 [512/54000 (1%)] Loss: -490.043762\n",
      "Train Epoch: 788 [11776/54000 (22%)] Loss: -641.596802\n",
      "Train Epoch: 788 [23040/54000 (43%)] Loss: -268.570129\n",
      "Train Epoch: 788 [34304/54000 (64%)] Loss: -507.743896\n",
      "Train Epoch: 788 [45568/54000 (84%)] Loss: -463.671936\n",
      "    epoch          : 788\n",
      "    loss           : -472.0982402038574\n",
      "    val_loss       : -424.69615913312884\n",
      "    val_log_likelihood: 549.9024444580078\n",
      "    val_log_marginal: 429.74166815392675\n",
      "Train Epoch: 789 [512/54000 (1%)] Loss: -278.889801\n",
      "Train Epoch: 789 [11776/54000 (22%)] Loss: -440.083313\n",
      "Train Epoch: 789 [23040/54000 (43%)] Loss: -532.587646\n",
      "Train Epoch: 789 [34304/54000 (64%)] Loss: -519.008850\n",
      "Train Epoch: 789 [45568/54000 (84%)] Loss: -508.651550\n",
      "    epoch          : 789\n",
      "    loss           : -473.17580444335937\n",
      "    val_loss       : -424.8336919862777\n",
      "    val_log_likelihood: 547.5452087402343\n",
      "    val_log_marginal: 428.99488426782193\n",
      "Train Epoch: 790 [512/54000 (1%)] Loss: -511.733032\n",
      "Train Epoch: 790 [11776/54000 (22%)] Loss: -535.360229\n",
      "Train Epoch: 790 [23040/54000 (43%)] Loss: -544.568787\n",
      "Train Epoch: 790 [34304/54000 (64%)] Loss: -329.091553\n",
      "Train Epoch: 790 [45568/54000 (84%)] Loss: -520.666016\n",
      "    epoch          : 790\n",
      "    loss           : -473.33746337890625\n",
      "    val_loss       : -428.327446295321\n",
      "    val_log_likelihood: 553.8287719726562\n",
      "    val_log_marginal: 434.08730153925717\n",
      "Train Epoch: 791 [512/54000 (1%)] Loss: -509.640045\n",
      "Train Epoch: 791 [11776/54000 (22%)] Loss: -633.818726\n",
      "Train Epoch: 791 [23040/54000 (43%)] Loss: -501.802856\n",
      "Train Epoch: 791 [34304/54000 (64%)] Loss: -305.737488\n",
      "Train Epoch: 791 [45568/54000 (84%)] Loss: -327.210083\n",
      "    epoch          : 791\n",
      "    loss           : -474.2251919555664\n",
      "    val_loss       : -425.0079812712967\n",
      "    val_log_likelihood: 552.2146057128906\n",
      "    val_log_marginal: 431.9589849805567\n",
      "Train Epoch: 792 [512/54000 (1%)] Loss: -535.980652\n",
      "Train Epoch: 792 [11776/54000 (22%)] Loss: -528.522278\n",
      "Train Epoch: 792 [23040/54000 (43%)] Loss: -642.898682\n",
      "Train Epoch: 792 [34304/54000 (64%)] Loss: -634.226562\n",
      "Train Epoch: 792 [45568/54000 (84%)] Loss: -502.067566\n",
      "    epoch          : 792\n",
      "    loss           : -474.4890991210938\n",
      "    val_loss       : -423.4739019693807\n",
      "    val_log_likelihood: 550.177197265625\n",
      "    val_log_marginal: 429.486712953821\n",
      "Train Epoch: 793 [512/54000 (1%)] Loss: -508.312866\n",
      "Train Epoch: 793 [11776/54000 (22%)] Loss: -503.485596\n",
      "Train Epoch: 793 [23040/54000 (43%)] Loss: -465.813202\n",
      "Train Epoch: 793 [34304/54000 (64%)] Loss: -519.652100\n",
      "Train Epoch: 793 [45568/54000 (84%)] Loss: -512.363892\n",
      "    epoch          : 793\n",
      "    loss           : -474.3104997253418\n",
      "    val_loss       : -427.71810224484653\n",
      "    val_log_likelihood: 552.6420227050781\n",
      "    val_log_marginal: 433.23552618660034\n",
      "Train Epoch: 794 [512/54000 (1%)] Loss: -525.631592\n",
      "Train Epoch: 794 [11776/54000 (22%)] Loss: -507.089966\n",
      "Train Epoch: 794 [23040/54000 (43%)] Loss: -468.362061\n",
      "Train Epoch: 794 [34304/54000 (64%)] Loss: -516.753845\n",
      "Train Epoch: 794 [45568/54000 (84%)] Loss: -502.398956\n",
      "    epoch          : 794\n",
      "    loss           : -473.88865051269534\n",
      "    val_loss       : -427.94579953923824\n",
      "    val_log_likelihood: 552.5879516601562\n",
      "    val_log_marginal: 432.43957395814357\n",
      "Train Epoch: 795 [512/54000 (1%)] Loss: -500.795929\n",
      "Train Epoch: 795 [11776/54000 (22%)] Loss: -510.094025\n",
      "Train Epoch: 795 [23040/54000 (43%)] Loss: -637.926758\n",
      "Train Epoch: 795 [34304/54000 (64%)] Loss: -521.055786\n",
      "Train Epoch: 795 [45568/54000 (84%)] Loss: -508.034424\n",
      "    epoch          : 795\n",
      "    loss           : -474.2729019165039\n",
      "    val_loss       : -427.77076187413184\n",
      "    val_log_likelihood: 554.0165649414063\n",
      "    val_log_marginal: 433.8820777129382\n",
      "Train Epoch: 796 [512/54000 (1%)] Loss: -535.387329\n",
      "Train Epoch: 796 [11776/54000 (22%)] Loss: -488.837585\n",
      "Train Epoch: 796 [23040/54000 (43%)] Loss: -277.594177\n",
      "Train Epoch: 796 [34304/54000 (64%)] Loss: -456.699768\n",
      "Train Epoch: 796 [45568/54000 (84%)] Loss: -490.625977\n",
      "    epoch          : 796\n",
      "    loss           : -474.1968130493164\n",
      "    val_loss       : -424.12767326850445\n",
      "    val_log_likelihood: 552.2227172851562\n",
      "    val_log_marginal: 431.78703468255696\n",
      "Train Epoch: 797 [512/54000 (1%)] Loss: -498.894714\n",
      "Train Epoch: 797 [11776/54000 (22%)] Loss: -526.702759\n",
      "Train Epoch: 797 [23040/54000 (43%)] Loss: -281.352020\n",
      "Train Epoch: 797 [34304/54000 (64%)] Loss: -498.738464\n",
      "Train Epoch: 797 [45568/54000 (84%)] Loss: -510.835571\n",
      "    epoch          : 797\n",
      "    loss           : -474.5065692138672\n",
      "    val_loss       : -425.06952773239465\n",
      "    val_log_likelihood: 547.768798828125\n",
      "    val_log_marginal: 429.4422186475247\n",
      "Train Epoch: 798 [512/54000 (1%)] Loss: -271.768982\n",
      "Train Epoch: 798 [11776/54000 (22%)] Loss: -488.327148\n",
      "Train Epoch: 798 [23040/54000 (43%)] Loss: -536.729797\n",
      "Train Epoch: 798 [34304/54000 (64%)] Loss: -629.408325\n",
      "Train Epoch: 798 [45568/54000 (84%)] Loss: -513.686829\n",
      "    epoch          : 798\n",
      "    loss           : -473.71172973632815\n",
      "    val_loss       : -427.5072405921295\n",
      "    val_log_likelihood: 550.4301635742188\n",
      "    val_log_marginal: 431.12307617219096\n",
      "Train Epoch: 799 [512/54000 (1%)] Loss: -518.655151\n",
      "Train Epoch: 799 [11776/54000 (22%)] Loss: -439.941376\n",
      "Train Epoch: 799 [23040/54000 (43%)] Loss: -456.827576\n",
      "Train Epoch: 799 [34304/54000 (64%)] Loss: -445.560150\n",
      "Train Epoch: 799 [45568/54000 (84%)] Loss: -522.893555\n",
      "    epoch          : 799\n",
      "    loss           : -473.30772399902344\n",
      "    val_loss       : -427.7202568102628\n",
      "    val_log_likelihood: 552.2294952392579\n",
      "    val_log_marginal: 431.75682749412954\n",
      "Train Epoch: 800 [512/54000 (1%)] Loss: -511.033630\n",
      "Train Epoch: 800 [11776/54000 (22%)] Loss: -529.456177\n",
      "Train Epoch: 800 [23040/54000 (43%)] Loss: -519.998352\n",
      "Train Epoch: 800 [34304/54000 (64%)] Loss: -487.221863\n",
      "Train Epoch: 800 [45568/54000 (84%)] Loss: -312.799805\n",
      "    epoch          : 800\n",
      "    loss           : -474.13376586914063\n",
      "    val_loss       : -426.641010158509\n",
      "    val_log_likelihood: 550.92255859375\n",
      "    val_log_marginal: 431.29728723205625\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch800.pth ...\n",
      "Train Epoch: 801 [512/54000 (1%)] Loss: -524.305786\n",
      "Train Epoch: 801 [11776/54000 (22%)] Loss: -525.170898\n",
      "Train Epoch: 801 [23040/54000 (43%)] Loss: -543.794800\n",
      "Train Epoch: 801 [34304/54000 (64%)] Loss: -496.525513\n",
      "Train Epoch: 801 [45568/54000 (84%)] Loss: -515.563110\n",
      "    epoch          : 801\n",
      "    loss           : -474.0283804321289\n",
      "    val_loss       : -426.90922548249364\n",
      "    val_log_likelihood: 553.1781616210938\n",
      "    val_log_marginal: 432.72317905761304\n",
      "Train Epoch: 802 [512/54000 (1%)] Loss: -513.022888\n",
      "Train Epoch: 802 [11776/54000 (22%)] Loss: -539.279663\n",
      "Train Epoch: 802 [23040/54000 (43%)] Loss: -306.876862\n",
      "Train Epoch: 802 [34304/54000 (64%)] Loss: -307.924683\n",
      "Train Epoch: 802 [45568/54000 (84%)] Loss: -494.880920\n",
      "    epoch          : 802\n",
      "    loss           : -473.89775634765624\n",
      "    val_loss       : -423.8326554113999\n",
      "    val_log_likelihood: 549.8268737792969\n",
      "    val_log_marginal: 430.6072608326464\n",
      "Train Epoch: 803 [512/54000 (1%)] Loss: -641.410034\n",
      "Train Epoch: 803 [11776/54000 (22%)] Loss: -279.093079\n",
      "Train Epoch: 803 [23040/54000 (43%)] Loss: -513.355530\n",
      "Train Epoch: 803 [34304/54000 (64%)] Loss: -299.651489\n",
      "Train Epoch: 803 [45568/54000 (84%)] Loss: -455.433014\n",
      "    epoch          : 803\n",
      "    loss           : -474.7954528808594\n",
      "    val_loss       : -426.83021553084257\n",
      "    val_log_likelihood: 552.5247100830078\n",
      "    val_log_marginal: 432.04088603146374\n",
      "Train Epoch: 804 [512/54000 (1%)] Loss: -528.176636\n",
      "Train Epoch: 804 [11776/54000 (22%)] Loss: -545.132385\n",
      "Train Epoch: 804 [23040/54000 (43%)] Loss: -512.859863\n",
      "Train Epoch: 804 [34304/54000 (64%)] Loss: -261.000732\n",
      "Train Epoch: 804 [45568/54000 (84%)] Loss: -454.593353\n",
      "    epoch          : 804\n",
      "    loss           : -474.0220834350586\n",
      "    val_loss       : -427.5718432268128\n",
      "    val_log_likelihood: 551.9211730957031\n",
      "    val_log_marginal: 432.53844623975465\n",
      "Train Epoch: 805 [512/54000 (1%)] Loss: -454.391754\n",
      "Train Epoch: 805 [11776/54000 (22%)] Loss: -456.568573\n",
      "Train Epoch: 805 [23040/54000 (43%)] Loss: -521.451721\n",
      "Train Epoch: 805 [34304/54000 (64%)] Loss: -510.664001\n",
      "Train Epoch: 805 [45568/54000 (84%)] Loss: -274.219116\n",
      "    epoch          : 805\n",
      "    loss           : -473.33577728271484\n",
      "    val_loss       : -424.6661365972832\n",
      "    val_log_likelihood: 550.7904357910156\n",
      "    val_log_marginal: 430.11690483130513\n",
      "Train Epoch: 806 [512/54000 (1%)] Loss: -481.767303\n",
      "Train Epoch: 806 [11776/54000 (22%)] Loss: -634.289062\n",
      "Train Epoch: 806 [23040/54000 (43%)] Loss: -490.384369\n",
      "Train Epoch: 806 [34304/54000 (64%)] Loss: -509.741211\n",
      "Train Epoch: 806 [45568/54000 (84%)] Loss: -503.714447\n",
      "    epoch          : 806\n",
      "    loss           : -474.01791778564456\n",
      "    val_loss       : -428.6727030266076\n",
      "    val_log_likelihood: 549.7796752929687\n",
      "    val_log_marginal: 431.216161538288\n",
      "Train Epoch: 807 [512/54000 (1%)] Loss: -481.892548\n",
      "Train Epoch: 807 [11776/54000 (22%)] Loss: -531.947510\n",
      "Train Epoch: 807 [23040/54000 (43%)] Loss: -535.747437\n",
      "Train Epoch: 807 [34304/54000 (64%)] Loss: -635.943604\n",
      "Train Epoch: 807 [45568/54000 (84%)] Loss: -524.944580\n",
      "    epoch          : 807\n",
      "    loss           : -474.6683676147461\n",
      "    val_loss       : -427.34848340675234\n",
      "    val_log_likelihood: 554.7565032958985\n",
      "    val_log_marginal: 434.2283525716513\n",
      "Train Epoch: 808 [512/54000 (1%)] Loss: -439.706665\n",
      "Train Epoch: 808 [11776/54000 (22%)] Loss: -263.528931\n",
      "Train Epoch: 808 [23040/54000 (43%)] Loss: -275.186401\n",
      "Train Epoch: 808 [34304/54000 (64%)] Loss: -534.697571\n",
      "Train Epoch: 808 [45568/54000 (84%)] Loss: -506.183533\n",
      "    epoch          : 808\n",
      "    loss           : -474.4159381103516\n",
      "    val_loss       : -428.53121815081687\n",
      "    val_log_likelihood: 554.120883178711\n",
      "    val_log_marginal: 433.5128413427621\n",
      "Train Epoch: 809 [512/54000 (1%)] Loss: -481.378784\n",
      "Train Epoch: 809 [11776/54000 (22%)] Loss: -302.203308\n",
      "Train Epoch: 809 [23040/54000 (43%)] Loss: -331.593079\n",
      "Train Epoch: 809 [34304/54000 (64%)] Loss: -278.553223\n",
      "Train Epoch: 809 [45568/54000 (84%)] Loss: -520.208252\n",
      "    epoch          : 809\n",
      "    loss           : -475.4117370605469\n",
      "    val_loss       : -425.10972284022716\n",
      "    val_log_likelihood: 552.1015625\n",
      "    val_log_marginal: 430.9029660601169\n",
      "Train Epoch: 810 [512/54000 (1%)] Loss: -451.245850\n",
      "Train Epoch: 810 [11776/54000 (22%)] Loss: -287.577850\n",
      "Train Epoch: 810 [23040/54000 (43%)] Loss: -641.532898\n",
      "Train Epoch: 810 [34304/54000 (64%)] Loss: -478.621857\n",
      "Train Epoch: 810 [45568/54000 (84%)] Loss: -428.209198\n",
      "    epoch          : 810\n",
      "    loss           : -474.6484080505371\n",
      "    val_loss       : -428.3063835028559\n",
      "    val_log_likelihood: 553.6633178710938\n",
      "    val_log_marginal: 433.666184868291\n",
      "Train Epoch: 811 [512/54000 (1%)] Loss: -458.647278\n",
      "Train Epoch: 811 [11776/54000 (22%)] Loss: -500.201294\n",
      "Train Epoch: 811 [23040/54000 (43%)] Loss: -254.986542\n",
      "Train Epoch: 811 [34304/54000 (64%)] Loss: -308.189087\n",
      "Train Epoch: 811 [45568/54000 (84%)] Loss: -531.072754\n",
      "    epoch          : 811\n",
      "    loss           : -474.84316589355467\n",
      "    val_loss       : -426.0682241005823\n",
      "    val_log_likelihood: 549.2172119140625\n",
      "    val_log_marginal: 430.1058549169451\n",
      "Train Epoch: 812 [512/54000 (1%)] Loss: -635.338989\n",
      "Train Epoch: 812 [11776/54000 (22%)] Loss: -454.879089\n",
      "Train Epoch: 812 [23040/54000 (43%)] Loss: -495.821167\n",
      "Train Epoch: 812 [34304/54000 (64%)] Loss: -636.598511\n",
      "Train Epoch: 812 [45568/54000 (84%)] Loss: -314.585144\n",
      "    epoch          : 812\n",
      "    loss           : -474.2741641235352\n",
      "    val_loss       : -428.2818363891914\n",
      "    val_log_likelihood: 553.6783111572265\n",
      "    val_log_marginal: 432.88003190942106\n",
      "Train Epoch: 813 [512/54000 (1%)] Loss: -262.372833\n",
      "Train Epoch: 813 [11776/54000 (22%)] Loss: -484.307312\n",
      "Train Epoch: 813 [23040/54000 (43%)] Loss: -523.829407\n",
      "Train Epoch: 813 [34304/54000 (64%)] Loss: -433.104767\n",
      "Train Epoch: 813 [45568/54000 (84%)] Loss: -452.529877\n",
      "    epoch          : 813\n",
      "    loss           : -475.3151776123047\n",
      "    val_loss       : -428.46337223090234\n",
      "    val_log_likelihood: 554.7742492675782\n",
      "    val_log_marginal: 434.07078917063774\n",
      "Train Epoch: 814 [512/54000 (1%)] Loss: -297.549744\n",
      "Train Epoch: 814 [11776/54000 (22%)] Loss: -531.500061\n",
      "Train Epoch: 814 [23040/54000 (43%)] Loss: -519.165894\n",
      "Train Epoch: 814 [34304/54000 (64%)] Loss: -483.012421\n",
      "Train Epoch: 814 [45568/54000 (84%)] Loss: -459.141205\n",
      "    epoch          : 814\n",
      "    loss           : -474.87937927246094\n",
      "    val_loss       : -428.8489595489576\n",
      "    val_log_likelihood: 554.4623229980468\n",
      "    val_log_marginal: 434.1259319793433\n",
      "Train Epoch: 815 [512/54000 (1%)] Loss: -448.426056\n",
      "Train Epoch: 815 [11776/54000 (22%)] Loss: -246.664673\n",
      "Train Epoch: 815 [23040/54000 (43%)] Loss: -497.733124\n",
      "Train Epoch: 815 [34304/54000 (64%)] Loss: -531.232727\n",
      "Train Epoch: 815 [45568/54000 (84%)] Loss: -516.133118\n",
      "    epoch          : 815\n",
      "    loss           : -474.4708193969727\n",
      "    val_loss       : -426.1715928034857\n",
      "    val_log_likelihood: 553.1962707519531\n",
      "    val_log_marginal: 431.87328292541207\n",
      "Train Epoch: 816 [512/54000 (1%)] Loss: -275.123596\n",
      "Train Epoch: 816 [11776/54000 (22%)] Loss: -267.540527\n",
      "Train Epoch: 816 [23040/54000 (43%)] Loss: -536.466431\n",
      "Train Epoch: 816 [34304/54000 (64%)] Loss: -285.121857\n",
      "Train Epoch: 816 [45568/54000 (84%)] Loss: -515.292847\n",
      "    epoch          : 816\n",
      "    loss           : -475.03056854248047\n",
      "    val_loss       : -426.2342463033274\n",
      "    val_log_likelihood: 552.4299438476562\n",
      "    val_log_marginal: 431.37892909757795\n",
      "Train Epoch: 817 [512/54000 (1%)] Loss: -511.464783\n",
      "Train Epoch: 817 [11776/54000 (22%)] Loss: -276.091583\n",
      "Train Epoch: 817 [23040/54000 (43%)] Loss: -518.725830\n",
      "Train Epoch: 817 [34304/54000 (64%)] Loss: -648.046204\n",
      "Train Epoch: 817 [45568/54000 (84%)] Loss: -313.199097\n",
      "    epoch          : 817\n",
      "    loss           : -474.93488250732423\n",
      "    val_loss       : -427.149238826707\n",
      "    val_log_likelihood: 554.6622924804688\n",
      "    val_log_marginal: 433.97761490754783\n",
      "Train Epoch: 818 [512/54000 (1%)] Loss: -466.313965\n",
      "Train Epoch: 818 [11776/54000 (22%)] Loss: -489.499695\n",
      "Train Epoch: 818 [23040/54000 (43%)] Loss: -523.406799\n",
      "Train Epoch: 818 [34304/54000 (64%)] Loss: -632.383423\n",
      "Train Epoch: 818 [45568/54000 (84%)] Loss: -513.507996\n",
      "    epoch          : 818\n",
      "    loss           : -474.8745175170898\n",
      "    val_loss       : -428.29367618188263\n",
      "    val_log_likelihood: 554.9588073730469\n",
      "    val_log_marginal: 434.0266699966043\n",
      "Train Epoch: 819 [512/54000 (1%)] Loss: -544.215454\n",
      "Train Epoch: 819 [11776/54000 (22%)] Loss: -642.910522\n",
      "Train Epoch: 819 [23040/54000 (43%)] Loss: -543.187683\n",
      "Train Epoch: 819 [34304/54000 (64%)] Loss: -510.277649\n",
      "Train Epoch: 819 [45568/54000 (84%)] Loss: -309.929565\n",
      "    epoch          : 819\n",
      "    loss           : -476.01494659423827\n",
      "    val_loss       : -428.3383743882179\n",
      "    val_log_likelihood: 552.1445922851562\n",
      "    val_log_marginal: 432.2452606547624\n",
      "Train Epoch: 820 [512/54000 (1%)] Loss: -534.027344\n",
      "Train Epoch: 820 [11776/54000 (22%)] Loss: -499.819153\n",
      "Train Epoch: 820 [23040/54000 (43%)] Loss: -474.858673\n",
      "Train Epoch: 820 [34304/54000 (64%)] Loss: -455.964233\n",
      "Train Epoch: 820 [45568/54000 (84%)] Loss: -510.971863\n",
      "    epoch          : 820\n",
      "    loss           : -475.12851348876956\n",
      "    val_loss       : -428.86823004502804\n",
      "    val_log_likelihood: 554.7435302734375\n",
      "    val_log_marginal: 434.14021535404027\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch820.pth ...\n",
      "Train Epoch: 821 [512/54000 (1%)] Loss: -511.494385\n",
      "Train Epoch: 821 [11776/54000 (22%)] Loss: -496.672089\n",
      "Train Epoch: 821 [23040/54000 (43%)] Loss: -465.798279\n",
      "Train Epoch: 821 [34304/54000 (64%)] Loss: -528.118042\n",
      "Train Epoch: 821 [45568/54000 (84%)] Loss: -513.963135\n",
      "    epoch          : 821\n",
      "    loss           : -475.1679251098633\n",
      "    val_loss       : -429.87671737000346\n",
      "    val_log_likelihood: 555.44033203125\n",
      "    val_log_marginal: 434.4265838865191\n",
      "Train Epoch: 822 [512/54000 (1%)] Loss: -628.580566\n",
      "Train Epoch: 822 [11776/54000 (22%)] Loss: -549.777954\n",
      "Train Epoch: 822 [23040/54000 (43%)] Loss: -459.439697\n",
      "Train Epoch: 822 [34304/54000 (64%)] Loss: -511.535706\n",
      "Train Epoch: 822 [45568/54000 (84%)] Loss: -480.953735\n",
      "    epoch          : 822\n",
      "    loss           : -475.26619583129883\n",
      "    val_loss       : -428.7435057314113\n",
      "    val_log_likelihood: 553.1146423339844\n",
      "    val_log_marginal: 432.3288771118969\n",
      "Train Epoch: 823 [512/54000 (1%)] Loss: -509.155212\n",
      "Train Epoch: 823 [11776/54000 (22%)] Loss: -295.291626\n",
      "Train Epoch: 823 [23040/54000 (43%)] Loss: -518.061646\n",
      "Train Epoch: 823 [34304/54000 (64%)] Loss: -631.451050\n",
      "Train Epoch: 823 [45568/54000 (84%)] Loss: -296.589142\n",
      "    epoch          : 823\n",
      "    loss           : -475.41545608520505\n",
      "    val_loss       : -429.18297781888396\n",
      "    val_log_likelihood: 553.074267578125\n",
      "    val_log_marginal: 433.4998823549598\n",
      "Train Epoch: 824 [512/54000 (1%)] Loss: -517.672363\n",
      "Train Epoch: 824 [11776/54000 (22%)] Loss: -642.720520\n",
      "Train Epoch: 824 [23040/54000 (43%)] Loss: -525.323975\n",
      "Train Epoch: 824 [34304/54000 (64%)] Loss: -456.681305\n",
      "Train Epoch: 824 [45568/54000 (84%)] Loss: -302.495331\n",
      "    epoch          : 824\n",
      "    loss           : -475.15910858154297\n",
      "    val_loss       : -429.03723147977144\n",
      "    val_log_likelihood: 555.7737976074219\n",
      "    val_log_marginal: 434.49178419224916\n",
      "Train Epoch: 825 [512/54000 (1%)] Loss: -266.645874\n",
      "Train Epoch: 825 [11776/54000 (22%)] Loss: -522.450439\n",
      "Train Epoch: 825 [23040/54000 (43%)] Loss: -495.185760\n",
      "Train Epoch: 825 [34304/54000 (64%)] Loss: -525.465820\n",
      "Train Epoch: 825 [45568/54000 (84%)] Loss: -520.314697\n",
      "    epoch          : 825\n",
      "    loss           : -474.6703820800781\n",
      "    val_loss       : -429.3612984672189\n",
      "    val_log_likelihood: 554.8917633056641\n",
      "    val_log_marginal: 433.6601522076875\n",
      "Train Epoch: 826 [512/54000 (1%)] Loss: -533.959900\n",
      "Train Epoch: 826 [11776/54000 (22%)] Loss: -268.865784\n",
      "Train Epoch: 826 [23040/54000 (43%)] Loss: -524.784607\n",
      "Train Epoch: 826 [34304/54000 (64%)] Loss: -528.342773\n",
      "Train Epoch: 826 [45568/54000 (84%)] Loss: -483.747101\n",
      "    epoch          : 826\n",
      "    loss           : -475.16187377929685\n",
      "    val_loss       : -426.18215336725115\n",
      "    val_log_likelihood: 553.8065002441406\n",
      "    val_log_marginal: 432.5657698538178\n",
      "Train Epoch: 827 [512/54000 (1%)] Loss: -504.700317\n",
      "Train Epoch: 827 [11776/54000 (22%)] Loss: -631.688354\n",
      "Train Epoch: 827 [23040/54000 (43%)] Loss: -309.003967\n",
      "Train Epoch: 827 [34304/54000 (64%)] Loss: -504.277405\n",
      "Train Epoch: 827 [45568/54000 (84%)] Loss: -327.236877\n",
      "    epoch          : 827\n",
      "    loss           : -475.19469131469725\n",
      "    val_loss       : -426.90510575640945\n",
      "    val_log_likelihood: 552.5735931396484\n",
      "    val_log_marginal: 431.92323263324795\n",
      "Train Epoch: 828 [512/54000 (1%)] Loss: -531.583740\n",
      "Train Epoch: 828 [11776/54000 (22%)] Loss: -490.774139\n",
      "Train Epoch: 828 [23040/54000 (43%)] Loss: -633.240601\n",
      "Train Epoch: 828 [34304/54000 (64%)] Loss: -464.457214\n",
      "Train Epoch: 828 [45568/54000 (84%)] Loss: -443.536133\n",
      "    epoch          : 828\n",
      "    loss           : -474.7803633117676\n",
      "    val_loss       : -423.5340605933219\n",
      "    val_log_likelihood: 550.1659790039063\n",
      "    val_log_marginal: 429.57348813079295\n",
      "Train Epoch: 829 [512/54000 (1%)] Loss: -481.868835\n",
      "Train Epoch: 829 [11776/54000 (22%)] Loss: -494.419861\n",
      "Train Epoch: 829 [23040/54000 (43%)] Loss: -491.689819\n",
      "Train Epoch: 829 [34304/54000 (64%)] Loss: -523.709839\n",
      "Train Epoch: 829 [45568/54000 (84%)] Loss: -529.273560\n",
      "    epoch          : 829\n",
      "    loss           : -475.4408360290527\n",
      "    val_loss       : -430.65993453729897\n",
      "    val_log_likelihood: 555.69296875\n",
      "    val_log_marginal: 435.51583097688854\n",
      "Train Epoch: 830 [512/54000 (1%)] Loss: -500.916016\n",
      "Train Epoch: 830 [11776/54000 (22%)] Loss: -284.062256\n",
      "Train Epoch: 830 [23040/54000 (43%)] Loss: -464.309998\n",
      "Train Epoch: 830 [34304/54000 (64%)] Loss: -452.029266\n",
      "Train Epoch: 830 [45568/54000 (84%)] Loss: -321.176392\n",
      "    epoch          : 830\n",
      "    loss           : -475.97777404785154\n",
      "    val_loss       : -431.6505748940632\n",
      "    val_log_likelihood: 557.7106170654297\n",
      "    val_log_marginal: 436.89841478578745\n",
      "Train Epoch: 831 [512/54000 (1%)] Loss: -322.404907\n",
      "Train Epoch: 831 [11776/54000 (22%)] Loss: -300.895477\n",
      "Train Epoch: 831 [23040/54000 (43%)] Loss: -537.961426\n",
      "Train Epoch: 831 [34304/54000 (64%)] Loss: -275.751221\n",
      "Train Epoch: 831 [45568/54000 (84%)] Loss: -514.758789\n",
      "    epoch          : 831\n",
      "    loss           : -475.1842256164551\n",
      "    val_loss       : -428.032067893818\n",
      "    val_log_likelihood: 551.7580963134766\n",
      "    val_log_marginal: 432.7417494769892\n",
      "Train Epoch: 832 [512/54000 (1%)] Loss: -635.940613\n",
      "Train Epoch: 832 [11776/54000 (22%)] Loss: -545.956238\n",
      "Train Epoch: 832 [23040/54000 (43%)] Loss: -522.260559\n",
      "Train Epoch: 832 [34304/54000 (64%)] Loss: -312.559326\n",
      "Train Epoch: 832 [45568/54000 (84%)] Loss: -508.013794\n",
      "    epoch          : 832\n",
      "    loss           : -475.36791046142577\n",
      "    val_loss       : -427.2008292149752\n",
      "    val_log_likelihood: 553.1887481689453\n",
      "    val_log_marginal: 431.61035710498055\n",
      "Train Epoch: 833 [512/54000 (1%)] Loss: -529.592163\n",
      "Train Epoch: 833 [11776/54000 (22%)] Loss: -534.100708\n",
      "Train Epoch: 833 [23040/54000 (43%)] Loss: -314.031067\n",
      "Train Epoch: 833 [34304/54000 (64%)] Loss: -531.221313\n",
      "Train Epoch: 833 [45568/54000 (84%)] Loss: -329.535339\n",
      "    epoch          : 833\n",
      "    loss           : -475.19580368041994\n",
      "    val_loss       : -429.54727426376195\n",
      "    val_log_likelihood: 556.2129058837891\n",
      "    val_log_marginal: 435.21491837985815\n",
      "Train Epoch: 834 [512/54000 (1%)] Loss: -510.852600\n",
      "Train Epoch: 834 [11776/54000 (22%)] Loss: -542.987976\n",
      "Train Epoch: 834 [23040/54000 (43%)] Loss: -514.439819\n",
      "Train Epoch: 834 [34304/54000 (64%)] Loss: -508.180237\n",
      "Train Epoch: 834 [45568/54000 (84%)] Loss: -526.528564\n",
      "    epoch          : 834\n",
      "    loss           : -475.53950927734377\n",
      "    val_loss       : -424.32804788723587\n",
      "    val_log_likelihood: 552.7408142089844\n",
      "    val_log_marginal: 431.769523428008\n",
      "Train Epoch: 835 [512/54000 (1%)] Loss: -638.471924\n",
      "Train Epoch: 835 [11776/54000 (22%)] Loss: -521.134705\n",
      "Train Epoch: 835 [23040/54000 (43%)] Loss: -534.702698\n",
      "Train Epoch: 835 [34304/54000 (64%)] Loss: -447.791138\n",
      "Train Epoch: 835 [45568/54000 (84%)] Loss: -508.483398\n",
      "    epoch          : 835\n",
      "    loss           : -474.6863539123535\n",
      "    val_loss       : -429.51657591313125\n",
      "    val_log_likelihood: 555.1830963134765\n",
      "    val_log_marginal: 433.75430936275905\n",
      "Train Epoch: 836 [512/54000 (1%)] Loss: -519.834473\n",
      "Train Epoch: 836 [11776/54000 (22%)] Loss: -641.241943\n",
      "Train Epoch: 836 [23040/54000 (43%)] Loss: -522.308350\n",
      "Train Epoch: 836 [34304/54000 (64%)] Loss: -328.537659\n",
      "Train Epoch: 836 [45568/54000 (84%)] Loss: -523.687317\n",
      "    epoch          : 836\n",
      "    loss           : -476.5067483520508\n",
      "    val_loss       : -428.18129188735037\n",
      "    val_log_likelihood: 554.3989868164062\n",
      "    val_log_marginal: 432.8441167656332\n",
      "Train Epoch: 837 [512/54000 (1%)] Loss: -539.624023\n",
      "Train Epoch: 837 [11776/54000 (22%)] Loss: -546.270142\n",
      "Train Epoch: 837 [23040/54000 (43%)] Loss: -451.633545\n",
      "Train Epoch: 837 [34304/54000 (64%)] Loss: -499.636505\n",
      "Train Epoch: 837 [45568/54000 (84%)] Loss: -461.883301\n",
      "    epoch          : 837\n",
      "    loss           : -476.60985290527344\n",
      "    val_loss       : -427.01393439564856\n",
      "    val_log_likelihood: 553.1198486328125\n",
      "    val_log_marginal: 432.0331808362156\n",
      "Train Epoch: 838 [512/54000 (1%)] Loss: -452.872223\n",
      "Train Epoch: 838 [11776/54000 (22%)] Loss: -528.500366\n",
      "Train Epoch: 838 [23040/54000 (43%)] Loss: -460.521545\n",
      "Train Epoch: 838 [34304/54000 (64%)] Loss: -453.608734\n",
      "Train Epoch: 838 [45568/54000 (84%)] Loss: -460.289215\n",
      "    epoch          : 838\n",
      "    loss           : -475.51500122070314\n",
      "    val_loss       : -424.0433781435713\n",
      "    val_log_likelihood: 550.0069915771485\n",
      "    val_log_marginal: 429.8428603094071\n",
      "Train Epoch: 839 [512/54000 (1%)] Loss: -282.670288\n",
      "Train Epoch: 839 [11776/54000 (22%)] Loss: -643.537231\n",
      "Train Epoch: 839 [23040/54000 (43%)] Loss: -494.254578\n",
      "Train Epoch: 839 [34304/54000 (64%)] Loss: -514.341858\n",
      "Train Epoch: 839 [45568/54000 (84%)] Loss: -532.328308\n",
      "    epoch          : 839\n",
      "    loss           : -475.73913421630857\n",
      "    val_loss       : -429.95908740609883\n",
      "    val_log_likelihood: 555.8529815673828\n",
      "    val_log_marginal: 434.57404726056404\n",
      "Train Epoch: 840 [512/54000 (1%)] Loss: -530.915039\n",
      "Train Epoch: 840 [11776/54000 (22%)] Loss: -502.259094\n",
      "Train Epoch: 840 [23040/54000 (43%)] Loss: -644.835144\n",
      "Train Epoch: 840 [34304/54000 (64%)] Loss: -521.670959\n",
      "Train Epoch: 840 [45568/54000 (84%)] Loss: -518.116943\n",
      "    epoch          : 840\n",
      "    loss           : -475.1221116638184\n",
      "    val_loss       : -429.22672244757416\n",
      "    val_log_likelihood: 556.5058898925781\n",
      "    val_log_marginal: 435.2746077185163\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch840.pth ...\n",
      "Train Epoch: 841 [512/54000 (1%)] Loss: -453.588715\n",
      "Train Epoch: 841 [11776/54000 (22%)] Loss: -291.837097\n",
      "Train Epoch: 841 [23040/54000 (43%)] Loss: -317.041168\n",
      "Train Epoch: 841 [34304/54000 (64%)] Loss: -512.326599\n",
      "Train Epoch: 841 [45568/54000 (84%)] Loss: -304.229401\n",
      "    epoch          : 841\n",
      "    loss           : -475.91887420654297\n",
      "    val_loss       : -426.71489304136486\n",
      "    val_log_likelihood: 553.2891845703125\n",
      "    val_log_marginal: 432.43323883078995\n",
      "Train Epoch: 842 [512/54000 (1%)] Loss: -446.835571\n",
      "Train Epoch: 842 [11776/54000 (22%)] Loss: -635.803467\n",
      "Train Epoch: 842 [23040/54000 (43%)] Loss: -538.047668\n",
      "Train Epoch: 842 [34304/54000 (64%)] Loss: -277.731750\n",
      "Train Epoch: 842 [45568/54000 (84%)] Loss: -529.810364\n",
      "    epoch          : 842\n",
      "    loss           : -475.57986053466794\n",
      "    val_loss       : -430.2190840007737\n",
      "    val_log_likelihood: 552.9022247314454\n",
      "    val_log_marginal: 432.6073939443265\n",
      "Train Epoch: 843 [512/54000 (1%)] Loss: -653.416748\n",
      "Train Epoch: 843 [11776/54000 (22%)] Loss: -638.926636\n",
      "Train Epoch: 843 [23040/54000 (43%)] Loss: -453.529846\n",
      "Train Epoch: 843 [34304/54000 (64%)] Loss: -534.664917\n",
      "Train Epoch: 843 [45568/54000 (84%)] Loss: -525.636108\n",
      "    epoch          : 843\n",
      "    loss           : -475.88284454345705\n",
      "    val_loss       : -427.59088786952196\n",
      "    val_log_likelihood: 554.1332336425781\n",
      "    val_log_marginal: 432.47315053754\n",
      "Train Epoch: 844 [512/54000 (1%)] Loss: -447.452911\n",
      "Train Epoch: 844 [11776/54000 (22%)] Loss: -271.146118\n",
      "Train Epoch: 844 [23040/54000 (43%)] Loss: -307.203186\n",
      "Train Epoch: 844 [34304/54000 (64%)] Loss: -449.401062\n",
      "Train Epoch: 844 [45568/54000 (84%)] Loss: -310.733612\n",
      "    epoch          : 844\n",
      "    loss           : -475.83877136230467\n",
      "    val_loss       : -429.7549098279327\n",
      "    val_log_likelihood: 554.8199340820313\n",
      "    val_log_marginal: 433.57533105276536\n",
      "Train Epoch: 845 [512/54000 (1%)] Loss: -539.455017\n",
      "Train Epoch: 845 [11776/54000 (22%)] Loss: -541.696045\n",
      "Train Epoch: 845 [23040/54000 (43%)] Loss: -511.663513\n",
      "Train Epoch: 845 [34304/54000 (64%)] Loss: -494.327545\n",
      "Train Epoch: 845 [45568/54000 (84%)] Loss: -437.436798\n",
      "    epoch          : 845\n",
      "    loss           : -475.67501510620116\n",
      "    val_loss       : -428.43872899953277\n",
      "    val_log_likelihood: 554.1001831054688\n",
      "    val_log_marginal: 433.761030902341\n",
      "Train Epoch: 846 [512/54000 (1%)] Loss: -511.737762\n",
      "Train Epoch: 846 [11776/54000 (22%)] Loss: -504.142120\n",
      "Train Epoch: 846 [23040/54000 (43%)] Loss: -260.633423\n",
      "Train Epoch: 846 [34304/54000 (64%)] Loss: -289.036438\n",
      "Train Epoch: 846 [45568/54000 (84%)] Loss: -446.122833\n",
      "    epoch          : 846\n",
      "    loss           : -476.36643676757814\n",
      "    val_loss       : -428.2805814757943\n",
      "    val_log_likelihood: 554.3071655273437\n",
      "    val_log_marginal: 432.58038352094593\n",
      "Train Epoch: 847 [512/54000 (1%)] Loss: -523.455078\n",
      "Train Epoch: 847 [11776/54000 (22%)] Loss: -534.439453\n",
      "Train Epoch: 847 [23040/54000 (43%)] Loss: -505.040527\n",
      "Train Epoch: 847 [34304/54000 (64%)] Loss: -496.044281\n",
      "Train Epoch: 847 [45568/54000 (84%)] Loss: -504.468445\n",
      "    epoch          : 847\n",
      "    loss           : -476.28275421142575\n",
      "    val_loss       : -429.9252959663048\n",
      "    val_log_likelihood: 555.0484313964844\n",
      "    val_log_marginal: 434.1900122318417\n",
      "Train Epoch: 848 [512/54000 (1%)] Loss: -470.725647\n",
      "Train Epoch: 848 [11776/54000 (22%)] Loss: -516.001709\n",
      "Train Epoch: 848 [23040/54000 (43%)] Loss: -648.105469\n",
      "Train Epoch: 848 [34304/54000 (64%)] Loss: -296.287720\n",
      "Train Epoch: 848 [45568/54000 (84%)] Loss: -310.745056\n",
      "    epoch          : 848\n",
      "    loss           : -476.33775939941404\n",
      "    val_loss       : -429.5974457917735\n",
      "    val_log_likelihood: 555.9579711914063\n",
      "    val_log_marginal: 434.1791369054466\n",
      "Train Epoch: 849 [512/54000 (1%)] Loss: -492.798767\n",
      "Train Epoch: 849 [11776/54000 (22%)] Loss: -306.219330\n",
      "Train Epoch: 849 [23040/54000 (43%)] Loss: -321.434631\n",
      "Train Epoch: 849 [34304/54000 (64%)] Loss: -307.689178\n",
      "Train Epoch: 849 [45568/54000 (84%)] Loss: -509.404541\n",
      "    epoch          : 849\n",
      "    loss           : -477.5128463745117\n",
      "    val_loss       : -427.51679775249215\n",
      "    val_log_likelihood: 552.1321075439453\n",
      "    val_log_marginal: 431.37167056267754\n",
      "Train Epoch: 850 [512/54000 (1%)] Loss: -549.179565\n",
      "Train Epoch: 850 [11776/54000 (22%)] Loss: -520.830505\n",
      "Train Epoch: 850 [23040/54000 (43%)] Loss: -528.484924\n",
      "Train Epoch: 850 [34304/54000 (64%)] Loss: -524.750488\n",
      "Train Epoch: 850 [45568/54000 (84%)] Loss: -506.762177\n",
      "    epoch          : 850\n",
      "    loss           : -476.1900933837891\n",
      "    val_loss       : -429.3259605448693\n",
      "    val_log_likelihood: 553.5763122558594\n",
      "    val_log_marginal: 433.49054695116973\n",
      "Train Epoch: 851 [512/54000 (1%)] Loss: -543.727661\n",
      "Train Epoch: 851 [11776/54000 (22%)] Loss: -452.011108\n",
      "Train Epoch: 851 [23040/54000 (43%)] Loss: -273.218933\n",
      "Train Epoch: 851 [34304/54000 (64%)] Loss: -632.781677\n",
      "Train Epoch: 851 [45568/54000 (84%)] Loss: -517.471680\n",
      "    epoch          : 851\n",
      "    loss           : -476.5681512451172\n",
      "    val_loss       : -428.6468458738178\n",
      "    val_log_likelihood: 555.3900329589844\n",
      "    val_log_marginal: 433.9382346060127\n",
      "Train Epoch: 852 [512/54000 (1%)] Loss: -321.444458\n",
      "Train Epoch: 852 [11776/54000 (22%)] Loss: -496.506104\n",
      "Train Epoch: 852 [23040/54000 (43%)] Loss: -537.013367\n",
      "Train Epoch: 852 [34304/54000 (64%)] Loss: -501.359863\n",
      "Train Epoch: 852 [45568/54000 (84%)] Loss: -516.595886\n",
      "    epoch          : 852\n",
      "    loss           : -476.116591796875\n",
      "    val_loss       : -427.2217651983723\n",
      "    val_log_likelihood: 552.5677337646484\n",
      "    val_log_marginal: 431.69392720498143\n",
      "Train Epoch: 853 [512/54000 (1%)] Loss: -538.771484\n",
      "Train Epoch: 853 [11776/54000 (22%)] Loss: -653.136963\n",
      "Train Epoch: 853 [23040/54000 (43%)] Loss: -317.598938\n",
      "Train Epoch: 853 [34304/54000 (64%)] Loss: -511.423096\n",
      "Train Epoch: 853 [45568/54000 (84%)] Loss: -520.082275\n",
      "    epoch          : 853\n",
      "    loss           : -477.1923046875\n",
      "    val_loss       : -428.7635655585676\n",
      "    val_log_likelihood: 556.1564086914062\n",
      "    val_log_marginal: 435.2876954022795\n",
      "Train Epoch: 854 [512/54000 (1%)] Loss: -458.017731\n",
      "Train Epoch: 854 [11776/54000 (22%)] Loss: -485.789856\n",
      "Train Epoch: 854 [23040/54000 (43%)] Loss: -635.743896\n",
      "Train Epoch: 854 [34304/54000 (64%)] Loss: -505.642700\n",
      "Train Epoch: 854 [45568/54000 (84%)] Loss: -296.499268\n",
      "    epoch          : 854\n",
      "    loss           : -476.8161618041992\n",
      "    val_loss       : -428.3910782892257\n",
      "    val_log_likelihood: 555.5114349365234\n",
      "    val_log_marginal: 433.7533105541021\n",
      "Train Epoch: 855 [512/54000 (1%)] Loss: -639.228027\n",
      "Train Epoch: 855 [11776/54000 (22%)] Loss: -466.173889\n",
      "Train Epoch: 855 [23040/54000 (43%)] Loss: -482.582092\n",
      "Train Epoch: 855 [34304/54000 (64%)] Loss: -529.978760\n",
      "Train Epoch: 855 [45568/54000 (84%)] Loss: -520.875000\n",
      "    epoch          : 855\n",
      "    loss           : -476.40073364257813\n",
      "    val_loss       : -428.67086113840344\n",
      "    val_log_likelihood: 553.4760101318359\n",
      "    val_log_marginal: 432.81282969601455\n",
      "Train Epoch: 856 [512/54000 (1%)] Loss: -520.959534\n",
      "Train Epoch: 856 [11776/54000 (22%)] Loss: -271.023163\n",
      "Train Epoch: 856 [23040/54000 (43%)] Loss: -496.642090\n",
      "Train Epoch: 856 [34304/54000 (64%)] Loss: -449.758179\n",
      "Train Epoch: 856 [45568/54000 (84%)] Loss: -503.072266\n",
      "    epoch          : 856\n",
      "    loss           : -476.7788482666016\n",
      "    val_loss       : -428.8227338962257\n",
      "    val_log_likelihood: 553.7593719482422\n",
      "    val_log_marginal: 432.55135885886847\n",
      "Train Epoch: 857 [512/54000 (1%)] Loss: -502.080963\n",
      "Train Epoch: 857 [11776/54000 (22%)] Loss: -501.842651\n",
      "Train Epoch: 857 [23040/54000 (43%)] Loss: -271.075073\n",
      "Train Epoch: 857 [34304/54000 (64%)] Loss: -516.378296\n",
      "Train Epoch: 857 [45568/54000 (84%)] Loss: -327.658112\n",
      "    epoch          : 857\n",
      "    loss           : -476.59386169433594\n",
      "    val_loss       : -430.05087743289766\n",
      "    val_log_likelihood: 556.5234558105469\n",
      "    val_log_marginal: 434.9343717330412\n",
      "Train Epoch: 858 [512/54000 (1%)] Loss: -495.287231\n",
      "Train Epoch: 858 [11776/54000 (22%)] Loss: -638.333374\n",
      "Train Epoch: 858 [23040/54000 (43%)] Loss: -536.890137\n",
      "Train Epoch: 858 [34304/54000 (64%)] Loss: -493.609009\n",
      "Train Epoch: 858 [45568/54000 (84%)] Loss: -306.860260\n",
      "    epoch          : 858\n",
      "    loss           : -477.7304867553711\n",
      "    val_loss       : -426.18693028315903\n",
      "    val_log_likelihood: 554.4081085205078\n",
      "    val_log_marginal: 432.17410255931316\n",
      "Train Epoch: 859 [512/54000 (1%)] Loss: -642.974976\n",
      "Train Epoch: 859 [11776/54000 (22%)] Loss: -323.679352\n",
      "Train Epoch: 859 [23040/54000 (43%)] Loss: -486.939301\n",
      "Train Epoch: 859 [34304/54000 (64%)] Loss: -513.106689\n",
      "Train Epoch: 859 [45568/54000 (84%)] Loss: -319.565796\n",
      "    epoch          : 859\n",
      "    loss           : -476.39989730834964\n",
      "    val_loss       : -425.30979608986524\n",
      "    val_log_likelihood: 551.9563110351562\n",
      "    val_log_marginal: 431.0770738991152\n",
      "Train Epoch: 860 [512/54000 (1%)] Loss: -454.280975\n",
      "Train Epoch: 860 [11776/54000 (22%)] Loss: -487.368896\n",
      "Train Epoch: 860 [23040/54000 (43%)] Loss: -551.294312\n",
      "Train Epoch: 860 [34304/54000 (64%)] Loss: -520.767273\n",
      "Train Epoch: 860 [45568/54000 (84%)] Loss: -535.188232\n",
      "    epoch          : 860\n",
      "    loss           : -476.50996002197263\n",
      "    val_loss       : -430.5671705275774\n",
      "    val_log_likelihood: 556.5620697021484\n",
      "    val_log_marginal: 435.0241083841771\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch860.pth ...\n",
      "Train Epoch: 861 [512/54000 (1%)] Loss: -491.288208\n",
      "Train Epoch: 861 [11776/54000 (22%)] Loss: -506.160706\n",
      "Train Epoch: 861 [23040/54000 (43%)] Loss: -528.226013\n",
      "Train Epoch: 861 [34304/54000 (64%)] Loss: -512.078735\n",
      "Train Epoch: 861 [45568/54000 (84%)] Loss: -314.176880\n",
      "    epoch          : 861\n",
      "    loss           : -476.7262303161621\n",
      "    val_loss       : -429.13488170783967\n",
      "    val_log_likelihood: 555.0426025390625\n",
      "    val_log_marginal: 433.47438406459986\n",
      "Train Epoch: 862 [512/54000 (1%)] Loss: -529.014343\n",
      "Train Epoch: 862 [11776/54000 (22%)] Loss: -489.899017\n",
      "Train Epoch: 862 [23040/54000 (43%)] Loss: -464.470123\n",
      "Train Epoch: 862 [34304/54000 (64%)] Loss: -539.144531\n",
      "Train Epoch: 862 [45568/54000 (84%)] Loss: -520.058411\n",
      "    epoch          : 862\n",
      "    loss           : -476.7301416015625\n",
      "    val_loss       : -429.62530381772666\n",
      "    val_log_likelihood: 556.263394165039\n",
      "    val_log_marginal: 434.56074066341733\n",
      "Train Epoch: 863 [512/54000 (1%)] Loss: -529.867615\n",
      "Train Epoch: 863 [11776/54000 (22%)] Loss: -479.941589\n",
      "Train Epoch: 863 [23040/54000 (43%)] Loss: -283.777863\n",
      "Train Epoch: 863 [34304/54000 (64%)] Loss: -490.337311\n",
      "Train Epoch: 863 [45568/54000 (84%)] Loss: -523.999756\n",
      "    epoch          : 863\n",
      "    loss           : -476.95083908081057\n",
      "    val_loss       : -427.79252301808447\n",
      "    val_log_likelihood: 551.7809020996094\n",
      "    val_log_marginal: 431.98188066066706\n",
      "Train Epoch: 864 [512/54000 (1%)] Loss: -537.909241\n",
      "Train Epoch: 864 [11776/54000 (22%)] Loss: -488.470032\n",
      "Train Epoch: 864 [23040/54000 (43%)] Loss: -489.541931\n",
      "Train Epoch: 864 [34304/54000 (64%)] Loss: -283.140015\n",
      "Train Epoch: 864 [45568/54000 (84%)] Loss: -507.315247\n",
      "    epoch          : 864\n",
      "    loss           : -477.27844314575196\n",
      "    val_loss       : -432.2443711262196\n",
      "    val_log_likelihood: 554.8125823974609\n",
      "    val_log_marginal: 434.9557354282588\n",
      "Train Epoch: 865 [512/54000 (1%)] Loss: -642.351562\n",
      "Train Epoch: 865 [11776/54000 (22%)] Loss: -503.084351\n",
      "Train Epoch: 865 [23040/54000 (43%)] Loss: -533.396118\n",
      "Train Epoch: 865 [34304/54000 (64%)] Loss: -279.629700\n",
      "Train Epoch: 865 [45568/54000 (84%)] Loss: -454.420441\n",
      "    epoch          : 865\n",
      "    loss           : -477.7409976196289\n",
      "    val_loss       : -427.6923093259335\n",
      "    val_log_likelihood: 554.934457397461\n",
      "    val_log_marginal: 433.79177145771894\n",
      "Train Epoch: 866 [512/54000 (1%)] Loss: -269.965454\n",
      "Train Epoch: 866 [11776/54000 (22%)] Loss: -268.502594\n",
      "Train Epoch: 866 [23040/54000 (43%)] Loss: -506.518372\n",
      "Train Epoch: 866 [34304/54000 (64%)] Loss: -492.635162\n",
      "Train Epoch: 866 [45568/54000 (84%)] Loss: -497.373718\n",
      "    epoch          : 866\n",
      "    loss           : -476.8659387207031\n",
      "    val_loss       : -427.6656668657437\n",
      "    val_log_likelihood: 555.8107299804688\n",
      "    val_log_marginal: 433.702995203808\n",
      "Train Epoch: 867 [512/54000 (1%)] Loss: -445.155487\n",
      "Train Epoch: 867 [11776/54000 (22%)] Loss: -646.899658\n",
      "Train Epoch: 867 [23040/54000 (43%)] Loss: -321.725891\n",
      "Train Epoch: 867 [34304/54000 (64%)] Loss: -532.321106\n",
      "Train Epoch: 867 [45568/54000 (84%)] Loss: -510.070435\n",
      "    epoch          : 867\n",
      "    loss           : -477.815876159668\n",
      "    val_loss       : -430.5316875001416\n",
      "    val_log_likelihood: 557.1064392089844\n",
      "    val_log_marginal: 435.44713918676746\n",
      "Train Epoch: 868 [512/54000 (1%)] Loss: -455.832306\n",
      "Train Epoch: 868 [11776/54000 (22%)] Loss: -274.164032\n",
      "Train Epoch: 868 [23040/54000 (43%)] Loss: -524.220032\n",
      "Train Epoch: 868 [34304/54000 (64%)] Loss: -504.914276\n",
      "Train Epoch: 868 [45568/54000 (84%)] Loss: -517.894165\n",
      "    epoch          : 868\n",
      "    loss           : -477.5096353149414\n",
      "    val_loss       : -429.75325318519026\n",
      "    val_log_likelihood: 555.8522430419922\n",
      "    val_log_marginal: 434.9484477069229\n",
      "Train Epoch: 869 [512/54000 (1%)] Loss: -455.552124\n",
      "Train Epoch: 869 [11776/54000 (22%)] Loss: -456.833801\n",
      "Train Epoch: 869 [23040/54000 (43%)] Loss: -524.225708\n",
      "Train Epoch: 869 [34304/54000 (64%)] Loss: -272.665009\n",
      "Train Epoch: 869 [45568/54000 (84%)] Loss: -534.323181\n",
      "    epoch          : 869\n",
      "    loss           : -477.43202239990234\n",
      "    val_loss       : -429.0692913196981\n",
      "    val_log_likelihood: 556.7333923339844\n",
      "    val_log_marginal: 434.98927460052073\n",
      "Train Epoch: 870 [512/54000 (1%)] Loss: -268.338440\n",
      "Train Epoch: 870 [11776/54000 (22%)] Loss: -505.424744\n",
      "Train Epoch: 870 [23040/54000 (43%)] Loss: -504.143372\n",
      "Train Epoch: 870 [34304/54000 (64%)] Loss: -526.903198\n",
      "Train Epoch: 870 [45568/54000 (84%)] Loss: -517.334839\n",
      "    epoch          : 870\n",
      "    loss           : -476.8823631286621\n",
      "    val_loss       : -428.0627089390531\n",
      "    val_log_likelihood: 555.8185180664062\n",
      "    val_log_marginal: 433.5308070201427\n",
      "Train Epoch: 871 [512/54000 (1%)] Loss: -521.326416\n",
      "Train Epoch: 871 [11776/54000 (22%)] Loss: -518.926147\n",
      "Train Epoch: 871 [23040/54000 (43%)] Loss: -450.855652\n",
      "Train Epoch: 871 [34304/54000 (64%)] Loss: -263.546387\n",
      "Train Epoch: 871 [45568/54000 (84%)] Loss: -520.762695\n",
      "    epoch          : 871\n",
      "    loss           : -477.6096226501465\n",
      "    val_loss       : -430.42917246390135\n",
      "    val_log_likelihood: 555.9804016113281\n",
      "    val_log_marginal: 435.4078468362415\n",
      "Train Epoch: 872 [512/54000 (1%)] Loss: -275.885315\n",
      "Train Epoch: 872 [11776/54000 (22%)] Loss: -502.234619\n",
      "Train Epoch: 872 [23040/54000 (43%)] Loss: -525.875610\n",
      "Train Epoch: 872 [34304/54000 (64%)] Loss: -449.834900\n",
      "Train Epoch: 872 [45568/54000 (84%)] Loss: -515.629395\n",
      "    epoch          : 872\n",
      "    loss           : -478.01273376464843\n",
      "    val_loss       : -427.9739851523191\n",
      "    val_log_likelihood: 552.6475799560546\n",
      "    val_log_marginal: 431.80774969644847\n",
      "Train Epoch: 873 [512/54000 (1%)] Loss: -529.640564\n",
      "Train Epoch: 873 [11776/54000 (22%)] Loss: -500.259827\n",
      "Train Epoch: 873 [23040/54000 (43%)] Loss: -482.070801\n",
      "Train Epoch: 873 [34304/54000 (64%)] Loss: -524.339233\n",
      "Train Epoch: 873 [45568/54000 (84%)] Loss: -512.522217\n",
      "    epoch          : 873\n",
      "    loss           : -478.10274993896485\n",
      "    val_loss       : -428.02907064929605\n",
      "    val_log_likelihood: 556.9528106689453\n",
      "    val_log_marginal: 434.83995823673905\n",
      "Train Epoch: 874 [512/54000 (1%)] Loss: -503.869080\n",
      "Train Epoch: 874 [11776/54000 (22%)] Loss: -546.929382\n",
      "Train Epoch: 874 [23040/54000 (43%)] Loss: -470.372620\n",
      "Train Epoch: 874 [34304/54000 (64%)] Loss: -329.990936\n",
      "Train Epoch: 874 [45568/54000 (84%)] Loss: -331.503662\n",
      "    epoch          : 874\n",
      "    loss           : -477.57597595214844\n",
      "    val_loss       : -426.45277031157167\n",
      "    val_log_likelihood: 552.4891571044922\n",
      "    val_log_marginal: 431.80439388118685\n",
      "Train Epoch: 875 [512/54000 (1%)] Loss: -547.159546\n",
      "Train Epoch: 875 [11776/54000 (22%)] Loss: -528.008179\n",
      "Train Epoch: 875 [23040/54000 (43%)] Loss: -536.205200\n",
      "Train Epoch: 875 [34304/54000 (64%)] Loss: -482.121094\n",
      "Train Epoch: 875 [45568/54000 (84%)] Loss: -513.497986\n",
      "    epoch          : 875\n",
      "    loss           : -477.54113616943357\n",
      "    val_loss       : -428.92781543023887\n",
      "    val_log_likelihood: 556.7258911132812\n",
      "    val_log_marginal: 434.49310463927685\n",
      "Train Epoch: 876 [512/54000 (1%)] Loss: -537.501831\n",
      "Train Epoch: 876 [11776/54000 (22%)] Loss: -526.925415\n",
      "Train Epoch: 876 [23040/54000 (43%)] Loss: -513.878174\n",
      "Train Epoch: 876 [34304/54000 (64%)] Loss: -640.872559\n",
      "Train Epoch: 876 [45568/54000 (84%)] Loss: -308.630493\n",
      "    epoch          : 876\n",
      "    loss           : -476.8831051635742\n",
      "    val_loss       : -429.42045670300723\n",
      "    val_log_likelihood: 556.5762329101562\n",
      "    val_log_marginal: 434.8416255336265\n",
      "Train Epoch: 877 [512/54000 (1%)] Loss: -264.456390\n",
      "Train Epoch: 877 [11776/54000 (22%)] Loss: -518.162964\n",
      "Train Epoch: 877 [23040/54000 (43%)] Loss: -455.110840\n",
      "Train Epoch: 877 [34304/54000 (64%)] Loss: -451.610626\n",
      "Train Epoch: 877 [45568/54000 (84%)] Loss: -302.899475\n",
      "    epoch          : 877\n",
      "    loss           : -477.5580595397949\n",
      "    val_loss       : -432.36229575332254\n",
      "    val_log_likelihood: 558.3884887695312\n",
      "    val_log_marginal: 436.1114285688847\n",
      "Train Epoch: 878 [512/54000 (1%)] Loss: -455.962738\n",
      "Train Epoch: 878 [11776/54000 (22%)] Loss: -454.654236\n",
      "Train Epoch: 878 [23040/54000 (43%)] Loss: -487.392548\n",
      "Train Epoch: 878 [34304/54000 (64%)] Loss: -532.686768\n",
      "Train Epoch: 878 [45568/54000 (84%)] Loss: -455.396790\n",
      "    epoch          : 878\n",
      "    loss           : -477.93692810058593\n",
      "    val_loss       : -428.86714804768565\n",
      "    val_log_likelihood: 556.1913818359375\n",
      "    val_log_marginal: 434.2844322409481\n",
      "Train Epoch: 879 [512/54000 (1%)] Loss: -451.730774\n",
      "Train Epoch: 879 [11776/54000 (22%)] Loss: -634.506470\n",
      "Train Epoch: 879 [23040/54000 (43%)] Loss: -301.295410\n",
      "Train Epoch: 879 [34304/54000 (64%)] Loss: -449.985016\n",
      "Train Epoch: 879 [45568/54000 (84%)] Loss: -298.256470\n",
      "    epoch          : 879\n",
      "    loss           : -477.6537045288086\n",
      "    val_loss       : -428.84359836392105\n",
      "    val_log_likelihood: 554.7360595703125\n",
      "    val_log_marginal: 433.4769233416766\n",
      "Train Epoch: 880 [512/54000 (1%)] Loss: -446.429230\n",
      "Train Epoch: 880 [11776/54000 (22%)] Loss: -531.948853\n",
      "Train Epoch: 880 [23040/54000 (43%)] Loss: -457.010193\n",
      "Train Epoch: 880 [34304/54000 (64%)] Loss: -446.772858\n",
      "Train Epoch: 880 [45568/54000 (84%)] Loss: -538.506348\n",
      "    epoch          : 880\n",
      "    loss           : -477.4613641357422\n",
      "    val_loss       : -428.8699304524809\n",
      "    val_log_likelihood: 555.8638214111328\n",
      "    val_log_marginal: 434.3812028061599\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch880.pth ...\n",
      "Train Epoch: 881 [512/54000 (1%)] Loss: -537.560547\n",
      "Train Epoch: 881 [11776/54000 (22%)] Loss: -650.001404\n",
      "Train Epoch: 881 [23040/54000 (43%)] Loss: -487.770233\n",
      "Train Epoch: 881 [34304/54000 (64%)] Loss: -495.365540\n",
      "Train Epoch: 881 [45568/54000 (84%)] Loss: -538.333008\n",
      "    epoch          : 881\n",
      "    loss           : -477.6373873901367\n",
      "    val_loss       : -425.4950123924762\n",
      "    val_log_likelihood: 554.3814422607422\n",
      "    val_log_marginal: 431.9710393857211\n",
      "Train Epoch: 882 [512/54000 (1%)] Loss: -330.295288\n",
      "Train Epoch: 882 [11776/54000 (22%)] Loss: -532.053162\n",
      "Train Epoch: 882 [23040/54000 (43%)] Loss: -636.954407\n",
      "Train Epoch: 882 [34304/54000 (64%)] Loss: -533.318481\n",
      "Train Epoch: 882 [45568/54000 (84%)] Loss: -519.338745\n",
      "    epoch          : 882\n",
      "    loss           : -478.10099822998046\n",
      "    val_loss       : -429.3154637850821\n",
      "    val_log_likelihood: 556.2404815673829\n",
      "    val_log_marginal: 434.51981841884555\n",
      "Train Epoch: 883 [512/54000 (1%)] Loss: -451.080414\n",
      "Train Epoch: 883 [11776/54000 (22%)] Loss: -502.424194\n",
      "Train Epoch: 883 [23040/54000 (43%)] Loss: -497.121857\n",
      "Train Epoch: 883 [34304/54000 (64%)] Loss: -486.960114\n",
      "Train Epoch: 883 [45568/54000 (84%)] Loss: -311.324310\n",
      "    epoch          : 883\n",
      "    loss           : -477.9259143066406\n",
      "    val_loss       : -431.07059354893863\n",
      "    val_log_likelihood: 558.4205200195313\n",
      "    val_log_marginal: 436.1934370357543\n",
      "Train Epoch: 884 [512/54000 (1%)] Loss: -536.492310\n",
      "Train Epoch: 884 [11776/54000 (22%)] Loss: -504.399841\n",
      "Train Epoch: 884 [23040/54000 (43%)] Loss: -652.159302\n",
      "Train Epoch: 884 [34304/54000 (64%)] Loss: -452.628876\n",
      "Train Epoch: 884 [45568/54000 (84%)] Loss: -456.168213\n",
      "    epoch          : 884\n",
      "    loss           : -477.82859268188474\n",
      "    val_loss       : -431.62780764568595\n",
      "    val_log_likelihood: 557.8690246582031\n",
      "    val_log_marginal: 436.23964711539446\n",
      "Train Epoch: 885 [512/54000 (1%)] Loss: -525.151367\n",
      "Train Epoch: 885 [11776/54000 (22%)] Loss: -315.515411\n",
      "Train Epoch: 885 [23040/54000 (43%)] Loss: -540.046509\n",
      "Train Epoch: 885 [34304/54000 (64%)] Loss: -518.210205\n",
      "Train Epoch: 885 [45568/54000 (84%)] Loss: -449.119476\n",
      "    epoch          : 885\n",
      "    loss           : -478.28679748535154\n",
      "    val_loss       : -426.93902847506104\n",
      "    val_log_likelihood: 555.140414428711\n",
      "    val_log_marginal: 432.6268399145454\n",
      "Train Epoch: 886 [512/54000 (1%)] Loss: -505.195007\n",
      "Train Epoch: 886 [11776/54000 (22%)] Loss: -528.763916\n",
      "Train Epoch: 886 [23040/54000 (43%)] Loss: -642.454224\n",
      "Train Epoch: 886 [34304/54000 (64%)] Loss: -493.399902\n",
      "Train Epoch: 886 [45568/54000 (84%)] Loss: -502.199554\n",
      "    epoch          : 886\n",
      "    loss           : -478.18716445922854\n",
      "    val_loss       : -431.51155406367036\n",
      "    val_log_likelihood: 554.4970581054688\n",
      "    val_log_marginal: 433.69699657247185\n",
      "Train Epoch: 887 [512/54000 (1%)] Loss: -308.687195\n",
      "Train Epoch: 887 [11776/54000 (22%)] Loss: -529.511780\n",
      "Train Epoch: 887 [23040/54000 (43%)] Loss: -544.217651\n",
      "Train Epoch: 887 [34304/54000 (64%)] Loss: -522.533386\n",
      "Train Epoch: 887 [45568/54000 (84%)] Loss: -525.039673\n",
      "    epoch          : 887\n",
      "    loss           : -477.96875366210935\n",
      "    val_loss       : -427.82899581231175\n",
      "    val_log_likelihood: 555.8126647949218\n",
      "    val_log_marginal: 433.5702294867486\n",
      "Train Epoch: 888 [512/54000 (1%)] Loss: -439.307648\n",
      "Train Epoch: 888 [11776/54000 (22%)] Loss: -490.876068\n",
      "Train Epoch: 888 [23040/54000 (43%)] Loss: -482.968201\n",
      "Train Epoch: 888 [34304/54000 (64%)] Loss: -535.153442\n",
      "Train Epoch: 888 [45568/54000 (84%)] Loss: -297.410706\n",
      "    epoch          : 888\n",
      "    loss           : -477.1859460449219\n",
      "    val_loss       : -431.44398219753054\n",
      "    val_log_likelihood: 556.4099243164062\n",
      "    val_log_marginal: 436.03881368302166\n",
      "Train Epoch: 889 [512/54000 (1%)] Loss: -635.855896\n",
      "Train Epoch: 889 [11776/54000 (22%)] Loss: -522.400391\n",
      "Train Epoch: 889 [23040/54000 (43%)] Loss: -644.570740\n",
      "Train Epoch: 889 [34304/54000 (64%)] Loss: -453.936310\n",
      "Train Epoch: 889 [45568/54000 (84%)] Loss: -446.602875\n",
      "    epoch          : 889\n",
      "    loss           : -477.6282104492187\n",
      "    val_loss       : -428.3749763229862\n",
      "    val_log_likelihood: 556.2035369873047\n",
      "    val_log_marginal: 434.1807299707085\n",
      "Train Epoch: 890 [512/54000 (1%)] Loss: -274.564697\n",
      "Train Epoch: 890 [11776/54000 (22%)] Loss: -274.686676\n",
      "Train Epoch: 890 [23040/54000 (43%)] Loss: -510.821136\n",
      "Train Epoch: 890 [34304/54000 (64%)] Loss: -496.049011\n",
      "Train Epoch: 890 [45568/54000 (84%)] Loss: -512.421265\n",
      "    epoch          : 890\n",
      "    loss           : -478.2195916748047\n",
      "    val_loss       : -428.6649766534567\n",
      "    val_log_likelihood: 556.0740783691406\n",
      "    val_log_marginal: 433.9793077688664\n",
      "Train Epoch: 891 [512/54000 (1%)] Loss: -491.452942\n",
      "Train Epoch: 891 [11776/54000 (22%)] Loss: -326.220245\n",
      "Train Epoch: 891 [23040/54000 (43%)] Loss: -640.228882\n",
      "Train Epoch: 891 [34304/54000 (64%)] Loss: -530.460327\n",
      "Train Epoch: 891 [45568/54000 (84%)] Loss: -456.810211\n",
      "    epoch          : 891\n",
      "    loss           : -477.850032043457\n",
      "    val_loss       : -428.0354620672762\n",
      "    val_log_likelihood: 555.0950805664063\n",
      "    val_log_marginal: 433.0906419795007\n",
      "Train Epoch: 892 [512/54000 (1%)] Loss: -521.898987\n",
      "Train Epoch: 892 [11776/54000 (22%)] Loss: -537.180664\n",
      "Train Epoch: 892 [23040/54000 (43%)] Loss: -463.327515\n",
      "Train Epoch: 892 [34304/54000 (64%)] Loss: -310.476654\n",
      "Train Epoch: 892 [45568/54000 (84%)] Loss: -472.908020\n",
      "    epoch          : 892\n",
      "    loss           : -478.15214965820314\n",
      "    val_loss       : -431.47594885211436\n",
      "    val_log_likelihood: 557.7500457763672\n",
      "    val_log_marginal: 435.3449079696089\n",
      "Train Epoch: 893 [512/54000 (1%)] Loss: -496.408936\n",
      "Train Epoch: 893 [11776/54000 (22%)] Loss: -508.649872\n",
      "Train Epoch: 893 [23040/54000 (43%)] Loss: -514.679321\n",
      "Train Epoch: 893 [34304/54000 (64%)] Loss: -490.917236\n",
      "Train Epoch: 893 [45568/54000 (84%)] Loss: -516.688965\n",
      "    epoch          : 893\n",
      "    loss           : -478.3614974975586\n",
      "    val_loss       : -427.8380167072639\n",
      "    val_log_likelihood: 556.6604858398438\n",
      "    val_log_marginal: 433.9290777603841\n",
      "Train Epoch: 894 [512/54000 (1%)] Loss: -648.266052\n",
      "Train Epoch: 894 [11776/54000 (22%)] Loss: -549.081970\n",
      "Train Epoch: 894 [23040/54000 (43%)] Loss: -518.843201\n",
      "Train Epoch: 894 [34304/54000 (64%)] Loss: -509.286438\n",
      "Train Epoch: 894 [45568/54000 (84%)] Loss: -319.903748\n",
      "    epoch          : 894\n",
      "    loss           : -478.189944152832\n",
      "    val_loss       : -430.974708288908\n",
      "    val_log_likelihood: 558.541357421875\n",
      "    val_log_marginal: 436.2538680050522\n",
      "Train Epoch: 895 [512/54000 (1%)] Loss: -546.728821\n",
      "Train Epoch: 895 [11776/54000 (22%)] Loss: -447.774323\n",
      "Train Epoch: 895 [23040/54000 (43%)] Loss: -311.752625\n",
      "Train Epoch: 895 [34304/54000 (64%)] Loss: -531.802979\n",
      "Train Epoch: 895 [45568/54000 (84%)] Loss: -527.189026\n",
      "    epoch          : 895\n",
      "    loss           : -478.0312170410156\n",
      "    val_loss       : -427.5370438830927\n",
      "    val_log_likelihood: 556.2594543457031\n",
      "    val_log_marginal: 433.2909400116653\n",
      "Train Epoch: 896 [512/54000 (1%)] Loss: -273.723694\n",
      "Train Epoch: 896 [11776/54000 (22%)] Loss: -264.308533\n",
      "Train Epoch: 896 [23040/54000 (43%)] Loss: -460.948914\n",
      "Train Epoch: 896 [34304/54000 (64%)] Loss: -527.587402\n",
      "Train Epoch: 896 [45568/54000 (84%)] Loss: -508.834045\n",
      "    epoch          : 896\n",
      "    loss           : -479.2566931152344\n",
      "    val_loss       : -428.06351886037737\n",
      "    val_log_likelihood: 556.1104064941406\n",
      "    val_log_marginal: 433.2943490719046\n",
      "Train Epoch: 897 [512/54000 (1%)] Loss: -490.745361\n",
      "Train Epoch: 897 [11776/54000 (22%)] Loss: -643.223022\n",
      "Train Epoch: 897 [23040/54000 (43%)] Loss: -549.906128\n",
      "Train Epoch: 897 [34304/54000 (64%)] Loss: -293.598938\n",
      "Train Epoch: 897 [45568/54000 (84%)] Loss: -452.470978\n",
      "    epoch          : 897\n",
      "    loss           : -478.21694244384764\n",
      "    val_loss       : -429.31930925659833\n",
      "    val_log_likelihood: 557.5346405029297\n",
      "    val_log_marginal: 435.64265377409754\n",
      "Train Epoch: 898 [512/54000 (1%)] Loss: -549.624390\n",
      "Train Epoch: 898 [11776/54000 (22%)] Loss: -543.548950\n",
      "Train Epoch: 898 [23040/54000 (43%)] Loss: -475.988190\n",
      "Train Epoch: 898 [34304/54000 (64%)] Loss: -487.140320\n",
      "Train Epoch: 898 [45568/54000 (84%)] Loss: -515.704102\n",
      "    epoch          : 898\n",
      "    loss           : -478.823603515625\n",
      "    val_loss       : -429.69888441227374\n",
      "    val_log_likelihood: 557.9810607910156\n",
      "    val_log_marginal: 435.3358766105026\n",
      "Train Epoch: 899 [512/54000 (1%)] Loss: -286.168884\n",
      "Train Epoch: 899 [11776/54000 (22%)] Loss: -491.663422\n",
      "Train Epoch: 899 [23040/54000 (43%)] Loss: -514.918091\n",
      "Train Epoch: 899 [34304/54000 (64%)] Loss: -300.695404\n",
      "Train Epoch: 899 [45568/54000 (84%)] Loss: -489.881653\n",
      "    epoch          : 899\n",
      "    loss           : -478.8346940612793\n",
      "    val_loss       : -428.5744234221056\n",
      "    val_log_likelihood: 555.6300170898437\n",
      "    val_log_marginal: 434.9992667820305\n",
      "Train Epoch: 900 [512/54000 (1%)] Loss: -485.420471\n",
      "Train Epoch: 900 [11776/54000 (22%)] Loss: -506.598145\n",
      "Train Epoch: 900 [23040/54000 (43%)] Loss: -504.245605\n",
      "Train Epoch: 900 [34304/54000 (64%)] Loss: -451.851105\n",
      "Train Epoch: 900 [45568/54000 (84%)] Loss: -524.380371\n",
      "    epoch          : 900\n",
      "    loss           : -478.7216558837891\n",
      "    val_loss       : -429.30179056860504\n",
      "    val_log_likelihood: 554.672021484375\n",
      "    val_log_marginal: 432.64558729343116\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch900.pth ...\n",
      "Train Epoch: 901 [512/54000 (1%)] Loss: -294.246979\n",
      "Train Epoch: 901 [11776/54000 (22%)] Loss: -502.430969\n",
      "Train Epoch: 901 [23040/54000 (43%)] Loss: -528.681641\n",
      "Train Epoch: 901 [34304/54000 (64%)] Loss: -633.910767\n",
      "Train Epoch: 901 [45568/54000 (84%)] Loss: -509.151672\n",
      "    epoch          : 901\n",
      "    loss           : -478.5730197143555\n",
      "    val_loss       : -428.633960881643\n",
      "    val_log_likelihood: 555.0784790039063\n",
      "    val_log_marginal: 433.16463741324844\n",
      "Train Epoch: 902 [512/54000 (1%)] Loss: -519.966553\n",
      "Train Epoch: 902 [11776/54000 (22%)] Loss: -507.689697\n",
      "Train Epoch: 902 [23040/54000 (43%)] Loss: -270.648071\n",
      "Train Epoch: 902 [34304/54000 (64%)] Loss: -523.502136\n",
      "Train Epoch: 902 [45568/54000 (84%)] Loss: -640.964233\n",
      "    epoch          : 902\n",
      "    loss           : -478.23888381958005\n",
      "    val_loss       : -429.1146303962916\n",
      "    val_log_likelihood: 556.2749847412109\n",
      "    val_log_marginal: 433.66852455325426\n",
      "Train Epoch: 903 [512/54000 (1%)] Loss: -492.056488\n",
      "Train Epoch: 903 [11776/54000 (22%)] Loss: -279.984711\n",
      "Train Epoch: 903 [23040/54000 (43%)] Loss: -308.372681\n",
      "Train Epoch: 903 [34304/54000 (64%)] Loss: -461.897766\n",
      "Train Epoch: 903 [45568/54000 (84%)] Loss: -306.195740\n",
      "    epoch          : 903\n",
      "    loss           : -478.0592980957031\n",
      "    val_loss       : -430.03231489434836\n",
      "    val_log_likelihood: 556.7110321044922\n",
      "    val_log_marginal: 434.68278911028546\n",
      "Train Epoch: 904 [512/54000 (1%)] Loss: -453.898743\n",
      "Train Epoch: 904 [11776/54000 (22%)] Loss: -277.406982\n",
      "Train Epoch: 904 [23040/54000 (43%)] Loss: -483.050140\n",
      "Train Epoch: 904 [34304/54000 (64%)] Loss: -537.380676\n",
      "Train Epoch: 904 [45568/54000 (84%)] Loss: -521.476379\n",
      "    epoch          : 904\n",
      "    loss           : -479.4534396362305\n",
      "    val_loss       : -427.81769824754446\n",
      "    val_log_likelihood: 556.0574279785156\n",
      "    val_log_marginal: 434.26628179837144\n",
      "Train Epoch: 905 [512/54000 (1%)] Loss: -502.249603\n",
      "Train Epoch: 905 [11776/54000 (22%)] Loss: -547.278931\n",
      "Train Epoch: 905 [23040/54000 (43%)] Loss: -305.138824\n",
      "Train Epoch: 905 [34304/54000 (64%)] Loss: -643.041626\n",
      "Train Epoch: 905 [45568/54000 (84%)] Loss: -520.415466\n",
      "    epoch          : 905\n",
      "    loss           : -478.8422564697266\n",
      "    val_loss       : -431.08729206826536\n",
      "    val_log_likelihood: 557.2826385498047\n",
      "    val_log_marginal: 435.2439345370978\n",
      "Train Epoch: 906 [512/54000 (1%)] Loss: -540.977051\n",
      "Train Epoch: 906 [11776/54000 (22%)] Loss: -322.121857\n",
      "Train Epoch: 906 [23040/54000 (43%)] Loss: -262.433899\n",
      "Train Epoch: 906 [34304/54000 (64%)] Loss: -452.862793\n",
      "Train Epoch: 906 [45568/54000 (84%)] Loss: -302.030823\n",
      "    epoch          : 906\n",
      "    loss           : -478.5963375854492\n",
      "    val_loss       : -430.1108075628057\n",
      "    val_log_likelihood: 557.1741027832031\n",
      "    val_log_marginal: 435.04056071899834\n",
      "Train Epoch: 907 [512/54000 (1%)] Loss: -506.339233\n",
      "Train Epoch: 907 [11776/54000 (22%)] Loss: -642.789917\n",
      "Train Epoch: 907 [23040/54000 (43%)] Loss: -463.251770\n",
      "Train Epoch: 907 [34304/54000 (64%)] Loss: -507.750824\n",
      "Train Epoch: 907 [45568/54000 (84%)] Loss: -323.879822\n",
      "    epoch          : 907\n",
      "    loss           : -477.9356036376953\n",
      "    val_loss       : -430.6058187643066\n",
      "    val_log_likelihood: 559.0038879394531\n",
      "    val_log_marginal: 436.5609927337617\n",
      "Train Epoch: 908 [512/54000 (1%)] Loss: -504.144318\n",
      "Train Epoch: 908 [11776/54000 (22%)] Loss: -483.576660\n",
      "Train Epoch: 908 [23040/54000 (43%)] Loss: -536.826172\n",
      "Train Epoch: 908 [34304/54000 (64%)] Loss: -512.894287\n",
      "Train Epoch: 908 [45568/54000 (84%)] Loss: -515.016968\n",
      "    epoch          : 908\n",
      "    loss           : -479.10872482299806\n",
      "    val_loss       : -430.63412248305974\n",
      "    val_log_likelihood: 558.3355499267578\n",
      "    val_log_marginal: 435.4932709094137\n",
      "Train Epoch: 909 [512/54000 (1%)] Loss: -526.971191\n",
      "Train Epoch: 909 [11776/54000 (22%)] Loss: -480.682983\n",
      "Train Epoch: 909 [23040/54000 (43%)] Loss: -453.849701\n",
      "Train Epoch: 909 [34304/54000 (64%)] Loss: -447.947388\n",
      "Train Epoch: 909 [45568/54000 (84%)] Loss: -449.319946\n",
      "    epoch          : 909\n",
      "    loss           : -479.04637329101564\n",
      "    val_loss       : -428.26967396531256\n",
      "    val_log_likelihood: 556.6613403320313\n",
      "    val_log_marginal: 433.9065126504749\n",
      "Train Epoch: 910 [512/54000 (1%)] Loss: -245.604248\n",
      "Train Epoch: 910 [11776/54000 (22%)] Loss: -519.576660\n",
      "Train Epoch: 910 [23040/54000 (43%)] Loss: -495.636047\n",
      "Train Epoch: 910 [34304/54000 (64%)] Loss: -498.172119\n",
      "Train Epoch: 910 [45568/54000 (84%)] Loss: -533.133545\n",
      "    epoch          : 910\n",
      "    loss           : -478.574813079834\n",
      "    val_loss       : -429.1156015207991\n",
      "    val_log_likelihood: 555.2125518798828\n",
      "    val_log_marginal: 434.2445165980607\n",
      "Train Epoch: 911 [512/54000 (1%)] Loss: -496.768463\n",
      "Train Epoch: 911 [11776/54000 (22%)] Loss: -642.679443\n",
      "Train Epoch: 911 [23040/54000 (43%)] Loss: -530.953674\n",
      "Train Epoch: 911 [34304/54000 (64%)] Loss: -520.988770\n",
      "Train Epoch: 911 [45568/54000 (84%)] Loss: -527.970093\n",
      "    epoch          : 911\n",
      "    loss           : -479.09789978027345\n",
      "    val_loss       : -429.24598135296253\n",
      "    val_log_likelihood: 556.5427520751953\n",
      "    val_log_marginal: 433.7978038761765\n",
      "Train Epoch: 912 [512/54000 (1%)] Loss: -244.426117\n",
      "Train Epoch: 912 [11776/54000 (22%)] Loss: -481.719727\n",
      "Train Epoch: 912 [23040/54000 (43%)] Loss: -635.721191\n",
      "Train Epoch: 912 [34304/54000 (64%)] Loss: -477.821381\n",
      "Train Epoch: 912 [45568/54000 (84%)] Loss: -451.752380\n",
      "    epoch          : 912\n",
      "    loss           : -479.8423956298828\n",
      "    val_loss       : -428.74786696936934\n",
      "    val_log_likelihood: 558.6634490966796\n",
      "    val_log_marginal: 435.87399944756874\n",
      "Train Epoch: 913 [512/54000 (1%)] Loss: -542.036011\n",
      "Train Epoch: 913 [11776/54000 (22%)] Loss: -305.505646\n",
      "Train Epoch: 913 [23040/54000 (43%)] Loss: -514.136841\n",
      "Train Epoch: 913 [34304/54000 (64%)] Loss: -517.507019\n",
      "Train Epoch: 913 [45568/54000 (84%)] Loss: -297.151550\n",
      "    epoch          : 913\n",
      "    loss           : -479.15416107177737\n",
      "    val_loss       : -430.2301540510729\n",
      "    val_log_likelihood: 555.6445007324219\n",
      "    val_log_marginal: 434.42789522148666\n",
      "Train Epoch: 914 [512/54000 (1%)] Loss: -635.522400\n",
      "Train Epoch: 914 [11776/54000 (22%)] Loss: -517.012024\n",
      "Train Epoch: 914 [23040/54000 (43%)] Loss: -532.351868\n",
      "Train Epoch: 914 [34304/54000 (64%)] Loss: -541.473022\n",
      "Train Epoch: 914 [45568/54000 (84%)] Loss: -530.780396\n",
      "    epoch          : 914\n",
      "    loss           : -479.04385833740236\n",
      "    val_loss       : -430.68826578017325\n",
      "    val_log_likelihood: 555.108529663086\n",
      "    val_log_marginal: 434.0183846676596\n",
      "Train Epoch: 915 [512/54000 (1%)] Loss: -643.136719\n",
      "Train Epoch: 915 [11776/54000 (22%)] Loss: -253.047440\n",
      "Train Epoch: 915 [23040/54000 (43%)] Loss: -515.081665\n",
      "Train Epoch: 915 [34304/54000 (64%)] Loss: -523.194702\n",
      "Train Epoch: 915 [45568/54000 (84%)] Loss: -321.705200\n",
      "    epoch          : 915\n",
      "    loss           : -478.9848469543457\n",
      "    val_loss       : -427.94197454918174\n",
      "    val_log_likelihood: 552.895751953125\n",
      "    val_log_marginal: 432.9787134986371\n",
      "Train Epoch: 916 [512/54000 (1%)] Loss: -509.881653\n",
      "Train Epoch: 916 [11776/54000 (22%)] Loss: -649.211548\n",
      "Train Epoch: 916 [23040/54000 (43%)] Loss: -640.102295\n",
      "Train Epoch: 916 [34304/54000 (64%)] Loss: -517.559387\n",
      "Train Epoch: 916 [45568/54000 (84%)] Loss: -531.756836\n",
      "    epoch          : 916\n",
      "    loss           : -478.7683071899414\n",
      "    val_loss       : -427.55811711791904\n",
      "    val_log_likelihood: 555.1578216552734\n",
      "    val_log_marginal: 433.009047026565\n",
      "Train Epoch: 917 [512/54000 (1%)] Loss: -495.454468\n",
      "Train Epoch: 917 [11776/54000 (22%)] Loss: -454.041351\n",
      "Train Epoch: 917 [23040/54000 (43%)] Loss: -460.032532\n",
      "Train Epoch: 917 [34304/54000 (64%)] Loss: -507.662476\n",
      "Train Epoch: 917 [45568/54000 (84%)] Loss: -516.142883\n",
      "    epoch          : 917\n",
      "    loss           : -479.56607070922854\n",
      "    val_loss       : -430.29731117896733\n",
      "    val_log_likelihood: 554.8468688964844\n",
      "    val_log_marginal: 433.719859546055\n",
      "Train Epoch: 918 [512/54000 (1%)] Loss: -524.555786\n",
      "Train Epoch: 918 [11776/54000 (22%)] Loss: -539.913269\n",
      "Train Epoch: 918 [23040/54000 (43%)] Loss: -272.167877\n",
      "Train Epoch: 918 [34304/54000 (64%)] Loss: -277.802094\n",
      "Train Epoch: 918 [45568/54000 (84%)] Loss: -527.445862\n",
      "    epoch          : 918\n",
      "    loss           : -479.202268371582\n",
      "    val_loss       : -431.2166925514117\n",
      "    val_log_likelihood: 557.1455657958984\n",
      "    val_log_marginal: 435.25159578062596\n",
      "Train Epoch: 919 [512/54000 (1%)] Loss: -637.563232\n",
      "Train Epoch: 919 [11776/54000 (22%)] Loss: -632.485962\n",
      "Train Epoch: 919 [23040/54000 (43%)] Loss: -489.132019\n",
      "Train Epoch: 919 [34304/54000 (64%)] Loss: -439.900024\n",
      "Train Epoch: 919 [45568/54000 (84%)] Loss: -521.578491\n",
      "    epoch          : 919\n",
      "    loss           : -479.1285934448242\n",
      "    val_loss       : -429.6964715080336\n",
      "    val_log_likelihood: 557.9185607910156\n",
      "    val_log_marginal: 435.02751118429006\n",
      "Train Epoch: 920 [512/54000 (1%)] Loss: -522.625671\n",
      "Train Epoch: 920 [11776/54000 (22%)] Loss: -531.176636\n",
      "Train Epoch: 920 [23040/54000 (43%)] Loss: -455.544861\n",
      "Train Epoch: 920 [34304/54000 (64%)] Loss: -485.570587\n",
      "Train Epoch: 920 [45568/54000 (84%)] Loss: -511.535187\n",
      "    epoch          : 920\n",
      "    loss           : -479.7520227050781\n",
      "    val_loss       : -429.4066841494292\n",
      "    val_log_likelihood: 557.2047149658204\n",
      "    val_log_marginal: 435.3906424436718\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch920.pth ...\n",
      "Train Epoch: 921 [512/54000 (1%)] Loss: -309.725403\n",
      "Train Epoch: 921 [11776/54000 (22%)] Loss: -654.358826\n",
      "Train Epoch: 921 [23040/54000 (43%)] Loss: -481.237213\n",
      "Train Epoch: 921 [34304/54000 (64%)] Loss: -456.177856\n",
      "Train Epoch: 921 [45568/54000 (84%)] Loss: -539.627319\n",
      "    epoch          : 921\n",
      "    loss           : -479.4998828125\n",
      "    val_loss       : -429.41507470291106\n",
      "    val_log_likelihood: 556.8143432617187\n",
      "    val_log_marginal: 434.67388042844834\n",
      "Train Epoch: 922 [512/54000 (1%)] Loss: -489.322327\n",
      "Train Epoch: 922 [11776/54000 (22%)] Loss: -512.013184\n",
      "Train Epoch: 922 [23040/54000 (43%)] Loss: -291.454987\n",
      "Train Epoch: 922 [34304/54000 (64%)] Loss: -535.665588\n",
      "Train Epoch: 922 [45568/54000 (84%)] Loss: -520.269043\n",
      "    epoch          : 922\n",
      "    loss           : -478.9295539855957\n",
      "    val_loss       : -429.3900485295802\n",
      "    val_log_likelihood: 558.8606689453125\n",
      "    val_log_marginal: 436.5627997595817\n",
      "Train Epoch: 923 [512/54000 (1%)] Loss: -510.807678\n",
      "Train Epoch: 923 [11776/54000 (22%)] Loss: -531.451538\n",
      "Train Epoch: 923 [23040/54000 (43%)] Loss: -543.635925\n",
      "Train Epoch: 923 [34304/54000 (64%)] Loss: -324.642303\n",
      "Train Epoch: 923 [45568/54000 (84%)] Loss: -503.301208\n",
      "    epoch          : 923\n",
      "    loss           : -479.2970654296875\n",
      "    val_loss       : -429.6853605672717\n",
      "    val_log_likelihood: 556.2846862792969\n",
      "    val_log_marginal: 434.19952033646405\n",
      "Train Epoch: 924 [512/54000 (1%)] Loss: -459.115631\n",
      "Train Epoch: 924 [11776/54000 (22%)] Loss: -506.288269\n",
      "Train Epoch: 924 [23040/54000 (43%)] Loss: -537.296753\n",
      "Train Epoch: 924 [34304/54000 (64%)] Loss: -289.188080\n",
      "Train Epoch: 924 [45568/54000 (84%)] Loss: -304.309387\n",
      "    epoch          : 924\n",
      "    loss           : -478.6911700439453\n",
      "    val_loss       : -431.91311790328473\n",
      "    val_log_likelihood: 557.9156707763672\n",
      "    val_log_marginal: 436.1491498272866\n",
      "Train Epoch: 925 [512/54000 (1%)] Loss: -642.196106\n",
      "Train Epoch: 925 [11776/54000 (22%)] Loss: -643.970093\n",
      "Train Epoch: 925 [23040/54000 (43%)] Loss: -512.475830\n",
      "Train Epoch: 925 [34304/54000 (64%)] Loss: -534.164062\n",
      "Train Epoch: 925 [45568/54000 (84%)] Loss: -538.518799\n",
      "    epoch          : 925\n",
      "    loss           : -479.15053802490235\n",
      "    val_loss       : -431.5136826187372\n",
      "    val_log_likelihood: 557.9925903320312\n",
      "    val_log_marginal: 435.31830503977835\n",
      "Train Epoch: 926 [512/54000 (1%)] Loss: -530.122681\n",
      "Train Epoch: 926 [11776/54000 (22%)] Loss: -516.759033\n",
      "Train Epoch: 926 [23040/54000 (43%)] Loss: -640.586670\n",
      "Train Epoch: 926 [34304/54000 (64%)] Loss: -532.196045\n",
      "Train Epoch: 926 [45568/54000 (84%)] Loss: -309.655487\n",
      "    epoch          : 926\n",
      "    loss           : -479.4085032653809\n",
      "    val_loss       : -429.7075445771217\n",
      "    val_log_likelihood: 556.6562042236328\n",
      "    val_log_marginal: 434.6815619971603\n",
      "Train Epoch: 927 [512/54000 (1%)] Loss: -543.588806\n",
      "Train Epoch: 927 [11776/54000 (22%)] Loss: -486.200867\n",
      "Train Epoch: 927 [23040/54000 (43%)] Loss: -634.874268\n",
      "Train Epoch: 927 [34304/54000 (64%)] Loss: -461.200684\n",
      "Train Epoch: 927 [45568/54000 (84%)] Loss: -525.902161\n",
      "    epoch          : 927\n",
      "    loss           : -479.81488555908203\n",
      "    val_loss       : -427.4454453166574\n",
      "    val_log_likelihood: 557.001171875\n",
      "    val_log_marginal: 434.12756157405676\n",
      "Train Epoch: 928 [512/54000 (1%)] Loss: -454.413574\n",
      "Train Epoch: 928 [11776/54000 (22%)] Loss: -536.480957\n",
      "Train Epoch: 928 [23040/54000 (43%)] Loss: -504.607422\n",
      "Train Epoch: 928 [34304/54000 (64%)] Loss: -325.060760\n",
      "Train Epoch: 928 [45568/54000 (84%)] Loss: -525.192139\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "    epoch          : 928\n",
      "    loss           : -480.0365071105957\n",
      "    val_loss       : -428.07770595718176\n",
      "    val_log_likelihood: 557.15673828125\n",
      "    val_log_marginal: 434.421289363876\n",
      "Train Epoch: 929 [512/54000 (1%)] Loss: -645.646729\n",
      "Train Epoch: 929 [11776/54000 (22%)] Loss: -519.411438\n",
      "Train Epoch: 929 [23040/54000 (43%)] Loss: -336.296997\n",
      "Train Epoch: 929 [34304/54000 (64%)] Loss: -473.085571\n",
      "Train Epoch: 929 [45568/54000 (84%)] Loss: -455.682892\n",
      "    epoch          : 929\n",
      "    loss           : -479.23298217773436\n",
      "    val_loss       : -429.56376473810525\n",
      "    val_log_likelihood: 557.3083404541015\n",
      "    val_log_marginal: 434.29840371348956\n",
      "Train Epoch: 930 [512/54000 (1%)] Loss: -515.433411\n",
      "Train Epoch: 930 [11776/54000 (22%)] Loss: -507.202545\n",
      "Train Epoch: 930 [23040/54000 (43%)] Loss: -490.409393\n",
      "Train Epoch: 930 [34304/54000 (64%)] Loss: -296.309906\n",
      "Train Epoch: 930 [45568/54000 (84%)] Loss: -530.801270\n",
      "    epoch          : 930\n",
      "    loss           : -480.01146911621095\n",
      "    val_loss       : -430.14896750357\n",
      "    val_log_likelihood: 557.8894653320312\n",
      "    val_log_marginal: 435.0592104860526\n",
      "Train Epoch: 931 [512/54000 (1%)] Loss: -546.242615\n",
      "Train Epoch: 931 [11776/54000 (22%)] Loss: -523.264465\n",
      "Train Epoch: 931 [23040/54000 (43%)] Loss: -513.957031\n",
      "Train Epoch: 931 [34304/54000 (64%)] Loss: -535.641724\n",
      "Train Epoch: 931 [45568/54000 (84%)] Loss: -525.778198\n",
      "    epoch          : 931\n",
      "    loss           : -479.5599053955078\n",
      "    val_loss       : -428.43757215552034\n",
      "    val_log_likelihood: 558.5728240966797\n",
      "    val_log_marginal: 435.83357601724566\n",
      "Train Epoch: 932 [512/54000 (1%)] Loss: -270.722717\n",
      "Train Epoch: 932 [11776/54000 (22%)] Loss: -304.840515\n",
      "Train Epoch: 932 [23040/54000 (43%)] Loss: -491.090088\n",
      "Train Epoch: 932 [34304/54000 (64%)] Loss: -546.321838\n",
      "Train Epoch: 932 [45568/54000 (84%)] Loss: -320.015564\n",
      "    epoch          : 932\n",
      "    loss           : -480.21488525390623\n",
      "    val_loss       : -431.92800466325133\n",
      "    val_log_likelihood: 558.7977294921875\n",
      "    val_log_marginal: 436.57343287058177\n",
      "Train Epoch: 933 [512/54000 (1%)] Loss: -539.078369\n",
      "Train Epoch: 933 [11776/54000 (22%)] Loss: -344.229370\n",
      "Train Epoch: 933 [23040/54000 (43%)] Loss: -534.477539\n",
      "Train Epoch: 933 [34304/54000 (64%)] Loss: -326.198914\n",
      "Train Epoch: 933 [45568/54000 (84%)] Loss: -317.830109\n",
      "    epoch          : 933\n",
      "    loss           : -480.5084594726562\n",
      "    val_loss       : -428.05230794213713\n",
      "    val_log_likelihood: 557.0353271484375\n",
      "    val_log_marginal: 433.99846316538896\n",
      "Train Epoch: 934 [512/54000 (1%)] Loss: -647.293091\n",
      "Train Epoch: 934 [11776/54000 (22%)] Loss: -527.351196\n",
      "Train Epoch: 934 [23040/54000 (43%)] Loss: -525.342285\n",
      "Train Epoch: 934 [34304/54000 (64%)] Loss: -522.824951\n",
      "Train Epoch: 934 [45568/54000 (84%)] Loss: -466.158173\n",
      "    epoch          : 934\n",
      "    loss           : -479.77700775146485\n",
      "    val_loss       : -429.68360153939574\n",
      "    val_log_likelihood: 553.4144348144531\n",
      "    val_log_marginal: 432.27556510617177\n",
      "Train Epoch: 935 [512/54000 (1%)] Loss: -488.066650\n",
      "Train Epoch: 935 [11776/54000 (22%)] Loss: -494.121887\n",
      "Train Epoch: 935 [23040/54000 (43%)] Loss: -510.185913\n",
      "Train Epoch: 935 [34304/54000 (64%)] Loss: -508.816315\n",
      "Train Epoch: 935 [45568/54000 (84%)] Loss: -506.731110\n",
      "    epoch          : 935\n",
      "    loss           : -479.7342468261719\n",
      "    val_loss       : -427.8113063760102\n",
      "    val_log_likelihood: 555.9834228515625\n",
      "    val_log_marginal: 433.458571607247\n",
      "Train Epoch: 936 [512/54000 (1%)] Loss: -547.074829\n",
      "Train Epoch: 936 [11776/54000 (22%)] Loss: -523.113281\n",
      "Train Epoch: 936 [23040/54000 (43%)] Loss: -486.648651\n",
      "Train Epoch: 936 [34304/54000 (64%)] Loss: -458.181122\n",
      "Train Epoch: 936 [45568/54000 (84%)] Loss: -321.334625\n",
      "    epoch          : 936\n",
      "    loss           : -480.59630462646487\n",
      "    val_loss       : -430.1665178356692\n",
      "    val_log_likelihood: 557.6771911621094\n",
      "    val_log_marginal: 435.1593026753515\n",
      "Train Epoch: 937 [512/54000 (1%)] Loss: -533.029358\n",
      "Train Epoch: 937 [11776/54000 (22%)] Loss: -538.696838\n",
      "Train Epoch: 937 [23040/54000 (43%)] Loss: -641.552063\n",
      "Train Epoch: 937 [34304/54000 (64%)] Loss: -502.645630\n",
      "Train Epoch: 937 [45568/54000 (84%)] Loss: -457.682861\n",
      "    epoch          : 937\n",
      "    loss           : -479.7604455566406\n",
      "    val_loss       : -429.4028141781688\n",
      "    val_log_likelihood: 555.9894226074218\n",
      "    val_log_marginal: 434.47032977863853\n",
      "Train Epoch: 938 [512/54000 (1%)] Loss: -536.473633\n",
      "Train Epoch: 938 [11776/54000 (22%)] Loss: -435.065094\n",
      "Train Epoch: 938 [23040/54000 (43%)] Loss: -545.798828\n",
      "Train Epoch: 938 [34304/54000 (64%)] Loss: -507.462280\n",
      "Train Epoch: 938 [45568/54000 (84%)] Loss: -537.684814\n",
      "    epoch          : 938\n",
      "    loss           : -480.0027639770508\n",
      "    val_loss       : -430.6947035135701\n",
      "    val_log_likelihood: 557.2733154296875\n",
      "    val_log_marginal: 434.76528561227025\n",
      "Train Epoch: 939 [512/54000 (1%)] Loss: -560.122131\n",
      "Train Epoch: 939 [11776/54000 (22%)] Loss: -267.703583\n",
      "Train Epoch: 939 [23040/54000 (43%)] Loss: -509.177185\n",
      "Train Epoch: 939 [34304/54000 (64%)] Loss: -516.470093\n",
      "Train Epoch: 939 [45568/54000 (84%)] Loss: -647.836548\n",
      "    epoch          : 939\n",
      "    loss           : -479.76433013916017\n",
      "    val_loss       : -430.3617894982919\n",
      "    val_log_likelihood: 559.1062438964843\n",
      "    val_log_marginal: 436.1432946156726\n",
      "Train Epoch: 940 [512/54000 (1%)] Loss: -542.807678\n",
      "Train Epoch: 940 [11776/54000 (22%)] Loss: -490.908417\n",
      "Train Epoch: 940 [23040/54000 (43%)] Loss: -521.878113\n",
      "Train Epoch: 940 [34304/54000 (64%)] Loss: -550.218689\n",
      "Train Epoch: 940 [45568/54000 (84%)] Loss: -516.208252\n",
      "    epoch          : 940\n",
      "    loss           : -480.0377828979492\n",
      "    val_loss       : -430.8147354003042\n",
      "    val_log_likelihood: 555.1017395019531\n",
      "    val_log_marginal: 433.9513875748962\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch940.pth ...\n",
      "Train Epoch: 941 [512/54000 (1%)] Loss: -648.681030\n",
      "Train Epoch: 941 [11776/54000 (22%)] Loss: -535.039001\n",
      "Train Epoch: 941 [23040/54000 (43%)] Loss: -526.404968\n",
      "Train Epoch: 941 [34304/54000 (64%)] Loss: -541.141235\n",
      "Train Epoch: 941 [45568/54000 (84%)] Loss: -302.894043\n",
      "    epoch          : 941\n",
      "    loss           : -480.21406707763674\n",
      "    val_loss       : -430.3288241868839\n",
      "    val_log_likelihood: 557.8202270507812\n",
      "    val_log_marginal: 435.6018927622587\n",
      "Train Epoch: 942 [512/54000 (1%)] Loss: -488.141449\n",
      "Train Epoch: 942 [11776/54000 (22%)] Loss: -652.660645\n",
      "Train Epoch: 942 [23040/54000 (43%)] Loss: -554.187256\n",
      "Train Epoch: 942 [34304/54000 (64%)] Loss: -518.925110\n",
      "Train Epoch: 942 [45568/54000 (84%)] Loss: -309.458679\n",
      "    epoch          : 942\n",
      "    loss           : -480.35476348876955\n",
      "    val_loss       : -432.6905449965969\n",
      "    val_log_likelihood: 556.8922302246094\n",
      "    val_log_marginal: 436.1856762026713\n",
      "Train Epoch: 943 [512/54000 (1%)] Loss: -521.496033\n",
      "Train Epoch: 943 [11776/54000 (22%)] Loss: -532.795715\n",
      "Train Epoch: 943 [23040/54000 (43%)] Loss: -507.647156\n",
      "Train Epoch: 943 [34304/54000 (64%)] Loss: -452.990234\n",
      "Train Epoch: 943 [45568/54000 (84%)] Loss: -517.834778\n",
      "    epoch          : 943\n",
      "    loss           : -480.25269805908204\n",
      "    val_loss       : -430.6924038590863\n",
      "    val_log_likelihood: 557.4721496582031\n",
      "    val_log_marginal: 434.90478490553795\n",
      "Train Epoch: 944 [512/54000 (1%)] Loss: -521.805664\n",
      "Train Epoch: 944 [11776/54000 (22%)] Loss: -451.127075\n",
      "Train Epoch: 944 [23040/54000 (43%)] Loss: -534.054138\n",
      "Train Epoch: 944 [34304/54000 (64%)] Loss: -455.153442\n",
      "Train Epoch: 944 [45568/54000 (84%)] Loss: -643.720886\n",
      "    epoch          : 944\n",
      "    loss           : -480.5843978881836\n",
      "    val_loss       : -429.4967923687771\n",
      "    val_log_likelihood: 556.8646697998047\n",
      "    val_log_marginal: 434.10599038489164\n",
      "Train Epoch: 945 [512/54000 (1%)] Loss: -452.907959\n",
      "Train Epoch: 945 [11776/54000 (22%)] Loss: -530.343628\n",
      "Train Epoch: 945 [23040/54000 (43%)] Loss: -468.539459\n",
      "Train Epoch: 945 [34304/54000 (64%)] Loss: -443.441437\n",
      "Train Epoch: 945 [45568/54000 (84%)] Loss: -532.193970\n",
      "    epoch          : 945\n",
      "    loss           : -479.4496148681641\n",
      "    val_loss       : -431.99760803841053\n",
      "    val_log_likelihood: 555.8284606933594\n",
      "    val_log_marginal: 434.5947287987918\n",
      "Train Epoch: 946 [512/54000 (1%)] Loss: -519.645264\n",
      "Train Epoch: 946 [11776/54000 (22%)] Loss: -647.345398\n",
      "Train Epoch: 946 [23040/54000 (43%)] Loss: -496.324127\n",
      "Train Epoch: 946 [34304/54000 (64%)] Loss: -530.011963\n",
      "Train Epoch: 946 [45568/54000 (84%)] Loss: -307.591217\n",
      "    epoch          : 946\n",
      "    loss           : -480.36534790039065\n",
      "    val_loss       : -429.76687845420093\n",
      "    val_log_likelihood: 558.6857299804688\n",
      "    val_log_marginal: 435.6051725410965\n",
      "Train Epoch: 947 [512/54000 (1%)] Loss: -278.109741\n",
      "Train Epoch: 947 [11776/54000 (22%)] Loss: -539.973206\n",
      "Train Epoch: 947 [23040/54000 (43%)] Loss: -505.976593\n",
      "Train Epoch: 947 [34304/54000 (64%)] Loss: -311.417236\n",
      "Train Epoch: 947 [45568/54000 (84%)] Loss: -544.757141\n",
      "    epoch          : 947\n",
      "    loss           : -480.14739501953125\n",
      "    val_loss       : -430.25604371074587\n",
      "    val_log_likelihood: 558.9970520019531\n",
      "    val_log_marginal: 436.1687584210187\n",
      "Train Epoch: 948 [512/54000 (1%)] Loss: -265.847290\n",
      "Train Epoch: 948 [11776/54000 (22%)] Loss: -504.269501\n",
      "Train Epoch: 948 [23040/54000 (43%)] Loss: -505.678009\n",
      "Train Epoch: 948 [34304/54000 (64%)] Loss: -310.957397\n",
      "Train Epoch: 948 [45568/54000 (84%)] Loss: -643.601440\n",
      "    epoch          : 948\n",
      "    loss           : -479.50563659667966\n",
      "    val_loss       : -428.02774359621105\n",
      "    val_log_likelihood: 556.0341430664063\n",
      "    val_log_marginal: 432.92202536799016\n",
      "Train Epoch: 949 [512/54000 (1%)] Loss: -463.675415\n",
      "Train Epoch: 949 [11776/54000 (22%)] Loss: -515.086243\n",
      "Train Epoch: 949 [23040/54000 (43%)] Loss: -513.652100\n",
      "Train Epoch: 949 [34304/54000 (64%)] Loss: -639.990601\n",
      "Train Epoch: 949 [45568/54000 (84%)] Loss: -533.563599\n",
      "    epoch          : 949\n",
      "    loss           : -479.6479217529297\n",
      "    val_loss       : -428.076739672944\n",
      "    val_log_likelihood: 553.3201568603515\n",
      "    val_log_marginal: 430.75939849726853\n",
      "Train Epoch: 950 [512/54000 (1%)] Loss: -450.587494\n",
      "Train Epoch: 950 [11776/54000 (22%)] Loss: -264.253967\n",
      "Train Epoch: 950 [23040/54000 (43%)] Loss: -458.573303\n",
      "Train Epoch: 950 [34304/54000 (64%)] Loss: -323.423981\n",
      "Train Epoch: 950 [45568/54000 (84%)] Loss: -458.110779\n",
      "    epoch          : 950\n",
      "    loss           : -480.5257260131836\n",
      "    val_loss       : -430.1820768652484\n",
      "    val_log_likelihood: 559.5361145019531\n",
      "    val_log_marginal: 436.7289583634585\n",
      "Train Epoch: 951 [512/54000 (1%)] Loss: -508.596191\n",
      "Train Epoch: 951 [11776/54000 (22%)] Loss: -527.233765\n",
      "Train Epoch: 951 [23040/54000 (43%)] Loss: -514.767944\n",
      "Train Epoch: 951 [34304/54000 (64%)] Loss: -533.782471\n",
      "Train Epoch: 951 [45568/54000 (84%)] Loss: -526.390808\n",
      "    epoch          : 951\n",
      "    loss           : -479.69761138916016\n",
      "    val_loss       : -428.4113022580743\n",
      "    val_log_likelihood: 555.1248229980469\n",
      "    val_log_marginal: 432.59854426048696\n",
      "Train Epoch: 952 [512/54000 (1%)] Loss: -261.250122\n",
      "Train Epoch: 952 [11776/54000 (22%)] Loss: -255.894409\n",
      "Train Epoch: 952 [23040/54000 (43%)] Loss: -508.525879\n",
      "Train Epoch: 952 [34304/54000 (64%)] Loss: -290.726715\n",
      "Train Epoch: 952 [45568/54000 (84%)] Loss: -274.454651\n",
      "    epoch          : 952\n",
      "    loss           : -480.1202993774414\n",
      "    val_loss       : -432.7463338630274\n",
      "    val_log_likelihood: 559.6094970703125\n",
      "    val_log_marginal: 436.52365413419903\n",
      "Train Epoch: 953 [512/54000 (1%)] Loss: -275.667480\n",
      "Train Epoch: 953 [11776/54000 (22%)] Loss: -288.134888\n",
      "Train Epoch: 953 [23040/54000 (43%)] Loss: -539.470398\n",
      "Train Epoch: 953 [34304/54000 (64%)] Loss: -554.805542\n",
      "Train Epoch: 953 [45568/54000 (84%)] Loss: -265.406372\n",
      "    epoch          : 953\n",
      "    loss           : -479.35523361206054\n",
      "    val_loss       : -430.4749464007094\n",
      "    val_log_likelihood: 558.3859436035157\n",
      "    val_log_marginal: 435.6413265619427\n",
      "Train Epoch: 954 [512/54000 (1%)] Loss: -527.879272\n",
      "Train Epoch: 954 [11776/54000 (22%)] Loss: -450.383698\n",
      "Train Epoch: 954 [23040/54000 (43%)] Loss: -316.034058\n",
      "Train Epoch: 954 [34304/54000 (64%)] Loss: -452.821350\n",
      "Train Epoch: 954 [45568/54000 (84%)] Loss: -331.883942\n",
      "    epoch          : 954\n",
      "    loss           : -479.4003915405273\n",
      "    val_loss       : -428.951675276272\n",
      "    val_log_likelihood: 557.0719848632813\n",
      "    val_log_marginal: 434.1581855993718\n",
      "Train Epoch: 955 [512/54000 (1%)] Loss: -518.346558\n",
      "Train Epoch: 955 [11776/54000 (22%)] Loss: -538.288574\n",
      "Train Epoch: 955 [23040/54000 (43%)] Loss: -284.330414\n",
      "Train Epoch: 955 [34304/54000 (64%)] Loss: -309.082123\n",
      "Train Epoch: 955 [45568/54000 (84%)] Loss: -276.665283\n",
      "    epoch          : 955\n",
      "    loss           : -480.4579345703125\n",
      "    val_loss       : -429.72917623519896\n",
      "    val_log_likelihood: 557.4153442382812\n",
      "    val_log_marginal: 435.3471469786018\n",
      "Train Epoch: 956 [512/54000 (1%)] Loss: -500.190033\n",
      "Train Epoch: 956 [11776/54000 (22%)] Loss: -434.297241\n",
      "Train Epoch: 956 [23040/54000 (43%)] Loss: -496.621155\n",
      "Train Epoch: 956 [34304/54000 (64%)] Loss: -313.704590\n",
      "Train Epoch: 956 [45568/54000 (84%)] Loss: -459.979553\n",
      "    epoch          : 956\n",
      "    loss           : -479.4149557495117\n",
      "    val_loss       : -432.1425646651536\n",
      "    val_log_likelihood: 557.9508239746094\n",
      "    val_log_marginal: 435.4804767850786\n",
      "Train Epoch: 957 [512/54000 (1%)] Loss: -509.697235\n",
      "Train Epoch: 957 [11776/54000 (22%)] Loss: -503.504883\n",
      "Train Epoch: 957 [23040/54000 (43%)] Loss: -491.787048\n",
      "Train Epoch: 957 [34304/54000 (64%)] Loss: -647.646057\n",
      "Train Epoch: 957 [45568/54000 (84%)] Loss: -505.280334\n",
      "    epoch          : 957\n",
      "    loss           : -480.02375549316406\n",
      "    val_loss       : -429.2552565574646\n",
      "    val_log_likelihood: 557.7164886474609\n",
      "    val_log_marginal: 434.75048586763444\n",
      "Train Epoch: 958 [512/54000 (1%)] Loss: -316.084381\n",
      "Train Epoch: 958 [11776/54000 (22%)] Loss: -293.974487\n",
      "Train Epoch: 958 [23040/54000 (43%)] Loss: -544.472351\n",
      "Train Epoch: 958 [34304/54000 (64%)] Loss: -529.928589\n",
      "Train Epoch: 958 [45568/54000 (84%)] Loss: -527.235107\n",
      "    epoch          : 958\n",
      "    loss           : -479.9260478210449\n",
      "    val_loss       : -431.4285373702645\n",
      "    val_log_likelihood: 556.9628997802735\n",
      "    val_log_marginal: 435.56322413124144\n",
      "Train Epoch: 959 [512/54000 (1%)] Loss: -497.371063\n",
      "Train Epoch: 959 [11776/54000 (22%)] Loss: -524.210205\n",
      "Train Epoch: 959 [23040/54000 (43%)] Loss: -266.343201\n",
      "Train Epoch: 959 [34304/54000 (64%)] Loss: -507.364441\n",
      "Train Epoch: 959 [45568/54000 (84%)] Loss: -642.215942\n",
      "    epoch          : 959\n",
      "    loss           : -479.38068634033203\n",
      "    val_loss       : -432.7741067122668\n",
      "    val_log_likelihood: 559.6941986083984\n",
      "    val_log_marginal: 437.576696145907\n",
      "Train Epoch: 960 [512/54000 (1%)] Loss: -480.574585\n",
      "Train Epoch: 960 [11776/54000 (22%)] Loss: -542.004089\n",
      "Train Epoch: 960 [23040/54000 (43%)] Loss: -482.914185\n",
      "Train Epoch: 960 [34304/54000 (64%)] Loss: -646.644348\n",
      "Train Epoch: 960 [45568/54000 (84%)] Loss: -454.899414\n",
      "    epoch          : 960\n",
      "    loss           : -480.15840576171877\n",
      "    val_loss       : -429.8591818859801\n",
      "    val_log_likelihood: 557.5854675292969\n",
      "    val_log_marginal: 435.2625450912863\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch960.pth ...\n",
      "Train Epoch: 961 [512/54000 (1%)] Loss: -471.121246\n",
      "Train Epoch: 961 [11776/54000 (22%)] Loss: -485.775360\n",
      "Train Epoch: 961 [23040/54000 (43%)] Loss: -462.246338\n",
      "Train Epoch: 961 [34304/54000 (64%)] Loss: -535.483215\n",
      "Train Epoch: 961 [45568/54000 (84%)] Loss: -538.298218\n",
      "    epoch          : 961\n",
      "    loss           : -479.74338150024414\n",
      "    val_loss       : -430.72981691472233\n",
      "    val_log_likelihood: 555.9864624023437\n",
      "    val_log_marginal: 434.48006885461507\n",
      "Train Epoch: 962 [512/54000 (1%)] Loss: -539.913879\n",
      "Train Epoch: 962 [11776/54000 (22%)] Loss: -457.845215\n",
      "Train Epoch: 962 [23040/54000 (43%)] Loss: -544.057922\n",
      "Train Epoch: 962 [34304/54000 (64%)] Loss: -526.557861\n",
      "Train Epoch: 962 [45568/54000 (84%)] Loss: -520.596619\n",
      "    epoch          : 962\n",
      "    loss           : -480.2720401000977\n",
      "    val_loss       : -431.8443856174126\n",
      "    val_log_likelihood: 559.4169982910156\n",
      "    val_log_marginal: 436.8020780678838\n",
      "Train Epoch: 963 [512/54000 (1%)] Loss: -537.969238\n",
      "Train Epoch: 963 [11776/54000 (22%)] Loss: -549.647339\n",
      "Train Epoch: 963 [23040/54000 (43%)] Loss: -332.509216\n",
      "Train Epoch: 963 [34304/54000 (64%)] Loss: -533.814453\n",
      "Train Epoch: 963 [45568/54000 (84%)] Loss: -326.187347\n",
      "    epoch          : 963\n",
      "    loss           : -480.1894543457031\n",
      "    val_loss       : -430.66601426526904\n",
      "    val_log_likelihood: 560.1592346191406\n",
      "    val_log_marginal: 437.01519164629406\n",
      "Train Epoch: 964 [512/54000 (1%)] Loss: -278.769073\n",
      "Train Epoch: 964 [11776/54000 (22%)] Loss: -518.856628\n",
      "Train Epoch: 964 [23040/54000 (43%)] Loss: -483.460541\n",
      "Train Epoch: 964 [34304/54000 (64%)] Loss: -518.531738\n",
      "Train Epoch: 964 [45568/54000 (84%)] Loss: -501.483704\n",
      "    epoch          : 964\n",
      "    loss           : -480.37358428955076\n",
      "    val_loss       : -433.9805470902473\n",
      "    val_log_likelihood: 561.4270416259766\n",
      "    val_log_marginal: 438.83041003681757\n",
      "Train Epoch: 965 [512/54000 (1%)] Loss: -542.729004\n",
      "Train Epoch: 965 [11776/54000 (22%)] Loss: -534.677002\n",
      "Train Epoch: 965 [23040/54000 (43%)] Loss: -644.955322\n",
      "Train Epoch: 965 [34304/54000 (64%)] Loss: -509.342133\n",
      "Train Epoch: 965 [45568/54000 (84%)] Loss: -517.980591\n",
      "    epoch          : 965\n",
      "    loss           : -480.7566683959961\n",
      "    val_loss       : -429.2712429916486\n",
      "    val_log_likelihood: 557.9572448730469\n",
      "    val_log_marginal: 434.6465236399323\n",
      "Train Epoch: 966 [512/54000 (1%)] Loss: -310.493225\n",
      "Train Epoch: 966 [11776/54000 (22%)] Loss: -268.925171\n",
      "Train Epoch: 966 [23040/54000 (43%)] Loss: -499.882080\n",
      "Train Epoch: 966 [34304/54000 (64%)] Loss: -487.966431\n",
      "Train Epoch: 966 [45568/54000 (84%)] Loss: -481.439606\n",
      "    epoch          : 966\n",
      "    loss           : -480.09056213378904\n",
      "    val_loss       : -428.5416461849585\n",
      "    val_log_likelihood: 556.7276458740234\n",
      "    val_log_marginal: 433.9057877053341\n",
      "Train Epoch: 967 [512/54000 (1%)] Loss: -502.142365\n",
      "Train Epoch: 967 [11776/54000 (22%)] Loss: -482.675873\n",
      "Train Epoch: 967 [23040/54000 (43%)] Loss: -516.661438\n",
      "Train Epoch: 967 [34304/54000 (64%)] Loss: -536.827942\n",
      "Train Epoch: 967 [45568/54000 (84%)] Loss: -542.840393\n",
      "    epoch          : 967\n",
      "    loss           : -479.9451892089844\n",
      "    val_loss       : -431.2083463203162\n",
      "    val_log_likelihood: 559.182583618164\n",
      "    val_log_marginal: 436.53217800073327\n",
      "Train Epoch: 968 [512/54000 (1%)] Loss: -651.951355\n",
      "Train Epoch: 968 [11776/54000 (22%)] Loss: -502.126953\n",
      "Train Epoch: 968 [23040/54000 (43%)] Loss: -517.909424\n",
      "Train Epoch: 968 [34304/54000 (64%)] Loss: -278.175507\n",
      "Train Epoch: 968 [45568/54000 (84%)] Loss: -332.956909\n",
      "    epoch          : 968\n",
      "    loss           : -479.3938632202148\n",
      "    val_loss       : -428.3840985994786\n",
      "    val_log_likelihood: 558.7068023681641\n",
      "    val_log_marginal: 436.1838011141866\n",
      "Train Epoch: 969 [512/54000 (1%)] Loss: -645.963623\n",
      "Train Epoch: 969 [11776/54000 (22%)] Loss: -502.326385\n",
      "Train Epoch: 969 [23040/54000 (43%)] Loss: -509.727448\n",
      "Train Epoch: 969 [34304/54000 (64%)] Loss: -260.427826\n",
      "Train Epoch: 969 [45568/54000 (84%)] Loss: -523.691345\n",
      "    epoch          : 969\n",
      "    loss           : -479.4422805786133\n",
      "    val_loss       : -427.73958558030427\n",
      "    val_log_likelihood: 552.89228515625\n",
      "    val_log_marginal: 432.09701184883335\n",
      "Train Epoch: 970 [512/54000 (1%)] Loss: -267.944519\n",
      "Train Epoch: 970 [11776/54000 (22%)] Loss: -279.653931\n",
      "Train Epoch: 970 [23040/54000 (43%)] Loss: -529.459961\n",
      "Train Epoch: 970 [34304/54000 (64%)] Loss: -453.126770\n",
      "Train Epoch: 970 [45568/54000 (84%)] Loss: -493.720245\n",
      "    epoch          : 970\n",
      "    loss           : -480.19217681884766\n",
      "    val_loss       : -432.45792106967417\n",
      "    val_log_likelihood: 559.4782318115234\n",
      "    val_log_marginal: 437.67004303456343\n",
      "Train Epoch: 971 [512/54000 (1%)] Loss: -451.167175\n",
      "Train Epoch: 971 [11776/54000 (22%)] Loss: -505.246765\n",
      "Train Epoch: 971 [23040/54000 (43%)] Loss: -296.796875\n",
      "Train Epoch: 971 [34304/54000 (64%)] Loss: -310.890991\n",
      "Train Epoch: 971 [45568/54000 (84%)] Loss: -526.273010\n",
      "    epoch          : 971\n",
      "    loss           : -479.59824523925784\n",
      "    val_loss       : -429.9869769524783\n",
      "    val_log_likelihood: 558.6109802246094\n",
      "    val_log_marginal: 435.820923608169\n",
      "Train Epoch: 972 [512/54000 (1%)] Loss: -530.206177\n",
      "Train Epoch: 972 [11776/54000 (22%)] Loss: -542.371948\n",
      "Train Epoch: 972 [23040/54000 (43%)] Loss: -636.164001\n",
      "Train Epoch: 972 [34304/54000 (64%)] Loss: -534.396240\n",
      "Train Epoch: 972 [45568/54000 (84%)] Loss: -650.992310\n",
      "    epoch          : 972\n",
      "    loss           : -479.72780242919924\n",
      "    val_loss       : -431.1533465569839\n",
      "    val_log_likelihood: 558.402685546875\n",
      "    val_log_marginal: 435.6889308158308\n",
      "Train Epoch: 973 [512/54000 (1%)] Loss: -550.467773\n",
      "Train Epoch: 973 [11776/54000 (22%)] Loss: -531.379150\n",
      "Train Epoch: 973 [23040/54000 (43%)] Loss: -651.651306\n",
      "Train Epoch: 973 [34304/54000 (64%)] Loss: -487.113342\n",
      "Train Epoch: 973 [45568/54000 (84%)] Loss: -508.805359\n",
      "    epoch          : 973\n",
      "    loss           : -480.5579861450195\n",
      "    val_loss       : -430.68247189391406\n",
      "    val_log_likelihood: 556.2957885742187\n",
      "    val_log_marginal: 434.19013575054703\n",
      "Train Epoch: 974 [512/54000 (1%)] Loss: -547.707886\n",
      "Train Epoch: 974 [11776/54000 (22%)] Loss: -500.329742\n",
      "Train Epoch: 974 [23040/54000 (43%)] Loss: -466.928070\n",
      "Train Epoch: 974 [34304/54000 (64%)] Loss: -533.069824\n",
      "Train Epoch: 974 [45568/54000 (84%)] Loss: -450.897583\n",
      "    epoch          : 974\n",
      "    loss           : -479.5228770446777\n",
      "    val_loss       : -428.9739263691008\n",
      "    val_log_likelihood: 553.48955078125\n",
      "    val_log_marginal: 432.3601346809419\n",
      "Train Epoch: 975 [512/54000 (1%)] Loss: -531.817932\n",
      "Train Epoch: 975 [11776/54000 (22%)] Loss: -315.152344\n",
      "Train Epoch: 975 [23040/54000 (43%)] Loss: -532.119629\n",
      "Train Epoch: 975 [34304/54000 (64%)] Loss: -460.133514\n",
      "Train Epoch: 975 [45568/54000 (84%)] Loss: -504.747498\n",
      "    epoch          : 975\n",
      "    loss           : -480.6068228149414\n",
      "    val_loss       : -432.9525547843426\n",
      "    val_log_likelihood: 559.2015716552735\n",
      "    val_log_marginal: 436.4121262623222\n",
      "Train Epoch: 976 [512/54000 (1%)] Loss: -479.299164\n",
      "Train Epoch: 976 [11776/54000 (22%)] Loss: -494.418030\n",
      "Train Epoch: 976 [23040/54000 (43%)] Loss: -494.788696\n",
      "Train Epoch: 976 [34304/54000 (64%)] Loss: -653.013062\n",
      "Train Epoch: 976 [45568/54000 (84%)] Loss: -530.093262\n",
      "    epoch          : 976\n",
      "    loss           : -479.8629574584961\n",
      "    val_loss       : -430.31786228604614\n",
      "    val_log_likelihood: 557.8472625732422\n",
      "    val_log_marginal: 434.89214504919937\n",
      "Train Epoch: 977 [512/54000 (1%)] Loss: -491.101746\n",
      "Train Epoch: 977 [11776/54000 (22%)] Loss: -270.365448\n",
      "Train Epoch: 977 [23040/54000 (43%)] Loss: -487.131531\n",
      "Train Epoch: 977 [34304/54000 (64%)] Loss: -520.060913\n",
      "Train Epoch: 977 [45568/54000 (84%)] Loss: -514.163452\n",
      "    epoch          : 977\n",
      "    loss           : -480.006516418457\n",
      "    val_loss       : -431.9687359049916\n",
      "    val_log_likelihood: 557.4902435302735\n",
      "    val_log_marginal: 435.67652003355323\n",
      "Train Epoch: 978 [512/54000 (1%)] Loss: -490.970612\n",
      "Train Epoch: 978 [11776/54000 (22%)] Loss: -528.343018\n",
      "Train Epoch: 978 [23040/54000 (43%)] Loss: -502.923370\n",
      "Train Epoch: 978 [34304/54000 (64%)] Loss: -255.823883\n",
      "Train Epoch: 978 [45568/54000 (84%)] Loss: -311.237213\n",
      "    epoch          : 978\n",
      "    loss           : -480.0619094848633\n",
      "    val_loss       : -429.7577712515369\n",
      "    val_log_likelihood: 556.913784790039\n",
      "    val_log_marginal: 434.8419688832015\n",
      "Train Epoch: 979 [512/54000 (1%)] Loss: -639.102356\n",
      "Train Epoch: 979 [11776/54000 (22%)] Loss: -515.922119\n",
      "Train Epoch: 979 [23040/54000 (43%)] Loss: -469.256775\n",
      "Train Epoch: 979 [34304/54000 (64%)] Loss: -630.586426\n",
      "Train Epoch: 979 [45568/54000 (84%)] Loss: -449.173950\n",
      "    epoch          : 979\n",
      "    loss           : -480.3156884765625\n",
      "    val_loss       : -428.8582134557888\n",
      "    val_log_likelihood: 555.8994689941406\n",
      "    val_log_marginal: 433.7248191449791\n",
      "Train Epoch: 980 [512/54000 (1%)] Loss: -484.987518\n",
      "Train Epoch: 980 [11776/54000 (22%)] Loss: -539.846741\n",
      "Train Epoch: 980 [23040/54000 (43%)] Loss: -532.818604\n",
      "Train Epoch: 980 [34304/54000 (64%)] Loss: -527.868530\n",
      "Train Epoch: 980 [45568/54000 (84%)] Loss: -542.348572\n",
      "    epoch          : 980\n",
      "    loss           : -480.253772277832\n",
      "    val_loss       : -432.3150092767552\n",
      "    val_log_likelihood: 559.2613677978516\n",
      "    val_log_marginal: 437.83739236660324\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch980.pth ...\n",
      "Train Epoch: 981 [512/54000 (1%)] Loss: -503.886108\n",
      "Train Epoch: 981 [11776/54000 (22%)] Loss: -534.203613\n",
      "Train Epoch: 981 [23040/54000 (43%)] Loss: -539.226868\n",
      "Train Epoch: 981 [34304/54000 (64%)] Loss: -326.858429\n",
      "Train Epoch: 981 [45568/54000 (84%)] Loss: -516.111755\n",
      "    epoch          : 981\n",
      "    loss           : -480.014267578125\n",
      "    val_loss       : -430.13973574079574\n",
      "    val_log_likelihood: 556.1428863525391\n",
      "    val_log_marginal: 433.9652471784502\n",
      "Train Epoch: 982 [512/54000 (1%)] Loss: -536.810425\n",
      "Train Epoch: 982 [11776/54000 (22%)] Loss: -496.729858\n",
      "Train Epoch: 982 [23040/54000 (43%)] Loss: -507.844177\n",
      "Train Epoch: 982 [34304/54000 (64%)] Loss: -513.233643\n",
      "Train Epoch: 982 [45568/54000 (84%)] Loss: -539.797974\n",
      "    epoch          : 982\n",
      "    loss           : -479.3551739501953\n",
      "    val_loss       : -429.4061564108357\n",
      "    val_log_likelihood: 559.2293853759766\n",
      "    val_log_marginal: 436.1898096028738\n",
      "Train Epoch: 983 [512/54000 (1%)] Loss: -495.091064\n",
      "Train Epoch: 983 [11776/54000 (22%)] Loss: -454.629486\n",
      "Train Epoch: 983 [23040/54000 (43%)] Loss: -313.131927\n",
      "Train Epoch: 983 [34304/54000 (64%)] Loss: -645.896240\n",
      "Train Epoch: 983 [45568/54000 (84%)] Loss: -507.474182\n",
      "    epoch          : 983\n",
      "    loss           : -480.17711853027345\n",
      "    val_loss       : -431.5413510041311\n",
      "    val_log_likelihood: 558.7589874267578\n",
      "    val_log_marginal: 435.6851762738079\n",
      "Train Epoch: 984 [512/54000 (1%)] Loss: -540.424988\n",
      "Train Epoch: 984 [11776/54000 (22%)] Loss: -647.265625\n",
      "Train Epoch: 984 [23040/54000 (43%)] Loss: -646.349670\n",
      "Train Epoch: 984 [34304/54000 (64%)] Loss: -520.310425\n",
      "Train Epoch: 984 [45568/54000 (84%)] Loss: -524.816467\n",
      "    epoch          : 984\n",
      "    loss           : -480.42506805419924\n",
      "    val_loss       : -429.8547849768773\n",
      "    val_log_likelihood: 557.0713500976562\n",
      "    val_log_marginal: 435.57348425602515\n",
      "Train Epoch: 985 [512/54000 (1%)] Loss: -269.886536\n",
      "Train Epoch: 985 [11776/54000 (22%)] Loss: -279.022736\n",
      "Train Epoch: 985 [23040/54000 (43%)] Loss: -484.414917\n",
      "Train Epoch: 985 [34304/54000 (64%)] Loss: -249.368774\n",
      "Train Epoch: 985 [45568/54000 (84%)] Loss: -338.271759\n",
      "    epoch          : 985\n",
      "    loss           : -480.05780838012697\n",
      "    val_loss       : -433.661300217174\n",
      "    val_log_likelihood: 560.0266326904297\n",
      "    val_log_marginal: 437.5134121965617\n",
      "Train Epoch: 986 [512/54000 (1%)] Loss: -517.040527\n",
      "Train Epoch: 986 [11776/54000 (22%)] Loss: -498.040894\n",
      "Train Epoch: 986 [23040/54000 (43%)] Loss: -539.012451\n",
      "Train Epoch: 986 [34304/54000 (64%)] Loss: -273.396484\n",
      "Train Epoch: 986 [45568/54000 (84%)] Loss: -527.051392\n",
      "    epoch          : 986\n",
      "    loss           : -480.3210559082031\n",
      "    val_loss       : -431.54482169039545\n",
      "    val_log_likelihood: 556.7773681640625\n",
      "    val_log_marginal: 435.072595296646\n",
      "Train Epoch: 987 [512/54000 (1%)] Loss: -553.593689\n",
      "Train Epoch: 987 [11776/54000 (22%)] Loss: -645.876099\n",
      "Train Epoch: 987 [23040/54000 (43%)] Loss: -493.740906\n",
      "Train Epoch: 987 [34304/54000 (64%)] Loss: -272.681580\n",
      "Train Epoch: 987 [45568/54000 (84%)] Loss: -311.726257\n",
      "    epoch          : 987\n",
      "    loss           : -480.5328924560547\n",
      "    val_loss       : -429.2337401403114\n",
      "    val_log_likelihood: 558.7178527832032\n",
      "    val_log_marginal: 436.1163447972387\n",
      "Train Epoch: 988 [512/54000 (1%)] Loss: -478.320923\n",
      "Train Epoch: 988 [11776/54000 (22%)] Loss: -492.717346\n",
      "Train Epoch: 988 [23040/54000 (43%)] Loss: -467.817017\n",
      "Train Epoch: 988 [34304/54000 (64%)] Loss: -522.022705\n",
      "Train Epoch: 988 [45568/54000 (84%)] Loss: -517.858887\n",
      "    epoch          : 988\n",
      "    loss           : -479.72309097290037\n",
      "    val_loss       : -429.4159683894366\n",
      "    val_log_likelihood: 555.8805541992188\n",
      "    val_log_marginal: 433.4906416412443\n",
      "Train Epoch: 989 [512/54000 (1%)] Loss: -542.969299\n",
      "Train Epoch: 989 [11776/54000 (22%)] Loss: -494.546356\n",
      "Train Epoch: 989 [23040/54000 (43%)] Loss: -651.072449\n",
      "Train Epoch: 989 [34304/54000 (64%)] Loss: -526.721375\n",
      "Train Epoch: 989 [45568/54000 (84%)] Loss: -456.705811\n",
      "    epoch          : 989\n",
      "    loss           : -480.04577087402345\n",
      "    val_loss       : -430.75020545236765\n",
      "    val_log_likelihood: 556.6866485595704\n",
      "    val_log_marginal: 435.9305439558414\n",
      "Train Epoch: 990 [512/54000 (1%)] Loss: -646.092163\n",
      "Train Epoch: 990 [11776/54000 (22%)] Loss: -653.916870\n",
      "Train Epoch: 990 [23040/54000 (43%)] Loss: -272.303650\n",
      "Train Epoch: 990 [34304/54000 (64%)] Loss: -480.468140\n",
      "Train Epoch: 990 [45568/54000 (84%)] Loss: -469.175293\n",
      "    epoch          : 990\n",
      "    loss           : -480.40226013183593\n",
      "    val_loss       : -432.98801028374584\n",
      "    val_log_likelihood: 560.2640502929687\n",
      "    val_log_marginal: 438.1433801818639\n",
      "Train Epoch: 991 [512/54000 (1%)] Loss: -516.032837\n",
      "Train Epoch: 991 [11776/54000 (22%)] Loss: -459.700897\n",
      "Train Epoch: 991 [23040/54000 (43%)] Loss: -531.426819\n",
      "Train Epoch: 991 [34304/54000 (64%)] Loss: -462.809174\n",
      "Train Epoch: 991 [45568/54000 (84%)] Loss: -522.166199\n",
      "    epoch          : 991\n",
      "    loss           : -480.4121255493164\n",
      "    val_loss       : -430.5898038757965\n",
      "    val_log_likelihood: 557.0826568603516\n",
      "    val_log_marginal: 435.53699842356144\n",
      "Train Epoch: 992 [512/54000 (1%)] Loss: -538.469360\n",
      "Train Epoch: 992 [11776/54000 (22%)] Loss: -516.563538\n",
      "Train Epoch: 992 [23040/54000 (43%)] Loss: -525.801147\n",
      "Train Epoch: 992 [34304/54000 (64%)] Loss: -515.803711\n",
      "Train Epoch: 992 [45568/54000 (84%)] Loss: -346.858917\n",
      "    epoch          : 992\n",
      "    loss           : -480.4811122131348\n",
      "    val_loss       : -431.8851407211274\n",
      "    val_log_likelihood: 560.003173828125\n",
      "    val_log_marginal: 437.7032529298216\n",
      "Train Epoch: 993 [512/54000 (1%)] Loss: -267.876160\n",
      "Train Epoch: 993 [11776/54000 (22%)] Loss: -514.081909\n",
      "Train Epoch: 993 [23040/54000 (43%)] Loss: -494.364594\n",
      "Train Epoch: 993 [34304/54000 (64%)] Loss: -644.930481\n",
      "Train Epoch: 993 [45568/54000 (84%)] Loss: -517.743774\n",
      "    epoch          : 993\n",
      "    loss           : -480.1588870239258\n",
      "    val_loss       : -428.99432363864037\n",
      "    val_log_likelihood: 559.0279052734375\n",
      "    val_log_marginal: 436.13689638115466\n",
      "Train Epoch: 994 [512/54000 (1%)] Loss: -543.852783\n",
      "Train Epoch: 994 [11776/54000 (22%)] Loss: -517.356262\n",
      "Train Epoch: 994 [23040/54000 (43%)] Loss: -525.888000\n",
      "Train Epoch: 994 [34304/54000 (64%)] Loss: -534.096436\n",
      "Train Epoch: 994 [45568/54000 (84%)] Loss: -509.331238\n",
      "    epoch          : 994\n",
      "    loss           : -479.65832946777346\n",
      "    val_loss       : -427.2981496229768\n",
      "    val_log_likelihood: 557.951708984375\n",
      "    val_log_marginal: 435.0810363631696\n",
      "Train Epoch: 995 [512/54000 (1%)] Loss: -532.418579\n",
      "Train Epoch: 995 [11776/54000 (22%)] Loss: -462.346497\n",
      "Train Epoch: 995 [23040/54000 (43%)] Loss: -514.075500\n",
      "Train Epoch: 995 [34304/54000 (64%)] Loss: -534.974548\n",
      "Train Epoch: 995 [45568/54000 (84%)] Loss: -516.651672\n",
      "    epoch          : 995\n",
      "    loss           : -480.11805053710935\n",
      "    val_loss       : -431.63659748006614\n",
      "    val_log_likelihood: 553.9200561523437\n",
      "    val_log_marginal: 433.6795204821974\n",
      "Train Epoch: 996 [512/54000 (1%)] Loss: -313.686279\n",
      "Train Epoch: 996 [11776/54000 (22%)] Loss: -269.537598\n",
      "Train Epoch: 996 [23040/54000 (43%)] Loss: -540.719971\n",
      "Train Epoch: 996 [34304/54000 (64%)] Loss: -533.271606\n",
      "Train Epoch: 996 [45568/54000 (84%)] Loss: -522.955017\n",
      "    epoch          : 996\n",
      "    loss           : -480.3101974487305\n",
      "    val_loss       : -429.8720102377236\n",
      "    val_log_likelihood: 558.0279663085937\n",
      "    val_log_marginal: 434.9945681158453\n",
      "Train Epoch: 997 [512/54000 (1%)] Loss: -481.884674\n",
      "Train Epoch: 997 [11776/54000 (22%)] Loss: -648.168335\n",
      "Train Epoch: 997 [23040/54000 (43%)] Loss: -644.973694\n",
      "Train Epoch: 997 [34304/54000 (64%)] Loss: -502.793945\n",
      "Train Epoch: 997 [45568/54000 (84%)] Loss: -489.813629\n",
      "    epoch          : 997\n",
      "    loss           : -479.41150756835935\n",
      "    val_loss       : -427.8938335366547\n",
      "    val_log_likelihood: 556.233984375\n",
      "    val_log_marginal: 433.32217655517195\n",
      "Train Epoch: 998 [512/54000 (1%)] Loss: -518.628052\n",
      "Train Epoch: 998 [11776/54000 (22%)] Loss: -543.577087\n",
      "Train Epoch: 998 [23040/54000 (43%)] Loss: -324.933258\n",
      "Train Epoch: 998 [34304/54000 (64%)] Loss: -313.878998\n",
      "Train Epoch: 998 [45568/54000 (84%)] Loss: -295.018127\n",
      "    epoch          : 998\n",
      "    loss           : -480.4306800842285\n",
      "    val_loss       : -431.25363787449896\n",
      "    val_log_likelihood: 560.3522064208985\n",
      "    val_log_marginal: 437.5522371273488\n",
      "Train Epoch: 999 [512/54000 (1%)] Loss: -482.668823\n",
      "Train Epoch: 999 [11776/54000 (22%)] Loss: -646.714111\n",
      "Train Epoch: 999 [23040/54000 (43%)] Loss: -658.265747\n",
      "Train Epoch: 999 [34304/54000 (64%)] Loss: -542.874390\n",
      "Train Epoch: 999 [45568/54000 (84%)] Loss: -528.128174\n",
      "    epoch          : 999\n",
      "    loss           : -480.23226837158205\n",
      "    val_loss       : -433.5783709237352\n",
      "    val_log_likelihood: 560.5133056640625\n",
      "    val_log_marginal: 437.5463432941586\n",
      "Train Epoch: 1000 [512/54000 (1%)] Loss: -288.051819\n",
      "Train Epoch: 1000 [11776/54000 (22%)] Loss: -545.606506\n",
      "Train Epoch: 1000 [23040/54000 (43%)] Loss: -474.270660\n",
      "Train Epoch: 1000 [34304/54000 (64%)] Loss: -272.250854\n",
      "Train Epoch: 1000 [45568/54000 (84%)] Loss: -519.677246\n",
      "    epoch          : 1000\n",
      "    loss           : -480.4288430786133\n",
      "    val_loss       : -429.3980681506917\n",
      "    val_log_likelihood: 551.1950012207031\n",
      "    val_log_marginal: 431.5595091279596\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseCategory/0711_122903/checkpoint-epoch1000.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GlimpseCategoryModel(\n",
       "  (_category): FreeCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=98, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=252, bias=True)\n",
       "        (1): LayerNorm((252,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=252, out_features=252, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(28, 14, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(14, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(14, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=98, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=252, bias=True)\n",
       "        (1): LayerNorm((252,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=252, out_features=252, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(28, 14, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(14, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(14, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=49, out_features=252, bias=True)\n",
       "        (1): LayerNorm((252,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=252, out_features=252, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(28, 14, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(14, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(14, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_6): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=49, bias=True)\n",
       "        (1): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=49, out_features=49, bias=True)\n",
       "        (4): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=49, out_features=49, bias=True)\n",
       "        (7): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=49, out_features=98, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=49, bias=True)\n",
       "        (1): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=98, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (7): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=196, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=392, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_10): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_11): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_12): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=24, out_features=49, bias=True)\n",
       "        (1): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=49, out_features=49, bias=True)\n",
       "        (4): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=49, out_features=49, bias=True)\n",
       "        (7): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=49, out_features=98, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_13): SpatialTransformerWriter(\n",
       "      (glimpse_conv): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Conv2d(56, 112, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (glimpse_selector): Softmax2d()\n",
       "      (glimpse_dense): Linear(in_features=9, out_features=6, bias=True)\n",
       "      (coordinates_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_14): GaussianLikelihood()\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=128, out_features=48, bias=True)\n",
       "  )\n",
       "  (encoders): ModuleDict(\n",
       "    ($p(Z^{32} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=96, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{49} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=147, out_features=147, bias=True)\n",
       "        (3): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=147, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{49} | \\mathbb{R}^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=147, out_features=147, bias=True)\n",
       "        (3): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=147, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{49})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=98, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{32} \\mid \\mathbb{R}^{16} \\times \\mathbb{R}^{2}):\\mathbb{R}^{16} \\times \\mathbb{R}^{2} -> \\mathbb{R}^{32}$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=96, out_features=36, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{49} \\mid \\mathbb{R}^{32} \\times \\mathbb{R}^{2}):\\mathbb{R}^{32} \\times \\mathbb{R}^{2} -> \\mathbb{R}^{49}$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=147, out_features=147, bias=True)\n",
       "        (3): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=147, out_features=68, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} \\mid \\mathbb{R}^{49} \\times \\mathbb{R}^{2}):\\mathbb{R}^{49} \\times \\mathbb{R}^{2} -> \\mathbb{R}^{196}$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=102, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{784} \\mid \\mathbb{R}^{196} \\times \\mathbb{R}^{2}):\\mathbb{R}^{196} \\times \\mathbb{R}^{2} -> \\mathbb{R}^{784}$): MlpEncoder(\n",
       "      (incoder): DenseIncoder(\n",
       "        (dense): Sequential(\n",
       "          (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "          (1): LayerNorm((1568,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Linear(in_features=1568, out_features=1568, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((1568,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=1568, out_features=1568, bias=True)\n",
       "        (3): LayerNorm((1568,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=1568, out_features=396, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{16}, Z^{32}): Ty() -> \\mathbb{R}^{32}$): RecurrentEncoder(\n",
       "      (recurrent): GRUCell(48, 64)\n",
       "    )\n",
       "    ($p(Z^{8}, Z^{16}): Ty() -> \\mathbb{R}^{16}$): RecurrentEncoder(\n",
       "      (recurrent): GRUCell(24, 32)\n",
       "    )\n",
       "    ($p(Z^{24}, Z^{49}): Ty() -> \\mathbb{R}^{49}$): RecurrentEncoder(\n",
       "      (recurrent): GRUCell(73, 98)\n",
       "    )\n",
       "    ($p(Z^{3} \\mid \\mathbb{R}^{784} \\times \\mathbb{R}^{196})$): MlpEncoder(\n",
       "      (incoder): DenseIncoder(\n",
       "        (dense): Sequential(\n",
       "          (0): Linear(in_features=784, out_features=6, bias=True)\n",
       "          (1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Linear(in_features=6, out_features=6, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((787,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=787, out_features=787, bias=True)\n",
       "        (3): LayerNorm((787,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=787, out_features=1176, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} \\mid \\mathbb{R}^{784})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{49})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{32})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{16})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{2})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEFCAYAAADOqip4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAap0lEQVR4nO2dW2wc13nHv28vvCy55HJJiqSo68qSbdmpY4mOA9dJjZZOc4ORBnRatCiKFq38XBSQYfSxfZGf+tKiFhqgDwWa2GrRh6QFLLWJi+Zii5YVX+JLLEqy7uJtSZHcC3f39IHDeLPi9x9qqSVXJ/8fQHB3vjlzzp6Z/87s/Oc7R51zQgjxk8hWN4AQ0jgocEI8hgInxGMocEI8hgInxGNiW90An1HVMRFJi8iMiGRFJOOcO97gOkdF5CXn3L51rn9IREZE5LBz7rlGto1sPjyDNwhVzYjIY8654865E7Ii8lSj63XOnRKRiTso8oKIvNys4lbVc1vdhnsZCrxxZERkevWNc+6M3JnwNouUcy671Y0AHN7qBtzLUOCNY1xEXlDVo8HZXIIzuYisXEoHf8dUNVW1bFZVDwWvX1LVTPD+pdXtVK132zZqUdUjwTpHa9cJLs/TwToZVR1T1XPB+q9UtWssWDYW/ARYd1tr6jPbvVbdQfverCq/VjvWbDMJcM7xr0F/InJIRE6KiJOVAzVVFXsp+D8qIseqlp8UkUPB62MictRY75fbC+p5pXobVcuPBa9Tq3XWtPFk7fugXKZqG0er211V77raWrN92O7qutf4LLAd1eX4t/LHM3gDcc6dcc497ZxTETklKyJYjVX/5k3VFF29lJ+uej2zxvazq/XIiqhq+X0RmQ7OhJngL4x00O7Vep8TkTNV8XM1da2rretsd23d1aB2oHK/1lDgDWL1EnIV59zzUiWw4PJ0VIBwA7K18TsgJSJngoP/jHPu6XWUgeIMSK++uIttXW/da7XjTsv92kCBN45UYJOJiEjw23AieH1ERKbdyh3v1fihO62g6vdrRlauEGp5RUSerlr/jusItlFd7jGjrnWzjnZvSjt+HaDAG0xwE2hMRI6IyPPB4lMisq/mLJ9evZSuujH3tIg8GwjiOREZrbl5NRps4zkR+YugvtVtHAm+QFZvQN12CV9TXypYZyT4AhKRX9pu2dWbW7LyO36ijrZWs1a7b6t7jc+yVjtuK0c+RYObFOQeQ1XfdM7dcxbSvdruexWewQnxGAr8HiS4LM3ca5el92q772V4iU6Ix/AMTojHUOCEeEzD00VjbR2uJZk245ES/okQKVbMWKUFfz9pGbctUizBeCkZN2NhP2w0bIWQeKSMV9Blu1+KPVG88Va77HpwJYXx2IIdD+uXaB7vtEoc73MXteuOFPC2tYTjpc4WGA87llHbNGR/V1pwny9NXZ5yzvXXLq9L4IEPmZV15De3JNPywDf+0ownpnGnJi4vmbGl4QQsG1vE2267OAvjk18YMGMO97doiIaiRbxD22ZD2n7D7pfz3+yCZXX/AoxXKvjDLc+2wfjg/9oiDPvcyQ/nYDw/3AnjxW77kE5O4M8dncR1T39hGMbbJ/EJA7Utfgvv78UhLNUz3/6ri2stv+NL9NWns1afwlrrAQpCSHNQz2/wx+TTpIIJ+dXHB0XklymK46o6XsovbqR9hJANUI/AUzXve2tXcCujmIw450ZibR11NYwQsnHqEXhWqrKJCCHNSz0CPy2fnsUzspJsTwhpQu74Lrpz7kSQQTQqKyNq4JQ9FanYbpMkLuLf6Eu77Uv8Ygf+foos4zu2hV09MN5x3b4rGnaXvJDCVlXP/13CGwjh6jd2m7HDT70Py+7vvAnjf9B9GsaXHD5s/nToT8xY17/gO/yVECuq/dw0jOc/ZzsftzL4DnzXMr6TnbyQh3EBNpiIyHLSPiZmDwCRiMjg69gBsKjLJnPOvRi8ZD4uIU0Mn2QjxGMocEI8hgInxGMocEI8hgInxGMocEI8puHpoloWaZ2z/ejcMH6UtQK8xdY5bEYvh/jkYSmbxS7btwzLiuq4jD3TW4dxZlKuF/vo85/PmbEwn/tLyXdg/Mc5PD9CMmrXLSLy5PB5M/bD+/HIzaU2nCHYnmqF8a4JO8vu2hPYBy+1pnAcN00iyzie+oV9TMQXsBRjl6bwxg14BifEYyhwQjyGAifEYyhwQjyGAifEYyhwQjym4TZZpOSkNWun4bkYTrGLL9hllzuxlVQOGYlSOuu30Zb6cdm5vdhTWU6GVH3wFox3txXN2IcLdsqkiMhrN/bjykPoasUW4K4EGMzy0XlYdi6O00kjeFxDaT9np1VuO4MP99w2nKqamMTppGgEYBGR2K2CGZvfgweyLDy5C8blO0abcClCyL0MBU6Ix1DghHgMBU6Ix1DghHgMBU6Ix1DghHhMw31wUex1z+/CTei+YOfgdVzBaYuVGP7+ii7i/L7ltO1Nts2GDJEb4tFPPoLjsRj2VHs77LTIN87tgWX3DU/C+NcGcTrpP773BRgv9Nr7NJfFfm80GZICnMD9PvP4NjPW+9plWDbfi1N4Zx7AQxt3X8Amfcu0/WBFWCpq8rL93AOCZ3BCPIYCJ8RjKHBCPIYCJ8RjKHBCPIYCJ8RjKHBCPKbhPng5rrI4aHu+XRexd+gitu8Z5nOXE/jjaRkPfdwyaU9tXOpph2VnD+Dc4sIw9jUrBdz2KJi/WCP4c+WWsZ/7D+98EcYHe3Cu+kcfbjdj0RT+3A/cdwXGz0/thXF19vGS3NMPy7Zm8bEYLeB+zafxsw2lB7rNWN9beHrgUsi0yhY8gxPiMXUJXFVnVfWkqh692w0ihNw96r1Ef9Y5d+qutoQQctep9xI9parm/DaqekRVx1V1vJS3f8cSQhpLvQJPi8iMqr60VtA5d9w5N+KcG4m14bnHCCGNoy6BBwLOikhWVcfubpMIIXeLOxZ4cPmNp4gkhDQF9dxke1lEMqtnbufcCbRypUVkYYftTXZcx5XdGra9xVQO+45Swb5lpQ2XL6btwcuz+7CXvDgc4rEnsR9cXMLbv1XE0+girlxOw3jfIB67/Npbg7iCbnv88O6knccuInJhBrdtaTf2qmNZe5/OPIhz0QdfxR788vYeGM/14aTu+KL97EIRjD2wsu2QY93gjgUeXJqfCf6guAkhWwsfdCHEYyhwQjyGAifEYyhwQjyGAifEYxqeLhotiHR/bFtG8QVsewyesof4XXgIp/9FitiqEsFD9KIwyEoUEZHYEl6hcB1bKj17wRS8InIVWF2RVjzNbdsnOPVwqpSC8fhubHV1tdsWYF8Cl93eMQfjPyqGpItO209Otk/j/V268AmM33rCToMVEYkv4ePNAbUtpbEUk5/YUw8jeAYnxGMocEI8hgInxGMocEI8hgInxGMocEI8hgInxGMa74PnK5L6hT1s08JOPPxwsdv2uhVb6JLrxx8vvhQ2Va39/dc2gz3P9NevwviB7psw/upbD8N4BKTKVirYg4+GzUQbx/1Svob32fKM7UVf/A28T/Ykp2H80E48BfD4pfvN2AJIPRYR6d69E8Z7zs7A+PThXhhHD08kbuCprPN9OH3YgmdwQjyGAifEYyhwQjyGAifEYyhwQjyGAifEYyhwQjym4T64FpcldtH2fIsHcX5vz4c5M1Zpwd9PS/3Y90RTE4uIVEDvLH4VT6G7MN8J4y2RkJztdB7G81O2Fx3L4t26tBPXHUYlicujNPxyFg8P/PE8zvG/lu2C8e6P7VglxErOfg7neycn8BS/7VP4wYwImK66ZdI+zkVEcn32EN6wzrpKEULuCShwQjyGAifEYyhwQjyGAifEYyhwQjyGAifEYxrug7t4TMrb7TzZ9LvYWywnbPMyPoO9w7YUNj4jyzinO75ox6cX8djiQwNZGC9WsEdfLocMvA6ILeCyZWxFi4TU3XoN92vcTv+X3BD29y9Pp2D8zw7+BMa/Ez9sxrJXsYeefh/7+/P3YS+6awJ8cBHJ99kdnxu2c+hFRFoWQsbwN+AZnBCPCRW4qo6p6sk1lo2q6pHGNY0QslFCBe6cO1H9XlXHguWngvejjWkaIWSj1HOJ/piITASvJ0TkUO0KqnpEVcdVdXy5hOeiIoQ0jnoEnqp5f9sdNOfccefciHNuJB7Dk+wRQhpHPQLPiog9tSUhpGmoR+Cn5dOzeEZETtqrEkK2klAfPLiJNqKqY865E865E6p6NFieWr3ZZhKNSKmr1QxPPoJN2dQ5O8d2fk8KltUQ6zBsLOqFYdvvHdiGx8h+ducZGP/OxREYj8exJ1vO2d/NpQT296N5/L1eSoTU3Yq3n3jU7pvYq3js8I6vX4fxh9svwXjZ2f0an8XPHkwfxHLofQ/P0Z09gL3sWMHut47L+JmOfH/YwwtGnWErBALuqVn2YvASi5sQsqXwQRdCPIYCJ8RjKHBCPIYCJ8RjKHBCPKbh6aKVqEoBpG0OvIFT7Ob22U/C9f0YT8HrErY9JyIyOdKN6z5gx/58x9uw7GBsDsaLJWzZlD7AqY2VXtvKapnDu7W4H1syWgqx0Qbw/MOzU3Za5e5n8LTKYcQFW3i/t9feL999+4uwbGsW23+lBN5n8SVcPtdn92vnefy5Oj7CtqwFz+CEeAwFTojHUOCEeAwFTojHUOCEeAwFTojHUOCEeEzjffC4yMKQ7R+2zIc0AYzgu/AgTj1smcPTuUZx9p+UOm1vcmoZTw+8u2UKxvs7sP/fOYK95ss3esxYfjfOk41GsF+rLdiTrUzj5wt6986asYvn8fTAkQTeZ8ciX4HxLw++Z8YctrFlcRgPF528jNtW7A45lsFuWdyFj6e5vfiZDflg7cU8gxPiMRQ4IR5DgRPiMRQ4IR5DgRPiMRQ4IR5DgRPiMQ33wSMlkcSk7asWu3ATIiXbs22bDDGysd0ruW0hU/S22+3+na6fw6IHW6Zh/O8KeBjcZ3a8A+MnIw+YsQsfD8CylVu4bteKffT2Iezhz35gz4sRCTniOgfwdNLfvf9fYfx/ctvtuot4f3dP4M+9MIynjG6fws8PJK/YPnq5Dbet7+2QY92AZ3BCPIYCJ8RjKHBCPIYCJ8RjKHBCPIYCJ8RjKHBCPKbhPriIiIb40YiuiSUzVonjBN+lQexbhhFvt6cXvl7C+bk7Y1kY39Vl50yLiPzn1Ydg/Pq0XX88hT3Tcsi45x2duPzCDZy7LN22HxyWa57L4X327exnYfz53l+Ysb/uwAdidh/ul9Q57JPH53AO/+IO+/mDSgz74Iq7zST0DK6qY6p6smbZrKqeVNWj9VVLCNkM1jM/+AlVfa5m8bPBvOGEkCam3t/gKVXN3NWWEELuOvUKPC0iM6r60lpBVT2iquOqOr5cwM8WE0IaR10Cd84dd85lRSSrqmNGfMQ5NxJvDbkhQwhpGHcs8ODsfKgRjSGE3F3Wcxd9VERGqs7ULwfLx0RWbsI1rnmEkI2wnrvop0Skp+p9VkTOBH+h4tayk9ii7R9GyiFzMnfYc4u3fXQDli2k7dxgEZF8H647GrXjZxd3wbJ/lLwG4+ezeEz3vSmcT97ZUl9+sIjIlTns4Yf53NGk/XyAiEg5Zz+fEP8Ej6ne+Vn8uS/mcb/9MHfOjJW6sJkclpOduI7Ph5FBnGcfAcOqx3LYY2+dxR67WWddpQgh9wQUOCEeQ4ET4jEUOCEeQ4ET4jEUOCEe0/B00XKbSvY+2+pyIV8xiUnbPsju2wnLJi/h6V7T72Kb7OZ2u91np3fAsu+lfwTjT2230xpFRC4u2UMPi4hcm+8yY4uLIcMi38BWlaaxDZZO4cePp8pJM9by8Bwsu7MLxx/quALjZ3J7zNjArhlYdvodPLVx8hK2qqY/g/u145p9LMdv4WM1fgW33YJncEI8hgInxGMocEI8hgInxGMocEI8hgInxGMocEI8puE+eGypIv1v2UMfL3fZXrOISKRoe4c9r16AZZcex8PG5dP4+80t2t3zlaH3YNm/vfw1GN/ejv3es5eHYfzh7XY66lQ7Tvcs9+K0yHgEpy5eutED4wd22Gm8I+lPYNnT07thfCKHveo/Tv/EjP39B1+CZd0QTsG99pv4+YLed3E6atuM/XxBKYGHAJ8bGYJxOb/2Yp7BCfEYCpwQj6HACfEYCpwQj6HACfEYCpwQj6HACfGYhvvgTlXKbcDjw5asZPfb08m29u+HZUshw+AuDeF88EjB/v57bQrX/c/3vQLjL1z5XRjvTGBP9vGeC3bdNz4Py+bncd7y9mGce/yVB34O469d3mfGPpx4HJa9P4OHm/5mzziMf+/WI3Yw5HTWch773LKBabBFRK4+aW+//yzOB69EQ4RiwDM4IR5DgRPiMRQ4IR5DgRPiMRQ4IR5DgRPiMRQ4IR7TcB+80qKyuN32sqMFbC6mztk5tHN7cC557zt2HrqISCWegPHFHbb32BnHPvWlEm7bQOs8jB8ewP3yg8kDZuwzQ1dh2bNlnGt+YxpPL5zZcRbGF4fs/f2jwl5YNoz/yB6G8R/ftLffMhMy/S8eDl763sVedbETb3/gtF1Brg/ngytONTeBAlfVlIhkgr/HnHPPB8vHRCQrIhnn3PH6qiaENJqwS/RviciIc+6EiIiqHgnELc65U8Gy0cY2kRBSL1DgzrnjVWfojIhMiMhjwX8J/h9qXPMIIRthXTfZVDUjIjPBWTtVE+5dY/0jqjququOl/OLGW0kIqYv13kUfc849F7zOigicGS84848450ZibR0baR8hZAOEClxVx5xzLwavD4nIafn0LJ4RkZMNax0hZEOE3UUfFZFjqvpCsOh559wJVT0axFKrN9vMbVRE4otgGN6QLLjYom1NdF3EZXNDOP2vHJJOWonbVtV7NwZh2VPdD8H4D67jdNPCMnYwv7rLTtm8lsc218Ehe1hjEZGWKLaDXr35IIzngUX4W3vPwbLvzuB+zS5ja3Mhb6fCVlqx9Tgwjj93tICHkw5LT67E7Xgsh9tWidWXLgqPokC8tyX3rp7RRQSKmxCytfBJNkI8hgInxGMocEI8hgInxGMocEI8hgInxGMani4aKZSlc2LBjBd7sVcdKdjeZLnfTksUEVkYxCl4Yamq0bztPRYuJGHZ5f247ie2GfO9BrRHce7i69N7zNiFG7c9PfwrfPPBszA+U8RPH3480wfjD/TeNGNzy3h/f3n7+zAexn9fediMDf0M728t4/j8Lny8JSZxTmfrbNGMFXrwtjs/wanPFjyDE+IxFDghHkOBE+IxFDghHkOBE+IxFDghHkOBE+IxjZ8+OBqR5bTtfZbbsV9cKtn+YGwJ5+d2XcL5vXN78McfeMP2oq88hct2R3MwfrOIffSYw5/toZQ9ze4zg2/Dsq/N4Fz0tpB88M8P4UT816/vMmOjOz6CZWdLON87KrhfXMT2srvfy+K6H+mB8c6r+NmEXD8+JuYy9mfr+Qhve2E37hd5Y+3FPIMT4jEUOCEeQ4ET4jEUOCEeQ4ET4jEUOCEeQ4ET4jEN98G17CQ+b+fBzh7ohOW7J2zf00XxWNGRZZzfm7yM83cXttvdE1vCdf92xwcwfiGPc7bDfHQBjw/0x/DUxI/34Fz0S3k4cY0cTODpiWfStme7WLLHLRcRGWydg/HPJfC46v8esacXXtrdBcvG8iH54CHTVac/yMN48hIIhuSiR4r4eRGzXF2lCCH3BBQ4IR5DgRPiMRQ4IR5DgRPiMRQ4IR5DgRPiMY33wZ0TLdr5xZ3XsBe9nLT9v7ZpnENbiePvr8QVPNZ0y5ydi15uw37u31z5Gox/o+8tGB+MYT/4n2580YxdL2K/d3+7PW65iMhgC/bR317cCeN/uO2nZuz7s4/Asq0RvE8/LGyH8ciifbxoBR9rXa8jo1pk+Sk7z11EJLqE8+hnDtrPfHSfw889xOcLMG4BFaCqKVU9pKpjqnqsavmsqp5U1aN11UoI2RTCLtG/JSIjzrkTIiKqeiRY/qxz7mnn3IsNbR0hZEPAS3Tn3PGqtxkRORm8Tqlqxjk30bCWEUI2zLpusqlqRkRmnHOngkVpEZlR1ZeM9Y+o6riqjhdL9c2pRAjZOOu9iz7mnHtu9Y1z7rhzLisiWVUdq105iI8450ZaYiGDxRFCGkboXXRVHVv9ra2qh0RkRETGnXNnGt04QsjGgAJX1VEROaaqLwSLnheRl0Uks3rmXr0BZ1FKRGX60ZQZjy/iYXBjOTuNzkVwyubiIE7v671yC8ajt+z0v/wX+mHZ3YkZGM+W8ZVNWHysf9yMfX8GW1Fh6aRhHEhch/Glim0hPpPG9mAyglMur5e6Ybxt0r4oTXyE2728Zxve9iy22bIH8LTLnVfA9MF9ePrgfHc7jMvptReH3WQ7JSL71gidCf6guAkhWwufZCPEYyhwQjyGAifEYyhwQjyGAifEYyhwQjym4emiIiIK7MOWeewtupjtdRe7Q6bw/Rg/JnvjSTw88MC/2VPdtk1hH/x7rzwB47mBEP9/ALd9ecn2TR+9D0/v+9OFtZzPT/nZzDCMt8dwSmehbO+XiOLhgS+M74DxCM7IlDaQdTnz+CAs2/Nf78O4+0wGxgvdeGhjNFV2qTUktflmyAc34BmcEI+hwAnxGAqcEI+hwAnxGAqcEI+hwAnxGAqcEI9R57AvueEKVCdFpNqY7RORqYZWWj9sW300a9uatV0id79tu51ztz2c0XCB31ah6rhzbmRTK10nbFt9NGvbmrVdIpvXNl6iE+IxFDghHrMVAj8evsqWwbbVR7O2rVnbJbJJbdv03+CEkM2Dl+iEeAwFTojHbKrAg1lKR6smMWwKmnG21KCvTq6xbMv7z2jblvYhmAl3y/tsK2fp3TSBV02UcCp4P7pZda+DppsttXZCiWbqP2Oyi63uw9tmwm2iPtuyWXo38wz+mIiszkY6ISKHNrHuMFLBBIvNTDP3n8gW92EwH97qnemMrPRRU/SZ0TaRTeizzRR4quZ97ybWHQacLbVJSNW8b6b+E2mSPqyZCTdVE97SPrvTWXrvBpsp8KysfKCmI2y21CYhK03afyJN1YfVM+Fmpbn67I5m6b0bbKbAT8un36gZETlpr7p5BL/Vmu1ydy2asv9EmqcP15gJt2n6rLZtm9Vnmybw4AZDJrjRkaq6TNlqXhb5lZtYTTGhYtBPIzXtaor+q22bNEEfVs2E+6aqviki6Wbps7XaJpvUZ3ySjRCP4YMuhHgMBU6Ix1DghHgMBU6Ix1DghHgMBU6Ix1DghHjM/wNU50ZdXCliKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEFCAYAAADOqip4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAaT0lEQVR4nO2d2W8cV3bGv9Mbd7JJStQuS23L9ozssU1TM5nEycOETjAJJpME9MwfEER+Sh5lOC95yYv9H1gPA0yQAImtJA/BBAEowJksCDKmBY932bL2hbK4NJcm2ezl5oFFu6fF+xXVZJOtO98PINhdp27dU7fr66quU+cec85BCBEmid12QAjRPCRwIQJGAhciYCRwIQJGAhciYFK77UDImNkYgAEAMwDyAHLOubNN7nMUwBvOuUc3uf4wgBEAzzvnXm6mb2Ln0Rm8SZhZDsAp59xZ59w5rIk82+x+nXPnAVx+gCavAnizVcVtZl/stg8PMxJ488gBmF5/45y7gAcT3k6Rdc7ld9sJwvO77cDDjATePCYAvGpmZ6KzOaIzOYC1S+no7zUzy9YsmzWz4ej1G2aWi96/sb6dmvXu20Y9ZnY6WudM/TrR5flAtE7OzMbM7Ito/bdq/BqLlo1FPwE27Wtdf16/N+o78u/dmvYb+bGhzyLCOae/Jv0BGAYwDsBh7UDN1tjeiP6PAnitZvk4gOHo9WsAznjW+2p7UT9v1W6jZvlr0evsep91Po7Xv4/a5Wq2cabW75p+N+Vr3fap37V9b7Av1I/advpb+9MZvIk45y445150zhmA81gTwbqt9jdvtq7p+qX8dM3rmQ22n1/vB2uiqufHAKajM2Eu+otjIPJ7vd+XAVyosX9R19emfN2k3/V918L8YO1+rZHAm8T6JeQ6zrlXUCOw6PJ0FES4Efl6+wOQBXAhOvgvOOde3EQbKs6IgfUX2+jrZvveyI8HbfdrgwTePLJRmAwAEP02vBy9Pg1g2q3d8V63Dz9oBzW/X3NYu0Ko5y0AL9as/8B9RNuobXfK09em2YTfO+LHrwMSeJOJbgKNATgN4JVo8XkAj9ad5QfWL6Vrbsy9COClSBAvAxitu3k1Gm3jZQB/HvW3vo3T0RfI+g2o+y7h6/rLRuuMRF9AAL4Ku+XXb25h7Xf85QZ8rWUjv+/re4N92ciP+9qJr7HoJoV4yDCzd51zD10I6WH1+2FFZ3AhAkYCfwiJLktzD9tl6cPq98OMLtGFCBidwYUIGAlciIBperpoOtPl2jv6vXaL+YVgpYrXVmlP0rYJf9PYba+tYF5TuYP3nZpfoXZX5n1bJkPtq/1p0pg2jcXK3F7piNlA1W9KL/GmyaUStbsMP2St4B93187HFEl+vrMiHxiX5scEPd7IsQYALsO3vbBwa8o5t7d+eUMCj+KQeWwiv7m9ox/PvfCXXntqmR/ombsLXtviE/4vDgDI5PkHwrYN8A8sfzJL2/aPf07tlalpak8dOkrtN//ksNdWjhNgzJdqx5d8hZlniIIBJJf9Qhma4G2z796l9uLRAWpP/+JTr809cZy2Lfe1UXvbNf6Zlff1UXtqMu+1uQz5wgawepBv++23/+raRssf+BJ9/ems9aewNnqAQgjRGjTyG/wUvk4quIxffXwQwFcpihNmNlFaLWzFPyHEFmhE4Nm694P1K7i1WUxGnHMj6UxXQ44JIbZOIwLPoyabSAjRujQi8Hfw9Vk8h7VkeyFEC/LAd9Gdc+eiDKJRrM2oQVP2zPE75Zmbs7Q/Kyx7bd2f8Lu9rp3fmVw5wu9MWtm//ezHc7zvQ0PUvjB6gtpnH+ffvauP+8clJuKCI0M8ffrWNB8Xt8TDTZ1H5722u0f4nerFQwep/dA4v5Ntg/6Ly0qSD0xyhUddVmPu4LPjBQBWj9z3a/Yr0tP8XlV61v95MxoKkznnXo9eKh9XiBZGT7IJETASuBABI4ELETASuBABI4ELETASuBAB0/R0USuWaRZOtSsm9YkEdUtDPbRpNcO/vzKzRWpP3vHHi10Hj+cu53jMdPokj8mu7uNpk+mkPyvLEjwee/2DA9Ru+3mqKyrc9/nb/s/FSvwzqcZkdC6e4DH6TN7/aHSqwMcUVT5umatTvHkPfyy70us/Zsr9nbStVXgWng+dwYUIGAlciICRwIUIGAlciICRwIUIGAlciIBpepgsDhY6AAAr+1M+UzEpdLGzXMYUfSh8y5+6mJldpW3njvNU1Uo7NaP7Em+/dMC/by7F9ysunTR9kYds0MtDNpkZ/3kjbqbbwpGYFar8kO254e+7fZrveHoxbkZX/plYhfvu0n7f4o7lRtEZXIiAkcCFCBgJXIiAkcCFCBgJXIiAkcCFCBgJXIiAaXocvNKZwsIz+7z2rkv+KXYBACn/d5DdjZlCN8vTSfPDfGrj1R5/3HT5GR4TjavwmYnZ7cJTPGWz4xN/IH35AI/HpudjqmjGhKJTi7y9I48frPTzGLpr4/ZSL++7sN9vr7TF5KI6bh9c4OnFlY8u8u0PPuu3Jfh+lbtjfPdttqFWQoiHAglciICRwIUIGAlciICRwIUIGAlciICRwIUImKbHwROrVXTe8JdGdR0x8WSWs33AX44VACpdMaVqD8fEHsmMzqvZmGlsXVypWt68632eMF447g9Wd16PyYOPiXOvDPF88uwnvP3iEf++d5F8bQAo9XD78nGeh58o+ePF5a64fHBqxvJh/lyFe+QUtSdX/MdMJUYHmWt8ymYfOoMLETANCdzMZs1s3MzObLdDQojto9FL9Jecc+e31RMhxLbT6CV61sxyPqOZnTazCTObKJWXGuxCCLFVGhX4AIAZM3tjI6Nz7qxzbsQ5N5JOxUzgJ4RoGg0JPBJwHkDezMa21yUhxHbxwAKPLr+Hm+GMEGJ7aeQm25sAcutnbufcObaySyVQ3Ou/TG+b5gHhSpc/PpiZXqBtp4f7qb1wmMeyq90kYFyKiXMv8e/O9mluX+2lZnTc9Me6+67E7FfMp17p4L4tHOdx8u4bfruLOaUsHouZ032RO8/i5LbA2zrjzs3leKx64BOeL56e8d+PSixyHbgFfqz7eGCBR5fmF6I/Km4hxO6iB12ECBgJXIiAkcCFCBgJXIiAkcCFCJjmp4sul9D5wS2vvXSMT12cvudPNa12kXxOAKVuHsqqdpep3Yr+779kzNTB3d+Ypfbi3AC3D/Gczr5P/GGylWzMfme4vdIWky76GTXTMFxmnm+791LMtMiHePvMHX+66PJJXqK3WOXpxckS963Sxu1pEr10Bf5Id+kZ75Pha/zHxot1BhciYCRwIQJGAhciYCRwIQJGAhciYCRwIQJGAhciYJoeB0cqieqgP/cxsRozh6/5Y7Yrh7pp07i0yP59vIZvPt/ltVVi0jnnrmb5CjFx7lRMiV+QcHC5k8e5U8s8ltxzhXe9NMS33/mlf/vJEu+7fZanupZjUllX9vi3n7jDp6KuZrhvi8f4cxODH/JxYVOEu6P+EtsAkJ5qbOozncGFCBgJXIiAkcCFCBgJXIiAkcCFCBgJXIiAkcCFCJjmx8EBgIQ2Sz3+/F0AALEnizxmOv8ULzWbWuT55Jj19+16eEwUGe5b1zU+9OVOHpOtsJAuD8diaR9fIcMfD0CSzw6M9JJ/30sxMfr54/ycUxzk49p9bM5rW17h0x5bJa7sMvd97lhMnD3tb7/azfe79yr3zYfO4EIEjAQuRMBI4EIEjAQuRMBI4EIEjAQuRMBI4EIETNPj4M4Al/bH8Npu5nn7Tv9c1YvHe2I65zHTbC/PsZ1L+Nvv7+flXG9c30PtcXN0d/XwcrKFq31eW3KZx2tL/TwXvdLOY65tM3z7U0/726/G9G0V/pkl9/PPbGHO/2xDW2eJtu1o52P+SD+f6/7i9ePUnlz1j8vg+4u0bTnueREPOoMLETCxAjezMTMb32DZqJmdbp5rQoitEitw59y52vdmNhYtPx+9H22Oa0KIrdLIJfopAJej15cBDNevYGanzWzCzCZK5cbmkhJCbJ1GBJ6tez9Yv4Jz7qxzbsQ5N5JOdTbkmBBi6zQi8DwAXhpTCNESNCLwd/D1WTwHYNy/qhBiN4mNg0c30UbMbMw5d845d87MzkTLs+s328gG4NL+7xEr8bzq+cf88eTlQf79lDt+h9orVd4+k/X79t0hPnl4bxuPqVYdjyXfmecTr3cc9cfhC3me556e5HnR5cM84dscr6NdJWH04ee+oG1vLfrj+wCQJs8mAMDv7r/otf39x6do2z987CNqvzB7hNrjng8Y/NAf6652cCm2TfI4uY9YgUcC7q9b9nr0kotbCLGr6EEXIQJGAhciYCRwIQJGAhciYCRwIQKm+emiCaOpbskCn2q285Y/3LR40F/eFwAG2wvUvqeN2+dLft+WKzzU9GWBlzYe6uJhj3SKp1WWWYhvhX9v7x2+S+23b9z3cOKvkHqSz6u8t8c/ru9+doy2TXfxqa5zQ9PUfnHRX4b3yYN8v6dL/Hh6vPdLav/8KA+jld73HzOZWR5WLWdjpvj2oDO4EAEjgQsRMBK4EAEjgQsRMBK4EAEjgQsRMBK4EAHT9Di4VRzS8/7YZqWHpx4WB/32Eg8149IMn7r4doanJn6j3x83LTs+tfDJwUlqf3fyMLUvkul/AaB/wB9HHziSp20XVviYP/34DWr/4JOj1D6T9Kd0Hjt6j7adzPM02UySPx/Qk/bHk/OrfEzvrfAD6k6B+9Z9jZ8vCwf9z4N0fHiTtk308Bi9t11DrYQQDwUSuBABI4ELETASuBABI4ELETASuBABI4ELETBNj4MDDnDO78DkHG3dXvXHqq3CywcnE/5+AaBY5rs/R/LBf7j3Pdr2Xpn7FpdP3rGfl7pNmT/WfOHLQ7Tt8/t5zPXT2SFqzx7g+eALJMc/FTPtcRy3YqaTPtDhP556Mzzn+v07B6l9D8lzB4Aq/0jRfcv/PMjqCd53eqqxaZN1BhciYCRwIQJGAhciYCRwIQJGAhciYCRwIQJGAhciYHYgDr41kgV/7DAmJRvJmJhrqcK/32aLnV7b7VKWtn373hPUfn22n9q/te82tc+W/bnFv3/4U9r2Z9dOUntPOy8fvBg3l32nv/3sJI9jn3ycx+iv57PUznK6u9N8v4orPJC9fz+P/0+lD1B7ouw/HouDvO/UfGNSjT2Dm9mYmY3XLZs1s3EzO9NQr0KIHWEz9cHPmdnLdYtfiuqGCyFamEZ/g2fNLLetngghtp1GBT4AYMbM3tjIaGanzWzCzCZK5aXGvRNCbImGBO6cO+ucywPIm9mYxz7inBtJp/w3qoQQzeWBBR6dnYeb4YwQYnvZzF30UQAjNWfqN6PlY8DaTbjmuSeE2AqbuYt+HkB/zfs8gAvRX6y4XTKBUq9/Hu5qhseDS73+eG9xgMe5/2D/F9R+7sLz1N5z1B+DP9HGa01f6d5L7Y/18PnB08bn/76x7B+3SwXe98m9fM72awv8M3nuKJ83/eKUP5/8icd4fP9GTJx7Mc9/8h0+nPfa/vs2vy/8vROfUftjnbw++MTBx6i9mvKfTztv8Fxzu8X79qEn2YQIGAlciICRwIUIGAlciICRwIUIGAlciIBpfvlg55As+kM+iVUeDmqbWvbaum7y1MN//vl3qP2RkzxcNNDuD138YpGHXN6f5tPgnsjyMNndZT7t8nzRn7JZrvLv7b42Pn3wC/suU/vENC8f/OjAlNd2NT9A21ZifD9+mI/bZ/P+EF2xxA/3/7lxnNo/7t5H7b0Xef5yasH/2Hapj6fgVr4dk/rxs40X6wwuRMBI4EIEjAQuRMBI4EIEjAQuRMBI4EIEjAQuRMA0Pw5eriI164//lfs6aPvUjD8W3X3bP0UuAFR+Z4Hapws89bAz7U8XLVb50P3w8C+pvS/pj+8DwFsFnso6v+JPwf2jYx/StgMpnpr4D9d4308P3qH2D6b90wcf7cvTtneX+Ge6t4OX0Z1e6fLalhf9YwYANuNPTQaAheP8mY04NSXn/J/5yhDXQefPP+Eb96AzuBABI4ELETASuBABI4ELETASuBABI4ELETASuBAB0/zywWZwGX83iXc+os2rT/vL8E4/ZbTtD459TO2fL/hzhwHglxdJ3vMJ2hRfJPjUxQVS/hcAetI8Z7vc5f9uvlzYQ9t+5vh+H+ubofaEOWrvafOX6W1PlWjbIz15av/BHv58wb9OPeO1DT7G4/8shg4Aly7vp/bkIT6Nd/Fwn9fW/TGfFnn1WT4lM/5z48U6gwsRMBK4EAEjgQsRMBK4EAEjgQsRMBK4EAEjgQsRMM2PgwOAI3FTEucGgOQcyQe/xudFf2fqEWqPywd/7smrXlt/hudzH+6YpfZSlc+hvVjhucuzRb/v383yec2nyjzneq7Mc5Onirz9yaw/Xzxf4tu+u8Q/03+cPEXtC6v+ccskeD438xsAEDM1+aXP/XnwAJCe988vMPMbPMbee9k/pwKDCtzMsljbrRyAU865V6LlYwDyAHLOubMN9SyEaDpxl+g/AjDinDsHAGZ2OhI3nHPno2WjzXVRCNEoVODOubM1Z+gcgMsATkX/Ef0fbp57QoitsKmbbGaWAzATnbWzdebBDdY/bWYTZjaxWm7st4MQYuts9i76mHPu5eh1HgCtIBed+UeccyOZFL+RJYRoHrECN7Mx59zr0ethAO/g67N4DsB407wTQmyJuLvoowBeM7NXo0WvOOfOmdmZyJZdv9nmwyUM1TZ/Ny7Nw0XJOb9tNcvTRXtjyuRaTNrjldn7fn18xUI3Tz18spuHXD5d5iGVPW18euD+Nv9Pn+tFXqL3QIYMKoCLi7xM7qH2PLXfWfGnRf7eIE8PvtLJ02w7E/5QEwD8++Q3vbbDXXnati1RpnY25gCQ/ZBHnZf3+0OEvZf48WRVfqz6oB5F4n10g+WvRy+puIUQu4ueZBMiYCRwIQJGAhciYCRwIQJGAhciYCRwIQKm+eWDiyUkr0x67e4Qj3u6tN/Fzkk+Te3FST498MEBHg8e6vbHovvbeUz0e1283OsHC4eo/fu971P7QOq+6OVXrFTTtO3lZT7m3+67Su370nzc3jP/dNN3S/4Y+XZwqMvv20KZp+C+/dnj1J5I8lh0f4Hbuz+867UtPcGPVZZqytAZXIiAkcCFCBgJXIiAkcCFCBgJXIiAkcCFCBgJXIiAaf60yckE0Eem2S3zWPbysazfyNPBUVrhu3drimwbQNX5O3j2yE3a9l/mnqf2pZjywT+991vUfrTDX+L3aGaatn2ineeqtyd4id+f3H6B2l/c438G4GCaTyf90zu/Se372heo/eKMP55cqfIDpqPLX/YYAIZ6eY7+tef51McdM/48+447fBpuK3Gd+NAZXIiAkcCFCBgJXIiAkcCFCBgJXIiAkcCFCBgJXIiAaX4cPJGA6/bPB+1S/DumY8JfCjfx7HHa1hI8P7e0zPOmT+ZueW3DfTdo25vFfmr//p4Pqf1KkedsH8r448l3SlnadqHSTu0nO3iMn8W5AaCNxNH/6R5/PmC1wufJ70rxWPV39l3z2qZWu2jbxRLPF//8v45Rez8fNrTl/fP0p27xZxfKR/bwjXvQGVyIgJHAhQgYCVyIgJHAhQgYCVyIgJHAhQgYCVyIgGl+HLxaRWLeP4e4a+OxaOvx55JXMvz7KXWN51zjBK/JfHu+12v7tI3X0P7rg/9G7X9z5/vU/s3u29TOctWvLDUWM13niwTft+lVkt8PHm/OdU7Rtk/18P0uusYP2fem+Vz0t27468EDQIqHydE+G/PcRaff90RMnDs5x/PFvdtlRjPLmtmwmY2Z2Ws1y2fNbNzMzjTUqxBiR4i7RP8RgBHn3DkAMLPT0fKXnHMvOudeb6p3QogtQa93nHNna97mAIxHr7NmlnPO+Z8jFULsOpu6yWZmOQAzzrnz0aIBADNm9oZn/dNmNmFmE6sVXsNLCNE8NnsXfcw59/L6G+fcWedcHkDezMbqV47sI865kUyyc5tcFUI8KLG3JM1sbP23tpkNAxgBMOGcu9Bs54QQW4MK3MxGAbxmZq9Gi14B8CaA3PqZe/0GnA+XSqK0318yNj3JS9FW+nu8tvZ7PHSwb4JfPeRneLhn9YWK1/bRFJ8iFwe5+cd7fkHt+Qr3fX/KP27D+67StlXHL9w+KvJw0l8McN/bzZ/y2Z3gqarvFXk66JcV/pm9Nf1tr+3ujD/sCQA9n/KQbWaOh8FSS3xq4673/enHrpOPS3mvXwfUJ2aMfnNvVIj6QvRHxS2E2F30JJsQASOBCxEwErgQASOBCxEwErgQASOBCxEwzU8XNQDmT22s9POpbEtZf3yw7TovRdvexnfvwG0eR590/vh9oZ/HRP+4fJra/zT3S2r/s/7/o/a/y/unH/5e98e07d9O89LExzvuUfuFYpbaS/DHwScKOdr2R30T1P6Tu79N7b9474TX1n2ZT8lsZWrG0P/6SzYDwMoBHqMvPEOeL4gphZ0q+J/JYOgMLkTASOBCBIwELkTASOBCBIwELkTASOBCBIwELkTAmHM8nrvlDszuAait6boHAJ87d/eQb43Rqr61ql/A9vv2iHPuvprTTRf4fR2aTTjnRna0000i3xqjVX1rVb+AnfNNl+hCBIwELkTA7IbAz8avsmvIt8ZoVd9a1S9gh3zb8d/gQoidQ5foQgSMBC5EwOyowKMqpaM1RQxbglaslhqN1fgGy3Z9/Dy+7eoYkkq4uz5mu1mld8cEXlMo4Xz0fnSn+t4ELVcttb6gRCuNn6fYxW6P4X2VcFtozHatSu9OnsFPAVivRnoZwPAO9h1HNiqw2Mq08vgBuzyGUT289TvTOayNUUuMmcc3YAfGbCcFnq17P7iDfcdBq6W2CNm69600fkCLjGFdJdxsnXlXx+xBq/RuBzsp8DzWdqjliKuW2iLk0aLjB7TUGNZWws2jtcbsgar0bgc7KfB38PU3ag7AuH/VnSP6rdZql7sb0ZLjB7TOGG5QCbdlxqzet50asx0TeHSDIRfd6MjWXKbsNm8Cv3ITqyUKKkbjNFLnV0uMX71vaIExrKmE+66ZvQtgoFXGbCPfsENjpifZhAgYPegiRMBI4EIEjAQuRMBI4EIEjAQuRMBI4EIEjAQuRMD8PyOdKW+T4fzpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
