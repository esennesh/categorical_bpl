{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.5,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [256/54000 (0%)] Loss: 4941.209961\n",
      "Train Epoch: 1 [4352/54000 (8%)] Loss: 4854.637695\n",
      "Train Epoch: 1 [8448/54000 (16%)] Loss: -2839.271484\n",
      "Train Epoch: 1 [12544/54000 (23%)] Loss: -5416.826172\n",
      "Train Epoch: 1 [16640/54000 (31%)] Loss: -30592.886719\n",
      "Train Epoch: 1 [20736/54000 (38%)] Loss: -70402.460938\n",
      "Train Epoch: 1 [24832/54000 (46%)] Loss: -139702.640625\n",
      "Train Epoch: 1 [28928/54000 (54%)] Loss: -95680.179688\n",
      "Train Epoch: 1 [33024/54000 (61%)] Loss: -71532.656250\n",
      "Train Epoch: 1 [37120/54000 (69%)] Loss: -92301.820312\n",
      "Train Epoch: 1 [41216/54000 (76%)] Loss: -120494.812500\n",
      "Train Epoch: 1 [45312/54000 (84%)] Loss: -145235.531250\n",
      "Train Epoch: 1 [49408/54000 (91%)] Loss: -125269.039062\n",
      "    epoch          : 1\n",
      "    loss           : -80062.52878277119\n",
      "    val_loss       : -146697.74045410156\n",
      "Train Epoch: 2 [256/54000 (0%)] Loss: -199235.875000\n",
      "Train Epoch: 2 [4352/54000 (8%)] Loss: -93627.656250\n",
      "Train Epoch: 2 [8448/54000 (16%)] Loss: -91384.015625\n",
      "Train Epoch: 2 [12544/54000 (23%)] Loss: -187821.562500\n",
      "Train Epoch: 2 [16640/54000 (31%)] Loss: -130197.328125\n",
      "Train Epoch: 2 [20736/54000 (38%)] Loss: -129231.968750\n",
      "Train Epoch: 2 [24832/54000 (46%)] Loss: -110122.359375\n",
      "Train Epoch: 2 [28928/54000 (54%)] Loss: -133429.906250\n",
      "Train Epoch: 2 [33024/54000 (61%)] Loss: -87980.140625\n",
      "Train Epoch: 2 [37120/54000 (69%)] Loss: -237807.687500\n",
      "Train Epoch: 2 [41216/54000 (76%)] Loss: -150897.906250\n",
      "Train Epoch: 2 [45312/54000 (84%)] Loss: -109553.656250\n",
      "Train Epoch: 2 [49408/54000 (91%)] Loss: -185689.750000\n",
      "    epoch          : 2\n",
      "    loss           : -154679.77118389422\n",
      "    val_loss       : -162828.56450195314\n",
      "Train Epoch: 3 [256/54000 (0%)] Loss: -148134.937500\n",
      "Train Epoch: 3 [4352/54000 (8%)] Loss: -135633.468750\n",
      "Train Epoch: 3 [8448/54000 (16%)] Loss: -143685.718750\n",
      "Train Epoch: 3 [12544/54000 (23%)] Loss: -187512.687500\n",
      "Train Epoch: 3 [16640/54000 (31%)] Loss: -113881.687500\n",
      "Train Epoch: 3 [20736/54000 (38%)] Loss: -143431.281250\n",
      "Train Epoch: 3 [24832/54000 (46%)] Loss: -228389.453125\n",
      "Train Epoch: 3 [28928/54000 (54%)] Loss: -189012.765625\n",
      "Train Epoch: 3 [33024/54000 (61%)] Loss: -120414.890625\n",
      "Train Epoch: 3 [37120/54000 (69%)] Loss: -101400.203125\n",
      "Train Epoch: 3 [41216/54000 (76%)] Loss: -206942.281250\n",
      "Train Epoch: 3 [45312/54000 (84%)] Loss: -137656.234375\n",
      "Train Epoch: 3 [49408/54000 (91%)] Loss: -134760.093750\n",
      "    epoch          : 3\n",
      "    loss           : -163080.98069411056\n",
      "    val_loss       : -167411.46716308594\n",
      "Train Epoch: 4 [256/54000 (0%)] Loss: -278082.656250\n",
      "Train Epoch: 4 [4352/54000 (8%)] Loss: -146062.656250\n",
      "Train Epoch: 4 [8448/54000 (16%)] Loss: -122258.390625\n",
      "Train Epoch: 4 [12544/54000 (23%)] Loss: -113908.140625\n",
      "Train Epoch: 4 [16640/54000 (31%)] Loss: -122569.703125\n",
      "Train Epoch: 4 [20736/54000 (38%)] Loss: -142536.687500\n",
      "Train Epoch: 4 [24832/54000 (46%)] Loss: -103101.867188\n",
      "Train Epoch: 4 [28928/54000 (54%)] Loss: -92278.101562\n",
      "Train Epoch: 4 [33024/54000 (61%)] Loss: -118769.523438\n",
      "Train Epoch: 4 [37120/54000 (69%)] Loss: -247826.125000\n",
      "Train Epoch: 4 [41216/54000 (76%)] Loss: -145625.843750\n",
      "Train Epoch: 4 [45312/54000 (84%)] Loss: -95777.429688\n",
      "Train Epoch: 4 [49408/54000 (91%)] Loss: -147379.703125\n",
      "    epoch          : 4\n",
      "    loss           : -167049.08124248797\n",
      "    val_loss       : -169303.80576171874\n",
      "Train Epoch: 5 [256/54000 (0%)] Loss: -190934.250000\n",
      "Train Epoch: 5 [4352/54000 (8%)] Loss: -145592.062500\n",
      "Train Epoch: 5 [8448/54000 (16%)] Loss: -123760.679688\n",
      "Train Epoch: 5 [12544/54000 (23%)] Loss: -151410.343750\n",
      "Train Epoch: 5 [16640/54000 (31%)] Loss: -145383.437500\n",
      "Train Epoch: 5 [20736/54000 (38%)] Loss: -201294.187500\n",
      "Train Epoch: 5 [24832/54000 (46%)] Loss: -97335.359375\n",
      "Train Epoch: 5 [28928/54000 (54%)] Loss: -145292.750000\n",
      "Train Epoch: 5 [33024/54000 (61%)] Loss: -103295.015625\n",
      "Train Epoch: 5 [37120/54000 (69%)] Loss: -116202.476562\n",
      "Train Epoch: 5 [41216/54000 (76%)] Loss: -98460.203125\n",
      "Train Epoch: 5 [45312/54000 (84%)] Loss: -241374.828125\n",
      "Train Epoch: 5 [49408/54000 (91%)] Loss: -279397.812500\n",
      "    epoch          : 5\n",
      "    loss           : -170724.72596153847\n",
      "    val_loss       : -174407.0093261719\n",
      "Train Epoch: 6 [256/54000 (0%)] Loss: -149172.343750\n",
      "Train Epoch: 6 [4352/54000 (8%)] Loss: -123509.828125\n",
      "Train Epoch: 6 [8448/54000 (16%)] Loss: -277661.218750\n",
      "Train Epoch: 6 [12544/54000 (23%)] Loss: -115728.414062\n",
      "Train Epoch: 6 [16640/54000 (31%)] Loss: -207885.984375\n",
      "Train Epoch: 6 [20736/54000 (38%)] Loss: -153638.937500\n",
      "Train Epoch: 6 [24832/54000 (46%)] Loss: -204966.984375\n",
      "Train Epoch: 6 [28928/54000 (54%)] Loss: -144221.390625\n",
      "Train Epoch: 6 [33024/54000 (61%)] Loss: -126845.156250\n",
      "Train Epoch: 6 [37120/54000 (69%)] Loss: -152558.593750\n",
      "Train Epoch: 6 [41216/54000 (76%)] Loss: -115760.656250\n",
      "Train Epoch: 6 [45312/54000 (84%)] Loss: -215859.640625\n",
      "Train Epoch: 6 [49408/54000 (91%)] Loss: -154041.828125\n",
      "    epoch          : 6\n",
      "    loss           : -178616.12560096153\n",
      "    val_loss       : -184492.40310058594\n",
      "Train Epoch: 7 [256/54000 (0%)] Loss: -104547.031250\n",
      "Train Epoch: 7 [4352/54000 (8%)] Loss: -159369.859375\n",
      "Train Epoch: 7 [8448/54000 (16%)] Loss: -271543.375000\n",
      "Train Epoch: 7 [12544/54000 (23%)] Loss: -155059.609375\n",
      "Train Epoch: 7 [16640/54000 (31%)] Loss: -126574.453125\n",
      "Train Epoch: 7 [20736/54000 (38%)] Loss: -251271.500000\n",
      "Train Epoch: 7 [24832/54000 (46%)] Loss: -273398.968750\n",
      "Train Epoch: 7 [28928/54000 (54%)] Loss: -240170.765625\n",
      "Train Epoch: 7 [33024/54000 (61%)] Loss: -112286.851562\n",
      "Train Epoch: 7 [37120/54000 (69%)] Loss: -272090.000000\n",
      "Train Epoch: 7 [41216/54000 (76%)] Loss: -171717.390625\n",
      "Train Epoch: 7 [45312/54000 (84%)] Loss: -243356.453125\n",
      "Train Epoch: 7 [49408/54000 (91%)] Loss: -162815.687500\n",
      "    epoch          : 7\n",
      "    loss           : -187261.3446890024\n",
      "    val_loss       : -194843.87099609376\n",
      "Train Epoch: 8 [256/54000 (0%)] Loss: -134385.593750\n",
      "Train Epoch: 8 [4352/54000 (8%)] Loss: -245389.968750\n",
      "Train Epoch: 8 [8448/54000 (16%)] Loss: -169684.000000\n",
      "Train Epoch: 8 [12544/54000 (23%)] Loss: -194448.125000\n",
      "Train Epoch: 8 [16640/54000 (31%)] Loss: -264134.812500\n",
      "Train Epoch: 8 [20736/54000 (38%)] Loss: -230531.390625\n",
      "Train Epoch: 8 [24832/54000 (46%)] Loss: -163465.015625\n",
      "Train Epoch: 8 [28928/54000 (54%)] Loss: -244554.500000\n",
      "Train Epoch: 8 [33024/54000 (61%)] Loss: -170611.296875\n",
      "Train Epoch: 8 [37120/54000 (69%)] Loss: -289858.062500\n",
      "Train Epoch: 8 [41216/54000 (76%)] Loss: -105702.476562\n",
      "Train Epoch: 8 [45312/54000 (84%)] Loss: -139129.484375\n",
      "Train Epoch: 8 [49408/54000 (91%)] Loss: -268810.843750\n",
      "    epoch          : 8\n",
      "    loss           : -196455.26592548078\n",
      "    val_loss       : -204931.63376464843\n",
      "Train Epoch: 9 [256/54000 (0%)] Loss: -172415.531250\n",
      "Train Epoch: 9 [4352/54000 (8%)] Loss: -165372.359375\n",
      "Train Epoch: 9 [8448/54000 (16%)] Loss: -193170.031250\n",
      "Train Epoch: 9 [12544/54000 (23%)] Loss: -263443.093750\n",
      "Train Epoch: 9 [16640/54000 (31%)] Loss: -116416.421875\n",
      "Train Epoch: 9 [20736/54000 (38%)] Loss: -141094.578125\n",
      "Train Epoch: 9 [24832/54000 (46%)] Loss: -169940.828125\n",
      "Train Epoch: 9 [28928/54000 (54%)] Loss: -180760.296875\n",
      "Train Epoch: 9 [33024/54000 (61%)] Loss: -275447.031250\n",
      "Train Epoch: 9 [37120/54000 (69%)] Loss: -180094.921875\n",
      "Train Epoch: 9 [41216/54000 (76%)] Loss: -172428.625000\n",
      "Train Epoch: 9 [45312/54000 (84%)] Loss: -265544.218750\n",
      "Train Epoch: 9 [49408/54000 (91%)] Loss: -299918.968750\n",
      "    epoch          : 9\n",
      "    loss           : -206322.59375\n",
      "    val_loss       : -216778.51604003907\n",
      "Train Epoch: 10 [256/54000 (0%)] Loss: -179153.375000\n",
      "Train Epoch: 10 [4352/54000 (8%)] Loss: -173382.156250\n",
      "Train Epoch: 10 [8448/54000 (16%)] Loss: -258503.140625\n",
      "Train Epoch: 10 [12544/54000 (23%)] Loss: -177544.312500\n",
      "Train Epoch: 10 [16640/54000 (31%)] Loss: -190369.468750\n",
      "Train Epoch: 10 [20736/54000 (38%)] Loss: -205892.484375\n",
      "Train Epoch: 10 [24832/54000 (46%)] Loss: -275117.250000\n",
      "Train Epoch: 10 [28928/54000 (54%)] Loss: -286270.343750\n",
      "Train Epoch: 10 [33024/54000 (61%)] Loss: -272655.375000\n",
      "Train Epoch: 10 [37120/54000 (69%)] Loss: -128685.187500\n",
      "Train Epoch: 10 [41216/54000 (76%)] Loss: -314186.437500\n",
      "Train Epoch: 10 [45312/54000 (84%)] Loss: -283964.156250\n",
      "Train Epoch: 10 [49408/54000 (91%)] Loss: -132522.156250\n",
      "    epoch          : 10\n",
      "    loss           : -219814.34900841347\n",
      "    val_loss       : -234123.608203125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0702_141658/checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 11 [256/54000 (0%)] Loss: -129487.390625\n",
      "Train Epoch: 11 [4352/54000 (8%)] Loss: -320936.875000\n",
      "Train Epoch: 11 [8448/54000 (16%)] Loss: -187102.703125\n",
      "Train Epoch: 11 [12544/54000 (23%)] Loss: -179122.375000\n",
      "Train Epoch: 11 [16640/54000 (31%)] Loss: -208533.375000\n",
      "Train Epoch: 11 [20736/54000 (38%)] Loss: -128841.468750\n",
      "Train Epoch: 11 [24832/54000 (46%)] Loss: -306538.875000\n",
      "Train Epoch: 11 [28928/54000 (54%)] Loss: -186742.906250\n",
      "Train Epoch: 11 [33024/54000 (61%)] Loss: -292551.875000\n",
      "Train Epoch: 11 [37120/54000 (69%)] Loss: -316627.812500\n",
      "Train Epoch: 11 [41216/54000 (76%)] Loss: -135915.781250\n",
      "Train Epoch: 11 [45312/54000 (84%)] Loss: -221547.031250\n",
      "Train Epoch: 11 [49408/54000 (91%)] Loss: -190505.109375\n",
      "    epoch          : 11\n",
      "    loss           : -233545.34059495194\n",
      "    val_loss       : -241862.35415039063\n",
      "Train Epoch: 12 [256/54000 (0%)] Loss: -202456.140625\n",
      "Train Epoch: 12 [4352/54000 (8%)] Loss: -214354.093750\n",
      "Train Epoch: 12 [8448/54000 (16%)] Loss: -209639.750000\n",
      "Train Epoch: 12 [12544/54000 (23%)] Loss: -140384.750000\n",
      "Train Epoch: 12 [16640/54000 (31%)] Loss: -294993.687500\n",
      "Train Epoch: 12 [20736/54000 (38%)] Loss: -330450.125000\n",
      "Train Epoch: 12 [24832/54000 (46%)] Loss: -143396.500000\n",
      "Train Epoch: 12 [28928/54000 (54%)] Loss: -211551.281250\n",
      "Train Epoch: 12 [33024/54000 (61%)] Loss: -337417.687500\n",
      "Train Epoch: 12 [37120/54000 (69%)] Loss: -186395.984375\n",
      "Train Epoch: 12 [41216/54000 (76%)] Loss: -215560.937500\n",
      "Train Epoch: 12 [45312/54000 (84%)] Loss: -275889.406250\n",
      "Train Epoch: 12 [49408/54000 (91%)] Loss: -325310.375000\n",
      "    epoch          : 12\n",
      "    loss           : -239058.84427584134\n",
      "    val_loss       : -244260.0591308594\n",
      "Train Epoch: 13 [256/54000 (0%)] Loss: -332953.625000\n",
      "Train Epoch: 13 [4352/54000 (8%)] Loss: -190243.812500\n",
      "Train Epoch: 13 [8448/54000 (16%)] Loss: -332097.187500\n",
      "Train Epoch: 13 [12544/54000 (23%)] Loss: -298197.375000\n",
      "Train Epoch: 13 [16640/54000 (31%)] Loss: -191023.171875\n",
      "Train Epoch: 13 [20736/54000 (38%)] Loss: -196433.078125\n",
      "Train Epoch: 13 [24832/54000 (46%)] Loss: -139506.875000\n",
      "Train Epoch: 13 [28928/54000 (54%)] Loss: -192266.812500\n",
      "Train Epoch: 13 [33024/54000 (61%)] Loss: -288758.312500\n",
      "Train Epoch: 13 [37120/54000 (69%)] Loss: -330129.562500\n",
      "Train Epoch: 13 [41216/54000 (76%)] Loss: -192049.046875\n",
      "Train Epoch: 13 [45312/54000 (84%)] Loss: -211284.703125\n",
      "Train Epoch: 13 [49408/54000 (91%)] Loss: -203551.875000\n",
      "    epoch          : 13\n",
      "    loss           : -242607.12387319712\n",
      "    val_loss       : -246284.83154296875\n",
      "Train Epoch: 14 [256/54000 (0%)] Loss: -210019.625000\n",
      "Train Epoch: 14 [4352/54000 (8%)] Loss: -138561.671875\n",
      "Train Epoch: 14 [8448/54000 (16%)] Loss: -148629.531250\n",
      "Train Epoch: 14 [12544/54000 (23%)] Loss: -194230.328125\n",
      "Train Epoch: 14 [16640/54000 (31%)] Loss: -215924.671875\n",
      "Train Epoch: 14 [20736/54000 (38%)] Loss: -207302.500000\n",
      "Train Epoch: 14 [24832/54000 (46%)] Loss: -146619.406250\n",
      "Train Epoch: 14 [28928/54000 (54%)] Loss: -335429.875000\n",
      "Train Epoch: 14 [33024/54000 (61%)] Loss: -292797.875000\n",
      "Train Epoch: 14 [37120/54000 (69%)] Loss: -300919.125000\n",
      "Train Epoch: 14 [41216/54000 (76%)] Loss: -193283.343750\n",
      "Train Epoch: 14 [45312/54000 (84%)] Loss: -308812.125000\n",
      "Train Epoch: 14 [49408/54000 (91%)] Loss: -185143.109375\n",
      "    epoch          : 14\n",
      "    loss           : -245134.63146033653\n",
      "    val_loss       : -246609.36616210936\n",
      "Train Epoch: 15 [256/54000 (0%)] Loss: -296351.093750\n",
      "Train Epoch: 15 [4352/54000 (8%)] Loss: -233400.031250\n",
      "Train Epoch: 15 [8448/54000 (16%)] Loss: -192235.046875\n",
      "Train Epoch: 15 [12544/54000 (23%)] Loss: -337441.875000\n",
      "Train Epoch: 15 [16640/54000 (31%)] Loss: -210774.562500\n",
      "Train Epoch: 15 [20736/54000 (38%)] Loss: -294010.750000\n",
      "Train Epoch: 15 [24832/54000 (46%)] Loss: -137104.234375\n",
      "Train Epoch: 15 [28928/54000 (54%)] Loss: -333245.687500\n",
      "Train Epoch: 15 [33024/54000 (61%)] Loss: -180524.500000\n",
      "Train Epoch: 15 [37120/54000 (69%)] Loss: -310948.968750\n",
      "Train Epoch: 15 [41216/54000 (76%)] Loss: -220927.875000\n",
      "Train Epoch: 15 [45312/54000 (84%)] Loss: -214602.875000\n",
      "Train Epoch: 15 [49408/54000 (91%)] Loss: -327959.906250\n",
      "    epoch          : 15\n",
      "    loss           : -246781.57466947116\n",
      "    val_loss       : -250406.28337402345\n",
      "Train Epoch: 16 [256/54000 (0%)] Loss: -213317.359375\n",
      "Train Epoch: 16 [4352/54000 (8%)] Loss: -337071.687500\n",
      "Train Epoch: 16 [8448/54000 (16%)] Loss: -309388.875000\n",
      "Train Epoch: 16 [12544/54000 (23%)] Loss: -295365.187500\n",
      "Train Epoch: 16 [16640/54000 (31%)] Loss: -228973.281250\n",
      "Train Epoch: 16 [20736/54000 (38%)] Loss: -199419.687500\n",
      "Train Epoch: 16 [24832/54000 (46%)] Loss: -332807.406250\n",
      "Train Epoch: 16 [28928/54000 (54%)] Loss: -297794.187500\n",
      "Train Epoch: 16 [33024/54000 (61%)] Loss: -235231.031250\n",
      "Train Epoch: 16 [37120/54000 (69%)] Loss: -302575.500000\n",
      "Train Epoch: 16 [41216/54000 (76%)] Loss: -184235.562500\n",
      "Train Epoch: 16 [45312/54000 (84%)] Loss: -304163.187500\n",
      "Train Epoch: 16 [49408/54000 (91%)] Loss: -193258.062500\n",
      "    epoch          : 16\n",
      "    loss           : -248405.8671875\n",
      "    val_loss       : -252503.06369628906\n",
      "Train Epoch: 17 [256/54000 (0%)] Loss: -335456.781250\n",
      "Train Epoch: 17 [4352/54000 (8%)] Loss: -302718.531250\n",
      "Train Epoch: 17 [8448/54000 (16%)] Loss: -338950.062500\n",
      "Train Epoch: 17 [12544/54000 (23%)] Loss: -183114.875000\n",
      "Train Epoch: 17 [16640/54000 (31%)] Loss: -197819.906250\n",
      "Train Epoch: 17 [20736/54000 (38%)] Loss: -339738.500000\n",
      "Train Epoch: 17 [24832/54000 (46%)] Loss: -186959.828125\n",
      "Train Epoch: 17 [28928/54000 (54%)] Loss: -202413.750000\n",
      "Train Epoch: 17 [33024/54000 (61%)] Loss: -210546.765625\n",
      "Train Epoch: 17 [37120/54000 (69%)] Loss: -304337.375000\n",
      "Train Epoch: 17 [41216/54000 (76%)] Loss: -336118.093750\n",
      "Train Epoch: 17 [45312/54000 (84%)] Loss: -220172.531250\n",
      "Train Epoch: 17 [49408/54000 (91%)] Loss: -228028.218750\n",
      "    epoch          : 17\n",
      "    loss           : -249928.24984975962\n",
      "    val_loss       : -253368.1256591797\n",
      "Train Epoch: 18 [256/54000 (0%)] Loss: -199104.875000\n",
      "Train Epoch: 18 [4352/54000 (8%)] Loss: -303658.750000\n",
      "Train Epoch: 18 [8448/54000 (16%)] Loss: -191956.562500\n",
      "Train Epoch: 18 [12544/54000 (23%)] Loss: -300142.250000\n",
      "Train Epoch: 18 [16640/54000 (31%)] Loss: -194944.062500\n",
      "Train Epoch: 18 [20736/54000 (38%)] Loss: -345750.593750\n",
      "Train Epoch: 18 [24832/54000 (46%)] Loss: -200532.406250\n",
      "Train Epoch: 18 [28928/54000 (54%)] Loss: -146016.640625\n",
      "Train Epoch: 18 [33024/54000 (61%)] Loss: -296749.437500\n",
      "Train Epoch: 18 [37120/54000 (69%)] Loss: -200065.250000\n",
      "Train Epoch: 18 [41216/54000 (76%)] Loss: -351117.468750\n",
      "Train Epoch: 18 [45312/54000 (84%)] Loss: -193715.468750\n",
      "Train Epoch: 18 [49408/54000 (91%)] Loss: -342930.375000\n",
      "    epoch          : 18\n",
      "    loss           : -251930.91932091347\n",
      "    val_loss       : -256896.30505371094\n",
      "Train Epoch: 19 [256/54000 (0%)] Loss: -153877.875000\n",
      "Train Epoch: 19 [4352/54000 (8%)] Loss: -189306.281250\n",
      "Train Epoch: 19 [8448/54000 (16%)] Loss: -200816.828125\n",
      "Train Epoch: 19 [12544/54000 (23%)] Loss: -214051.328125\n",
      "Train Epoch: 19 [16640/54000 (31%)] Loss: -183911.843750\n",
      "Train Epoch: 19 [20736/54000 (38%)] Loss: -151057.703125\n",
      "Train Epoch: 19 [24832/54000 (46%)] Loss: -219518.875000\n",
      "Train Epoch: 19 [28928/54000 (54%)] Loss: -190198.156250\n",
      "Train Epoch: 19 [33024/54000 (61%)] Loss: -345372.000000\n",
      "Train Epoch: 19 [37120/54000 (69%)] Loss: -227872.125000\n",
      "Train Epoch: 19 [41216/54000 (76%)] Loss: -200040.718750\n",
      "Train Epoch: 19 [45312/54000 (84%)] Loss: -196758.906250\n",
      "Train Epoch: 19 [49408/54000 (91%)] Loss: -340185.187500\n",
      "    epoch          : 19\n",
      "    loss           : -253158.03635817306\n",
      "    val_loss       : -257935.6675292969\n",
      "Train Epoch: 20 [256/54000 (0%)] Loss: -295746.812500\n",
      "Train Epoch: 20 [4352/54000 (8%)] Loss: -230096.125000\n",
      "Train Epoch: 20 [8448/54000 (16%)] Loss: -195720.296875\n",
      "Train Epoch: 20 [12544/54000 (23%)] Loss: -313499.093750\n",
      "Train Epoch: 20 [16640/54000 (31%)] Loss: -334356.156250\n",
      "Train Epoch: 20 [20736/54000 (38%)] Loss: -208534.531250\n",
      "Train Epoch: 20 [24832/54000 (46%)] Loss: -339572.187500\n",
      "Train Epoch: 20 [28928/54000 (54%)] Loss: -148382.515625\n",
      "Train Epoch: 20 [33024/54000 (61%)] Loss: -257961.890625\n",
      "Train Epoch: 20 [37120/54000 (69%)] Loss: -340905.531250\n",
      "Train Epoch: 20 [41216/54000 (76%)] Loss: -144739.843750\n",
      "Train Epoch: 20 [45312/54000 (84%)] Loss: -232538.718750\n",
      "Train Epoch: 20 [49408/54000 (91%)] Loss: -221395.515625\n",
      "    epoch          : 20\n",
      "    loss           : -254475.49579326922\n",
      "    val_loss       : -259628.32023925782\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0702_141658/checkpoint-epoch20.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 21 [256/54000 (0%)] Loss: -152972.593750\n",
      "Train Epoch: 21 [4352/54000 (8%)] Loss: -187643.453125\n",
      "Train Epoch: 21 [8448/54000 (16%)] Loss: -348520.593750\n",
      "Train Epoch: 21 [12544/54000 (23%)] Loss: -303071.312500\n",
      "Train Epoch: 21 [16640/54000 (31%)] Loss: -308618.562500\n",
      "Train Epoch: 21 [20736/54000 (38%)] Loss: -158592.109375\n",
      "Train Epoch: 21 [24832/54000 (46%)] Loss: -208056.078125\n",
      "Train Epoch: 21 [28928/54000 (54%)] Loss: -333744.062500\n",
      "Train Epoch: 21 [33024/54000 (61%)] Loss: -308533.156250\n",
      "Train Epoch: 21 [37120/54000 (69%)] Loss: -234212.625000\n",
      "Train Epoch: 21 [41216/54000 (76%)] Loss: -257912.406250\n",
      "Train Epoch: 21 [45312/54000 (84%)] Loss: -228869.734375\n",
      "Train Epoch: 21 [49408/54000 (91%)] Loss: -220520.531250\n",
      "    epoch          : 21\n",
      "    loss           : -255788.66714242788\n",
      "    val_loss       : -260753.2546875\n",
      "Train Epoch: 22 [256/54000 (0%)] Loss: -158020.953125\n",
      "Train Epoch: 22 [4352/54000 (8%)] Loss: -194469.750000\n",
      "Train Epoch: 22 [8448/54000 (16%)] Loss: -346505.875000\n",
      "Train Epoch: 22 [12544/54000 (23%)] Loss: -301994.750000\n",
      "Train Epoch: 22 [16640/54000 (31%)] Loss: -198579.812500\n",
      "Train Epoch: 22 [20736/54000 (38%)] Loss: -337712.187500\n",
      "Train Epoch: 22 [24832/54000 (46%)] Loss: -219572.312500\n",
      "Train Epoch: 22 [28928/54000 (54%)] Loss: -155710.250000\n",
      "Train Epoch: 22 [33024/54000 (61%)] Loss: -321725.562500\n",
      "Train Epoch: 22 [37120/54000 (69%)] Loss: -336472.312500\n",
      "Train Epoch: 22 [41216/54000 (76%)] Loss: -224685.562500\n",
      "Train Epoch: 22 [45312/54000 (84%)] Loss: -302351.937500\n",
      "Train Epoch: 22 [49408/54000 (91%)] Loss: -163709.093750\n",
      "    epoch          : 22\n",
      "    loss           : -256764.6182391827\n",
      "    val_loss       : -262399.924609375\n",
      "Train Epoch: 23 [256/54000 (0%)] Loss: -203846.437500\n",
      "Train Epoch: 23 [4352/54000 (8%)] Loss: -203170.000000\n",
      "Train Epoch: 23 [8448/54000 (16%)] Loss: -253770.453125\n",
      "Train Epoch: 23 [12544/54000 (23%)] Loss: -230388.265625\n",
      "Train Epoch: 23 [16640/54000 (31%)] Loss: -303781.437500\n",
      "Train Epoch: 23 [20736/54000 (38%)] Loss: -335493.937500\n",
      "Train Epoch: 23 [24832/54000 (46%)] Loss: -218784.968750\n",
      "Train Epoch: 23 [28928/54000 (54%)] Loss: -253519.343750\n",
      "Train Epoch: 23 [33024/54000 (61%)] Loss: -192260.125000\n",
      "Train Epoch: 23 [37120/54000 (69%)] Loss: -343004.000000\n",
      "Train Epoch: 23 [41216/54000 (76%)] Loss: -345838.437500\n",
      "Train Epoch: 23 [45312/54000 (84%)] Loss: -230017.375000\n",
      "Train Epoch: 23 [49408/54000 (91%)] Loss: -203738.843750\n",
      "    epoch          : 23\n",
      "    loss           : -257525.2362530048\n",
      "    val_loss       : -263472.31667480466\n",
      "Train Epoch: 24 [256/54000 (0%)] Loss: -172135.078125\n",
      "Train Epoch: 24 [4352/54000 (8%)] Loss: -356608.875000\n",
      "Train Epoch: 24 [8448/54000 (16%)] Loss: -304473.343750\n",
      "Train Epoch: 24 [12544/54000 (23%)] Loss: -259403.515625\n",
      "Train Epoch: 24 [16640/54000 (31%)] Loss: -354945.781250\n",
      "Train Epoch: 24 [20736/54000 (38%)] Loss: -231953.406250\n",
      "Train Epoch: 24 [24832/54000 (46%)] Loss: -346988.281250\n",
      "Train Epoch: 24 [28928/54000 (54%)] Loss: -307694.125000\n",
      "Train Epoch: 24 [33024/54000 (61%)] Loss: -326313.125000\n",
      "Train Epoch: 24 [37120/54000 (69%)] Loss: -300919.093750\n",
      "Train Epoch: 24 [41216/54000 (76%)] Loss: -229894.875000\n",
      "Train Epoch: 24 [45312/54000 (84%)] Loss: -301262.812500\n",
      "Train Epoch: 24 [49408/54000 (91%)] Loss: -230922.593750\n",
      "    epoch          : 24\n",
      "    loss           : -259433.01645132212\n",
      "    val_loss       : -265493.3166259766\n",
      "Train Epoch: 25 [256/54000 (0%)] Loss: -174548.906250\n",
      "Train Epoch: 25 [4352/54000 (8%)] Loss: -208222.937500\n",
      "Train Epoch: 25 [8448/54000 (16%)] Loss: -234622.718750\n",
      "Train Epoch: 25 [12544/54000 (23%)] Loss: -348197.000000\n",
      "Train Epoch: 25 [16640/54000 (31%)] Loss: -299934.250000\n",
      "Train Epoch: 25 [20736/54000 (38%)] Loss: -211420.531250\n",
      "Train Epoch: 25 [24832/54000 (46%)] Loss: -297449.281250\n",
      "Train Epoch: 25 [28928/54000 (54%)] Loss: -350588.000000\n",
      "Train Epoch: 25 [33024/54000 (61%)] Loss: -163619.531250\n",
      "Train Epoch: 25 [37120/54000 (69%)] Loss: -306354.843750\n",
      "Train Epoch: 25 [41216/54000 (76%)] Loss: -356319.687500\n",
      "Train Epoch: 25 [45312/54000 (84%)] Loss: -306212.437500\n",
      "Train Epoch: 25 [49408/54000 (91%)] Loss: -358277.812500\n",
      "    epoch          : 25\n",
      "    loss           : -260559.06610576922\n",
      "    val_loss       : -266440.29704589845\n",
      "Train Epoch: 26 [256/54000 (0%)] Loss: -277062.593750\n",
      "Train Epoch: 26 [4352/54000 (8%)] Loss: -346497.937500\n",
      "Train Epoch: 26 [8448/54000 (16%)] Loss: -209826.765625\n",
      "Train Epoch: 26 [12544/54000 (23%)] Loss: -174462.656250\n",
      "Train Epoch: 26 [16640/54000 (31%)] Loss: -309563.000000\n",
      "Train Epoch: 26 [20736/54000 (38%)] Loss: -258833.609375\n",
      "Train Epoch: 26 [24832/54000 (46%)] Loss: -207955.218750\n",
      "Train Epoch: 26 [28928/54000 (54%)] Loss: -228223.609375\n",
      "Train Epoch: 26 [33024/54000 (61%)] Loss: -201120.062500\n",
      "Train Epoch: 26 [37120/54000 (69%)] Loss: -228554.093750\n",
      "Train Epoch: 26 [41216/54000 (76%)] Loss: -270451.968750\n",
      "Train Epoch: 26 [45312/54000 (84%)] Loss: -307042.812500\n",
      "Train Epoch: 26 [49408/54000 (91%)] Loss: -224384.890625\n",
      "    epoch          : 26\n",
      "    loss           : -261875.7735126202\n",
      "    val_loss       : -268474.12473144534\n",
      "Train Epoch: 27 [256/54000 (0%)] Loss: -345335.218750\n",
      "Train Epoch: 27 [4352/54000 (8%)] Loss: -234358.453125\n",
      "Train Epoch: 27 [8448/54000 (16%)] Loss: -218000.062500\n",
      "Train Epoch: 27 [12544/54000 (23%)] Loss: -307702.062500\n",
      "Train Epoch: 27 [16640/54000 (31%)] Loss: -325251.937500\n",
      "Train Epoch: 27 [20736/54000 (38%)] Loss: -171025.890625\n",
      "Train Epoch: 27 [24832/54000 (46%)] Loss: -231056.750000\n",
      "Train Epoch: 27 [28928/54000 (54%)] Loss: -168774.781250\n",
      "Train Epoch: 27 [33024/54000 (61%)] Loss: -259001.828125\n",
      "Train Epoch: 27 [37120/54000 (69%)] Loss: -279838.937500\n",
      "Train Epoch: 27 [41216/54000 (76%)] Loss: -214296.750000\n",
      "Train Epoch: 27 [45312/54000 (84%)] Loss: -228900.500000\n",
      "Train Epoch: 27 [49408/54000 (91%)] Loss: -239720.062500\n",
      "    epoch          : 27\n",
      "    loss           : -262655.77659254806\n",
      "    val_loss       : -269465.4300048828\n",
      "Train Epoch: 28 [256/54000 (0%)] Loss: -198160.890625\n",
      "Train Epoch: 28 [4352/54000 (8%)] Loss: -361879.281250\n",
      "Train Epoch: 28 [8448/54000 (16%)] Loss: -210825.125000\n",
      "Train Epoch: 28 [12544/54000 (23%)] Loss: -356597.687500\n",
      "Train Epoch: 28 [16640/54000 (31%)] Loss: -302438.500000\n",
      "Train Epoch: 28 [20736/54000 (38%)] Loss: -234043.906250\n",
      "Train Epoch: 28 [24832/54000 (46%)] Loss: -153400.062500\n",
      "Train Epoch: 28 [28928/54000 (54%)] Loss: -212358.406250\n",
      "Train Epoch: 28 [33024/54000 (61%)] Loss: -344422.687500\n",
      "Train Epoch: 28 [37120/54000 (69%)] Loss: -353661.437500\n",
      "Train Epoch: 28 [41216/54000 (76%)] Loss: -208737.468750\n",
      "Train Epoch: 28 [45312/54000 (84%)] Loss: -237426.515625\n",
      "Train Epoch: 28 [49408/54000 (91%)] Loss: -205573.000000\n",
      "    epoch          : 28\n",
      "    loss           : -263246.70778245194\n",
      "    val_loss       : -268086.6369140625\n",
      "Train Epoch: 29 [256/54000 (0%)] Loss: -232387.218750\n",
      "Train Epoch: 29 [4352/54000 (8%)] Loss: -356941.750000\n",
      "Train Epoch: 29 [8448/54000 (16%)] Loss: -232479.421875\n",
      "Train Epoch: 29 [12544/54000 (23%)] Loss: -232857.187500\n",
      "Train Epoch: 29 [16640/54000 (31%)] Loss: -194538.687500\n",
      "Train Epoch: 29 [20736/54000 (38%)] Loss: -264943.781250\n",
      "Train Epoch: 29 [24832/54000 (46%)] Loss: -193740.062500\n",
      "Train Epoch: 29 [28928/54000 (54%)] Loss: -236742.843750\n",
      "Train Epoch: 29 [33024/54000 (61%)] Loss: -362331.562500\n",
      "Train Epoch: 29 [37120/54000 (69%)] Loss: -274345.593750\n",
      "Train Epoch: 29 [41216/54000 (76%)] Loss: -181115.296875\n",
      "Train Epoch: 29 [45312/54000 (84%)] Loss: -199480.781250\n",
      "Train Epoch: 29 [49408/54000 (91%)] Loss: -351321.031250\n",
      "    epoch          : 29\n",
      "    loss           : -264060.75060096156\n",
      "    val_loss       : -270123.7171875\n",
      "Train Epoch: 30 [256/54000 (0%)] Loss: -358133.687500\n",
      "Train Epoch: 30 [4352/54000 (8%)] Loss: -346591.968750\n",
      "Train Epoch: 30 [8448/54000 (16%)] Loss: -348003.437500\n",
      "Train Epoch: 30 [12544/54000 (23%)] Loss: -202240.937500\n",
      "Train Epoch: 30 [16640/54000 (31%)] Loss: -354963.781250\n",
      "Train Epoch: 30 [20736/54000 (38%)] Loss: -232937.046875\n",
      "Train Epoch: 30 [24832/54000 (46%)] Loss: -204891.531250\n",
      "Train Epoch: 30 [28928/54000 (54%)] Loss: -216809.140625\n",
      "Train Epoch: 30 [33024/54000 (61%)] Loss: -305015.781250\n",
      "Train Epoch: 30 [37120/54000 (69%)] Loss: -213362.593750\n",
      "Train Epoch: 30 [41216/54000 (76%)] Loss: -241644.359375\n",
      "Train Epoch: 30 [45312/54000 (84%)] Loss: -203911.968750\n",
      "Train Epoch: 30 [49408/54000 (91%)] Loss: -361496.687500\n",
      "    epoch          : 30\n",
      "    loss           : -268326.8218149039\n",
      "    val_loss       : -272107.0739746094\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0702_141658/checkpoint-epoch30.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 31 [256/54000 (0%)] Loss: -347421.875000\n",
      "Train Epoch: 31 [4352/54000 (8%)] Loss: -306788.750000\n",
      "Train Epoch: 31 [8448/54000 (16%)] Loss: -336809.812500\n",
      "Train Epoch: 31 [12544/54000 (23%)] Loss: -314424.468750\n",
      "Train Epoch: 31 [16640/54000 (31%)] Loss: -236188.218750\n",
      "Train Epoch: 31 [20736/54000 (38%)] Loss: -351256.843750\n",
      "Train Epoch: 31 [24832/54000 (46%)] Loss: -208301.828125\n",
      "Train Epoch: 31 [28928/54000 (54%)] Loss: -228178.031250\n",
      "Train Epoch: 31 [33024/54000 (61%)] Loss: -199322.062500\n",
      "Train Epoch: 31 [37120/54000 (69%)] Loss: -328744.750000\n",
      "Train Epoch: 31 [41216/54000 (76%)] Loss: -357039.937500\n",
      "Train Epoch: 31 [45312/54000 (84%)] Loss: -238902.031250\n",
      "Train Epoch: 31 [49408/54000 (91%)] Loss: -208027.296875\n",
      "    epoch          : 31\n",
      "    loss           : -268078.8445763221\n",
      "    val_loss       : -273501.29626464844\n",
      "Train Epoch: 32 [256/54000 (0%)] Loss: -360375.906250\n",
      "Train Epoch: 32 [4352/54000 (8%)] Loss: -198895.406250\n",
      "Train Epoch: 32 [8448/54000 (16%)] Loss: -325143.687500\n",
      "Train Epoch: 32 [12544/54000 (23%)] Loss: -195516.343750\n",
      "Train Epoch: 32 [16640/54000 (31%)] Loss: -234093.531250\n",
      "Train Epoch: 32 [20736/54000 (38%)] Loss: -362447.125000\n",
      "Train Epoch: 32 [24832/54000 (46%)] Loss: -208145.437500\n",
      "Train Epoch: 32 [28928/54000 (54%)] Loss: -342269.312500\n",
      "Train Epoch: 32 [33024/54000 (61%)] Loss: -203003.687500\n",
      "Train Epoch: 32 [37120/54000 (69%)] Loss: -308521.687500\n",
      "Train Epoch: 32 [41216/54000 (76%)] Loss: -363765.843750\n",
      "Train Epoch: 32 [45312/54000 (84%)] Loss: -309694.000000\n",
      "Train Epoch: 32 [49408/54000 (91%)] Loss: -206628.234375\n",
      "    epoch          : 32\n",
      "    loss           : -269129.87462439906\n",
      "    val_loss       : -272726.1684814453\n",
      "Train Epoch: 33 [256/54000 (0%)] Loss: -201296.281250\n",
      "Train Epoch: 33 [4352/54000 (8%)] Loss: -241126.531250\n",
      "Train Epoch: 33 [8448/54000 (16%)] Loss: -353742.593750\n",
      "Train Epoch: 33 [12544/54000 (23%)] Loss: -188789.171875\n",
      "Train Epoch: 33 [16640/54000 (31%)] Loss: -354409.375000\n",
      "Train Epoch: 33 [20736/54000 (38%)] Loss: -314300.250000\n",
      "Train Epoch: 33 [24832/54000 (46%)] Loss: -211941.000000\n",
      "Train Epoch: 33 [28928/54000 (54%)] Loss: -326707.125000\n",
      "Train Epoch: 33 [33024/54000 (61%)] Loss: -238780.062500\n",
      "Train Epoch: 33 [37120/54000 (69%)] Loss: -312088.031250\n",
      "Train Epoch: 33 [41216/54000 (76%)] Loss: -362503.312500\n",
      "Train Epoch: 33 [45312/54000 (84%)] Loss: -240192.625000\n",
      "Train Epoch: 33 [49408/54000 (91%)] Loss: -242850.781250\n",
      "    epoch          : 33\n",
      "    loss           : -272422.22596153844\n",
      "    val_loss       : -275161.2837402344\n",
      "Train Epoch: 34 [256/54000 (0%)] Loss: -362180.687500\n",
      "Train Epoch: 34 [4352/54000 (8%)] Loss: -310441.843750\n",
      "Train Epoch: 34 [8448/54000 (16%)] Loss: -242486.531250\n",
      "Train Epoch: 34 [12544/54000 (23%)] Loss: -202354.171875\n",
      "Train Epoch: 34 [16640/54000 (31%)] Loss: -356331.687500\n",
      "Train Epoch: 34 [20736/54000 (38%)] Loss: -214176.156250\n",
      "Train Epoch: 34 [24832/54000 (46%)] Loss: -351262.375000\n",
      "Train Epoch: 34 [28928/54000 (54%)] Loss: -361727.062500\n",
      "Train Epoch: 34 [33024/54000 (61%)] Loss: -212431.812500\n",
      "Train Epoch: 34 [37120/54000 (69%)] Loss: -237165.750000\n",
      "Train Epoch: 34 [41216/54000 (76%)] Loss: -279323.281250\n",
      "Train Epoch: 34 [45312/54000 (84%)] Loss: -237270.859375\n",
      "Train Epoch: 34 [49408/54000 (91%)] Loss: -348010.312500\n",
      "    epoch          : 34\n",
      "    loss           : -273108.0576171875\n",
      "    val_loss       : -275073.9974121094\n",
      "Train Epoch: 35 [256/54000 (0%)] Loss: -309970.937500\n",
      "Train Epoch: 35 [4352/54000 (8%)] Loss: -240132.625000\n",
      "Train Epoch: 35 [8448/54000 (16%)] Loss: -365082.156250\n",
      "Train Epoch: 35 [12544/54000 (23%)] Loss: -352934.218750\n",
      "Train Epoch: 35 [16640/54000 (31%)] Loss: -204790.078125\n",
      "Train Epoch: 35 [20736/54000 (38%)] Loss: -360168.968750\n",
      "Train Epoch: 35 [24832/54000 (46%)] Loss: -236623.312500\n",
      "Train Epoch: 35 [28928/54000 (54%)] Loss: -287404.062500\n",
      "Train Epoch: 35 [33024/54000 (61%)] Loss: -355641.750000\n",
      "Train Epoch: 35 [37120/54000 (69%)] Loss: -313192.500000\n",
      "Train Epoch: 35 [41216/54000 (76%)] Loss: -360295.906250\n",
      "Train Epoch: 35 [45312/54000 (84%)] Loss: -357827.812500\n",
      "Train Epoch: 35 [49408/54000 (91%)] Loss: -210980.531250\n",
      "    epoch          : 35\n",
      "    loss           : -274063.4794170673\n",
      "    val_loss       : -276284.9041748047\n",
      "Train Epoch: 36 [256/54000 (0%)] Loss: -363058.562500\n",
      "Train Epoch: 36 [4352/54000 (8%)] Loss: -352973.218750\n",
      "Train Epoch: 36 [8448/54000 (16%)] Loss: -281269.437500\n",
      "Train Epoch: 36 [12544/54000 (23%)] Loss: -333170.562500\n",
      "Train Epoch: 36 [16640/54000 (31%)] Loss: -309322.000000\n",
      "Train Epoch: 36 [20736/54000 (38%)] Loss: -204598.906250\n",
      "Train Epoch: 36 [24832/54000 (46%)] Loss: -209197.812500\n",
      "Train Epoch: 36 [28928/54000 (54%)] Loss: -209926.625000\n",
      "Train Epoch: 36 [33024/54000 (61%)] Loss: -336505.187500\n",
      "Train Epoch: 36 [37120/54000 (69%)] Loss: -362253.406250\n",
      "Train Epoch: 36 [41216/54000 (76%)] Loss: -364234.312500\n",
      "Train Epoch: 36 [45312/54000 (84%)] Loss: -330993.687500\n",
      "Train Epoch: 36 [49408/54000 (91%)] Loss: -205653.703125\n",
      "    epoch          : 36\n",
      "    loss           : -273798.7110126202\n",
      "    val_loss       : -277700.8797607422\n",
      "Train Epoch: 37 [256/54000 (0%)] Loss: -360741.062500\n",
      "Train Epoch: 37 [4352/54000 (8%)] Loss: -315660.937500\n",
      "Train Epoch: 37 [8448/54000 (16%)] Loss: -215167.937500\n",
      "Train Epoch: 37 [12544/54000 (23%)] Loss: -350136.875000\n",
      "Train Epoch: 37 [16640/54000 (31%)] Loss: -207239.281250\n",
      "Train Epoch: 37 [20736/54000 (38%)] Loss: -364046.562500\n",
      "Train Epoch: 37 [24832/54000 (46%)] Loss: -277562.718750\n",
      "Train Epoch: 37 [28928/54000 (54%)] Loss: -358397.437500\n",
      "Train Epoch: 37 [33024/54000 (61%)] Loss: -231583.750000\n",
      "Train Epoch: 37 [37120/54000 (69%)] Loss: -208156.562500\n",
      "Train Epoch: 37 [41216/54000 (76%)] Loss: -211062.968750\n",
      "Train Epoch: 37 [45312/54000 (84%)] Loss: -231594.515625\n",
      "Train Epoch: 37 [49408/54000 (91%)] Loss: -221033.281250\n",
      "    epoch          : 37\n",
      "    loss           : -274729.5029296875\n",
      "    val_loss       : -278253.75458984374\n",
      "Train Epoch: 38 [256/54000 (0%)] Loss: -238767.421875\n",
      "Train Epoch: 38 [4352/54000 (8%)] Loss: -325540.250000\n",
      "Train Epoch: 38 [8448/54000 (16%)] Loss: -355185.562500\n",
      "Train Epoch: 38 [12544/54000 (23%)] Loss: -220677.250000\n",
      "Train Epoch: 38 [16640/54000 (31%)] Loss: -314445.062500\n",
      "Train Epoch: 38 [20736/54000 (38%)] Loss: -238937.593750\n",
      "Train Epoch: 38 [24832/54000 (46%)] Loss: -290831.906250\n",
      "Train Epoch: 38 [28928/54000 (54%)] Loss: -354883.593750\n",
      "Train Epoch: 38 [33024/54000 (61%)] Loss: -341706.187500\n",
      "Train Epoch: 38 [37120/54000 (69%)] Loss: -214380.171875\n",
      "Train Epoch: 38 [41216/54000 (76%)] Loss: -214378.843750\n",
      "Train Epoch: 38 [45312/54000 (84%)] Loss: -331760.656250\n",
      "Train Epoch: 38 [49408/54000 (91%)] Loss: -355283.906250\n",
      "    epoch          : 38\n",
      "    loss           : -277758.65234375\n",
      "    val_loss       : -278075.3675048828\n",
      "Train Epoch: 39 [256/54000 (0%)] Loss: -242647.375000\n",
      "Train Epoch: 39 [4352/54000 (8%)] Loss: -286145.062500\n",
      "Train Epoch: 39 [8448/54000 (16%)] Loss: -243568.968750\n",
      "Train Epoch: 39 [12544/54000 (23%)] Loss: -220464.890625\n",
      "Train Epoch: 39 [16640/54000 (31%)] Loss: -314583.531250\n",
      "Train Epoch: 39 [20736/54000 (38%)] Loss: -236268.750000\n",
      "Train Epoch: 39 [24832/54000 (46%)] Loss: -312410.937500\n",
      "Train Epoch: 39 [28928/54000 (54%)] Loss: -358773.031250\n",
      "Train Epoch: 39 [33024/54000 (61%)] Loss: -212651.718750\n",
      "Train Epoch: 39 [37120/54000 (69%)] Loss: -364064.437500\n",
      "Train Epoch: 39 [41216/54000 (76%)] Loss: -237909.046875\n",
      "Train Epoch: 39 [45312/54000 (84%)] Loss: -353036.000000\n",
      "Train Epoch: 39 [49408/54000 (91%)] Loss: -221332.000000\n",
      "    epoch          : 39\n",
      "    loss           : -276665.19215745194\n",
      "    val_loss       : -274819.7175537109\n",
      "Train Epoch: 40 [256/54000 (0%)] Loss: -287254.062500\n",
      "Train Epoch: 40 [4352/54000 (8%)] Loss: -216168.406250\n",
      "Train Epoch: 40 [8448/54000 (16%)] Loss: -218708.031250\n",
      "Train Epoch: 40 [12544/54000 (23%)] Loss: -211888.828125\n",
      "Train Epoch: 40 [16640/54000 (31%)] Loss: -343119.937500\n",
      "Train Epoch: 40 [20736/54000 (38%)] Loss: -363800.218750\n",
      "Train Epoch: 40 [24832/54000 (46%)] Loss: -248123.593750\n",
      "Train Epoch: 40 [28928/54000 (54%)] Loss: -240323.015625\n",
      "Train Epoch: 40 [33024/54000 (61%)] Loss: -311216.062500\n",
      "Train Epoch: 40 [37120/54000 (69%)] Loss: -354780.375000\n",
      "Train Epoch: 40 [41216/54000 (76%)] Loss: -221244.140625\n",
      "Train Epoch: 40 [45312/54000 (84%)] Loss: -340699.656250\n",
      "Train Epoch: 40 [49408/54000 (91%)] Loss: -221883.515625\n",
      "    epoch          : 40\n",
      "    loss           : -279788.3125\n",
      "    val_loss       : -272054.7506347656\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0702_141658/checkpoint-epoch40.pth ...\n",
      "Train Epoch: 41 [256/54000 (0%)] Loss: -220465.031250\n",
      "Train Epoch: 41 [4352/54000 (8%)] Loss: -366508.406250\n",
      "Train Epoch: 41 [8448/54000 (16%)] Loss: -287578.125000\n",
      "Train Epoch: 41 [12544/54000 (23%)] Loss: -208251.859375\n",
      "Train Epoch: 41 [16640/54000 (31%)] Loss: -276921.718750\n",
      "Train Epoch: 41 [20736/54000 (38%)] Loss: -356421.156250\n",
      "Train Epoch: 41 [24832/54000 (46%)] Loss: -213027.906250\n",
      "Train Epoch: 41 [28928/54000 (54%)] Loss: -354985.250000\n",
      "Train Epoch: 41 [33024/54000 (61%)] Loss: -219793.281250\n",
      "Train Epoch: 41 [37120/54000 (69%)] Loss: -291367.781250\n",
      "Train Epoch: 41 [41216/54000 (76%)] Loss: -240385.031250\n",
      "Train Epoch: 41 [45312/54000 (84%)] Loss: -292984.968750\n",
      "Train Epoch: 41 [49408/54000 (91%)] Loss: -218071.437500\n",
      "    epoch          : 41\n",
      "    loss           : -280260.58037860575\n",
      "    val_loss       : -276982.5484863281\n",
      "Train Epoch: 42 [256/54000 (0%)] Loss: -244659.796875\n",
      "Train Epoch: 42 [4352/54000 (8%)] Loss: -366267.000000\n",
      "Train Epoch: 42 [8448/54000 (16%)] Loss: -294450.937500\n",
      "Train Epoch: 42 [12544/54000 (23%)] Loss: -251159.906250\n",
      "Train Epoch: 42 [16640/54000 (31%)] Loss: -369228.312500\n",
      "Train Epoch: 42 [20736/54000 (38%)] Loss: -211320.562500\n",
      "Train Epoch: 42 [24832/54000 (46%)] Loss: -311796.281250\n",
      "Train Epoch: 42 [28928/54000 (54%)] Loss: -357874.437500\n",
      "Train Epoch: 42 [33024/54000 (61%)] Loss: -203704.093750\n",
      "Train Epoch: 42 [37120/54000 (69%)] Loss: -284596.687500\n",
      "Train Epoch: 42 [41216/54000 (76%)] Loss: -211483.140625\n",
      "Train Epoch: 42 [45312/54000 (84%)] Loss: -361096.437500\n",
      "Train Epoch: 42 [49408/54000 (91%)] Loss: -367610.875000\n",
      "    epoch          : 42\n",
      "    loss           : -278916.34607872594\n",
      "    val_loss       : -271562.3831298828\n",
      "Train Epoch: 43 [256/54000 (0%)] Loss: -293998.062500\n",
      "Train Epoch: 43 [4352/54000 (8%)] Loss: -224888.968750\n",
      "Train Epoch: 43 [8448/54000 (16%)] Loss: -290668.125000\n",
      "Train Epoch: 43 [12544/54000 (23%)] Loss: -218735.156250\n",
      "Train Epoch: 43 [16640/54000 (31%)] Loss: -244070.062500\n",
      "Train Epoch: 43 [20736/54000 (38%)] Loss: -357121.656250\n",
      "Train Epoch: 43 [24832/54000 (46%)] Loss: -342579.718750\n",
      "Train Epoch: 43 [28928/54000 (54%)] Loss: -219995.921875\n",
      "Train Epoch: 43 [33024/54000 (61%)] Loss: -322476.343750\n",
      "Train Epoch: 43 [37120/54000 (69%)] Loss: -242562.031250\n",
      "Train Epoch: 43 [41216/54000 (76%)] Loss: -225308.937500\n",
      "Train Epoch: 43 [45312/54000 (84%)] Loss: -348698.531250\n",
      "Train Epoch: 43 [49408/54000 (91%)] Loss: -215646.656250\n",
      "    epoch          : 43\n",
      "    loss           : -281960.490234375\n",
      "    val_loss       : -277119.6439453125\n",
      "Train Epoch: 44 [256/54000 (0%)] Loss: -322793.437500\n",
      "Train Epoch: 44 [4352/54000 (8%)] Loss: -367122.343750\n",
      "Train Epoch: 44 [8448/54000 (16%)] Loss: -357389.468750\n",
      "Train Epoch: 44 [12544/54000 (23%)] Loss: -335616.875000\n",
      "Train Epoch: 44 [16640/54000 (31%)] Loss: -358793.812500\n",
      "Train Epoch: 44 [20736/54000 (38%)] Loss: -243641.625000\n",
      "Train Epoch: 44 [24832/54000 (46%)] Loss: -216095.000000\n",
      "Train Epoch: 44 [28928/54000 (54%)] Loss: -224489.562500\n",
      "Train Epoch: 44 [33024/54000 (61%)] Loss: -285480.812500\n",
      "Train Epoch: 44 [37120/54000 (69%)] Loss: -331356.843750\n",
      "Train Epoch: 44 [41216/54000 (76%)] Loss: -314548.625000\n",
      "Train Epoch: 44 [45312/54000 (84%)] Loss: -247690.390625\n",
      "Train Epoch: 44 [49408/54000 (91%)] Loss: -325454.312500\n",
      "    epoch          : 44\n",
      "    loss           : -280773.49639423075\n",
      "    val_loss       : -273271.10322265624\n",
      "Train Epoch: 45 [256/54000 (0%)] Loss: -220978.343750\n",
      "Train Epoch: 45 [4352/54000 (8%)] Loss: -223689.875000\n",
      "Train Epoch: 45 [8448/54000 (16%)] Loss: -341237.687500\n",
      "Train Epoch: 45 [12544/54000 (23%)] Loss: -249774.156250\n",
      "Train Epoch: 45 [16640/54000 (31%)] Loss: -293810.000000\n",
      "Train Epoch: 45 [20736/54000 (38%)] Loss: -363984.375000\n",
      "Train Epoch: 45 [24832/54000 (46%)] Loss: -225553.875000\n",
      "Train Epoch: 45 [28928/54000 (54%)] Loss: -225172.125000\n",
      "Train Epoch: 45 [33024/54000 (61%)] Loss: -240842.906250\n",
      "Train Epoch: 45 [37120/54000 (69%)] Loss: -328261.468750\n",
      "Train Epoch: 45 [41216/54000 (76%)] Loss: -369326.250000\n",
      "Train Epoch: 45 [45312/54000 (84%)] Loss: -321425.968750\n",
      "Train Epoch: 45 [49408/54000 (91%)] Loss: -215234.437500\n",
      "    epoch          : 45\n",
      "    loss           : -283863.24278846156\n",
      "    val_loss       : -267846.35671386716\n",
      "Train Epoch: 46 [256/54000 (0%)] Loss: -252466.671875\n",
      "Train Epoch: 46 [4352/54000 (8%)] Loss: -219886.250000\n",
      "Train Epoch: 46 [8448/54000 (16%)] Loss: -317366.312500\n",
      "Train Epoch: 46 [12544/54000 (23%)] Loss: -250840.687500\n",
      "Train Epoch: 46 [16640/54000 (31%)] Loss: -367562.281250\n",
      "Train Epoch: 46 [20736/54000 (38%)] Loss: -249318.687500\n",
      "Train Epoch: 46 [24832/54000 (46%)] Loss: -219585.156250\n",
      "Train Epoch: 46 [28928/54000 (54%)] Loss: -213526.546875\n",
      "Train Epoch: 46 [33024/54000 (61%)] Loss: -296757.593750\n",
      "Train Epoch: 46 [37120/54000 (69%)] Loss: -221953.437500\n",
      "Train Epoch: 46 [41216/54000 (76%)] Loss: -218769.078125\n",
      "Train Epoch: 46 [45312/54000 (84%)] Loss: -325766.250000\n",
      "Train Epoch: 46 [49408/54000 (91%)] Loss: -224341.687500\n",
      "    epoch          : 46\n",
      "    loss           : -284388.0567908654\n",
      "    val_loss       : -221944.933203125\n",
      "Train Epoch: 47 [256/54000 (0%)] Loss: -227397.109375\n",
      "Train Epoch: 47 [4352/54000 (8%)] Loss: -246023.421875\n",
      "Train Epoch: 47 [8448/54000 (16%)] Loss: -325389.843750\n",
      "Train Epoch: 47 [12544/54000 (23%)] Loss: -291531.750000\n",
      "Train Epoch: 47 [16640/54000 (31%)] Loss: -366697.562500\n",
      "Train Epoch: 47 [20736/54000 (38%)] Loss: -253937.937500\n",
      "Train Epoch: 47 [24832/54000 (46%)] Loss: -245813.093750\n",
      "Train Epoch: 47 [28928/54000 (54%)] Loss: -241540.718750\n",
      "Train Epoch: 47 [33024/54000 (61%)] Loss: -321595.750000\n",
      "Train Epoch: 47 [37120/54000 (69%)] Loss: -360579.062500\n",
      "Train Epoch: 47 [41216/54000 (76%)] Loss: -252462.468750\n",
      "Train Epoch: 47 [45312/54000 (84%)] Loss: -366109.125000\n",
      "Train Epoch: 47 [49408/54000 (91%)] Loss: -221993.656250\n",
      "    epoch          : 47\n",
      "    loss           : -284406.7207782452\n",
      "    val_loss       : -269980.6039794922\n",
      "Train Epoch: 48 [256/54000 (0%)] Loss: -216143.343750\n",
      "Train Epoch: 48 [4352/54000 (8%)] Loss: -324592.656250\n",
      "Train Epoch: 48 [8448/54000 (16%)] Loss: -218586.421875\n",
      "Train Epoch: 48 [12544/54000 (23%)] Loss: -324118.000000\n",
      "Train Epoch: 48 [16640/54000 (31%)] Loss: -224463.203125\n",
      "Train Epoch: 48 [20736/54000 (38%)] Loss: -368842.906250\n",
      "Train Epoch: 48 [24832/54000 (46%)] Loss: -242246.031250\n",
      "Train Epoch: 48 [28928/54000 (54%)] Loss: -220918.406250\n",
      "Train Epoch: 48 [33024/54000 (61%)] Loss: -248829.937500\n",
      "Train Epoch: 48 [37120/54000 (69%)] Loss: -255586.312500\n",
      "Train Epoch: 48 [41216/54000 (76%)] Loss: -359817.187500\n",
      "Train Epoch: 48 [45312/54000 (84%)] Loss: -314803.406250\n",
      "Train Epoch: 48 [49408/54000 (91%)] Loss: -365899.625000\n",
      "    epoch          : 48\n",
      "    loss           : -285614.00262920675\n",
      "    val_loss       : -232552.43513183593\n",
      "Train Epoch: 49 [256/54000 (0%)] Loss: -223103.812500\n",
      "Train Epoch: 49 [4352/54000 (8%)] Loss: -223735.015625\n",
      "Train Epoch: 49 [8448/54000 (16%)] Loss: -250444.390625\n",
      "Train Epoch: 49 [12544/54000 (23%)] Loss: -217456.062500\n",
      "Train Epoch: 49 [16640/54000 (31%)] Loss: -224039.031250\n",
      "Train Epoch: 49 [20736/54000 (38%)] Loss: -210890.828125\n",
      "Train Epoch: 49 [24832/54000 (46%)] Loss: -346037.875000\n",
      "Train Epoch: 49 [28928/54000 (54%)] Loss: -363871.125000\n",
      "Train Epoch: 49 [33024/54000 (61%)] Loss: -295412.562500\n",
      "Train Epoch: 49 [37120/54000 (69%)] Loss: -366728.281250\n",
      "Train Epoch: 49 [41216/54000 (76%)] Loss: -207422.437500\n",
      "Train Epoch: 49 [45312/54000 (84%)] Loss: -291313.750000\n",
      "Train Epoch: 49 [49408/54000 (91%)] Loss: -358036.593750\n",
      "    epoch          : 49\n",
      "    loss           : -285117.1886268029\n",
      "    val_loss       : -276357.0581787109\n",
      "Train Epoch: 50 [256/54000 (0%)] Loss: -249330.578125\n",
      "Train Epoch: 50 [4352/54000 (8%)] Loss: -370863.375000\n",
      "Train Epoch: 50 [8448/54000 (16%)] Loss: -246789.937500\n",
      "Train Epoch: 50 [12544/54000 (23%)] Loss: -224656.000000\n",
      "Train Epoch: 50 [16640/54000 (31%)] Loss: -360281.500000\n",
      "Train Epoch: 50 [20736/54000 (38%)] Loss: -213197.250000\n",
      "Train Epoch: 50 [24832/54000 (46%)] Loss: -233506.031250\n",
      "Train Epoch: 50 [28928/54000 (54%)] Loss: -222165.890625\n",
      "Train Epoch: 50 [33024/54000 (61%)] Loss: -344583.687500\n",
      "Train Epoch: 50 [37120/54000 (69%)] Loss: -254878.812500\n",
      "Train Epoch: 50 [41216/54000 (76%)] Loss: -366987.937500\n",
      "Train Epoch: 50 [45312/54000 (84%)] Loss: -252588.062500\n",
      "Train Epoch: 50 [49408/54000 (91%)] Loss: -222163.968750\n",
      "    epoch          : 50\n",
      "    loss           : -284010.8231670673\n",
      "    val_loss       : -281313.187109375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0702_141658/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [256/54000 (0%)] Loss: -219956.000000\n",
      "Train Epoch: 51 [4352/54000 (8%)] Loss: -327804.437500\n",
      "Train Epoch: 51 [8448/54000 (16%)] Loss: -217425.656250\n",
      "Train Epoch: 51 [12544/54000 (23%)] Loss: -369756.437500\n",
      "Train Epoch: 51 [16640/54000 (31%)] Loss: -295439.906250\n",
      "Train Epoch: 51 [20736/54000 (38%)] Loss: -358539.781250\n",
      "Train Epoch: 51 [24832/54000 (46%)] Loss: -229376.906250\n",
      "Train Epoch: 51 [28928/54000 (54%)] Loss: -365424.375000\n",
      "Train Epoch: 51 [33024/54000 (61%)] Loss: -224996.406250\n",
      "Train Epoch: 51 [37120/54000 (69%)] Loss: -374909.562500\n",
      "Train Epoch: 51 [41216/54000 (76%)] Loss: -370522.062500\n",
      "Train Epoch: 51 [45312/54000 (84%)] Loss: -219635.000000\n",
      "Train Epoch: 51 [49408/54000 (91%)] Loss: -255314.843750\n",
      "    epoch          : 51\n",
      "    loss           : -287148.86192908656\n",
      "    val_loss       : -276057.2572753906\n",
      "Train Epoch: 52 [256/54000 (0%)] Loss: -324168.750000\n",
      "Train Epoch: 52 [4352/54000 (8%)] Loss: -351194.625000\n",
      "Train Epoch: 52 [8448/54000 (16%)] Loss: -340203.968750\n",
      "Train Epoch: 52 [12544/54000 (23%)] Loss: -346592.875000\n",
      "Train Epoch: 52 [16640/54000 (31%)] Loss: -361874.218750\n",
      "Train Epoch: 52 [20736/54000 (38%)] Loss: -366582.812500\n",
      "Train Epoch: 52 [24832/54000 (46%)] Loss: -225779.296875\n",
      "Train Epoch: 52 [28928/54000 (54%)] Loss: -369411.281250\n",
      "Train Epoch: 52 [33024/54000 (61%)] Loss: -226474.484375\n",
      "Train Epoch: 52 [37120/54000 (69%)] Loss: -257690.312500\n",
      "Train Epoch: 52 [41216/54000 (76%)] Loss: -368290.000000\n",
      "Train Epoch: 52 [45312/54000 (84%)] Loss: -220822.390625\n",
      "Train Epoch: 52 [49408/54000 (91%)] Loss: -340704.875000\n",
      "    epoch          : 52\n",
      "    loss           : -287221.4448617789\n",
      "    val_loss       : -268197.2056640625\n",
      "Train Epoch: 53 [256/54000 (0%)] Loss: -226860.437500\n",
      "Train Epoch: 53 [4352/54000 (8%)] Loss: -294092.781250\n",
      "Train Epoch: 53 [8448/54000 (16%)] Loss: -370124.531250\n",
      "Train Epoch: 53 [12544/54000 (23%)] Loss: -346254.750000\n",
      "Train Epoch: 53 [16640/54000 (31%)] Loss: -228774.000000\n",
      "Train Epoch: 53 [20736/54000 (38%)] Loss: -365662.531250\n",
      "Train Epoch: 53 [24832/54000 (46%)] Loss: -294144.125000\n",
      "Train Epoch: 53 [28928/54000 (54%)] Loss: -370975.625000\n",
      "Train Epoch: 53 [33024/54000 (61%)] Loss: -344900.687500\n",
      "Train Epoch: 53 [37120/54000 (69%)] Loss: -297114.687500\n",
      "Train Epoch: 53 [41216/54000 (76%)] Loss: -219296.968750\n",
      "Train Epoch: 53 [45312/54000 (84%)] Loss: -292191.437500\n",
      "Train Epoch: 53 [49408/54000 (91%)] Loss: -371678.812500\n",
      "    epoch          : 53\n",
      "    loss           : -287559.22678786056\n",
      "    val_loss       : -266712.4654785156\n",
      "Train Epoch: 54 [256/54000 (0%)] Loss: -372460.343750\n",
      "Train Epoch: 54 [4352/54000 (8%)] Loss: -218860.687500\n",
      "Train Epoch: 54 [8448/54000 (16%)] Loss: -328667.187500\n",
      "Train Epoch: 54 [12544/54000 (23%)] Loss: -341491.843750\n",
      "Train Epoch: 54 [16640/54000 (31%)] Loss: -354925.312500\n",
      "Train Epoch: 54 [20736/54000 (38%)] Loss: -362258.562500\n",
      "Train Epoch: 54 [24832/54000 (46%)] Loss: -327661.937500\n",
      "Train Epoch: 54 [28928/54000 (54%)] Loss: -220748.812500\n",
      "Train Epoch: 54 [33024/54000 (61%)] Loss: -371105.093750\n",
      "Train Epoch: 54 [37120/54000 (69%)] Loss: -229666.828125\n",
      "Train Epoch: 54 [41216/54000 (76%)] Loss: -359982.718750\n",
      "Train Epoch: 54 [45312/54000 (84%)] Loss: -220823.343750\n",
      "Train Epoch: 54 [49408/54000 (91%)] Loss: -362158.281250\n",
      "    epoch          : 54\n",
      "    loss           : -287516.8426983173\n",
      "    val_loss       : -254319.32705078126\n",
      "Train Epoch: 55 [256/54000 (0%)] Loss: -299879.187500\n",
      "Train Epoch: 55 [4352/54000 (8%)] Loss: -221883.296875\n",
      "Train Epoch: 55 [8448/54000 (16%)] Loss: -375192.812500\n",
      "Train Epoch: 55 [12544/54000 (23%)] Loss: -223491.750000\n",
      "Train Epoch: 55 [16640/54000 (31%)] Loss: -225665.546875\n",
      "Train Epoch: 55 [20736/54000 (38%)] Loss: -370960.843750\n",
      "Train Epoch: 55 [24832/54000 (46%)] Loss: -297914.437500\n",
      "Train Epoch: 55 [28928/54000 (54%)] Loss: -227046.796875\n",
      "Train Epoch: 55 [33024/54000 (61%)] Loss: -358813.843750\n",
      "Train Epoch: 55 [37120/54000 (69%)] Loss: -230525.671875\n",
      "Train Epoch: 55 [41216/54000 (76%)] Loss: -370734.718750\n",
      "Train Epoch: 55 [45312/54000 (84%)] Loss: -366899.125000\n",
      "Train Epoch: 55 [49408/54000 (91%)] Loss: -256490.203125\n",
      "    epoch          : 55\n",
      "    loss           : -288485.0532602164\n",
      "    val_loss       : -243887.473828125\n",
      "Train Epoch: 56 [256/54000 (0%)] Loss: -329006.968750\n",
      "Train Epoch: 56 [4352/54000 (8%)] Loss: -369960.812500\n",
      "Train Epoch: 56 [8448/54000 (16%)] Loss: -222831.859375\n",
      "Train Epoch: 56 [12544/54000 (23%)] Loss: -252927.343750\n",
      "Train Epoch: 56 [16640/54000 (31%)] Loss: -339287.468750\n",
      "Train Epoch: 56 [20736/54000 (38%)] Loss: -234219.093750\n",
      "Train Epoch: 56 [24832/54000 (46%)] Loss: -296385.125000\n",
      "Train Epoch: 56 [28928/54000 (54%)] Loss: -233021.031250\n",
      "Train Epoch: 56 [33024/54000 (61%)] Loss: -369868.625000\n",
      "Train Epoch: 56 [37120/54000 (69%)] Loss: -228877.625000\n",
      "Train Epoch: 56 [41216/54000 (76%)] Loss: -233288.875000\n",
      "Train Epoch: 56 [45312/54000 (84%)] Loss: -370834.000000\n",
      "Train Epoch: 56 [49408/54000 (91%)] Loss: -252857.203125\n",
      "    epoch          : 56\n",
      "    loss           : -289131.6186899039\n",
      "    val_loss       : -272613.1797607422\n",
      "Train Epoch: 57 [256/54000 (0%)] Loss: -226692.125000\n",
      "Train Epoch: 57 [4352/54000 (8%)] Loss: -327677.781250\n",
      "Train Epoch: 57 [8448/54000 (16%)] Loss: -373035.312500\n",
      "Train Epoch: 57 [12544/54000 (23%)] Loss: -236141.750000\n",
      "Train Epoch: 57 [16640/54000 (31%)] Loss: -296877.125000\n",
      "Train Epoch: 57 [20736/54000 (38%)] Loss: -229053.156250\n",
      "Train Epoch: 57 [24832/54000 (46%)] Loss: -374003.406250\n",
      "Train Epoch: 57 [28928/54000 (54%)] Loss: -225632.875000\n",
      "Train Epoch: 57 [33024/54000 (61%)] Loss: -249037.187500\n",
      "Train Epoch: 57 [37120/54000 (69%)] Loss: -349098.406250\n",
      "Train Epoch: 57 [41216/54000 (76%)] Loss: -227059.984375\n",
      "Train Epoch: 57 [45312/54000 (84%)] Loss: -370413.000000\n",
      "Train Epoch: 57 [49408/54000 (91%)] Loss: -232019.546875\n",
      "    epoch          : 57\n",
      "    loss           : -289461.9125600961\n",
      "    val_loss       : -286323.86650390626\n",
      "Train Epoch: 58 [256/54000 (0%)] Loss: -254145.593750\n",
      "Train Epoch: 58 [4352/54000 (8%)] Loss: -259982.796875\n",
      "Train Epoch: 58 [8448/54000 (16%)] Loss: -368051.250000\n",
      "Train Epoch: 58 [12544/54000 (23%)] Loss: -359845.750000\n",
      "Train Epoch: 58 [16640/54000 (31%)] Loss: -290877.687500\n",
      "Train Epoch: 58 [20736/54000 (38%)] Loss: -246023.812500\n",
      "Train Epoch: 58 [24832/54000 (46%)] Loss: -346719.125000\n",
      "Train Epoch: 58 [28928/54000 (54%)] Loss: -327964.968750\n",
      "Train Epoch: 58 [33024/54000 (61%)] Loss: -226991.906250\n",
      "Train Epoch: 58 [37120/54000 (69%)] Loss: -324159.875000\n",
      "Train Epoch: 58 [41216/54000 (76%)] Loss: -232570.203125\n",
      "Train Epoch: 58 [45312/54000 (84%)] Loss: -294471.375000\n",
      "Train Epoch: 58 [49408/54000 (91%)] Loss: -257809.281250\n",
      "    epoch          : 58\n",
      "    loss           : -287937.71026141825\n",
      "    val_loss       : -274851.5084472656\n",
      "Train Epoch: 59 [256/54000 (0%)] Loss: -227573.531250\n",
      "Train Epoch: 59 [4352/54000 (8%)] Loss: -229507.218750\n",
      "Train Epoch: 59 [8448/54000 (16%)] Loss: -333916.125000\n",
      "Train Epoch: 59 [12544/54000 (23%)] Loss: -346946.031250\n",
      "Train Epoch: 59 [16640/54000 (31%)] Loss: -230062.562500\n",
      "Train Epoch: 59 [20736/54000 (38%)] Loss: -255607.000000\n",
      "Train Epoch: 59 [24832/54000 (46%)] Loss: -224972.781250\n",
      "Train Epoch: 59 [28928/54000 (54%)] Loss: -375408.625000\n",
      "Train Epoch: 59 [33024/54000 (61%)] Loss: -348219.218750\n",
      "Train Epoch: 59 [37120/54000 (69%)] Loss: -303947.781250\n",
      "Train Epoch: 59 [41216/54000 (76%)] Loss: -266013.781250\n",
      "Train Epoch: 59 [45312/54000 (84%)] Loss: -373088.812500\n",
      "Train Epoch: 59 [49408/54000 (91%)] Loss: -323957.250000\n",
      "    epoch          : 59\n",
      "    loss           : -290759.9608623798\n",
      "    val_loss       : -280451.4087158203\n",
      "Train Epoch: 60 [256/54000 (0%)] Loss: -369038.218750\n",
      "Train Epoch: 60 [4352/54000 (8%)] Loss: -326968.937500\n",
      "Train Epoch: 60 [8448/54000 (16%)] Loss: -325681.687500\n",
      "Train Epoch: 60 [12544/54000 (23%)] Loss: -348745.187500\n",
      "Train Epoch: 60 [16640/54000 (31%)] Loss: -326475.437500\n",
      "Train Epoch: 60 [20736/54000 (38%)] Loss: -368593.062500\n",
      "Train Epoch: 60 [24832/54000 (46%)] Loss: -233637.281250\n",
      "Train Epoch: 60 [28928/54000 (54%)] Loss: -328068.062500\n",
      "Train Epoch: 60 [33024/54000 (61%)] Loss: -256341.687500\n",
      "Train Epoch: 60 [37120/54000 (69%)] Loss: -346973.656250\n",
      "Train Epoch: 60 [41216/54000 (76%)] Loss: -233928.656250\n",
      "Train Epoch: 60 [45312/54000 (84%)] Loss: -226948.937500\n",
      "Train Epoch: 60 [49408/54000 (91%)] Loss: -250467.031250\n",
      "    epoch          : 60\n",
      "    loss           : -290012.41774338944\n",
      "    val_loss       : -273592.7876708984\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0702_141658/checkpoint-epoch60.pth ...\n",
      "Train Epoch: 61 [256/54000 (0%)] Loss: -347358.843750\n",
      "Train Epoch: 61 [4352/54000 (8%)] Loss: -301991.343750\n",
      "Train Epoch: 61 [8448/54000 (16%)] Loss: -231976.203125\n",
      "Train Epoch: 61 [12544/54000 (23%)] Loss: -227002.734375\n",
      "Train Epoch: 61 [16640/54000 (31%)] Loss: -365088.000000\n",
      "Train Epoch: 61 [20736/54000 (38%)] Loss: -373624.906250\n",
      "Train Epoch: 61 [24832/54000 (46%)] Loss: -324694.937500\n",
      "Train Epoch: 61 [28928/54000 (54%)] Loss: -361674.906250\n",
      "Train Epoch: 61 [33024/54000 (61%)] Loss: -256945.687500\n",
      "Train Epoch: 61 [37120/54000 (69%)] Loss: -229331.296875\n",
      "Train Epoch: 61 [41216/54000 (76%)] Loss: -230526.437500\n",
      "Train Epoch: 61 [45312/54000 (84%)] Loss: -254334.281250\n",
      "Train Epoch: 61 [49408/54000 (91%)] Loss: -227127.906250\n",
      "    epoch          : 61\n",
      "    loss           : -290918.8731971154\n",
      "    val_loss       : -259490.51430664063\n",
      "Train Epoch: 62 [256/54000 (0%)] Loss: -298477.750000\n",
      "Train Epoch: 62 [4352/54000 (8%)] Loss: -255641.937500\n",
      "Train Epoch: 62 [8448/54000 (16%)] Loss: -226461.843750\n",
      "Train Epoch: 62 [12544/54000 (23%)] Loss: -253442.921875\n",
      "Train Epoch: 62 [16640/54000 (31%)] Loss: -371608.593750\n",
      "Train Epoch: 62 [20736/54000 (38%)] Loss: -299123.875000\n",
      "Train Epoch: 62 [24832/54000 (46%)] Loss: -247514.187500\n",
      "Train Epoch: 62 [28928/54000 (54%)] Loss: -226582.984375\n",
      "Train Epoch: 62 [33024/54000 (61%)] Loss: -331180.968750\n",
      "Train Epoch: 62 [37120/54000 (69%)] Loss: -368195.375000\n",
      "Train Epoch: 62 [41216/54000 (76%)] Loss: -369067.125000\n",
      "Train Epoch: 62 [45312/54000 (84%)] Loss: -334595.875000\n",
      "Train Epoch: 62 [49408/54000 (91%)] Loss: -225755.437500\n",
      "    epoch          : 62\n",
      "    loss           : -291322.6293569711\n",
      "    val_loss       : -190973.28564453125\n",
      "Train Epoch: 63 [256/54000 (0%)] Loss: -373958.312500\n",
      "Train Epoch: 63 [4352/54000 (8%)] Loss: -352888.250000\n",
      "Train Epoch: 63 [8448/54000 (16%)] Loss: -299067.687500\n",
      "Train Epoch: 63 [12544/54000 (23%)] Loss: -370971.687500\n",
      "Train Epoch: 63 [16640/54000 (31%)] Loss: -233554.515625\n",
      "Train Epoch: 63 [20736/54000 (38%)] Loss: -259783.093750\n",
      "Train Epoch: 63 [24832/54000 (46%)] Loss: -326693.687500\n",
      "Train Epoch: 63 [28928/54000 (54%)] Loss: -301489.125000\n",
      "Train Epoch: 63 [33024/54000 (61%)] Loss: -222641.281250\n",
      "Train Epoch: 63 [37120/54000 (69%)] Loss: -260329.312500\n",
      "Train Epoch: 63 [41216/54000 (76%)] Loss: -231802.000000\n",
      "Train Epoch: 63 [45312/54000 (84%)] Loss: -364952.937500\n",
      "Train Epoch: 63 [49408/54000 (91%)] Loss: -260426.062500\n",
      "    epoch          : 63\n",
      "    loss           : -292554.66090745194\n",
      "    val_loss       : -279027.5349609375\n",
      "Train Epoch: 64 [256/54000 (0%)] Loss: -224438.250000\n",
      "Train Epoch: 64 [4352/54000 (8%)] Loss: -337842.343750\n",
      "Train Epoch: 64 [8448/54000 (16%)] Loss: -228942.203125\n",
      "Train Epoch: 64 [12544/54000 (23%)] Loss: -233459.921875\n",
      "Train Epoch: 64 [16640/54000 (31%)] Loss: -373525.375000\n",
      "Train Epoch: 64 [20736/54000 (38%)] Loss: -235435.171875\n",
      "Train Epoch: 64 [24832/54000 (46%)] Loss: -334150.468750\n",
      "Train Epoch: 64 [28928/54000 (54%)] Loss: -235722.406250\n",
      "Train Epoch: 64 [33024/54000 (61%)] Loss: -328578.062500\n",
      "Train Epoch: 64 [37120/54000 (69%)] Loss: -374253.937500\n",
      "Train Epoch: 64 [41216/54000 (76%)] Loss: -225690.859375\n",
      "Train Epoch: 64 [45312/54000 (84%)] Loss: -372059.718750\n",
      "Train Epoch: 64 [49408/54000 (91%)] Loss: -228472.250000\n",
      "    epoch          : 64\n",
      "    loss           : -293012.1073467548\n",
      "    val_loss       : -213742.9760253906\n",
      "Train Epoch: 65 [256/54000 (0%)] Loss: -222520.687500\n",
      "Train Epoch: 65 [4352/54000 (8%)] Loss: -303971.312500\n",
      "Train Epoch: 65 [8448/54000 (16%)] Loss: -255684.343750\n",
      "Train Epoch: 65 [12544/54000 (23%)] Loss: -343894.750000\n",
      "Train Epoch: 65 [16640/54000 (31%)] Loss: -340556.750000\n",
      "Train Epoch: 65 [20736/54000 (38%)] Loss: -236324.812500\n",
      "Train Epoch: 65 [24832/54000 (46%)] Loss: -368124.125000\n",
      "Train Epoch: 65 [28928/54000 (54%)] Loss: -373387.125000\n",
      "Train Epoch: 65 [33024/54000 (61%)] Loss: -350593.500000\n",
      "Train Epoch: 65 [37120/54000 (69%)] Loss: -217123.312500\n",
      "Train Epoch: 65 [41216/54000 (76%)] Loss: -372577.000000\n",
      "Train Epoch: 65 [45312/54000 (84%)] Loss: -226478.406250\n",
      "Train Epoch: 65 [49408/54000 (91%)] Loss: -236765.718750\n",
      "    epoch          : 65\n",
      "    loss           : -291285.5741436298\n",
      "    val_loss       : -124927.43178710938\n",
      "Train Epoch: 66 [256/54000 (0%)] Loss: -264102.000000\n",
      "Train Epoch: 66 [4352/54000 (8%)] Loss: -349539.625000\n",
      "Train Epoch: 66 [8448/54000 (16%)] Loss: -226795.031250\n",
      "Train Epoch: 66 [12544/54000 (23%)] Loss: -303447.250000\n",
      "Train Epoch: 66 [16640/54000 (31%)] Loss: -254566.062500\n",
      "Train Epoch: 66 [20736/54000 (38%)] Loss: -229339.765625\n",
      "Train Epoch: 66 [24832/54000 (46%)] Loss: -349633.812500\n",
      "Train Epoch: 66 [28928/54000 (54%)] Loss: -254397.937500\n",
      "Train Epoch: 66 [33024/54000 (61%)] Loss: -336024.125000\n",
      "Train Epoch: 66 [37120/54000 (69%)] Loss: -304384.937500\n",
      "Train Epoch: 66 [41216/54000 (76%)] Loss: -373017.937500\n",
      "Train Epoch: 66 [45312/54000 (84%)] Loss: -330366.937500\n",
      "Train Epoch: 66 [49408/54000 (91%)] Loss: -225589.296875\n",
      "    epoch          : 66\n",
      "    loss           : -293844.0115685096\n",
      "    val_loss       : -278305.4173095703\n",
      "Train Epoch: 67 [256/54000 (0%)] Loss: -354434.906250\n",
      "Train Epoch: 67 [4352/54000 (8%)] Loss: -220906.875000\n",
      "Train Epoch: 67 [8448/54000 (16%)] Loss: -332861.187500\n",
      "Train Epoch: 67 [12544/54000 (23%)] Loss: -235844.125000\n",
      "Train Epoch: 67 [16640/54000 (31%)] Loss: -251862.218750\n",
      "Train Epoch: 67 [20736/54000 (38%)] Loss: -347428.375000\n",
      "Train Epoch: 67 [24832/54000 (46%)] Loss: -330158.062500\n",
      "Train Epoch: 67 [28928/54000 (54%)] Loss: -297279.812500\n",
      "Train Epoch: 67 [33024/54000 (61%)] Loss: -333060.781250\n",
      "Train Epoch: 67 [37120/54000 (69%)] Loss: -372403.625000\n",
      "Train Epoch: 67 [41216/54000 (76%)] Loss: -371459.468750\n",
      "Train Epoch: 67 [45312/54000 (84%)] Loss: -239579.359375\n",
      "Train Epoch: 67 [49408/54000 (91%)] Loss: -235220.609375\n",
      "    epoch          : 67\n",
      "    loss           : -293352.57196514425\n",
      "    val_loss       : -188216.652734375\n",
      "Train Epoch: 68 [256/54000 (0%)] Loss: -370054.125000\n",
      "Train Epoch: 68 [4352/54000 (8%)] Loss: -258960.968750\n",
      "Train Epoch: 68 [8448/54000 (16%)] Loss: -375032.062500\n",
      "Train Epoch: 68 [12544/54000 (23%)] Loss: -256870.000000\n",
      "Train Epoch: 68 [16640/54000 (31%)] Loss: -349754.125000\n",
      "Train Epoch: 68 [20736/54000 (38%)] Loss: -372308.281250\n",
      "Train Epoch: 68 [24832/54000 (46%)] Loss: -257854.734375\n",
      "Train Epoch: 68 [28928/54000 (54%)] Loss: -356433.656250\n",
      "Train Epoch: 68 [33024/54000 (61%)] Loss: -224421.109375\n",
      "Train Epoch: 68 [37120/54000 (69%)] Loss: -250791.968750\n",
      "Train Epoch: 68 [41216/54000 (76%)] Loss: -378754.250000\n",
      "Train Epoch: 68 [45312/54000 (84%)] Loss: -336030.093750\n",
      "Train Epoch: 68 [49408/54000 (91%)] Loss: -376678.468750\n",
      "    epoch          : 68\n",
      "    loss           : -294196.8993389423\n",
      "    val_loss       : -259480.3682861328\n",
      "Train Epoch: 69 [256/54000 (0%)] Loss: -372501.562500\n",
      "Train Epoch: 69 [4352/54000 (8%)] Loss: -307860.312500\n",
      "Train Epoch: 69 [8448/54000 (16%)] Loss: -261810.593750\n",
      "Train Epoch: 69 [12544/54000 (23%)] Loss: -231918.000000\n",
      "Train Epoch: 69 [16640/54000 (31%)] Loss: -229350.312500\n",
      "Train Epoch: 69 [20736/54000 (38%)] Loss: -372731.312500\n",
      "Train Epoch: 69 [24832/54000 (46%)] Loss: -237124.968750\n",
      "Train Epoch: 69 [28928/54000 (54%)] Loss: -256880.593750\n",
      "Train Epoch: 69 [33024/54000 (61%)] Loss: -346474.500000\n",
      "Train Epoch: 69 [37120/54000 (69%)] Loss: -372247.718750\n",
      "Train Epoch: 69 [41216/54000 (76%)] Loss: -238001.781250\n",
      "Train Epoch: 69 [45312/54000 (84%)] Loss: -256666.781250\n",
      "Train Epoch: 69 [49408/54000 (91%)] Loss: -377053.562500\n",
      "    epoch          : 69\n",
      "    loss           : -294904.35524338944\n",
      "    val_loss       : -178912.24631347656\n",
      "Train Epoch: 70 [256/54000 (0%)] Loss: -242724.968750\n",
      "Train Epoch: 70 [4352/54000 (8%)] Loss: -377626.625000\n",
      "Train Epoch: 70 [8448/54000 (16%)] Loss: -338373.687500\n",
      "Train Epoch: 70 [12544/54000 (23%)] Loss: -332755.625000\n",
      "Train Epoch: 70 [16640/54000 (31%)] Loss: -258375.734375\n",
      "Train Epoch: 70 [20736/54000 (38%)] Loss: -377642.593750\n",
      "Train Epoch: 70 [24832/54000 (46%)] Loss: -332520.656250\n",
      "Train Epoch: 70 [28928/54000 (54%)] Loss: -382077.406250\n",
      "Train Epoch: 70 [33024/54000 (61%)] Loss: -257573.531250\n",
      "Train Epoch: 70 [37120/54000 (69%)] Loss: -259561.296875\n",
      "Train Epoch: 70 [41216/54000 (76%)] Loss: -229946.812500\n",
      "Train Epoch: 70 [45312/54000 (84%)] Loss: -328326.843750\n",
      "Train Epoch: 70 [49408/54000 (91%)] Loss: -257875.968750\n",
      "    epoch          : 70\n",
      "    loss           : -295605.12770432694\n",
      "    val_loss       : -102075.4833984375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0702_141658/checkpoint-epoch70.pth ...\n",
      "Train Epoch: 71 [256/54000 (0%)] Loss: -253240.812500\n",
      "Train Epoch: 71 [4352/54000 (8%)] Loss: -337889.000000\n",
      "Train Epoch: 71 [8448/54000 (16%)] Loss: -356874.375000\n",
      "Train Epoch: 71 [12544/54000 (23%)] Loss: -370519.875000\n",
      "Train Epoch: 71 [16640/54000 (31%)] Loss: -238257.281250\n",
      "Train Epoch: 71 [20736/54000 (38%)] Loss: -245709.984375\n",
      "Train Epoch: 71 [24832/54000 (46%)] Loss: -353930.875000\n",
      "Train Epoch: 71 [28928/54000 (54%)] Loss: -376740.093750\n",
      "Train Epoch: 71 [33024/54000 (61%)] Loss: -370309.812500\n",
      "Train Epoch: 71 [37120/54000 (69%)] Loss: -237205.156250\n",
      "Train Epoch: 71 [41216/54000 (76%)] Loss: -235599.187500\n",
      "Train Epoch: 71 [45312/54000 (84%)] Loss: -234327.843750\n",
      "Train Epoch: 71 [49408/54000 (91%)] Loss: -239271.156250\n",
      "    epoch          : 71\n",
      "    loss           : -295579.8025841346\n",
      "    val_loss       : -262129.44350585938\n",
      "Train Epoch: 72 [256/54000 (0%)] Loss: -233392.203125\n",
      "Train Epoch: 72 [4352/54000 (8%)] Loss: -370383.437500\n",
      "Train Epoch: 72 [8448/54000 (16%)] Loss: -234612.593750\n",
      "Train Epoch: 72 [12544/54000 (23%)] Loss: -302766.531250\n",
      "Train Epoch: 72 [16640/54000 (31%)] Loss: -233684.187500\n",
      "Train Epoch: 72 [20736/54000 (38%)] Loss: -374065.531250\n",
      "Train Epoch: 72 [24832/54000 (46%)] Loss: -307417.468750\n",
      "Train Epoch: 72 [28928/54000 (54%)] Loss: -374861.625000\n",
      "Train Epoch: 72 [33024/54000 (61%)] Loss: -334209.437500\n",
      "Train Epoch: 72 [37120/54000 (69%)] Loss: -258785.015625\n",
      "Train Epoch: 72 [41216/54000 (76%)] Loss: -238081.781250\n",
      "Train Epoch: 72 [45312/54000 (84%)] Loss: -333370.468750\n",
      "Train Epoch: 72 [49408/54000 (91%)] Loss: -254743.125000\n",
      "    epoch          : 72\n",
      "    loss           : -296411.18487079325\n",
      "    val_loss       : -241660.18872070312\n",
      "Train Epoch: 73 [256/54000 (0%)] Loss: -373920.000000\n",
      "Train Epoch: 73 [4352/54000 (8%)] Loss: -255174.656250\n",
      "Train Epoch: 73 [8448/54000 (16%)] Loss: -306522.718750\n",
      "Train Epoch: 73 [12544/54000 (23%)] Loss: -235230.250000\n",
      "Train Epoch: 73 [16640/54000 (31%)] Loss: -232519.000000\n",
      "Train Epoch: 73 [20736/54000 (38%)] Loss: -234540.390625\n",
      "Train Epoch: 73 [24832/54000 (46%)] Loss: -376826.781250\n",
      "Train Epoch: 73 [28928/54000 (54%)] Loss: -263672.875000\n",
      "Train Epoch: 73 [33024/54000 (61%)] Loss: -264587.437500\n",
      "Train Epoch: 73 [37120/54000 (69%)] Loss: -375791.375000\n",
      "Train Epoch: 73 [41216/54000 (76%)] Loss: -232559.328125\n",
      "Train Epoch: 73 [45312/54000 (84%)] Loss: -224489.859375\n",
      "Train Epoch: 73 [49408/54000 (91%)] Loss: -261598.687500\n",
      "    epoch          : 73\n",
      "    loss           : -296415.5978064904\n",
      "    val_loss       : -258766.28310546876\n",
      "Train Epoch: 74 [256/54000 (0%)] Loss: -377245.500000\n",
      "Train Epoch: 74 [4352/54000 (8%)] Loss: -352098.062500\n",
      "Train Epoch: 74 [8448/54000 (16%)] Loss: -255232.593750\n",
      "Train Epoch: 74 [12544/54000 (23%)] Loss: -308494.968750\n",
      "Train Epoch: 74 [16640/54000 (31%)] Loss: -378768.750000\n",
      "Train Epoch: 74 [20736/54000 (38%)] Loss: -258339.187500\n",
      "Train Epoch: 74 [24832/54000 (46%)] Loss: -231834.125000\n",
      "Train Epoch: 74 [28928/54000 (54%)] Loss: -231307.156250\n",
      "Train Epoch: 74 [33024/54000 (61%)] Loss: -359651.125000\n",
      "Train Epoch: 74 [37120/54000 (69%)] Loss: -233015.031250\n",
      "Train Epoch: 74 [41216/54000 (76%)] Loss: -372871.718750\n",
      "Train Epoch: 74 [45312/54000 (84%)] Loss: -235118.156250\n",
      "Train Epoch: 74 [49408/54000 (91%)] Loss: -374019.937500\n",
      "    epoch          : 74\n",
      "    loss           : -297115.03545673075\n",
      "    val_loss       : -270341.3811035156\n",
      "Train Epoch: 75 [256/54000 (0%)] Loss: -379078.750000\n",
      "Train Epoch: 75 [4352/54000 (8%)] Loss: -305475.406250\n",
      "Train Epoch: 75 [8448/54000 (16%)] Loss: -264195.781250\n",
      "Train Epoch: 75 [12544/54000 (23%)] Loss: -306530.687500\n",
      "Train Epoch: 75 [16640/54000 (31%)] Loss: -349810.437500\n",
      "Train Epoch: 75 [20736/54000 (38%)] Loss: -262067.281250\n",
      "Train Epoch: 75 [24832/54000 (46%)] Loss: -231623.078125\n",
      "Train Epoch: 75 [28928/54000 (54%)] Loss: -234464.046875\n",
      "Train Epoch: 75 [33024/54000 (61%)] Loss: -327723.187500\n",
      "Train Epoch: 75 [37120/54000 (69%)] Loss: -374588.937500\n",
      "Train Epoch: 75 [41216/54000 (76%)] Loss: -234471.296875\n",
      "Train Epoch: 75 [45312/54000 (84%)] Loss: -263114.406250\n",
      "Train Epoch: 75 [49408/54000 (91%)] Loss: -229941.968750\n",
      "    epoch          : 75\n",
      "    loss           : -297313.9409555289\n",
      "    val_loss       : -247876.89404296875\n",
      "Train Epoch: 76 [256/54000 (0%)] Loss: -232755.796875\n",
      "Train Epoch: 76 [4352/54000 (8%)] Loss: -300134.062500\n",
      "Train Epoch: 76 [8448/54000 (16%)] Loss: -269531.125000\n",
      "Train Epoch: 76 [12544/54000 (23%)] Loss: -374739.812500\n",
      "Train Epoch: 76 [16640/54000 (31%)] Loss: -238104.062500\n",
      "Train Epoch: 76 [20736/54000 (38%)] Loss: -375967.562500\n",
      "Train Epoch: 76 [24832/54000 (46%)] Loss: -378097.562500\n",
      "Train Epoch: 76 [28928/54000 (54%)] Loss: -378429.656250\n",
      "Train Epoch: 76 [33024/54000 (61%)] Loss: -234696.171875\n",
      "Train Epoch: 76 [37120/54000 (69%)] Loss: -241226.437500\n",
      "Train Epoch: 76 [41216/54000 (76%)] Loss: -254171.515625\n",
      "Train Epoch: 76 [45312/54000 (84%)] Loss: -250772.484375\n",
      "Train Epoch: 76 [49408/54000 (91%)] Loss: -257389.843750\n",
      "    epoch          : 76\n",
      "    loss           : -296999.5028545673\n",
      "    val_loss       : -281481.2056640625\n",
      "Train Epoch: 77 [256/54000 (0%)] Loss: -228057.265625\n",
      "Train Epoch: 77 [4352/54000 (8%)] Loss: -229769.171875\n",
      "Train Epoch: 77 [8448/54000 (16%)] Loss: -263478.906250\n",
      "Train Epoch: 77 [12544/54000 (23%)] Loss: -262607.468750\n",
      "Train Epoch: 77 [16640/54000 (31%)] Loss: -341790.250000\n",
      "Train Epoch: 77 [20736/54000 (38%)] Loss: -374548.656250\n",
      "Train Epoch: 77 [24832/54000 (46%)] Loss: -260139.375000\n",
      "Train Epoch: 77 [28928/54000 (54%)] Loss: -334194.250000\n",
      "Train Epoch: 77 [33024/54000 (61%)] Loss: -237538.468750\n",
      "Train Epoch: 77 [37120/54000 (69%)] Loss: -233291.750000\n",
      "Train Epoch: 77 [41216/54000 (76%)] Loss: -237875.937500\n",
      "Train Epoch: 77 [45312/54000 (84%)] Loss: -357458.750000\n",
      "Train Epoch: 77 [49408/54000 (91%)] Loss: -261344.687500\n",
      "    epoch          : 77\n",
      "    loss           : -297717.6415264423\n",
      "    val_loss       : -241976.5623779297\n",
      "Train Epoch: 78 [256/54000 (0%)] Loss: -234645.609375\n",
      "Train Epoch: 78 [4352/54000 (8%)] Loss: -263638.562500\n",
      "Train Epoch: 78 [8448/54000 (16%)] Loss: -233794.937500\n",
      "Train Epoch: 78 [12544/54000 (23%)] Loss: -307157.968750\n",
      "Train Epoch: 78 [16640/54000 (31%)] Loss: -231176.656250\n",
      "Train Epoch: 78 [20736/54000 (38%)] Loss: -261594.062500\n",
      "Train Epoch: 78 [24832/54000 (46%)] Loss: -333518.968750\n",
      "Train Epoch: 78 [28928/54000 (54%)] Loss: -257548.500000\n",
      "Train Epoch: 78 [33024/54000 (61%)] Loss: -236945.218750\n",
      "Train Epoch: 78 [37120/54000 (69%)] Loss: -236024.078125\n",
      "Train Epoch: 78 [41216/54000 (76%)] Loss: -263503.375000\n",
      "Train Epoch: 78 [45312/54000 (84%)] Loss: -313522.437500\n",
      "Train Epoch: 78 [49408/54000 (91%)] Loss: -266252.062500\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    78: reducing learning rate of group 0 to 5.0000e-05.\n",
      "    epoch          : 78\n",
      "    loss           : -298270.9306640625\n",
      "    val_loss       : -115327.88125\n",
      "Train Epoch: 79 [256/54000 (0%)] Loss: -253938.640625\n",
      "Train Epoch: 79 [4352/54000 (8%)] Loss: -379452.906250\n",
      "Train Epoch: 79 [8448/54000 (16%)] Loss: -354987.156250\n",
      "Train Epoch: 79 [12544/54000 (23%)] Loss: -257796.156250\n",
      "Train Epoch: 79 [16640/54000 (31%)] Loss: -237945.656250\n",
      "Train Epoch: 79 [20736/54000 (38%)] Loss: -310115.031250\n",
      "Train Epoch: 79 [24832/54000 (46%)] Loss: -261909.218750\n",
      "Train Epoch: 79 [28928/54000 (54%)] Loss: -234373.390625\n",
      "Train Epoch: 79 [33024/54000 (61%)] Loss: -351967.312500\n",
      "Train Epoch: 79 [37120/54000 (69%)] Loss: -260383.921875\n",
      "Train Epoch: 79 [41216/54000 (76%)] Loss: -376612.000000\n",
      "Train Epoch: 79 [45312/54000 (84%)] Loss: -262923.625000\n",
      "Train Epoch: 79 [49408/54000 (91%)] Loss: -383137.218750\n",
      "    epoch          : 79\n",
      "    loss           : -298827.3426231971\n",
      "    val_loss       : -266401.0756347656\n",
      "Train Epoch: 80 [256/54000 (0%)] Loss: -240204.312500\n",
      "Train Epoch: 80 [4352/54000 (8%)] Loss: -379923.437500\n",
      "Train Epoch: 80 [8448/54000 (16%)] Loss: -262568.687500\n",
      "Train Epoch: 80 [12544/54000 (23%)] Loss: -232205.843750\n",
      "Train Epoch: 80 [16640/54000 (31%)] Loss: -380094.937500\n",
      "Train Epoch: 80 [20736/54000 (38%)] Loss: -378242.500000\n",
      "Train Epoch: 80 [24832/54000 (46%)] Loss: -310673.625000\n",
      "Train Epoch: 80 [28928/54000 (54%)] Loss: -242129.500000\n",
      "Train Epoch: 80 [33024/54000 (61%)] Loss: -231654.468750\n",
      "Train Epoch: 80 [37120/54000 (69%)] Loss: -262362.656250\n",
      "Train Epoch: 80 [41216/54000 (76%)] Loss: -257230.593750\n",
      "Train Epoch: 80 [45312/54000 (84%)] Loss: -305073.000000\n",
      "Train Epoch: 80 [49408/54000 (91%)] Loss: -374717.093750\n",
      "    epoch          : 80\n",
      "    loss           : -299058.74969951925\n",
      "    val_loss       : -242835.169921875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0702_141658/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [256/54000 (0%)] Loss: -262985.937500\n",
      "Train Epoch: 81 [4352/54000 (8%)] Loss: -378575.437500\n",
      "Train Epoch: 81 [8448/54000 (16%)] Loss: -376937.000000\n",
      "Train Epoch: 81 [12544/54000 (23%)] Loss: -239030.312500\n",
      "Train Epoch: 81 [16640/54000 (31%)] Loss: -260452.593750\n",
      "Train Epoch: 81 [20736/54000 (38%)] Loss: -379932.093750\n",
      "Train Epoch: 81 [24832/54000 (46%)] Loss: -260240.156250\n",
      "Train Epoch: 81 [28928/54000 (54%)] Loss: -380137.562500\n",
      "Train Epoch: 81 [33024/54000 (61%)] Loss: -268447.562500\n",
      "Train Epoch: 81 [37120/54000 (69%)] Loss: -238267.562500\n",
      "Train Epoch: 81 [41216/54000 (76%)] Loss: -238756.250000\n",
      "Train Epoch: 81 [45312/54000 (84%)] Loss: -377000.562500\n",
      "Train Epoch: 81 [49408/54000 (91%)] Loss: -237421.843750\n",
      "    epoch          : 81\n",
      "    loss           : -299228.2166466346\n",
      "    val_loss       : -63905.9009765625\n",
      "Train Epoch: 82 [256/54000 (0%)] Loss: -336261.437500\n",
      "Train Epoch: 82 [4352/54000 (8%)] Loss: -236425.406250\n",
      "Train Epoch: 82 [8448/54000 (16%)] Loss: -228474.000000\n",
      "Train Epoch: 82 [12544/54000 (23%)] Loss: -258297.250000\n",
      "Train Epoch: 82 [16640/54000 (31%)] Loss: -352222.437500\n",
      "Train Epoch: 82 [20736/54000 (38%)] Loss: -377954.312500\n",
      "Train Epoch: 82 [24832/54000 (46%)] Loss: -309741.375000\n",
      "Train Epoch: 82 [28928/54000 (54%)] Loss: -232783.609375\n",
      "Train Epoch: 82 [33024/54000 (61%)] Loss: -232778.750000\n",
      "Train Epoch: 82 [37120/54000 (69%)] Loss: -259247.734375\n",
      "Train Epoch: 82 [41216/54000 (76%)] Loss: -265179.000000\n",
      "Train Epoch: 82 [45312/54000 (84%)] Loss: -333568.562500\n",
      "Train Epoch: 82 [49408/54000 (91%)] Loss: -231054.812500\n",
      "    epoch          : 82\n",
      "    loss           : -299366.99864783656\n",
      "    val_loss       : -249004.92541503906\n",
      "Train Epoch: 83 [256/54000 (0%)] Loss: -363831.687500\n",
      "Train Epoch: 83 [4352/54000 (8%)] Loss: -381310.031250\n",
      "Train Epoch: 83 [8448/54000 (16%)] Loss: -312989.562500\n",
      "Train Epoch: 83 [12544/54000 (23%)] Loss: -235895.843750\n",
      "Train Epoch: 83 [16640/54000 (31%)] Loss: -336151.843750\n",
      "Train Epoch: 83 [20736/54000 (38%)] Loss: -235924.125000\n",
      "Train Epoch: 83 [24832/54000 (46%)] Loss: -378192.312500\n",
      "Train Epoch: 83 [28928/54000 (54%)] Loss: -379585.562500\n",
      "Train Epoch: 83 [33024/54000 (61%)] Loss: -353681.281250\n",
      "Train Epoch: 83 [37120/54000 (69%)] Loss: -336202.000000\n",
      "Train Epoch: 83 [41216/54000 (76%)] Loss: -377033.343750\n",
      "Train Epoch: 83 [45312/54000 (84%)] Loss: -312491.437500\n",
      "Train Epoch: 83 [49408/54000 (91%)] Loss: -335684.125000\n",
      "    epoch          : 83\n",
      "    loss           : -299546.20582932694\n",
      "    val_loss       : -288495.6853515625\n",
      "Train Epoch: 84 [256/54000 (0%)] Loss: -334268.000000\n",
      "Train Epoch: 84 [4352/54000 (8%)] Loss: -242309.718750\n",
      "Train Epoch: 84 [8448/54000 (16%)] Loss: -380369.625000\n",
      "Train Epoch: 84 [12544/54000 (23%)] Loss: -309651.562500\n",
      "Train Epoch: 84 [16640/54000 (31%)] Loss: -375923.750000\n",
      "Train Epoch: 84 [20736/54000 (38%)] Loss: -265681.093750\n",
      "Train Epoch: 84 [24832/54000 (46%)] Loss: -264188.000000\n",
      "Train Epoch: 84 [28928/54000 (54%)] Loss: -240300.781250\n",
      "Train Epoch: 84 [33024/54000 (61%)] Loss: -238376.718750\n",
      "Train Epoch: 84 [37120/54000 (69%)] Loss: -365640.843750\n",
      "Train Epoch: 84 [41216/54000 (76%)] Loss: -381012.500000\n",
      "Train Epoch: 84 [45312/54000 (84%)] Loss: -235606.312500\n",
      "Train Epoch: 84 [49408/54000 (91%)] Loss: -377420.750000\n",
      "    epoch          : 84\n",
      "    loss           : -299660.76787860575\n",
      "    val_loss       : -282023.6415039062\n",
      "Train Epoch: 85 [256/54000 (0%)] Loss: -241166.421875\n",
      "Train Epoch: 85 [4352/54000 (8%)] Loss: -262089.390625\n",
      "Train Epoch: 85 [8448/54000 (16%)] Loss: -376808.531250\n",
      "Train Epoch: 85 [12544/54000 (23%)] Loss: -336354.812500\n",
      "Train Epoch: 85 [16640/54000 (31%)] Loss: -236436.656250\n",
      "Train Epoch: 85 [20736/54000 (38%)] Loss: -258966.406250\n",
      "Train Epoch: 85 [24832/54000 (46%)] Loss: -310333.500000\n",
      "Train Epoch: 85 [28928/54000 (54%)] Loss: -239067.250000\n",
      "Train Epoch: 85 [33024/54000 (61%)] Loss: -245859.468750\n",
      "Train Epoch: 85 [37120/54000 (69%)] Loss: -238555.843750\n",
      "Train Epoch: 85 [41216/54000 (76%)] Loss: -266913.062500\n",
      "Train Epoch: 85 [45312/54000 (84%)] Loss: -380835.718750\n",
      "Train Epoch: 85 [49408/54000 (91%)] Loss: -264317.375000\n",
      "    epoch          : 85\n",
      "    loss           : -299834.8435997596\n",
      "    val_loss       : -269287.474609375\n",
      "Train Epoch: 86 [256/54000 (0%)] Loss: -341732.687500\n",
      "Train Epoch: 86 [4352/54000 (8%)] Loss: -262684.000000\n",
      "Train Epoch: 86 [8448/54000 (16%)] Loss: -377155.750000\n",
      "Train Epoch: 86 [12544/54000 (23%)] Loss: -239181.062500\n",
      "Train Epoch: 86 [16640/54000 (31%)] Loss: -232022.437500\n",
      "Train Epoch: 86 [20736/54000 (38%)] Loss: -378846.875000\n",
      "Train Epoch: 86 [24832/54000 (46%)] Loss: -338717.093750\n",
      "Train Epoch: 86 [28928/54000 (54%)] Loss: -311390.218750\n",
      "Train Epoch: 86 [33024/54000 (61%)] Loss: -365277.687500\n",
      "Train Epoch: 86 [37120/54000 (69%)] Loss: -264832.281250\n",
      "Train Epoch: 86 [41216/54000 (76%)] Loss: -242491.968750\n",
      "Train Epoch: 86 [45312/54000 (84%)] Loss: -265025.781250\n",
      "Train Epoch: 86 [49408/54000 (91%)] Loss: -383277.656250\n",
      "    epoch          : 86\n",
      "    loss           : -300070.60682091344\n",
      "    val_loss       : -269754.27846679685\n",
      "Train Epoch: 87 [256/54000 (0%)] Loss: -235962.656250\n",
      "Train Epoch: 87 [4352/54000 (8%)] Loss: -378517.625000\n",
      "Train Epoch: 87 [8448/54000 (16%)] Loss: -239046.875000\n",
      "Train Epoch: 87 [12544/54000 (23%)] Loss: -236342.968750\n",
      "Train Epoch: 87 [16640/54000 (31%)] Loss: -308048.750000\n",
      "Train Epoch: 87 [20736/54000 (38%)] Loss: -242265.703125\n",
      "Train Epoch: 87 [24832/54000 (46%)] Loss: -262828.531250\n",
      "Train Epoch: 87 [28928/54000 (54%)] Loss: -241828.750000\n",
      "Train Epoch: 87 [33024/54000 (61%)] Loss: -382624.375000\n",
      "Train Epoch: 87 [37120/54000 (69%)] Loss: -228068.312500\n",
      "Train Epoch: 87 [41216/54000 (76%)] Loss: -234510.171875\n",
      "Train Epoch: 87 [45312/54000 (84%)] Loss: -355931.750000\n",
      "Train Epoch: 87 [49408/54000 (91%)] Loss: -229855.140625\n",
      "    epoch          : 87\n",
      "    loss           : -300162.19553786056\n",
      "    val_loss       : -222208.6965576172\n",
      "Train Epoch: 88 [256/54000 (0%)] Loss: -239965.078125\n",
      "Train Epoch: 88 [4352/54000 (8%)] Loss: -344056.687500\n",
      "Train Epoch: 88 [8448/54000 (16%)] Loss: -264909.843750\n",
      "Train Epoch: 88 [12544/54000 (23%)] Loss: -314887.000000\n",
      "Train Epoch: 88 [16640/54000 (31%)] Loss: -381542.500000\n",
      "Train Epoch: 88 [20736/54000 (38%)] Loss: -241114.687500\n",
      "Train Epoch: 88 [24832/54000 (46%)] Loss: -363154.281250\n",
      "Train Epoch: 88 [28928/54000 (54%)] Loss: -377582.875000\n",
      "Train Epoch: 88 [33024/54000 (61%)] Loss: -232709.875000\n",
      "Train Epoch: 88 [37120/54000 (69%)] Loss: -262764.687500\n",
      "Train Epoch: 88 [41216/54000 (76%)] Loss: -236707.468750\n",
      "Train Epoch: 88 [45312/54000 (84%)] Loss: -264563.750000\n",
      "Train Epoch: 88 [49408/54000 (91%)] Loss: -232288.343750\n",
      "    epoch          : 88\n",
      "    loss           : -300332.60479266825\n",
      "    val_loss       : -279624.6646484375\n",
      "Train Epoch: 89 [256/54000 (0%)] Loss: -260913.828125\n",
      "Train Epoch: 89 [4352/54000 (8%)] Loss: -229865.375000\n",
      "Train Epoch: 89 [8448/54000 (16%)] Loss: -237005.234375\n",
      "Train Epoch: 89 [12544/54000 (23%)] Loss: -236618.109375\n",
      "Train Epoch: 89 [16640/54000 (31%)] Loss: -312144.250000\n",
      "Train Epoch: 89 [20736/54000 (38%)] Loss: -311871.875000\n",
      "Train Epoch: 89 [24832/54000 (46%)] Loss: -359910.093750\n",
      "Train Epoch: 89 [28928/54000 (54%)] Loss: -264909.875000\n",
      "Train Epoch: 89 [33024/54000 (61%)] Loss: -242876.687500\n",
      "Train Epoch: 89 [37120/54000 (69%)] Loss: -334620.000000\n",
      "Train Epoch: 89 [41216/54000 (76%)] Loss: -256643.484375\n",
      "Train Epoch: 89 [45312/54000 (84%)] Loss: -311544.625000\n",
      "Train Epoch: 89 [49408/54000 (91%)] Loss: -380120.937500\n",
      "    epoch          : 89\n",
      "    loss           : -300439.32774939906\n",
      "    val_loss       : -166411.264453125\n",
      "Train Epoch: 90 [256/54000 (0%)] Loss: -375670.593750\n",
      "Train Epoch: 90 [4352/54000 (8%)] Loss: -262171.968750\n",
      "Train Epoch: 90 [8448/54000 (16%)] Loss: -384435.937500\n",
      "Train Epoch: 90 [12544/54000 (23%)] Loss: -235451.765625\n",
      "Train Epoch: 90 [16640/54000 (31%)] Loss: -237097.312500\n",
      "Train Epoch: 90 [20736/54000 (38%)] Loss: -264339.062500\n",
      "Train Epoch: 90 [24832/54000 (46%)] Loss: -266442.093750\n",
      "Train Epoch: 90 [28928/54000 (54%)] Loss: -236456.031250\n",
      "Train Epoch: 90 [33024/54000 (61%)] Loss: -237337.609375\n",
      "Train Epoch: 90 [37120/54000 (69%)] Loss: -239483.734375\n",
      "Train Epoch: 90 [41216/54000 (76%)] Loss: -381470.406250\n",
      "Train Epoch: 90 [45312/54000 (84%)] Loss: -380534.125000\n",
      "Train Epoch: 90 [49408/54000 (91%)] Loss: -375377.187500\n",
      "    epoch          : 90\n",
      "    loss           : -300513.32189002406\n",
      "    val_loss       : -271590.93959960935\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0702_141658/checkpoint-epoch90.pth ...\n",
      "Train Epoch: 91 [256/54000 (0%)] Loss: -335263.625000\n",
      "Train Epoch: 91 [4352/54000 (8%)] Loss: -263151.750000\n",
      "Train Epoch: 91 [8448/54000 (16%)] Loss: -339982.375000\n",
      "Train Epoch: 91 [12544/54000 (23%)] Loss: -357569.437500\n",
      "Train Epoch: 91 [16640/54000 (31%)] Loss: -241208.593750\n",
      "Train Epoch: 91 [20736/54000 (38%)] Loss: -268522.062500\n",
      "Train Epoch: 91 [24832/54000 (46%)] Loss: -377993.343750\n",
      "Train Epoch: 91 [28928/54000 (54%)] Loss: -234028.093750\n",
      "Train Epoch: 91 [33024/54000 (61%)] Loss: -232161.531250\n",
      "Train Epoch: 91 [37120/54000 (69%)] Loss: -337321.531250\n",
      "Train Epoch: 91 [41216/54000 (76%)] Loss: -234512.453125\n",
      "Train Epoch: 91 [45312/54000 (84%)] Loss: -377753.625000\n",
      "Train Epoch: 91 [49408/54000 (91%)] Loss: -263857.937500\n",
      "    epoch          : 91\n",
      "    loss           : -300674.76239483175\n",
      "    val_loss       : -267531.3994140625\n",
      "Train Epoch: 92 [256/54000 (0%)] Loss: -266095.437500\n",
      "Train Epoch: 92 [4352/54000 (8%)] Loss: -371667.656250\n",
      "Train Epoch: 92 [8448/54000 (16%)] Loss: -262734.093750\n",
      "Train Epoch: 92 [12544/54000 (23%)] Loss: -313985.562500\n",
      "Train Epoch: 92 [16640/54000 (31%)] Loss: -263391.687500\n",
      "Train Epoch: 92 [20736/54000 (38%)] Loss: -266117.968750\n",
      "Train Epoch: 92 [24832/54000 (46%)] Loss: -237051.093750\n",
      "Train Epoch: 92 [28928/54000 (54%)] Loss: -239061.093750\n",
      "Train Epoch: 92 [33024/54000 (61%)] Loss: -239040.890625\n",
      "Train Epoch: 92 [37120/54000 (69%)] Loss: -233131.718750\n",
      "Train Epoch: 92 [41216/54000 (76%)] Loss: -377781.468750\n",
      "Train Epoch: 92 [45312/54000 (84%)] Loss: -263782.187500\n",
      "Train Epoch: 92 [49408/54000 (91%)] Loss: -377667.343750\n",
      "    epoch          : 92\n",
      "    loss           : -300804.5428185096\n",
      "    val_loss       : -290898.98798828124\n",
      "Train Epoch: 93 [256/54000 (0%)] Loss: -313977.562500\n",
      "Train Epoch: 93 [4352/54000 (8%)] Loss: -242493.187500\n",
      "Train Epoch: 93 [8448/54000 (16%)] Loss: -266465.812500\n",
      "Train Epoch: 93 [12544/54000 (23%)] Loss: -237673.843750\n",
      "Train Epoch: 93 [16640/54000 (31%)] Loss: -381241.187500\n",
      "Train Epoch: 93 [20736/54000 (38%)] Loss: -238969.968750\n",
      "Train Epoch: 93 [24832/54000 (46%)] Loss: -233474.437500\n",
      "Train Epoch: 93 [28928/54000 (54%)] Loss: -262852.500000\n",
      "Train Epoch: 93 [33024/54000 (61%)] Loss: -342368.875000\n",
      "Train Epoch: 93 [37120/54000 (69%)] Loss: -262935.781250\n",
      "Train Epoch: 93 [41216/54000 (76%)] Loss: -237044.531250\n",
      "Train Epoch: 93 [45312/54000 (84%)] Loss: -231135.906250\n",
      "Train Epoch: 93 [49408/54000 (91%)] Loss: -376121.625000\n",
      "    epoch          : 93\n",
      "    loss           : -300981.5028545673\n",
      "    val_loss       : -279782.309375\n",
      "Train Epoch: 94 [256/54000 (0%)] Loss: -332017.312500\n",
      "Train Epoch: 94 [4352/54000 (8%)] Loss: -236914.968750\n",
      "Train Epoch: 94 [8448/54000 (16%)] Loss: -233978.125000\n",
      "Train Epoch: 94 [12544/54000 (23%)] Loss: -363109.250000\n",
      "Train Epoch: 94 [16640/54000 (31%)] Loss: -337247.812500\n",
      "Train Epoch: 94 [20736/54000 (38%)] Loss: -383752.468750\n",
      "Train Epoch: 94 [24832/54000 (46%)] Loss: -338580.812500\n",
      "Train Epoch: 94 [28928/54000 (54%)] Loss: -261888.656250\n",
      "Train Epoch: 94 [33024/54000 (61%)] Loss: -232597.812500\n",
      "Train Epoch: 94 [37120/54000 (69%)] Loss: -264278.750000\n",
      "Train Epoch: 94 [41216/54000 (76%)] Loss: -310107.531250\n",
      "Train Epoch: 94 [45312/54000 (84%)] Loss: -264790.937500\n",
      "Train Epoch: 94 [49408/54000 (91%)] Loss: -375461.437500\n",
      "    epoch          : 94\n",
      "    loss           : -301086.07842548075\n",
      "    val_loss       : -288721.01044921874\n",
      "Train Epoch: 95 [256/54000 (0%)] Loss: -352437.937500\n",
      "Train Epoch: 95 [4352/54000 (8%)] Loss: -247027.031250\n",
      "Train Epoch: 95 [8448/54000 (16%)] Loss: -338968.781250\n",
      "Train Epoch: 95 [12544/54000 (23%)] Loss: -381419.562500\n",
      "Train Epoch: 95 [16640/54000 (31%)] Loss: -264771.812500\n",
      "Train Epoch: 95 [20736/54000 (38%)] Loss: -374819.312500\n",
      "Train Epoch: 95 [24832/54000 (46%)] Loss: -257638.468750\n",
      "Train Epoch: 95 [28928/54000 (54%)] Loss: -242174.828125\n",
      "Train Epoch: 95 [33024/54000 (61%)] Loss: -313489.937500\n",
      "Train Epoch: 95 [37120/54000 (69%)] Loss: -265102.156250\n",
      "Train Epoch: 95 [41216/54000 (76%)] Loss: -235784.125000\n",
      "Train Epoch: 95 [45312/54000 (84%)] Loss: -263992.312500\n",
      "Train Epoch: 95 [49408/54000 (91%)] Loss: -375926.781250\n",
      "    epoch          : 95\n",
      "    loss           : -301218.62755408656\n",
      "    val_loss       : -68429.19697265625\n",
      "Train Epoch: 96 [256/54000 (0%)] Loss: -237847.984375\n",
      "Train Epoch: 96 [4352/54000 (8%)] Loss: -257847.640625\n",
      "Train Epoch: 96 [8448/54000 (16%)] Loss: -236678.890625\n",
      "Train Epoch: 96 [12544/54000 (23%)] Loss: -378215.187500\n",
      "Train Epoch: 96 [16640/54000 (31%)] Loss: -376245.375000\n",
      "Train Epoch: 96 [20736/54000 (38%)] Loss: -265230.437500\n",
      "Train Epoch: 96 [24832/54000 (46%)] Loss: -314419.718750\n",
      "Train Epoch: 96 [28928/54000 (54%)] Loss: -239017.875000\n",
      "Train Epoch: 96 [33024/54000 (61%)] Loss: -365924.687500\n",
      "Train Epoch: 96 [37120/54000 (69%)] Loss: -333716.656250\n",
      "Train Epoch: 96 [41216/54000 (76%)] Loss: -234643.015625\n",
      "Train Epoch: 96 [45312/54000 (84%)] Loss: -373703.375000\n",
      "Train Epoch: 96 [49408/54000 (91%)] Loss: -245137.375000\n",
      "    epoch          : 96\n",
      "    loss           : -301302.79296875\n",
      "    val_loss       : -277160.66337890626\n",
      "Train Epoch: 97 [256/54000 (0%)] Loss: -309638.843750\n",
      "Train Epoch: 97 [4352/54000 (8%)] Loss: -234669.812500\n",
      "Train Epoch: 97 [8448/54000 (16%)] Loss: -262118.937500\n",
      "Train Epoch: 97 [12544/54000 (23%)] Loss: -230720.968750\n",
      "Train Epoch: 97 [16640/54000 (31%)] Loss: -233510.125000\n",
      "Train Epoch: 97 [20736/54000 (38%)] Loss: -266953.750000\n",
      "Train Epoch: 97 [24832/54000 (46%)] Loss: -378835.281250\n",
      "Train Epoch: 97 [28928/54000 (54%)] Loss: -237277.703125\n",
      "Train Epoch: 97 [33024/54000 (61%)] Loss: -311808.156250\n",
      "Train Epoch: 97 [37120/54000 (69%)] Loss: -265352.656250\n",
      "Train Epoch: 97 [41216/54000 (76%)] Loss: -241900.468750\n",
      "Train Epoch: 97 [45312/54000 (84%)] Loss: -268596.062500\n",
      "Train Epoch: 97 [49408/54000 (91%)] Loss: -379296.375000\n",
      "    epoch          : 97\n",
      "    loss           : -301500.841796875\n",
      "    val_loss       : -276226.82060546876\n",
      "Train Epoch: 98 [256/54000 (0%)] Loss: -233997.531250\n",
      "Train Epoch: 98 [4352/54000 (8%)] Loss: -377733.593750\n",
      "Train Epoch: 98 [8448/54000 (16%)] Loss: -246382.687500\n",
      "Train Epoch: 98 [12544/54000 (23%)] Loss: -256209.578125\n",
      "Train Epoch: 98 [16640/54000 (31%)] Loss: -242065.531250\n",
      "Train Epoch: 98 [20736/54000 (38%)] Loss: -379017.031250\n",
      "Train Epoch: 98 [24832/54000 (46%)] Loss: -361884.562500\n",
      "Train Epoch: 98 [28928/54000 (54%)] Loss: -381298.500000\n",
      "Train Epoch: 98 [33024/54000 (61%)] Loss: -227019.812500\n",
      "Train Epoch: 98 [37120/54000 (69%)] Loss: -239290.312500\n",
      "Train Epoch: 98 [41216/54000 (76%)] Loss: -383370.437500\n",
      "Train Epoch: 98 [45312/54000 (84%)] Loss: -315522.968750\n",
      "Train Epoch: 98 [49408/54000 (91%)] Loss: -241526.906250\n",
      "    epoch          : 98\n",
      "    loss           : -301618.07053786056\n",
      "    val_loss       : -263292.27099609375\n",
      "Train Epoch: 99 [256/54000 (0%)] Loss: -261997.890625\n",
      "Train Epoch: 99 [4352/54000 (8%)] Loss: -242295.750000\n",
      "Train Epoch: 99 [8448/54000 (16%)] Loss: -271376.781250\n",
      "Train Epoch: 99 [12544/54000 (23%)] Loss: -315721.437500\n",
      "Train Epoch: 99 [16640/54000 (31%)] Loss: -381706.250000\n",
      "Train Epoch: 99 [20736/54000 (38%)] Loss: -239583.078125\n",
      "Train Epoch: 99 [24832/54000 (46%)] Loss: -312397.031250\n",
      "Train Epoch: 99 [28928/54000 (54%)] Loss: -239322.421875\n",
      "Train Epoch: 99 [33024/54000 (61%)] Loss: -377842.000000\n",
      "Train Epoch: 99 [37120/54000 (69%)] Loss: -245779.781250\n",
      "Train Epoch: 99 [41216/54000 (76%)] Loss: -378045.750000\n",
      "Train Epoch: 99 [45312/54000 (84%)] Loss: -339418.000000\n",
      "Train Epoch: 99 [49408/54000 (91%)] Loss: -241059.125000\n",
      "    epoch          : 99\n",
      "    loss           : -301722.9501201923\n",
      "    val_loss       : -135453.74096679688\n",
      "Train Epoch: 100 [256/54000 (0%)] Loss: -337178.187500\n",
      "Train Epoch: 100 [4352/54000 (8%)] Loss: -235104.234375\n",
      "Train Epoch: 100 [8448/54000 (16%)] Loss: -239046.250000\n",
      "Train Epoch: 100 [12544/54000 (23%)] Loss: -314087.625000\n",
      "Train Epoch: 100 [16640/54000 (31%)] Loss: -260575.015625\n",
      "Train Epoch: 100 [20736/54000 (38%)] Loss: -268627.562500\n",
      "Train Epoch: 100 [24832/54000 (46%)] Loss: -382517.125000\n",
      "Train Epoch: 100 [28928/54000 (54%)] Loss: -268876.937500\n",
      "Train Epoch: 100 [33024/54000 (61%)] Loss: -363447.562500\n",
      "Train Epoch: 100 [37120/54000 (69%)] Loss: -240523.140625\n",
      "Train Epoch: 100 [41216/54000 (76%)] Loss: -381839.343750\n",
      "Train Epoch: 100 [45312/54000 (84%)] Loss: -377477.468750\n",
      "Train Epoch: 100 [49408/54000 (91%)] Loss: -337055.468750\n",
      "    epoch          : 100\n",
      "    loss           : -301862.25811298075\n",
      "    val_loss       : -272674.3009765625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0702_141658/checkpoint-epoch100.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAECategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=96, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=96, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=160, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=160, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=160, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=160, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=288, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=288, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=288, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=288, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=424, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=424, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_3_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=424, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=424, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=192, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=192, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=192, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=320, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=320, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=320, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=456, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=456, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=456, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=456, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=384, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=384, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=520, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=520, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=520, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=520, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=648, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=648, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_9_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=648, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=648, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0_0): StandardNormal()\n",
       "    (global_element_1_0): StandardNormal()\n",
       "    (global_element_2_0): StandardNormal()\n",
       "    (global_element_3_0): StandardNormal()\n",
       "    (global_element_4_0): StandardNormal()\n",
       "  )\n",
       "  (guide_embedding): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=49, bias=True)\n",
       "    (1): BatchNorm1d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=49, out_features=49, bias=True)\n",
       "    (4): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (guide_confidences): Sequential(\n",
       "    (0): Linear(in_features=49, out_features=2, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
