{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 62580.320312\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -90466.156250\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -67242.062500\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -78638.242188\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -92179.296875\n",
      "    epoch          : 1\n",
      "    loss           : -67478.39276367187\n",
      "    val_loss       : -86367.68728027344\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -76150.000000\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -78037.484375\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -82792.796875\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -56056.406250\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -118627.562500\n",
      "    epoch          : 2\n",
      "    loss           : -93389.1131640625\n",
      "    val_loss       : -102044.86750488282\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -102992.898438\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -99574.734375\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -101037.468750\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -90609.437500\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -120402.781250\n",
      "    epoch          : 3\n",
      "    loss           : -107428.306640625\n",
      "    val_loss       : -116142.50063476563\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -113165.507812\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -116106.468750\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -119475.523438\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -139958.750000\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -120965.062500\n",
      "    epoch          : 4\n",
      "    loss           : -121667.3446875\n",
      "    val_loss       : -129243.99782714844\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -111186.703125\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -132345.062500\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -125103.968750\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -104929.046875\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -155057.718750\n",
      "    epoch          : 5\n",
      "    loss           : -134804.393515625\n",
      "    val_loss       : -142290.10864257812\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -145112.140625\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -130700.984375\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -154869.625000\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -118315.140625\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -143410.812500\n",
      "    epoch          : 6\n",
      "    loss           : -145772.945546875\n",
      "    val_loss       : -150943.54831542968\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -142151.046875\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -140526.562500\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -127371.937500\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -178486.750000\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -188078.593750\n",
      "    epoch          : 7\n",
      "    loss           : -156950.309453125\n",
      "    val_loss       : -162407.12938232423\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -169040.625000\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -185369.687500\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -186584.718750\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -188201.906250\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -191535.062500\n",
      "    epoch          : 8\n",
      "    loss           : -168920.32890625\n",
      "    val_loss       : -175459.3140136719\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -161393.953125\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -170899.093750\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -174191.953125\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -189539.937500\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -205140.953125\n",
      "    epoch          : 9\n",
      "    loss           : -179437.21265625\n",
      "    val_loss       : -185459.0132446289\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -195279.234375\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -168893.421875\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -196779.515625\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -205436.781250\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -176739.625000\n",
      "    epoch          : 10\n",
      "    loss           : -189097.78484375\n",
      "    val_loss       : -194596.6662109375\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -177210.625000\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -206075.562500\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -218693.171875\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -233204.187500\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -215831.265625\n",
      "    epoch          : 11\n",
      "    loss           : -197818.68859375\n",
      "    val_loss       : -203751.473828125\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -234956.531250\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -239096.234375\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -213270.453125\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -189249.750000\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -189554.640625\n",
      "    epoch          : 12\n",
      "    loss           : -207271.91125\n",
      "    val_loss       : -212237.0927368164\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -215398.875000\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -230227.250000\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -252058.250000\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -195215.656250\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -199420.421875\n",
      "    epoch          : 13\n",
      "    loss           : -216211.5971875\n",
      "    val_loss       : -220942.1086669922\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -240030.843750\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -260632.593750\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -242277.609375\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -203080.234375\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -266777.500000\n",
      "    epoch          : 14\n",
      "    loss           : -224795.4325\n",
      "    val_loss       : -230105.50963134767\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -265595.312500\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -255446.375000\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -270928.937500\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -212748.203125\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -256101.921875\n",
      "    epoch          : 15\n",
      "    loss           : -231112.62875\n",
      "    val_loss       : -233139.66794433593\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -209450.531250\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -236344.906250\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -238257.281250\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -268934.468750\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -223630.312500\n",
      "    epoch          : 16\n",
      "    loss           : -238142.27078125\n",
      "    val_loss       : -244707.48076171876\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -245308.781250\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -233140.343750\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -289404.750000\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -273346.500000\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -265601.000000\n",
      "    epoch          : 17\n",
      "    loss           : -241723.39865234375\n",
      "    val_loss       : -251694.76007080078\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -236911.531250\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -238969.343750\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -273251.031250\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -274170.500000\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -222978.562500\n",
      "    epoch          : 18\n",
      "    loss           : -253837.581875\n",
      "    val_loss       : -258261.01232910156\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -284077.812500\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -243675.531250\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -292724.125000\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -283569.437500\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -231695.109375\n",
      "    epoch          : 19\n",
      "    loss           : -260313.23875\n",
      "    val_loss       : -264280.197253418\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -292476.937500\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -262791.875000\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -264576.437500\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -251009.593750\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -242050.109375\n",
      "    epoch          : 20\n",
      "    loss           : -266840.2184375\n",
      "    val_loss       : -270743.56916503905\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -315937.625000\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -236768.312500\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -237867.921875\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -264640.656250\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -307370.343750\n",
      "    epoch          : 21\n",
      "    loss           : -271697.596875\n",
      "    val_loss       : -275816.9163696289\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -245891.734375\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -243542.984375\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -266465.343750\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -301525.187500\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -240949.312500\n",
      "    epoch          : 22\n",
      "    loss           : -279115.009375\n",
      "    val_loss       : -283453.3767333984\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -316418.343750\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -335218.500000\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -333285.500000\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -321639.468750\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -247395.796875\n",
      "    epoch          : 23\n",
      "    loss           : -284776.789375\n",
      "    val_loss       : -289241.92443847656\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -310823.875000\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -246606.218750\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -265387.156250\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -252061.750000\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -276412.562500\n",
      "    epoch          : 24\n",
      "    loss           : -290388.65953125\n",
      "    val_loss       : -294320.0981933594\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -286961.781250\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -331415.125000\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -290957.718750\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -333184.125000\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -314386.812500\n",
      "    epoch          : 25\n",
      "    loss           : -295646.8053125\n",
      "    val_loss       : -298123.1171020508\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -336975.375000\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -294611.937500\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -261536.750000\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -295455.812500\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -324151.687500\n",
      "    epoch          : 26\n",
      "    loss           : -301675.56\n",
      "    val_loss       : -302455.15987548826\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -288147.312500\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -345359.593750\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -357614.093750\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -342777.843750\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -345090.937500\n",
      "    epoch          : 27\n",
      "    loss           : -303703.71875\n",
      "    val_loss       : -309792.52857666014\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -298550.437500\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -334226.343750\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -278237.656250\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -270453.875000\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -338894.000000\n",
      "    epoch          : 28\n",
      "    loss           : -312510.8815625\n",
      "    val_loss       : -316558.5855102539\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -267584.312500\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -357248.750000\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -339187.656250\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -274833.843750\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -338002.500000\n",
      "    epoch          : 29\n",
      "    loss           : -315430.61\n",
      "    val_loss       : -320415.4673583984\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -274694.843750\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -302250.125000\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -274280.500000\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -312548.375000\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -344895.062500\n",
      "    epoch          : 30\n",
      "    loss           : -322681.2275\n",
      "    val_loss       : -325802.19028320315\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -371961.156250\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -277523.781250\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -279108.812500\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -351301.750000\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -351924.375000\n",
      "    epoch          : 31\n",
      "    loss           : -326868.450625\n",
      "    val_loss       : -330839.9495239258\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -282356.343750\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -370205.500000\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -319668.875000\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -388696.375000\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -318601.062500\n",
      "    epoch          : 32\n",
      "    loss           : -330404.7409375\n",
      "    val_loss       : -332866.51004638674\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -381869.906250\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -337068.906250\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -341330.062500\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -402094.312500\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -342835.718750\n",
      "    epoch          : 33\n",
      "    loss           : -334578.3565625\n",
      "    val_loss       : -340307.4647216797\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -401910.906250\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -401742.812500\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -321249.343750\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -294967.343750\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -293567.000000\n",
      "    epoch          : 34\n",
      "    loss           : -341230.2865625\n",
      "    val_loss       : -343844.93790283205\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -291443.687500\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -359055.531250\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -362666.375000\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -286684.281250\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -367510.750000\n",
      "    epoch          : 35\n",
      "    loss           : -342667.004375\n",
      "    val_loss       : -348559.85736083984\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -332187.875000\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -399359.562500\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -295032.968750\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -296544.125000\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -358795.375000\n",
      "    epoch          : 36\n",
      "    loss           : -349319.5575\n",
      "    val_loss       : -352144.54377441405\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -338515.187500\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -299520.218750\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -366148.156250\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -332569.375000\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -297954.031250\n",
      "    epoch          : 37\n",
      "    loss           : -352158.8928125\n",
      "    val_loss       : -355334.0846557617\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -335511.812500\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -375433.156250\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -310929.250000\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -297912.750000\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -307359.062500\n",
      "    epoch          : 38\n",
      "    loss           : -355482.8625\n",
      "    val_loss       : -360691.8568359375\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -343959.062500\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -410994.875000\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -335730.281250\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -426253.687500\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -338530.281250\n",
      "    epoch          : 39\n",
      "    loss           : -358488.231875\n",
      "    val_loss       : -363444.8313842773\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -307987.375000\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -339624.531250\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -305535.750000\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -373460.906250\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -313477.750000\n",
      "    epoch          : 40\n",
      "    loss           : -362074.7440625\n",
      "    val_loss       : -366053.47376708983\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -314344.000000\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -436581.437500\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -381417.875000\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -426153.937500\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -337325.312500\n",
      "    epoch          : 41\n",
      "    loss           : -365173.5475\n",
      "    val_loss       : -367639.85390625\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -340557.812500\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -409063.625000\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -388632.625000\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -345513.750000\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -393312.593750\n",
      "    epoch          : 42\n",
      "    loss           : -370073.2725\n",
      "    val_loss       : -372841.54844970704\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -316830.312500\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -414042.531250\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -432967.125000\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -349378.031250\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -350235.500000\n",
      "    epoch          : 43\n",
      "    loss           : -372143.8434375\n",
      "    val_loss       : -373959.4936035156\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -444593.000000\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -318598.500000\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -446494.468750\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -358966.187500\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -400370.531250\n",
      "    epoch          : 44\n",
      "    loss           : -376167.8928125\n",
      "    val_loss       : -379552.8374267578\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -351872.687500\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -453230.937500\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -324452.468750\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -425675.625000\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -427658.968750\n",
      "    epoch          : 45\n",
      "    loss           : -381451.3153125\n",
      "    val_loss       : -384130.8723876953\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -442928.843750\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -327526.718750\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -331860.781250\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -332503.593750\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -330900.437500\n",
      "    epoch          : 46\n",
      "    loss           : -383135.146875\n",
      "    val_loss       : -386774.54497070314\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -334000.093750\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -432947.437500\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -445189.343750\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -430342.875000\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -406743.562500\n",
      "    epoch          : 47\n",
      "    loss           : -386293.2015625\n",
      "    val_loss       : -389736.4978149414\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -361930.937500\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -330942.062500\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -435214.343750\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -331314.687500\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -440042.812500\n",
      "    epoch          : 48\n",
      "    loss           : -391597.3834375\n",
      "    val_loss       : -395579.32001953124\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -412382.031250\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -334942.468750\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -417526.250000\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -377084.218750\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -440655.562500\n",
      "    epoch          : 49\n",
      "    loss           : -395009.5340625\n",
      "    val_loss       : -396984.24514160154\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -378522.187500\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -466791.062500\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -469242.281250\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -337042.687500\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -418703.281250\n",
      "    epoch          : 50\n",
      "    loss           : -393921.9465625\n",
      "    val_loss       : -396200.85604248045\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0717_104644/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -379471.625000\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -459726.312500\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -335117.750000\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -335177.125000\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -464976.531250\n",
      "    epoch          : 51\n",
      "    loss           : -396844.5390625\n",
      "    val_loss       : -402312.74296875\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -346137.375000\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -444403.031250\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -424249.437500\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -429028.843750\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -483309.000000\n",
      "    epoch          : 52\n",
      "    loss           : -402640.05125\n",
      "    val_loss       : -401651.9849243164\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -425161.687500\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -341576.500000\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -375644.937500\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -427908.281250\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -426029.593750\n",
      "    epoch          : 53\n",
      "    loss           : -404066.4521875\n",
      "    val_loss       : -406256.61096191406\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -344826.406250\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -444869.968750\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -471501.375000\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -464443.437500\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -346882.375000\n",
      "    epoch          : 54\n",
      "    loss           : -401746.4609375\n",
      "    val_loss       : -409726.17465820315\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -346732.625000\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -384489.875000\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -431176.531250\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -433902.906250\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -435691.437500\n",
      "    epoch          : 55\n",
      "    loss           : -410277.565\n",
      "    val_loss       : -413302.8049926758\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -464143.125000\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -439200.031250\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -490875.343750\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -496308.187500\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -383986.437500\n",
      "    epoch          : 56\n",
      "    loss           : -413676.13875\n",
      "    val_loss       : -415739.2696044922\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -377276.531250\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -389399.343750\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -428020.437500\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -347765.093750\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -438161.468750\n",
      "    epoch          : 57\n",
      "    loss           : -413367.7584375\n",
      "    val_loss       : -417330.33736572263\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -353187.375000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -392177.468750\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -396763.250000\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -358523.062500\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -440812.406250\n",
      "    epoch          : 58\n",
      "    loss           : -418715.2109375\n",
      "    val_loss       : -421373.4077758789\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -359250.281250\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -362865.937500\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -399860.375000\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -468709.812500\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -450389.750000\n",
      "    epoch          : 59\n",
      "    loss           : -420726.89625\n",
      "    val_loss       : -422046.6835083008\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -489721.062500\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -390855.218750\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -388082.312500\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -359998.343750\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -451186.062500\n",
      "    epoch          : 60\n",
      "    loss           : -423079.618125\n",
      "    val_loss       : -426011.24680175784\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -403500.250000\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -494281.718750\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -361767.125000\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -497843.687500\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -447498.468750\n",
      "    epoch          : 61\n",
      "    loss           : -423230.333125\n",
      "    val_loss       : -423196.75040283205\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -352672.062500\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -363051.093750\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -359854.187500\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -402267.781250\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -401574.312500\n",
      "    epoch          : 62\n",
      "    loss           : -427614.49125\n",
      "    val_loss       : -430821.10260009766\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -401563.000000\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -476589.812500\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -364224.531250\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -515020.968750\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -360659.625000\n",
      "    epoch          : 63\n",
      "    loss           : -428844.749375\n",
      "    val_loss       : -432710.49166259763\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -474223.562500\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -512729.375000\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -372783.250000\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -369191.687500\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -403790.468750\n",
      "    epoch          : 64\n",
      "    loss           : -431188.728125\n",
      "    val_loss       : -431350.4019165039\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -513224.843750\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -368380.500000\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -374120.093750\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -405457.468750\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -473767.281250\n",
      "    epoch          : 65\n",
      "    loss           : -431861.7703125\n",
      "    val_loss       : -434874.4471801758\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -505540.906250\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -522592.906250\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -395969.687500\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -398598.625000\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -357624.906250\n",
      "    epoch          : 66\n",
      "    loss           : -433870.24375\n",
      "    val_loss       : -438420.56235351565\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -521326.562500\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -364863.468750\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -411890.312500\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -369932.812500\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -408698.000000\n",
      "    epoch          : 67\n",
      "    loss           : -437767.2203125\n",
      "    val_loss       : -443320.1212036133\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -515096.375000\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -514865.656250\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -492438.562500\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -415234.218750\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -493025.343750\n",
      "    epoch          : 68\n",
      "    loss           : -442485.0321875\n",
      "    val_loss       : -445016.94420166017\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -379980.750000\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -417619.656250\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -378246.281250\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -410889.875000\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -411884.562500\n",
      "    epoch          : 69\n",
      "    loss           : -444716.284375\n",
      "    val_loss       : -446878.0535888672\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -468299.687500\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -466577.750000\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -378357.406250\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -479398.312500\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -532231.375000\n",
      "    epoch          : 70\n",
      "    loss           : -444753.6928125\n",
      "    val_loss       : -444926.8029785156\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -407170.875000\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -419931.437500\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -522537.687500\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -372330.750000\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -486226.093750\n",
      "    epoch          : 71\n",
      "    loss           : -445718.2125\n",
      "    val_loss       : -448885.3655517578\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -523819.812500\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -410370.406250\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -374612.218750\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -483036.781250\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -417809.218750\n",
      "    epoch          : 72\n",
      "    loss           : -445944.1625\n",
      "    val_loss       : -452101.49376220704\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -383171.812500\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -536422.937500\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -416021.906250\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -384080.812500\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -488412.062500\n",
      "    epoch          : 73\n",
      "    loss           : -452262.000625\n",
      "    val_loss       : -455150.15173339844\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -425915.875000\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -527850.562500\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -542376.375000\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -470075.031250\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -490811.500000\n",
      "    epoch          : 74\n",
      "    loss           : -451321.541875\n",
      "    val_loss       : -449993.3561401367\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -373736.687500\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -384623.937500\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -385967.031250\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -528216.625000\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -502398.593750\n",
      "    epoch          : 75\n",
      "    loss           : -453940.4521875\n",
      "    val_loss       : -456370.41616210935\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -427116.812500\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -389136.250000\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -422266.937500\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -380587.625000\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -530785.250000\n",
      "    epoch          : 76\n",
      "    loss           : -450151.863125\n",
      "    val_loss       : -455623.91009521484\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -478157.781250\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -392739.906250\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -428051.718750\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -379575.906250\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -503357.125000\n",
      "    epoch          : 77\n",
      "    loss           : -456108.454375\n",
      "    val_loss       : -459967.0301513672\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -392335.031250\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -481144.031250\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -503026.156250\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -485633.750000\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -479345.375000\n",
      "    epoch          : 78\n",
      "    loss           : -459693.8865625\n",
      "    val_loss       : -462950.9857910156\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -551218.187500\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -393793.250000\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -422302.312500\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -397738.625000\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -430721.656250\n",
      "    epoch          : 79\n",
      "    loss           : -461367.3940625\n",
      "    val_loss       : -464007.71293945314\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -391913.781250\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -390876.718750\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -431620.937500\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -439633.750000\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -476280.562500\n",
      "    epoch          : 80\n",
      "    loss           : -463327.7321875\n",
      "    val_loss       : -465301.39263916016\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -433881.687500\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -385980.562500\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -427217.375000\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -390158.625000\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -481570.625000\n",
      "    epoch          : 81\n",
      "    loss           : -461032.1325\n",
      "    val_loss       : -464421.36478271487\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -389188.250000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -529893.000000\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -540964.187500\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -395153.906250\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -430004.500000\n",
      "    epoch          : 82\n",
      "    loss           : -461575.5246875\n",
      "    val_loss       : -469800.84436035156\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -439343.531250\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -545697.437500\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -549519.250000\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -494139.281250\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -546595.437500\n",
      "    epoch          : 83\n",
      "    loss           : -469618.3084375\n",
      "    val_loss       : -470771.0100830078\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -388694.375000\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -394794.937500\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -429152.125000\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -387432.968750\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -519118.375000\n",
      "    epoch          : 84\n",
      "    loss           : -468606.785625\n",
      "    val_loss       : -473522.0586791992\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -402896.125000\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -438356.718750\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -436514.687500\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -431494.687500\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -527818.812500\n",
      "    epoch          : 85\n",
      "    loss           : -472730.235625\n",
      "    val_loss       : -474243.94383544923\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -554320.375000\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -401582.375000\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -568505.125000\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -398529.343750\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -497092.718750\n",
      "    epoch          : 86\n",
      "    loss           : -475141.529375\n",
      "    val_loss       : -477151.17987060547\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -571312.937500\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -556219.687500\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -400371.593750\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -487025.937500\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -494826.093750\n",
      "    epoch          : 87\n",
      "    loss           : -474156.2321875\n",
      "    val_loss       : -478234.4522583008\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -576279.312500\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -558608.500000\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -572748.000000\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -501074.531250\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -395071.812500\n",
      "    epoch          : 88\n",
      "    loss           : -478086.195\n",
      "    val_loss       : -478653.36494140624\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -527014.250000\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -529148.062500\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -404287.562500\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -401131.500000\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -492719.250000\n",
      "    epoch          : 89\n",
      "    loss           : -477104.381875\n",
      "    val_loss       : -477743.0186523438\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -403299.437500\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -507708.750000\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -503739.500000\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -412488.125000\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -531738.125000\n",
      "    epoch          : 90\n",
      "    loss           : -479349.264375\n",
      "    val_loss       : -482609.3057617188\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -447317.718750\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -449849.125000\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -522803.000000\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -521716.937500\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -402457.718750\n",
      "    epoch          : 91\n",
      "    loss           : -481530.5565625\n",
      "    val_loss       : -482113.72259521484\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -539623.750000\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -526485.437500\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -403302.000000\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -526546.250000\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -454109.437500\n",
      "    epoch          : 92\n",
      "    loss           : -481434.5346875\n",
      "    val_loss       : -485811.4315429687\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -458218.750000\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -526784.125000\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -580870.750000\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -449292.656250\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -406962.437500\n",
      "    epoch          : 93\n",
      "    loss           : -485481.0596875\n",
      "    val_loss       : -488079.94938964845\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -408884.906250\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -447464.625000\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -418558.437500\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -397419.593750\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -564903.875000\n",
      "    epoch          : 94\n",
      "    loss           : -485944.651875\n",
      "    val_loss       : -487257.9942504883\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -406167.187500\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -408235.968750\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -579823.250000\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -412601.562500\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -448228.625000\n",
      "    epoch          : 95\n",
      "    loss           : -483080.935625\n",
      "    val_loss       : -487827.12362060545\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -571108.750000\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -415107.375000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -516751.093750\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -590627.875000\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -403435.875000\n",
      "    epoch          : 96\n",
      "    loss           : -488346.2675\n",
      "    val_loss       : -489163.3576293945\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -409022.750000\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -545017.375000\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -567678.000000\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -447504.687500\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -581950.000000\n",
      "    epoch          : 97\n",
      "    loss           : -482635.518125\n",
      "    val_loss       : -488030.6605712891\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -404820.875000\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -589292.500000\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -422217.062500\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -552759.500000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -413943.000000\n",
      "    epoch          : 98\n",
      "    loss           : -491503.34125\n",
      "    val_loss       : -493280.7506469727\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -456765.250000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -454517.687500\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -462467.593750\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -417044.312500\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -536826.625000\n",
      "    epoch          : 99\n",
      "    loss           : -490373.585\n",
      "    val_loss       : -490297.56472167966\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -406947.375000\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -518119.625000\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -516788.187500\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -410599.812500\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -409729.468750\n",
      "    epoch          : 100\n",
      "    loss           : -491166.34625\n",
      "    val_loss       : -495484.0508789063\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0717_104644/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -588394.500000\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -454616.750000\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -415892.687500\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -542705.750000\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -458913.218750\n",
      "    epoch          : 101\n",
      "    loss           : -494399.9\n",
      "    val_loss       : -494563.087109375\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -596160.625000\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -579535.062500\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -518141.687500\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -521621.812500\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -418220.187500\n",
      "    epoch          : 102\n",
      "    loss           : -496789.3803125\n",
      "    val_loss       : -498746.05064697267\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -414687.187500\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -601587.687500\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -458816.312500\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -545712.500000\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -542597.375000\n",
      "    epoch          : 103\n",
      "    loss           : -498237.02625\n",
      "    val_loss       : -500479.39869384764\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -523838.281250\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -603336.250000\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -524126.937500\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -583962.625000\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -551640.062500\n",
      "    epoch          : 104\n",
      "    loss           : -499306.931875\n",
      "    val_loss       : -501322.90673828125\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -586114.312500\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -585268.062500\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -448912.500000\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -418220.718750\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -412818.656250\n",
      "    epoch          : 105\n",
      "    loss           : -496818.4521875\n",
      "    val_loss       : -502399.5329833984\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -469524.687500\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -553355.812500\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -427242.218750\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -412793.500000\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -523065.593750\n",
      "    epoch          : 106\n",
      "    loss           : -499127.5296875\n",
      "    val_loss       : -500224.48278808594\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -603536.875000\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -556003.812500\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -549228.125000\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -508668.687500\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -411498.843750\n",
      "    epoch          : 107\n",
      "    loss           : -497891.1425\n",
      "    val_loss       : -503471.98802490236\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -606569.375000\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -459579.218750\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -429051.625000\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -417217.593750\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -466570.500000\n",
      "    epoch          : 108\n",
      "    loss           : -502501.1140625\n",
      "    val_loss       : -506012.6647216797\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -603459.687500\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -554999.312500\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -554167.312500\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -415194.375000\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -610500.875000\n",
      "    epoch          : 109\n",
      "    loss           : -505020.4740625\n",
      "    val_loss       : -504910.06528320315\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -462059.687500\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -611789.875000\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -553702.062500\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -552752.125000\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -413861.625000\n",
      "    epoch          : 110\n",
      "    loss           : -504582.0940625\n",
      "    val_loss       : -501870.133996582\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -411238.593750\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -453972.031250\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -594015.562500\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -563191.875000\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -529142.187500\n",
      "    epoch          : 111\n",
      "    loss           : -501278.0271875\n",
      "    val_loss       : -506023.59261474607\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -417532.937500\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -427353.687500\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -566699.250000\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -470538.375000\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -562683.625000\n",
      "    epoch          : 112\n",
      "    loss           : -508710.67375\n",
      "    val_loss       : -511909.54865722655\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -479645.312500\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -469917.500000\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -459758.812500\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -613556.250000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -435138.562500\n",
      "    epoch          : 113\n",
      "    loss           : -508465.855625\n",
      "    val_loss       : -511449.93311767577\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -610831.437500\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -574638.000000\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -563445.937500\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -476825.000000\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -437696.968750\n",
      "    epoch          : 114\n",
      "    loss           : -509469.945625\n",
      "    val_loss       : -510087.97911376954\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -467734.875000\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -430413.875000\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -432839.468750\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -478946.375000\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -532803.500000\n",
      "    epoch          : 115\n",
      "    loss           : -512246.43375\n",
      "    val_loss       : -513447.56970214844\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -533799.250000\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -467608.500000\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -537494.125000\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -564433.125000\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -424163.375000\n",
      "    epoch          : 116\n",
      "    loss           : -512774.5990625\n",
      "    val_loss       : -514698.39775390626\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -603171.625000\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -601452.187500\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -604526.500000\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -556014.375000\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -567420.562500\n",
      "    epoch          : 117\n",
      "    loss           : -509366.0284375\n",
      "    val_loss       : -513925.1127441406\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -474755.187500\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -574844.750000\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -538707.125000\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -541926.750000\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -569166.875000\n",
      "    epoch          : 118\n",
      "    loss           : -514126.31125\n",
      "    val_loss       : -511386.6258666992\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -467232.031250\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -422288.062500\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -533537.125000\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -432473.343750\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -541963.687500\n",
      "    epoch          : 119\n",
      "    loss           : -512848.009375\n",
      "    val_loss       : -515280.30159912107\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -481347.093750\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -426908.531250\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -474236.562500\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -572706.000000\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -427752.593750\n",
      "    epoch          : 120\n",
      "    loss           : -514165.1971875\n",
      "    val_loss       : -515138.5741088867\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -473336.000000\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -476555.593750\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -545325.500000\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -479193.656250\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -575305.625000\n",
      "    epoch          : 121\n",
      "    loss           : -518149.104375\n",
      "    val_loss       : -521397.25948486326\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -574691.250000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -483393.437500\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -571533.125000\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -543162.437500\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -610277.250000\n",
      "    epoch          : 122\n",
      "    loss           : -517862.08625\n",
      "    val_loss       : -516646.9200195313\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -437421.718750\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -537262.375000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -427331.718750\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -474905.156250\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -545278.312500\n",
      "    epoch          : 123\n",
      "    loss           : -516492.273125\n",
      "    val_loss       : -522182.3464599609\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -441660.718750\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -435632.250000\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -540235.750000\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -432930.718750\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -431597.250000\n",
      "    epoch          : 124\n",
      "    loss           : -520194.0090625\n",
      "    val_loss       : -523507.138684082\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -484837.000000\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -579968.062500\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -481600.437500\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -433951.656250\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -477635.625000\n",
      "    epoch          : 125\n",
      "    loss           : -522670.3296875\n",
      "    val_loss       : -522707.28887939453\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -580462.375000\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -443106.906250\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -579166.187500\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -613304.125000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -489960.750000\n",
      "    epoch          : 126\n",
      "    loss           : -523474.594375\n",
      "    val_loss       : -523909.13807373046\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -585162.187500\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -437059.125000\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -488912.906250\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -575917.687500\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -579687.062500\n",
      "    epoch          : 127\n",
      "    loss           : -522108.5884375\n",
      "    val_loss       : -523931.8971435547\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -440293.562500\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -549159.062500\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -546816.875000\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -579421.500000\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -435547.312500\n",
      "    epoch          : 128\n",
      "    loss           : -523443.6778125\n",
      "    val_loss       : -525470.3294433594\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -554797.437500\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -435932.625000\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -441503.750000\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -430954.875000\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -579881.250000\n",
      "    epoch          : 129\n",
      "    loss           : -523120.236875\n",
      "    val_loss       : -526529.5131103515\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -481519.531250\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -631929.500000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -580800.500000\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -588509.375000\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -550049.562500\n",
      "    epoch          : 130\n",
      "    loss           : -525987.44625\n",
      "    val_loss       : -525807.7784667969\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -620300.062500\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -622395.437500\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -581689.500000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -633700.437500\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -438045.406250\n",
      "    epoch          : 131\n",
      "    loss           : -521346.13875\n",
      "    val_loss       : -527971.9924682617\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -621806.062500\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -494762.250000\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -437669.843750\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -593146.375000\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -550245.625000\n",
      "    epoch          : 132\n",
      "    loss           : -525654.53875\n",
      "    val_loss       : -524055.4858154297\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -621747.625000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -433086.875000\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -584498.625000\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -590255.125000\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -436862.312500\n",
      "    epoch          : 133\n",
      "    loss           : -527731.558125\n",
      "    val_loss       : -528928.3335083008\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -621610.250000\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -500036.406250\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -560375.750000\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -478911.750000\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -582784.562500\n",
      "    epoch          : 134\n",
      "    loss           : -530604.3375\n",
      "    val_loss       : -533737.8654296875\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -644431.125000\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -498515.156250\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -588814.562500\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -590585.500000\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -444277.031250\n",
      "    epoch          : 135\n",
      "    loss           : -531097.886875\n",
      "    val_loss       : -533104.0276855469\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -446612.562500\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -446455.281250\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -451804.656250\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -585554.250000\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -479994.687500\n",
      "    epoch          : 136\n",
      "    loss           : -528864.0940625\n",
      "    val_loss       : -524143.65130615234\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -448082.156250\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -438293.875000\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -591981.812500\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -589192.000000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -442596.281250\n",
      "    epoch          : 137\n",
      "    loss           : -530654.811875\n",
      "    val_loss       : -532491.8493530273\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -446361.625000\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -492072.281250\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -439172.500000\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -501395.906250\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -558622.750000\n",
      "    epoch          : 138\n",
      "    loss           : -532371.9271875\n",
      "    val_loss       : -535077.5499023438\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -563536.812500\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -448974.250000\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -652848.437500\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -592521.875000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -443276.625000\n",
      "    epoch          : 139\n",
      "    loss           : -532889.7703125\n",
      "    val_loss       : -534046.6165283204\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -560417.625000\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -647229.687500\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -493109.125000\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -450341.250000\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -595887.062500\n",
      "    epoch          : 140\n",
      "    loss           : -536103.3665625\n",
      "    val_loss       : -537206.2239990234\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -488692.250000\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -485740.687500\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -495595.843750\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -630033.000000\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -416787.593750\n",
      "    epoch          : 141\n",
      "    loss           : -531357.290625\n",
      "    val_loss       : -528630.7299682617\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -476051.562500\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -617260.125000\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -423286.812500\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -445199.125000\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -440353.312500\n",
      "    epoch          : 142\n",
      "    loss           : -532093.0521875\n",
      "    val_loss       : -537566.2887939453\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -595745.500000\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -489828.062500\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -450404.875000\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -599193.437500\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -446399.468750\n",
      "    epoch          : 143\n",
      "    loss           : -538558.9121875\n",
      "    val_loss       : -540247.3111450195\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -450567.531250\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -451311.625000\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -446977.843750\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -446245.406250\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -602483.625000\n",
      "    epoch          : 144\n",
      "    loss           : -535874.9925\n",
      "    val_loss       : -537271.448840332\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -489839.312500\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -563361.625000\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -646358.250000\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -589410.500000\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -600104.000000\n",
      "    epoch          : 145\n",
      "    loss           : -535633.6571875\n",
      "    val_loss       : -540836.1236938477\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -496085.531250\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -496668.750000\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -653833.812500\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -485061.875000\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -600493.312500\n",
      "    epoch          : 146\n",
      "    loss           : -537871.5884375\n",
      "    val_loss       : -534164.4901489258\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -429378.312500\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -632969.750000\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -598830.187500\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -561362.312500\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -563489.375000\n",
      "    epoch          : 147\n",
      "    loss           : -535494.8865625\n",
      "    val_loss       : -540438.9381835938\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -651912.250000\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -565720.125000\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -447275.500000\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -638564.875000\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -450044.937500\n",
      "    epoch          : 148\n",
      "    loss           : -541394.291875\n",
      "    val_loss       : -543787.2353881836\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -640697.125000\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -494390.968750\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -565316.875000\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -445031.937500\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -598196.437500\n",
      "    epoch          : 149\n",
      "    loss           : -541750.50375\n",
      "    val_loss       : -537537.35234375\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -641174.750000\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -493779.281250\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -440414.250000\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -451309.906250\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -561468.625000\n",
      "    epoch          : 150\n",
      "    loss           : -539659.549375\n",
      "    val_loss       : -539879.0349975586\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0717_104644/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -641027.500000\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -596354.375000\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -662086.000000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -447352.687500\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -602218.375000\n",
      "    epoch          : 151\n",
      "    loss           : -538118.92875\n",
      "    val_loss       : -541624.2315063477\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -439445.375000\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -606729.062500\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -447032.968750\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -569993.312500\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -512545.843750\n",
      "    epoch          : 152\n",
      "    loss           : -544167.360625\n",
      "    val_loss       : -546170.5815307617\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -603009.250000\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -450159.218750\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -662554.625000\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -601200.937500\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -564456.750000\n",
      "    epoch          : 153\n",
      "    loss           : -544079.7475\n",
      "    val_loss       : -544710.3743774414\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -448393.187500\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -565138.812500\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -498926.562500\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -496615.062500\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -612451.500000\n",
      "    epoch          : 154\n",
      "    loss           : -543981.0975\n",
      "    val_loss       : -544391.8435180665\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -603176.250000\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -447252.281250\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -454540.093750\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -447422.812500\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -449213.562500\n",
      "    epoch          : 155\n",
      "    loss           : -545559.74375\n",
      "    val_loss       : -548279.0111450196\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -502247.906250\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -506133.937500\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -660255.687500\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -500567.125000\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -567686.375000\n",
      "    epoch          : 156\n",
      "    loss           : -545733.1003125\n",
      "    val_loss       : -541897.2813720703\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -648431.812500\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -443935.031250\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -496452.750000\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -504835.625000\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -451378.500000\n",
      "    epoch          : 157\n",
      "    loss           : -546566.6296875\n",
      "    val_loss       : -549259.869543457\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -615845.000000\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -649889.875000\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -670479.062500\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -571538.625000\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -494867.562500\n",
      "    epoch          : 158\n",
      "    loss           : -548552.943125\n",
      "    val_loss       : -549351.304663086\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -643518.687500\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -454443.312500\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -661625.625000\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -495300.875000\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -666693.625000\n",
      "    epoch          : 159\n",
      "    loss           : -547087.5178125\n",
      "    val_loss       : -547430.0135864258\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -617319.500000\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -450510.343750\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -507697.000000\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -462761.375000\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -614844.437500\n",
      "    epoch          : 160\n",
      "    loss           : -549324.34125\n",
      "    val_loss       : -548169.3139160157\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -494606.750000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -575510.500000\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -455492.625000\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -608085.625000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -431955.968750\n",
      "    epoch          : 161\n",
      "    loss           : -546171.8115625\n",
      "    val_loss       : -547956.1631225586\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -454575.468750\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -456568.718750\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -575930.625000\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -451149.281250\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -576187.250000\n",
      "    epoch          : 162\n",
      "    loss           : -548489.18\n",
      "    val_loss       : -552143.4366577149\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -462474.625000\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -443583.781250\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -504992.312500\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -650540.375000\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -456490.343750\n",
      "    epoch          : 163\n",
      "    loss           : -549058.8378125\n",
      "    val_loss       : -553259.0688720703\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -462936.937500\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -455772.531250\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -677938.062500\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -657998.187500\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -613091.937500\n",
      "    epoch          : 164\n",
      "    loss           : -552853.6321875\n",
      "    val_loss       : -553267.8940917968\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -512869.531250\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -457584.281250\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -516567.125000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -501642.937500\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -610755.250000\n",
      "    epoch          : 165\n",
      "    loss           : -549882.2953125\n",
      "    val_loss       : -551040.5767089843\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -493023.593750\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -654012.312500\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -617525.250000\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -451955.031250\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -460571.750000\n",
      "    epoch          : 166\n",
      "    loss           : -552448.23875\n",
      "    val_loss       : -555347.4673461914\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -505336.375000\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -610397.500000\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -667999.937500\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -622723.625000\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -620365.437500\n",
      "    epoch          : 167\n",
      "    loss           : -554138.5871875\n",
      "    val_loss       : -554690.5830810547\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -613367.062500\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -461117.437500\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -656915.812500\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -446759.093750\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -431195.812500\n",
      "    epoch          : 168\n",
      "    loss           : -550207.8446875\n",
      "    val_loss       : -549531.8938964844\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -451332.406250\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -503024.781250\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -449700.812500\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -616804.437500\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -450201.031250\n",
      "    epoch          : 169\n",
      "    loss           : -551981.483125\n",
      "    val_loss       : -556094.8875366211\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -458294.875000\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -463973.750000\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -439711.750000\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -502158.375000\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -458308.281250\n",
      "    epoch          : 170\n",
      "    loss           : -554208.356875\n",
      "    val_loss       : -558316.1247680665\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -622180.062500\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -466804.312500\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -470478.750000\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -470147.875000\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -501996.625000\n",
      "    epoch          : 171\n",
      "    loss           : -556852.804375\n",
      "    val_loss       : -558317.133203125\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -468907.656250\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -508794.312500\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -584032.875000\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -676449.625000\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -622252.312500\n",
      "    epoch          : 172\n",
      "    loss           : -556855.760625\n",
      "    val_loss       : -558877.4071289062\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -624209.250000\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -627341.750000\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -512124.062500\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -457772.968750\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -624581.875000\n",
      "    epoch          : 173\n",
      "    loss           : -558077.961875\n",
      "    val_loss       : -557674.8691772461\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -511602.500000\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -627605.187500\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -519631.125000\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -501783.031250\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -623947.750000\n",
      "    epoch          : 174\n",
      "    loss           : -556879.0553125\n",
      "    val_loss       : -558461.3641357422\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -677745.750000\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -679600.750000\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -662076.625000\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -467390.500000\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -578812.312500\n",
      "    epoch          : 175\n",
      "    loss           : -554481.005\n",
      "    val_loss       : -556242.0079711914\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -456160.437500\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -681777.562500\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -511651.687500\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -617875.437500\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -459995.562500\n",
      "    epoch          : 176\n",
      "    loss           : -556734.67125\n",
      "    val_loss       : -559955.0270263671\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -512177.187500\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -622020.000000\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -663250.500000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -570216.750000\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -619215.812500\n",
      "    epoch          : 177\n",
      "    loss           : -555437.049375\n",
      "    val_loss       : -560427.6668334961\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -460614.312500\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -590527.937500\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -663885.250000\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -577849.062500\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -433836.562500\n",
      "    epoch          : 178\n",
      "    loss           : -556372.8153125\n",
      "    val_loss       : -557920.4512939453\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -628707.875000\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -458836.625000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -516158.375000\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -513337.812500\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -624440.000000\n",
      "    epoch          : 179\n",
      "    loss           : -559313.725625\n",
      "    val_loss       : -561895.8950317383\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -467794.500000\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -467453.062500\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -521217.593750\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -626904.437500\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -630440.875000\n",
      "    epoch          : 180\n",
      "    loss           : -561363.6559375\n",
      "    val_loss       : -562879.0810546875\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -466453.906250\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -509616.812500\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -518023.750000\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -683958.000000\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -579992.250000\n",
      "    epoch          : 181\n",
      "    loss           : -559364.94125\n",
      "    val_loss       : -553191.6866333007\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -447377.468750\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -665993.812500\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -589194.312500\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -581283.562500\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -630574.062500\n",
      "    epoch          : 182\n",
      "    loss           : -555345.61875\n",
      "    val_loss       : -562462.0864990235\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -518181.906250\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -455533.125000\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -630726.062500\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -680852.000000\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -458468.437500\n",
      "    epoch          : 183\n",
      "    loss           : -561598.253125\n",
      "    val_loss       : -563354.2137451172\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -687188.312500\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -462655.875000\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -669342.937500\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -516373.062500\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -509865.843750\n",
      "    epoch          : 184\n",
      "    loss           : -562357.2871875\n",
      "    val_loss       : -558728.8570678711\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -591784.312500\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -627736.500000\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -667197.812500\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -524899.937500\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -591686.812500\n",
      "    epoch          : 185\n",
      "    loss           : -561646.7853125\n",
      "    val_loss       : -564877.0331665039\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -511445.562500\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -668208.500000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -673230.312500\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -633926.125000\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -460800.750000\n",
      "    epoch          : 186\n",
      "    loss           : -563027.8803125\n",
      "    val_loss       : -563578.9402832031\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -631606.625000\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -635473.375000\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -462085.062500\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -451516.562500\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -627642.750000\n",
      "    epoch          : 187\n",
      "    loss           : -561726.52875\n",
      "    val_loss       : -562525.4296386719\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -669184.687500\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -689383.625000\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -458418.125000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -515595.531250\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -630380.375000\n",
      "    epoch          : 188\n",
      "    loss           : -562152.4659375\n",
      "    val_loss       : -564767.5122436524\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -506735.593750\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -631197.625000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -628264.437500\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -512884.875000\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -591217.500000\n",
      "    epoch          : 189\n",
      "    loss           : -560481.899375\n",
      "    val_loss       : -562211.2887695313\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -464334.500000\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -461134.562500\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -466000.937500\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -468127.281250\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -593735.562500\n",
      "    epoch          : 190\n",
      "    loss           : -565209.6403125\n",
      "    val_loss       : -567345.4907836914\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -517202.093750\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -592788.500000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -459205.937500\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -639694.937500\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -461003.562500\n",
      "    epoch          : 191\n",
      "    loss           : -566112.015625\n",
      "    val_loss       : -564012.3081665039\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -625222.500000\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -624805.625000\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -627127.000000\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -632204.562500\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -590192.375000\n",
      "    epoch          : 192\n",
      "    loss           : -563594.8453125\n",
      "    val_loss       : -565795.8989501953\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -671584.500000\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -674542.500000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -519338.250000\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -516080.718750\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -640955.875000\n",
      "    epoch          : 193\n",
      "    loss           : -562850.606875\n",
      "    val_loss       : -566643.8622436523\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -508617.812500\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -697786.375000\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -462788.250000\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -673743.125000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -593561.875000\n",
      "    epoch          : 194\n",
      "    loss           : -565129.228125\n",
      "    val_loss       : -567719.1782470703\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -468342.125000\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -459046.031250\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -465264.625000\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -510175.312500\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -630399.125000\n",
      "    epoch          : 195\n",
      "    loss           : -565052.2053125\n",
      "    val_loss       : -566184.1688476562\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -517983.187500\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -639899.937500\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -596176.562500\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -638165.875000\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -459127.656250\n",
      "    epoch          : 196\n",
      "    loss           : -567540.0546875\n",
      "    val_loss       : -568740.1674194336\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -677915.062500\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -514725.375000\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -527758.875000\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -636463.062500\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -638838.000000\n",
      "    epoch          : 197\n",
      "    loss           : -568561.4821875\n",
      "    val_loss       : -568581.3750366211\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -695235.500000\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -528257.062500\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -701073.875000\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -639532.625000\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -691319.125000\n",
      "    epoch          : 198\n",
      "    loss           : -568839.2484375\n",
      "    val_loss       : -569395.1676757813\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -464231.250000\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -466285.562500\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -592488.187500\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -641776.687500\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -644995.375000\n",
      "    epoch          : 199\n",
      "    loss           : -568353.833125\n",
      "    val_loss       : -570361.8030273437\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -526472.875000\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -515392.468750\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -472098.250000\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -628804.500000\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -458437.156250\n",
      "    epoch          : 200\n",
      "    loss           : -566053.1928125\n",
      "    val_loss       : -561601.5925170898\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0717_104644/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -446842.750000\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -612863.812500\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -645465.312500\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -635467.812500\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -521720.062500\n",
      "    epoch          : 201\n",
      "    loss           : -565827.67\n",
      "    val_loss       : -570524.9849365235\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -457401.968750\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -519658.656250\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -517016.718750\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -459682.187500\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -636396.125000\n",
      "    epoch          : 202\n",
      "    loss           : -568870.490625\n",
      "    val_loss       : -569095.5725952148\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -682945.812500\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -632336.937500\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -462928.156250\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -680311.000000\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -453232.656250\n",
      "    epoch          : 203\n",
      "    loss           : -565337.58\n",
      "    val_loss       : -568080.6049194336\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -594979.937500\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -640386.375000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -641906.437500\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -521921.781250\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -521362.531250\n",
      "    epoch          : 204\n",
      "    loss           : -567268.17625\n",
      "    val_loss       : -572195.6917602539\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -474974.000000\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -483270.812500\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -517902.468750\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -642779.875000\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -647927.875000\n",
      "    epoch          : 205\n",
      "    loss           : -571789.3421875\n",
      "    val_loss       : -573631.2034301758\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -517406.875000\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -638272.500000\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -457629.312500\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -468073.250000\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -455452.843750\n",
      "    epoch          : 206\n",
      "    loss           : -572429.6628125\n",
      "    val_loss       : -571350.4452270508\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -681178.062500\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -523820.437500\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -457180.156250\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -700344.687500\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -459584.406250\n",
      "    epoch          : 207\n",
      "    loss           : -570728.4759375\n",
      "    val_loss       : -571728.9465820312\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -525966.000000\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -700755.000000\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -642332.375000\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -529172.375000\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -516871.468750\n",
      "    epoch          : 208\n",
      "    loss           : -572036.931875\n",
      "    val_loss       : -572278.709008789\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -466873.125000\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -509287.468750\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -595295.812500\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -587556.250000\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -695079.062500\n",
      "    epoch          : 209\n",
      "    loss           : -568232.808125\n",
      "    val_loss       : -571785.0104980469\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -461180.625000\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -685706.750000\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -467157.625000\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -474471.500000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -463164.062500\n",
      "    epoch          : 210\n",
      "    loss           : -573170.3053125\n",
      "    val_loss       : -572889.992602539\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -460512.437500\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -453801.562500\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -692034.062500\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -683807.937500\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -468564.500000\n",
      "    epoch          : 211\n",
      "    loss           : -570599.2859375\n",
      "    val_loss       : -572599.5670410156\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -598354.812500\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -464073.437500\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -648775.250000\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -466904.312500\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -654384.750000\n",
      "    epoch          : 212\n",
      "    loss           : -573296.07375\n",
      "    val_loss       : -575061.1375854493\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -651537.562500\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -474131.718750\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -682632.062500\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -468667.125000\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -461615.562500\n",
      "    epoch          : 213\n",
      "    loss           : -571873.705625\n",
      "    val_loss       : -571801.986730957\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -687347.687500\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -473454.250000\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -604388.500000\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -698131.312500\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -650154.562500\n",
      "    epoch          : 214\n",
      "    loss           : -572014.500625\n",
      "    val_loss       : -574903.462121582\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -652961.625000\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -465457.375000\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -520316.468750\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -649197.937500\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -471727.187500\n",
      "    epoch          : 215\n",
      "    loss           : -573662.78375\n",
      "    val_loss       : -574507.3053955078\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -607748.750000\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -690727.500000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -701002.625000\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -640393.125000\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -639975.500000\n",
      "    epoch          : 216\n",
      "    loss           : -573773.58\n",
      "    val_loss       : -574661.1663452148\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -688215.562500\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -468076.562500\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -643453.000000\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -514431.375000\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -519956.906250\n",
      "    epoch          : 217\n",
      "    loss           : -570482.8671875\n",
      "    val_loss       : -574403.2162841797\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -707683.562500\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -524757.312500\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -525479.625000\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -529175.187500\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -606559.000000\n",
      "    epoch          : 218\n",
      "    loss           : -575766.7496875\n",
      "    val_loss       : -576876.0994995118\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -527358.500000\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -467535.156250\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -641259.187500\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -518908.625000\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -453278.281250\n",
      "    epoch          : 219\n",
      "    loss           : -573344.7446875\n",
      "    val_loss       : -573954.1338256836\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -653386.500000\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -473892.437500\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -690899.062500\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -635897.000000\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -519266.500000\n",
      "    epoch          : 220\n",
      "    loss           : -572651.569375\n",
      "    val_loss       : -577300.9955566407\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -461138.718750\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -467346.000000\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -695733.500000\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -483008.750000\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -524888.562500\n",
      "    epoch          : 221\n",
      "    loss           : -576680.0453125\n",
      "    val_loss       : -577123.1478271484\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -692052.750000\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -706959.375000\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -523920.656250\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -602044.187500\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -466691.406250\n",
      "    epoch          : 222\n",
      "    loss           : -576877.3921875\n",
      "    val_loss       : -575396.1542114258\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -648203.500000\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -485804.437500\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -607317.125000\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -519722.218750\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -526581.125000\n",
      "    epoch          : 223\n",
      "    loss           : -575712.0128125\n",
      "    val_loss       : -577903.2145141602\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -470606.343750\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -520761.812500\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -467847.312500\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -587289.375000\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -653756.687500\n",
      "    epoch          : 224\n",
      "    loss           : -571092.9428125\n",
      "    val_loss       : -576168.5554077148\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -482543.312500\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -524439.687500\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -524617.812500\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -640816.437500\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -657991.312500\n",
      "    epoch          : 225\n",
      "    loss           : -576163.260625\n",
      "    val_loss       : -579120.4474365234\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -532733.375000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -614225.125000\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -525233.000000\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -714387.125000\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -659795.375000\n",
      "    epoch          : 226\n",
      "    loss           : -578733.8559375\n",
      "    val_loss       : -580240.1869018555\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -475797.062500\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -651127.062500\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -644446.625000\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -471255.656250\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -611373.562500\n",
      "    epoch          : 227\n",
      "    loss           : -577535.8059375\n",
      "    val_loss       : -578572.5171386718\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -647057.625000\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -654286.375000\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -695333.375000\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -529463.000000\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -647497.375000\n",
      "    epoch          : 228\n",
      "    loss           : -578934.425\n",
      "    val_loss       : -580833.5371459961\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -652721.125000\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -650102.562500\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -528236.687500\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -644382.937500\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -463432.093750\n",
      "    epoch          : 229\n",
      "    loss           : -578783.5959375\n",
      "    val_loss       : -579964.1096801758\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -649584.875000\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -458603.125000\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -692834.250000\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -528570.625000\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -657102.500000\n",
      "    epoch          : 230\n",
      "    loss           : -579281.3809375\n",
      "    val_loss       : -579404.368395996\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -709464.812500\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -469711.750000\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -605365.125000\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -710106.500000\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -518932.906250\n",
      "    epoch          : 231\n",
      "    loss           : -577724.19\n",
      "    val_loss       : -579674.6942260743\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -612489.750000\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -607964.000000\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -694732.437500\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -598102.062500\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -651074.250000\n",
      "    epoch          : 232\n",
      "    loss           : -576114.2559375\n",
      "    val_loss       : -578526.4390869141\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -650271.250000\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -651450.437500\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -472141.906250\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -469027.187500\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -613996.000000\n",
      "    epoch          : 233\n",
      "    loss           : -578949.6375\n",
      "    val_loss       : -580276.4236083984\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -698131.750000\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -615616.062500\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -610011.250000\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -714694.125000\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -652633.562500\n",
      "    epoch          : 234\n",
      "    loss           : -580252.5925\n",
      "    val_loss       : -580576.8803222657\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -528020.937500\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -657178.500000\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -608325.312500\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -475465.593750\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -610652.250000\n",
      "    epoch          : 235\n",
      "    loss           : -579503.8065625\n",
      "    val_loss       : -577460.0964599609\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -658878.312500\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -655164.562500\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -515516.250000\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -605552.000000\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -654638.000000\n",
      "    epoch          : 236\n",
      "    loss           : -575525.8740625\n",
      "    val_loss       : -577012.5368652344\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -516878.562500\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -699388.625000\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -710645.312500\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -472988.312500\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -464694.375000\n",
      "    epoch          : 237\n",
      "    loss           : -576757.9634375\n",
      "    val_loss       : -580390.8704956055\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -472576.437500\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -718393.562500\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -523787.937500\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -651680.125000\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -454762.437500\n",
      "    epoch          : 238\n",
      "    loss           : -580600.7190625\n",
      "    val_loss       : -583310.6371826172\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -654980.250000\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -698182.812500\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -656812.250000\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -609843.437500\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -659507.812500\n",
      "    epoch          : 239\n",
      "    loss           : -578419.2909375\n",
      "    val_loss       : -574346.9866455079\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -640579.250000\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -652651.125000\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -699331.750000\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -656696.875000\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -606510.687500\n",
      "    epoch          : 240\n",
      "    loss           : -579680.135\n",
      "    val_loss       : -578267.7199951172\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -614468.312500\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -463078.437500\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -472068.843750\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -612163.750000\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -711628.750000\n",
      "    epoch          : 241\n",
      "    loss           : -576961.05625\n",
      "    val_loss       : -579027.1234375\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -660457.437500\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -462954.375000\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -474927.500000\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -659727.812500\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -647283.312500\n",
      "    epoch          : 242\n",
      "    loss           : -579159.0625\n",
      "    val_loss       : -581803.8812133789\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -475482.718750\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -526528.875000\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -461714.750000\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -469701.562500\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -647528.250000\n",
      "    epoch          : 243\n",
      "    loss           : -580746.6003125\n",
      "    val_loss       : -584391.8067993164\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -475937.187500\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -722855.937500\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -462911.531250\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -467667.312500\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -484871.437500\n",
      "    epoch          : 244\n",
      "    loss           : -582497.0965625\n",
      "    val_loss       : -582202.8379028321\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -713464.750000\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -470873.687500\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -475654.000000\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -466256.812500\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -455370.781250\n",
      "    epoch          : 245\n",
      "    loss           : -582511.9353125\n",
      "    val_loss       : -584064.5627441406\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -658157.937500\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -461308.750000\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -476449.593750\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -460151.406250\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -471934.187500\n",
      "    epoch          : 246\n",
      "    loss           : -582497.759375\n",
      "    val_loss       : -580663.2220214844\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -474590.812500\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -529036.437500\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -613568.500000\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -457502.187500\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -613713.687500\n",
      "    epoch          : 247\n",
      "    loss           : -581839.35125\n",
      "    val_loss       : -582379.8414672852\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -615666.437500\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -515001.875000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -723063.000000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -523645.156250\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -656245.250000\n",
      "    epoch          : 248\n",
      "    loss           : -580543.0090625\n",
      "    val_loss       : -578875.2662475586\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -467752.812500\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -472028.500000\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -531417.312500\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -660637.562500\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -720203.812500\n",
      "    epoch          : 249\n",
      "    loss           : -584167.1334375\n",
      "    val_loss       : -584936.204675293\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -459827.718750\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -530333.500000\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -702621.437500\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -527753.375000\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -617650.812500\n",
      "    epoch          : 250\n",
      "    loss           : -585002.53\n",
      "    val_loss       : -585861.8681518554\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0717_104644/checkpoint-epoch250.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -483927.500000\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -483416.812500\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -724156.750000\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -667377.125000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -657365.125000\n",
      "    epoch          : 251\n",
      "    loss           : -585972.8465625\n",
      "    val_loss       : -584971.2779541016\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -465924.781250\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -701714.812500\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -476879.218750\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -612366.062500\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -605836.062500\n",
      "    epoch          : 252\n",
      "    loss           : -582334.27\n",
      "    val_loss       : -580353.5311889648\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -665605.625000\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -466677.187500\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -469500.375000\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -667108.500000\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -464905.375000\n",
      "    epoch          : 253\n",
      "    loss           : -582780.426875\n",
      "    val_loss       : -586049.2607299804\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -468057.125000\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -472685.437500\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -522235.687500\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -526178.375000\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -612646.750000\n",
      "    epoch          : 254\n",
      "    loss           : -584516.8259375\n",
      "    val_loss       : -582317.0092041015\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -710003.812500\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -478931.000000\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -655325.125000\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -655704.812500\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -464287.312500\n",
      "    epoch          : 255\n",
      "    loss           : -579770.5784375\n",
      "    val_loss       : -583492.5242553711\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -658813.500000\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -720074.375000\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -528534.375000\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -658859.625000\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -614076.937500\n",
      "    epoch          : 256\n",
      "    loss           : -585819.215\n",
      "    val_loss       : -587629.7329833985\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -529182.125000\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -730770.125000\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -486340.968750\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -612306.750000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -667318.375000\n",
      "    epoch          : 257\n",
      "    loss           : -586158.8190625\n",
      "    val_loss       : -586209.0784057617\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -492381.500000\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -652433.187500\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -531904.187500\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -615060.875000\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -609762.375000\n",
      "    epoch          : 258\n",
      "    loss           : -581238.8296875\n",
      "    val_loss       : -579263.835913086\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -665345.625000\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -664853.125000\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -657623.750000\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -655403.437500\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -602197.375000\n",
      "    epoch          : 259\n",
      "    loss           : -583375.099375\n",
      "    val_loss       : -580197.3843383789\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -472429.593750\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -729219.812500\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -530681.250000\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -473631.625000\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -660852.250000\n",
      "    epoch          : 260\n",
      "    loss           : -585255.9396875\n",
      "    val_loss       : -587884.5459228515\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -467216.625000\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -726731.062500\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -710869.625000\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -620534.000000\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -671647.500000\n",
      "    epoch          : 261\n",
      "    loss           : -587408.1596875\n",
      "    val_loss       : -587106.9920532226\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -675647.187500\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -471397.062500\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -655096.812500\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -471048.125000\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -487862.625000\n",
      "    epoch          : 262\n",
      "    loss           : -587336.8446875\n",
      "    val_loss       : -585088.5061279297\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -532411.937500\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -709026.375000\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -464428.312500\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -665826.562500\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -661475.812500\n",
      "    epoch          : 263\n",
      "    loss           : -583294.78875\n",
      "    val_loss       : -586698.8716552735\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -725467.500000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -474478.281250\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -712743.375000\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -531015.375000\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -477207.468750\n",
      "    epoch          : 264\n",
      "    loss           : -587793.313125\n",
      "    val_loss       : -589339.9618652344\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -474065.093750\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -521841.687500\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -709507.187500\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -473649.343750\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -730606.562500\n",
      "    epoch          : 265\n",
      "    loss           : -589160.6234375\n",
      "    val_loss       : -589084.671105957\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -626342.500000\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -710934.375000\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -478534.812500\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -483311.906250\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -460977.250000\n",
      "    epoch          : 266\n",
      "    loss           : -587953.5065625\n",
      "    val_loss       : -587027.9143432617\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -717811.750000\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -658598.625000\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -661585.875000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -657156.500000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -511155.031250\n",
      "    epoch          : 267\n",
      "    loss           : -582333.464375\n",
      "    val_loss       : -586364.7135009766\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -462964.906250\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -731619.125000\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -675365.437500\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -725704.625000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -621145.562500\n",
      "    epoch          : 268\n",
      "    loss           : -587525.553125\n",
      "    val_loss       : -588687.5843383789\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -727826.875000\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -657228.875000\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -732506.875000\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -481687.375000\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -471894.312500\n",
      "    epoch          : 269\n",
      "    loss           : -589592.190625\n",
      "    val_loss       : -590406.869909668\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -534793.375000\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -534984.875000\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -620053.000000\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -673309.750000\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -675460.812500\n",
      "    epoch          : 270\n",
      "    loss           : -589786.8596875\n",
      "    val_loss       : -589803.2400024415\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -732008.375000\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -539114.250000\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -715088.125000\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -675135.687500\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -619215.250000\n",
      "    epoch          : 271\n",
      "    loss           : -588414.6984375\n",
      "    val_loss       : -586884.2763549804\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -527846.875000\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -536159.937500\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -466233.437500\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -469704.406250\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -619356.562500\n",
      "    epoch          : 272\n",
      "    loss           : -584726.4209375\n",
      "    val_loss       : -588803.1263793946\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -480072.906250\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -680209.187500\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -676134.750000\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -727589.937500\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -468753.156250\n",
      "    epoch          : 273\n",
      "    loss           : -586662.186875\n",
      "    val_loss       : -585228.5309082031\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -675901.375000\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -671910.500000\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -478015.500000\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -474096.687500\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -620624.312500\n",
      "    epoch          : 274\n",
      "    loss           : -585242.7653125\n",
      "    val_loss       : -584458.325012207\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -480958.937500\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -735604.187500\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -726445.625000\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -530856.500000\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -663020.937500\n",
      "    epoch          : 275\n",
      "    loss           : -588086.70625\n",
      "    val_loss       : -589094.8869750977\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -467986.968750\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -713732.875000\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -531723.500000\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -540818.250000\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -675266.000000\n",
      "    epoch          : 276\n",
      "    loss           : -588924.1\n",
      "    val_loss       : -589874.8070800782\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -535090.750000\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -471492.875000\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -483701.281250\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -674078.250000\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -535918.812500\n",
      "    epoch          : 277\n",
      "    loss           : -589671.2871875\n",
      "    val_loss       : -590519.703869629\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -530126.375000\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -519923.562500\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -532715.625000\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -528465.000000\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -673770.375000\n",
      "    epoch          : 278\n",
      "    loss           : -590662.3678125\n",
      "    val_loss       : -591020.5657226562\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -531526.625000\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -535409.000000\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -532490.937500\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -522857.062500\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -679106.000000\n",
      "    epoch          : 279\n",
      "    loss           : -590296.648125\n",
      "    val_loss       : -590101.4926391601\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -716887.312500\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -532768.062500\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -530060.312500\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -479012.593750\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -624516.937500\n",
      "    epoch          : 280\n",
      "    loss           : -589434.1965625\n",
      "    val_loss       : -589692.4855224609\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -471064.562500\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -529325.062500\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -539130.875000\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -626047.312500\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -472707.781250\n",
      "    epoch          : 281\n",
      "    loss           : -590185.7990625\n",
      "    val_loss       : -591506.8596679687\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -716762.062500\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -532859.062500\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -533437.500000\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -664246.875000\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -677821.312500\n",
      "    epoch          : 282\n",
      "    loss           : -591001.1446875\n",
      "    val_loss       : -592424.8647216797\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -474336.125000\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -676775.812500\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -620414.750000\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -670700.875000\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -615763.187500\n",
      "    epoch          : 283\n",
      "    loss           : -591031.783125\n",
      "    val_loss       : -589190.5852661133\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -472426.125000\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -659341.875000\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -530278.687500\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -534669.687500\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -475444.031250\n",
      "    epoch          : 284\n",
      "    loss           : -589152.3084375\n",
      "    val_loss       : -590702.5965332031\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -662296.062500\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -627060.625000\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -528587.437500\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -534732.125000\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -730141.250000\n",
      "    epoch          : 285\n",
      "    loss           : -590017.54125\n",
      "    val_loss       : -592658.6165039062\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -620177.125000\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -735934.750000\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -473630.562500\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -448545.562500\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -525338.000000\n",
      "    epoch          : 286\n",
      "    loss           : -586916.638125\n",
      "    val_loss       : -580419.0245239257\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -465032.500000\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -675539.500000\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -475405.125000\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -530613.250000\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -624787.625000\n",
      "    epoch          : 287\n",
      "    loss           : -590707.3465625\n",
      "    val_loss       : -593115.0141967774\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -719535.437500\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -736430.750000\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -672400.812500\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -486948.718750\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -477644.125000\n",
      "    epoch          : 288\n",
      "    loss           : -593414.19\n",
      "    val_loss       : -593874.9916381836\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -666139.000000\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -684840.625000\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -535162.125000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -532985.125000\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -521889.687500\n",
      "    epoch          : 289\n",
      "    loss           : -593009.2196875\n",
      "    val_loss       : -593626.7223510742\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -538319.750000\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -738998.187500\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -620856.875000\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -521380.218750\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -669360.500000\n",
      "    epoch          : 290\n",
      "    loss           : -589561.5215625\n",
      "    val_loss       : -592211.8754516601\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -735825.875000\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -717183.062500\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -737327.750000\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -729305.125000\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -676413.125000\n",
      "    epoch          : 291\n",
      "    loss           : -591865.780625\n",
      "    val_loss       : -592958.6094482422\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -671516.125000\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -737231.625000\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -718976.812500\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -721383.687500\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -678298.125000\n",
      "    epoch          : 292\n",
      "    loss           : -592710.358125\n",
      "    val_loss       : -592004.837866211\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -731062.625000\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -485851.000000\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -630753.812500\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -735103.125000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -528115.875000\n",
      "    epoch          : 293\n",
      "    loss           : -593660.49375\n",
      "    val_loss       : -594172.1734741211\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -627462.250000\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -731117.250000\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -533976.437500\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -533238.625000\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -683724.812500\n",
      "    epoch          : 294\n",
      "    loss           : -593853.2871875\n",
      "    val_loss       : -593587.7229980469\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -473823.093750\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -476810.843750\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -530990.187500\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -661590.375000\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -678036.000000\n",
      "    epoch          : 295\n",
      "    loss           : -589462.7340625\n",
      "    val_loss       : -591180.1675170899\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -666389.875000\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -668352.000000\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -663960.625000\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -674410.250000\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -623119.875000\n",
      "    epoch          : 296\n",
      "    loss           : -590959.5734375\n",
      "    val_loss       : -593651.6630249023\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -482814.093750\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -677894.937500\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -483269.375000\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -481915.500000\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -484583.343750\n",
      "    epoch          : 297\n",
      "    loss           : -592778.4571875\n",
      "    val_loss       : -592909.3897338867\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -718016.000000\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -741041.750000\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -466886.593750\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -683664.875000\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -677081.062500\n",
      "    epoch          : 298\n",
      "    loss           : -592835.7940625\n",
      "    val_loss       : -589283.7202026367\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -532566.250000\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -474230.937500\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -625797.750000\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -682634.187500\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -468612.000000\n",
      "    epoch          : 299\n",
      "    loss           : -591230.36\n",
      "    val_loss       : -594214.1461181641\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -718123.625000\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -533632.250000\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -480557.812500\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -672371.000000\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -465779.406250\n",
      "    epoch          : 300\n",
      "    loss           : -590256.1340625\n",
      "    val_loss       : -588691.2676147461\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0717_104644/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -539818.187500\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -525106.625000\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -626263.312500\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -671435.750000\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -664869.125000\n",
      "    epoch          : 301\n",
      "    loss           : -593563.128125\n",
      "    val_loss       : -595471.7245117187\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -480020.812500\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -535555.625000\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -680796.000000\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -463860.593750\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -684281.500000\n",
      "    epoch          : 302\n",
      "    loss           : -592599.06625\n",
      "    val_loss       : -592633.1978393554\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -622077.625000\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -486630.125000\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -491291.093750\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -626701.687500\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -463726.312500\n",
      "    epoch          : 303\n",
      "    loss           : -594260.5371875\n",
      "    val_loss       : -594564.0627929687\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -625850.250000\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -545593.875000\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -520936.250000\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -497985.812500\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -458368.375000\n",
      "    epoch          : 304\n",
      "    loss           : -586839.8121875\n",
      "    val_loss       : -589022.1954467774\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -741513.500000\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -625392.687500\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -491198.250000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -670508.875000\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -629244.312500\n",
      "    epoch          : 305\n",
      "    loss           : -593684.5871875\n",
      "    val_loss       : -595202.9893798828\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -742576.625000\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -533813.187500\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -487702.687500\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -739517.062500\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -670347.562500\n",
      "    epoch          : 306\n",
      "    loss           : -595445.008125\n",
      "    val_loss       : -595565.0862426758\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -480100.156250\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -534432.250000\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -672126.625000\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -687818.625000\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -679817.562500\n",
      "    epoch          : 307\n",
      "    loss           : -595911.729375\n",
      "    val_loss       : -596661.5808837891\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -471170.406250\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -531136.312500\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -534604.312500\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -537422.500000\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -685604.875000\n",
      "    epoch          : 308\n",
      "    loss           : -594992.63875\n",
      "    val_loss       : -593054.5348266602\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -736957.125000\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -484151.125000\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -475058.281250\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -443717.937500\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -666528.500000\n",
      "    epoch          : 309\n",
      "    loss           : -586900.955625\n",
      "    val_loss       : -593712.1885742188\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -626241.062500\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -680269.625000\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -722491.750000\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -723758.750000\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -687930.375000\n",
      "    epoch          : 310\n",
      "    loss           : -592343.725625\n",
      "    val_loss       : -596427.5979125977\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -668584.875000\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -673441.125000\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -537626.750000\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -530709.812500\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -467658.343750\n",
      "    epoch          : 311\n",
      "    loss           : -595067.7421875\n",
      "    val_loss       : -593798.3513549805\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -530635.625000\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -469517.531250\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -716790.750000\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -469281.187500\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -672816.437500\n",
      "    epoch          : 312\n",
      "    loss           : -591451.2153125\n",
      "    val_loss       : -594292.3010742187\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -729005.875000\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -474037.312500\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -726214.062500\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -744890.500000\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -683639.000000\n",
      "    epoch          : 313\n",
      "    loss           : -593558.9421875\n",
      "    val_loss       : -594706.678503418\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -745460.062500\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -535674.437500\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -481840.875000\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -625225.750000\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -546152.750000\n",
      "    epoch          : 314\n",
      "    loss           : -595485.2490625\n",
      "    val_loss       : -596897.6211059571\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -535323.687500\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -531310.125000\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -724948.250000\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -480264.812500\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -618319.875000\n",
      "    epoch          : 315\n",
      "    loss           : -593575.105625\n",
      "    val_loss       : -594511.700390625\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -486154.468750\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -486611.843750\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -731050.125000\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -673615.562500\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -674687.625000\n",
      "    epoch          : 316\n",
      "    loss           : -596019.5728125\n",
      "    val_loss       : -596504.5121948242\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -530661.062500\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -673996.437500\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -530961.625000\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -629363.687500\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -687915.125000\n",
      "    epoch          : 317\n",
      "    loss           : -594233.53375\n",
      "    val_loss       : -586589.2647583007\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -469486.375000\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -463414.406250\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -478510.843750\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -722529.500000\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -672512.125000\n",
      "    epoch          : 318\n",
      "    loss           : -587233.3771875\n",
      "    val_loss       : -593628.565246582\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -731849.875000\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -542001.312500\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -693525.937500\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -537793.312500\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -632577.187500\n",
      "    epoch          : 319\n",
      "    loss           : -596519.9684375\n",
      "    val_loss       : -596575.7911010742\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -673937.500000\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -480383.031250\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -750128.250000\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -744004.312500\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -669165.000000\n",
      "    epoch          : 320\n",
      "    loss           : -597465.1065625\n",
      "    val_loss       : -597769.9137207031\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -531417.375000\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -685143.625000\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -481233.656250\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -526668.750000\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -531183.750000\n",
      "    epoch          : 321\n",
      "    loss           : -595123.008125\n",
      "    val_loss       : -596086.7461669922\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -533631.000000\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -522011.562500\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -687529.500000\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -690977.187500\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -482968.781250\n",
      "    epoch          : 322\n",
      "    loss           : -592390.3375\n",
      "    val_loss       : -597596.0246948242\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -472552.156250\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -731338.625000\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -480804.468750\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -679648.937500\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -475837.625000\n",
      "    epoch          : 323\n",
      "    loss           : -598144.909375\n",
      "    val_loss       : -598491.9844970703\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -726904.562500\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -536628.000000\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -628790.062500\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -746153.250000\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -687325.937500\n",
      "    epoch          : 324\n",
      "    loss           : -598215.83625\n",
      "    val_loss       : -598740.3255004883\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -730762.187500\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -535535.187500\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -668719.250000\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -678369.625000\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -482293.937500\n",
      "    epoch          : 325\n",
      "    loss           : -597793.53375\n",
      "    val_loss       : -595643.0076660156\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -747900.875000\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -673041.125000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -746334.625000\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -481524.593750\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -622263.562500\n",
      "    epoch          : 326\n",
      "    loss           : -591749.16125\n",
      "    val_loss       : -592048.1541625976\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -519951.593750\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -477847.625000\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -629216.187500\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -485812.000000\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -533505.562500\n",
      "    epoch          : 327\n",
      "    loss           : -595644.8303125\n",
      "    val_loss       : -598291.7090576172\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -747103.437500\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -726937.937500\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -536119.812500\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -740227.812500\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -688405.750000\n",
      "    epoch          : 328\n",
      "    loss           : -598953.35625\n",
      "    val_loss       : -598810.5911743164\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -729465.937500\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -747216.625000\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -479143.593750\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -481243.031250\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -469194.531250\n",
      "    epoch          : 329\n",
      "    loss           : -598182.219375\n",
      "    val_loss       : -595853.9868286133\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -630736.375000\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -485152.000000\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -536980.875000\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -667289.312500\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -662631.625000\n",
      "    epoch          : 330\n",
      "    loss           : -597370.1246875\n",
      "    val_loss       : -597365.838635254\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -530907.062500\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -743566.062500\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -622030.250000\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -475292.500000\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -534109.437500\n",
      "    epoch          : 331\n",
      "    loss           : -594934.3309375\n",
      "    val_loss       : -597000.8864501953\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -743894.562500\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -534081.500000\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -731036.687500\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -476455.000000\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -486830.187500\n",
      "    epoch          : 332\n",
      "    loss           : -597949.8425\n",
      "    val_loss       : -597826.8600341796\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -478142.187500\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -685872.500000\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -481930.062500\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -667094.437500\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -475181.187500\n",
      "    epoch          : 333\n",
      "    loss           : -596411.7709375\n",
      "    val_loss       : -597673.9148681641\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -749954.562500\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -666974.500000\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -624830.625000\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -673815.000000\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -624768.375000\n",
      "    epoch          : 334\n",
      "    loss           : -597772.3634375\n",
      "    val_loss       : -599599.3008544922\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -539987.125000\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -747019.625000\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -481594.250000\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -673427.750000\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -693277.562500\n",
      "    epoch          : 335\n",
      "    loss           : -598962.5409375\n",
      "    val_loss       : -598682.5907226562\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -742290.000000\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -546280.375000\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -732443.562500\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -479506.906250\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -692413.687500\n",
      "    epoch          : 336\n",
      "    loss           : -598919.7175\n",
      "    val_loss       : -599574.1241210938\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -679008.687500\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -734093.062500\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -532415.125000\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -690003.187500\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -533781.312500\n",
      "    epoch          : 337\n",
      "    loss           : -600003.4225\n",
      "    val_loss       : -598136.9610961914\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -536283.062500\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -486477.437500\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -536610.500000\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -482837.812500\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -463164.968750\n",
      "    epoch          : 338\n",
      "    loss           : -598538.715625\n",
      "    val_loss       : -599027.5232910156\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -478037.531250\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -462135.375000\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -715618.500000\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -476228.250000\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -661491.750000\n",
      "    epoch          : 339\n",
      "    loss           : -590661.91875\n",
      "    val_loss       : -592134.4726928711\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -511017.656250\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -528365.187500\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -734449.812500\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -477545.906250\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -488469.562500\n",
      "    epoch          : 340\n",
      "    loss           : -596891.858125\n",
      "    val_loss       : -599777.1268188476\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -485593.937500\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -473198.125000\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -628714.375000\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -632269.937500\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -475793.125000\n",
      "    epoch          : 341\n",
      "    loss           : -599799.976875\n",
      "    val_loss       : -599324.8548706055\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -493588.375000\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -483585.875000\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -636284.937500\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -632389.750000\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -463805.718750\n",
      "    epoch          : 342\n",
      "    loss           : -600771.135625\n",
      "    val_loss       : -598784.6235107422\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -479056.625000\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -531904.875000\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -628879.937500\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -477729.437500\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -474940.937500\n",
      "    epoch          : 343\n",
      "    loss           : -598982.6815625\n",
      "    val_loss       : -599725.5983032227\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -489462.687500\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -536375.000000\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -697994.125000\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -477471.125000\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -535102.625000\n",
      "    epoch          : 344\n",
      "    loss           : -600162.674375\n",
      "    val_loss       : -598761.0930664062\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -628297.875000\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -736677.500000\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -484984.843750\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -536961.875000\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -529151.625000\n",
      "    epoch          : 345\n",
      "    loss           : -598464.1675\n",
      "    val_loss       : -596086.8598754883\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -746048.250000\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -737775.562500\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -754257.812500\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -466760.562500\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -532769.625000\n",
      "    epoch          : 346\n",
      "    loss           : -594382.1078125\n",
      "    val_loss       : -596852.7106933594\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -479006.312500\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -635993.375000\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -538178.187500\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -758177.562500\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -694896.312500\n",
      "    epoch          : 347\n",
      "    loss           : -599164.9071875\n",
      "    val_loss       : -601369.24375\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -739192.750000\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -473648.093750\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -535326.875000\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -468913.281250\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -535972.937500\n",
      "    epoch          : 348\n",
      "    loss           : -601055.54875\n",
      "    val_loss       : -601384.867590332\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -739404.000000\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -483344.812500\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -535003.750000\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -631559.187500\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -473664.250000\n",
      "    epoch          : 349\n",
      "    loss           : -600715.4603125\n",
      "    val_loss       : -598812.4869995117\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -736801.375000\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -489212.437500\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -677054.375000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -625638.000000\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -623743.500000\n",
      "    epoch          : 350\n",
      "    loss           : -600402.473125\n",
      "    val_loss       : -600323.6639648437\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0717_104644/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -690653.750000\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -733828.375000\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -753273.625000\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -691662.375000\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -678571.562500\n",
      "    epoch          : 351\n",
      "    loss           : -601602.550625\n",
      "    val_loss       : -601575.0166259765\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -699020.750000\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -476225.125000\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -533816.625000\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -635461.375000\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -630191.187500\n",
      "    epoch          : 352\n",
      "    loss           : -597315.253125\n",
      "    val_loss       : -599231.6563842774\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -531712.875000\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -482032.062500\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -694234.375000\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -482604.593750\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -473105.031250\n",
      "    epoch          : 353\n",
      "    loss           : -600490.7990625\n",
      "    val_loss       : -600392.2663208008\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -632925.500000\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -633131.250000\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -674019.000000\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -731912.187500\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -681315.562500\n",
      "    epoch          : 354\n",
      "    loss           : -599668.8428125\n",
      "    val_loss       : -596631.6485473632\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -738006.437500\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -495214.656250\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -487459.656250\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -536415.562500\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -669602.375000\n",
      "    epoch          : 355\n",
      "    loss           : -594886.6934375\n",
      "    val_loss       : -594012.8841308594\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -624900.375000\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -535843.000000\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -689695.875000\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -538810.250000\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -700556.375000\n",
      "    epoch          : 356\n",
      "    loss           : -598012.0678125\n",
      "    val_loss       : -598729.0557983399\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -751047.625000\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -673090.250000\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -697933.625000\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -672012.312500\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -686697.750000\n",
      "    epoch          : 357\n",
      "    loss           : -600877.19625\n",
      "    val_loss       : -600356.5630859375\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -491219.375000\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -678348.125000\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -753449.250000\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -676594.937500\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -477270.000000\n",
      "    epoch          : 358\n",
      "    loss           : -602395.4853125\n",
      "    val_loss       : -601916.4270263672\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -633746.750000\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -743625.875000\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -740236.562500\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -471895.468750\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -634454.187500\n",
      "    epoch          : 359\n",
      "    loss           : -602652.5196875\n",
      "    val_loss       : -602595.5770263672\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -534555.562500\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -625672.500000\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -492203.656250\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -636340.937500\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -492657.125000\n",
      "    epoch          : 360\n",
      "    loss           : -602024.129375\n",
      "    val_loss       : -601560.0771972656\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -477638.750000\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -545048.500000\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -731041.000000\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -678721.437500\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -490931.781250\n",
      "    epoch          : 361\n",
      "    loss           : -600097.913125\n",
      "    val_loss       : -600185.0448852539\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -699377.500000\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -486955.093750\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -630068.000000\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -694617.375000\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -469743.093750\n",
      "    epoch          : 362\n",
      "    loss           : -599807.8259375\n",
      "    val_loss       : -600788.1625732422\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -760611.250000\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -672607.250000\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -481790.312500\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -740264.625000\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -530794.187500\n",
      "    epoch          : 363\n",
      "    loss           : -600317.9234375\n",
      "    val_loss       : -600762.1393676758\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -755119.000000\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -750358.875000\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -473004.500000\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -538228.500000\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -699508.937500\n",
      "    epoch          : 364\n",
      "    loss           : -599580.1128125\n",
      "    val_loss       : -603742.8164916992\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -487506.093750\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -489614.656250\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -740648.312500\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -697586.625000\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -627099.125000\n",
      "    epoch          : 365\n",
      "    loss           : -598943.226875\n",
      "    val_loss       : -599761.7628417969\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -682844.687500\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -481161.937500\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -525627.437500\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -534205.875000\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -691237.625000\n",
      "    epoch          : 366\n",
      "    loss           : -599148.85125\n",
      "    val_loss       : -599094.5408569336\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -738565.750000\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -687481.812500\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -636730.812500\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -467409.312500\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -631146.000000\n",
      "    epoch          : 367\n",
      "    loss           : -600908.8003125\n",
      "    val_loss       : -600388.102307129\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -695154.125000\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -480500.031250\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -754449.250000\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -542202.750000\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -701325.187500\n",
      "    epoch          : 368\n",
      "    loss           : -600014.8284375\n",
      "    val_loss       : -601450.0864501953\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -738841.812500\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -738108.000000\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -478499.187500\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -746899.562500\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -480352.687500\n",
      "    epoch          : 369\n",
      "    loss           : -601460.2265625\n",
      "    val_loss       : -601166.1381103515\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -531341.250000\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -755090.875000\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -537024.875000\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -625505.750000\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -669931.062500\n",
      "    epoch          : 370\n",
      "    loss           : -600537.4415625\n",
      "    val_loss       : -598837.9030761719\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -528351.500000\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -748360.062500\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -533315.812500\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -683398.812500\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -463959.187500\n",
      "    epoch          : 371\n",
      "    loss           : -600077.036875\n",
      "    val_loss       : -599919.1410766601\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -536681.875000\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -538974.312500\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -690465.125000\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -470777.343750\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -480021.875000\n",
      "    epoch          : 372\n",
      "    loss           : -601514.1609375\n",
      "    val_loss       : -602089.88203125\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -484154.000000\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -543117.250000\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -535737.250000\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -530217.687500\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -685013.750000\n",
      "    epoch          : 373\n",
      "    loss           : -601143.275\n",
      "    val_loss       : -597715.1342895508\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -472780.000000\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -752328.812500\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -534636.875000\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -491874.031250\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -626438.000000\n",
      "    epoch          : 374\n",
      "    loss           : -601676.48625\n",
      "    val_loss       : -601737.2934448242\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -753752.500000\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -489427.593750\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -540997.625000\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -700755.812500\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -473029.500000\n",
      "    epoch          : 375\n",
      "    loss           : -603820.4315625\n",
      "    val_loss       : -604251.8762817383\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -482534.906250\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -480791.125000\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -538286.375000\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -682187.750000\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -637128.500000\n",
      "    epoch          : 376\n",
      "    loss           : -604455.556875\n",
      "    val_loss       : -604926.0408081055\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -495813.250000\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -695383.875000\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -704762.875000\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -687651.687500\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -674168.125000\n",
      "    epoch          : 377\n",
      "    loss           : -604143.7340625\n",
      "    val_loss       : -601671.1533447265\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -481799.812500\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -756060.437500\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -625008.375000\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -732043.000000\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -729749.125000\n",
      "    epoch          : 378\n",
      "    loss           : -597284.054375\n",
      "    val_loss       : -593715.7835571289\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -529626.625000\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -530542.500000\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -637238.625000\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -538592.375000\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -700835.375000\n",
      "    epoch          : 379\n",
      "    loss           : -601641.775\n",
      "    val_loss       : -604714.1794555665\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -683110.062500\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -757393.875000\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -761556.750000\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -675397.125000\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -630211.250000\n",
      "    epoch          : 380\n",
      "    loss           : -604341.9978125\n",
      "    val_loss       : -601711.063671875\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -744465.500000\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -485371.500000\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -701735.875000\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -477447.937500\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -481752.656250\n",
      "    epoch          : 381\n",
      "    loss           : -599582.3728125\n",
      "    val_loss       : -601450.7459472656\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -532943.187500\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -744314.187500\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -752802.062500\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -473551.562500\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -636564.375000\n",
      "    epoch          : 382\n",
      "    loss           : -603199.840625\n",
      "    val_loss       : -603055.5447265625\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -747042.750000\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -753698.250000\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -537638.687500\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -484010.000000\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -680254.625000\n",
      "    epoch          : 383\n",
      "    loss           : -602428.676875\n",
      "    val_loss       : -603184.5045166016\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -680459.125000\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -541843.750000\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -681018.937500\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -475140.750000\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -479774.843750\n",
      "    epoch          : 384\n",
      "    loss           : -601968.496875\n",
      "    val_loss       : -602390.2668945312\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -538113.125000\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -537841.437500\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -540258.500000\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -536128.375000\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -701028.500000\n",
      "    epoch          : 385\n",
      "    loss           : -603093.9721875\n",
      "    val_loss       : -601863.1607543945\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -743792.312500\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -750552.812500\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -531756.687500\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -526424.750000\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -700872.750000\n",
      "    epoch          : 386\n",
      "    loss           : -602978.303125\n",
      "    val_loss       : -602524.3478515625\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -756805.437500\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -757755.187500\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -677974.750000\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -756136.375000\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -465929.812500\n",
      "    epoch          : 387\n",
      "    loss           : -601966.21125\n",
      "    val_loss       : -602911.3827514648\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -534469.250000\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -738655.250000\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -479684.562500\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -480070.062500\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -636284.250000\n",
      "    epoch          : 388\n",
      "    loss           : -603883.64375\n",
      "    val_loss       : -602123.2559448242\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -474186.312500\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -540618.625000\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -531590.375000\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -634130.562500\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -679874.937500\n",
      "    epoch          : 389\n",
      "    loss           : -604470.3390625\n",
      "    val_loss       : -604113.2090942382\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -637871.062500\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -698437.187500\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -530391.937500\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -742933.375000\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -478476.500000\n",
      "    epoch          : 390\n",
      "    loss           : -600831.8390625\n",
      "    val_loss       : -599579.7809814453\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -481399.937500\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -678887.562500\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -538601.312500\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -475106.843750\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -749357.000000\n",
      "    epoch          : 391\n",
      "    loss           : -601696.3875\n",
      "    val_loss       : -602525.9126708985\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -743852.687500\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -705357.375000\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -679248.500000\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -491005.687500\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -680288.625000\n",
      "    epoch          : 392\n",
      "    loss           : -604839.6896875\n",
      "    val_loss       : -605124.6296875\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -476746.437500\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -744941.375000\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -669769.375000\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -702270.875000\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -534900.125000\n",
      "    epoch          : 393\n",
      "    loss           : -604319.2784375\n",
      "    val_loss       : -603496.5265869141\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -700446.625000\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -481028.062500\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -497802.812500\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -535968.437500\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -684252.875000\n",
      "    epoch          : 394\n",
      "    loss           : -605011.656875\n",
      "    val_loss       : -604573.5726318359\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -489181.531250\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -635318.125000\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -706175.937500\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -632500.000000\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -627027.937500\n",
      "    epoch          : 395\n",
      "    loss           : -604484.1978125\n",
      "    val_loss       : -604487.1401367188\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -476264.500000\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -481247.250000\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -483670.937500\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -702855.000000\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -535552.375000\n",
      "    epoch          : 396\n",
      "    loss           : -605438.083125\n",
      "    val_loss       : -605322.665637207\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -739417.937500\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -548980.625000\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -753718.750000\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -700769.062500\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -684705.187500\n",
      "    epoch          : 397\n",
      "    loss           : -605176.9696875\n",
      "    val_loss       : -604224.0606811524\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -745718.875000\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -540124.062500\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -674092.625000\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -756374.500000\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -485833.437500\n",
      "    epoch          : 398\n",
      "    loss           : -604487.5015625\n",
      "    val_loss       : -604026.6332519532\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -478531.500000\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -628609.562500\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -627890.125000\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -694015.062500\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -632083.000000\n",
      "    epoch          : 399\n",
      "    loss           : -600666.8659375\n",
      "    val_loss       : -601948.6104614257\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -755743.000000\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -480019.812500\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -744968.000000\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -628658.437500\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -473105.812500\n",
      "    epoch          : 400\n",
      "    loss           : -604629.9865625\n",
      "    val_loss       : -605354.0170410157\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0717_104644/checkpoint-epoch400.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -486954.125000\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -484055.125000\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -698856.937500\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -483668.406250\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -700397.375000\n",
      "    epoch          : 401\n",
      "    loss           : -605162.39875\n",
      "    val_loss       : -604268.4645629883\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -467896.218750\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -634055.187500\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -757179.812500\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -474416.562500\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -627044.937500\n",
      "    epoch          : 402\n",
      "    loss           : -604628.074375\n",
      "    val_loss       : -603751.4877319336\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -490081.218750\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -478217.406250\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -686538.875000\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -478332.625000\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -761352.125000\n",
      "    epoch          : 403\n",
      "    loss           : -605285.2240625\n",
      "    val_loss       : -600922.6941162109\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -743215.125000\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -536631.562500\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -743026.500000\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -706642.937500\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -701854.000000\n",
      "    epoch          : 404\n",
      "    loss           : -598553.9478125\n",
      "    val_loss       : -603929.9412231445\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -469916.875000\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -494568.375000\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -684245.375000\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -482526.343750\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -481086.625000\n",
      "    epoch          : 405\n",
      "    loss           : -605481.35625\n",
      "    val_loss       : -605286.2803833007\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -531290.000000\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -747430.875000\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -760534.375000\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -501064.906250\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -687728.750000\n",
      "    epoch          : 406\n",
      "    loss           : -605605.005625\n",
      "    val_loss       : -606063.6998657227\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -683272.562500\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -499001.531250\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -540503.250000\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -531670.812500\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -475968.937500\n",
      "    epoch          : 407\n",
      "    loss           : -606278.6625\n",
      "    val_loss       : -604913.5925048828\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -763209.437500\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -480069.750000\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -765216.750000\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -761302.625000\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -546697.687500\n",
      "    epoch          : 408\n",
      "    loss           : -604040.61625\n",
      "    val_loss       : -597309.8837646485\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -742053.062500\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -631692.375000\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -708491.750000\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -463292.437500\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -541212.125000\n",
      "    epoch          : 409\n",
      "    loss           : -601751.1940625\n",
      "    val_loss       : -601692.5914916992\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -487348.312500\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -632243.375000\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -541274.687500\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -639743.375000\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -475402.406250\n",
      "    epoch          : 410\n",
      "    loss           : -604660.495625\n",
      "    val_loss       : -605847.9124023437\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -747179.062500\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -676393.562500\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -485027.218750\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -478541.812500\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -530774.625000\n",
      "    epoch          : 411\n",
      "    loss           : -604121.61625\n",
      "    val_loss       : -600367.8277832031\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -467798.500000\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -541673.812500\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -680952.437500\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -534488.500000\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -536856.437500\n",
      "    epoch          : 412\n",
      "    loss           : -604623.2084375\n",
      "    val_loss       : -604282.835961914\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -480439.312500\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -746825.750000\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -481114.125000\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -687419.937500\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -679740.187500\n",
      "    epoch          : 413\n",
      "    loss           : -605073.3728125\n",
      "    val_loss       : -601269.6423339844\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -484613.750000\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -495605.937500\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -538519.625000\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -709751.625000\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -635625.187500\n",
      "    epoch          : 414\n",
      "    loss           : -605065.10875\n",
      "    val_loss       : -606405.8265991211\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -686417.375000\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -488757.437500\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -474952.968750\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -747015.625000\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -709596.687500\n",
      "    epoch          : 415\n",
      "    loss           : -607353.914375\n",
      "    val_loss       : -605947.0018676758\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -683081.687500\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -485333.437500\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -751867.750000\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -473175.125000\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -632139.312500\n",
      "    epoch          : 416\n",
      "    loss           : -606788.5309375\n",
      "    val_loss       : -603213.9520996094\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -473892.156250\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -632760.125000\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -745302.250000\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -492143.593750\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -480614.906250\n",
      "    epoch          : 417\n",
      "    loss           : -605104.2453125\n",
      "    val_loss       : -605936.3183715821\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -638708.875000\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -490348.687500\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -771000.625000\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -635861.000000\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -533845.125000\n",
      "    epoch          : 418\n",
      "    loss           : -607464.4140625\n",
      "    val_loss       : -605937.9112670899\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -702125.187500\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -683134.437500\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -753639.937500\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -539696.687500\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -688701.687500\n",
      "    epoch          : 419\n",
      "    loss           : -607621.6846875\n",
      "    val_loss       : -607426.3974487304\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -538579.937500\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -764196.250000\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -702374.375000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -489279.812500\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -710228.250000\n",
      "    epoch          : 420\n",
      "    loss           : -607663.3053125\n",
      "    val_loss       : -605040.753930664\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -750586.062500\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -680210.375000\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -708806.250000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -707161.000000\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -700772.750000\n",
      "    epoch          : 421\n",
      "    loss           : -605505.9134375\n",
      "    val_loss       : -602505.9971801757\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -488034.312500\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -702687.875000\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -487399.750000\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -637781.187500\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -639317.625000\n",
      "    epoch          : 422\n",
      "    loss           : -606230.9971875\n",
      "    val_loss       : -607313.4213256836\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -489207.062500\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -761797.250000\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -536795.687500\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -713306.000000\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -703671.875000\n",
      "    epoch          : 423\n",
      "    loss           : -606045.675\n",
      "    val_loss       : -605954.1584228516\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -543922.562500\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -528974.312500\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -754741.312500\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -481282.937500\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -704912.687500\n",
      "    epoch          : 424\n",
      "    loss           : -604965.0234375\n",
      "    val_loss       : -605827.2295776367\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -544098.375000\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -480770.750000\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -710034.437500\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -763507.562500\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -467165.843750\n",
      "    epoch          : 425\n",
      "    loss           : -605536.51\n",
      "    val_loss       : -606026.468005371\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -489415.156250\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -763140.625000\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -760429.937500\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -492657.531250\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -684646.937500\n",
      "    epoch          : 426\n",
      "    loss           : -606271.4396875\n",
      "    val_loss       : -599276.0195190429\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -685097.750000\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -760016.937500\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -673500.750000\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -474730.625000\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -707866.187500\n",
      "    epoch          : 427\n",
      "    loss           : -602743.4953125\n",
      "    val_loss       : -606495.8969726562\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -767387.625000\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -684561.250000\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -542860.812500\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -534866.687500\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -691400.437500\n",
      "    epoch          : 428\n",
      "    loss           : -606652.9034375\n",
      "    val_loss       : -606717.8461425782\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -538232.125000\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -489751.562500\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -708631.250000\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -677533.375000\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -706510.312500\n",
      "    epoch          : 429\n",
      "    loss           : -607876.51125\n",
      "    val_loss       : -606500.6299926757\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -483407.125000\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -760684.000000\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -751472.812500\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -750794.937500\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -680540.375000\n",
      "    epoch          : 430\n",
      "    loss           : -606945.575625\n",
      "    val_loss       : -605948.3109008789\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -481177.437500\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -477661.375000\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -750497.000000\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -748225.500000\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -481277.250000\n",
      "    epoch          : 431\n",
      "    loss           : -606225.7071875\n",
      "    val_loss       : -601063.1005981446\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -763177.000000\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -759073.812500\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -710912.625000\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -497266.187500\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -687287.125000\n",
      "    epoch          : 432\n",
      "    loss           : -605372.6509375\n",
      "    val_loss       : -606575.0603027344\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -689795.375000\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -542468.500000\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -641679.500000\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -702441.500000\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -479655.125000\n",
      "    epoch          : 433\n",
      "    loss           : -608434.33125\n",
      "    val_loss       : -607858.034362793\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -770287.062500\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -769460.250000\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -763320.375000\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -491100.937500\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -632280.625000\n",
      "    epoch          : 434\n",
      "    loss           : -608891.611875\n",
      "    val_loss       : -607354.5832763672\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -750652.000000\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -751217.312500\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -767014.375000\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -684062.625000\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -705665.000000\n",
      "    epoch          : 435\n",
      "    loss           : -608356.5503125\n",
      "    val_loss       : -603331.4079956055\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -700720.250000\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -703105.937500\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -475997.937500\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -713900.875000\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -746462.437500\n",
      "    epoch          : 436\n",
      "    loss           : -604626.17875\n",
      "    val_loss       : -602328.4203979492\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -751953.687500\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -757820.000000\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -539693.562500\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -745568.312500\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -703802.062500\n",
      "    epoch          : 437\n",
      "    loss           : -607247.895\n",
      "    val_loss       : -607116.1836059571\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -640075.812500\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -751422.312500\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -536413.000000\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -684400.437500\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -479990.375000\n",
      "    epoch          : 438\n",
      "    loss           : -607288.71875\n",
      "    val_loss       : -606674.2200439454\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -473850.000000\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -529549.812500\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -715449.062500\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -689081.875000\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -551908.062500\n",
      "    epoch          : 439\n",
      "    loss           : -608252.7225\n",
      "    val_loss       : -607646.7787597657\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -483720.218750\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -488185.625000\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -764565.375000\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -529987.875000\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -674654.375000\n",
      "    epoch          : 440\n",
      "    loss           : -605589.333125\n",
      "    val_loss       : -606223.4986816406\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -540027.875000\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -474209.687500\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -545106.750000\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -538089.187500\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -682687.500000\n",
      "    epoch          : 441\n",
      "    loss           : -606004.96875\n",
      "    val_loss       : -604866.763293457\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -472754.406250\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -748883.250000\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -758791.312500\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -754919.375000\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -637184.000000\n",
      "    epoch          : 442\n",
      "    loss           : -607602.1871875\n",
      "    val_loss       : -605292.9397216797\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -534821.562500\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -489080.187500\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -643261.750000\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -477294.687500\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -686107.250000\n",
      "    epoch          : 443\n",
      "    loss           : -608763.9775\n",
      "    val_loss       : -607050.0575561523\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -686999.250000\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -686251.500000\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -544252.687500\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -483230.750000\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -485349.875000\n",
      "    epoch          : 444\n",
      "    loss           : -609370.1490625\n",
      "    val_loss       : -606921.6677124023\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -683191.187500\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -626184.500000\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -496097.500000\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -747669.312500\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -673480.062500\n",
      "    epoch          : 445\n",
      "    loss           : -603799.61375\n",
      "    val_loss       : -600392.5050292969\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -535558.687500\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -479388.437500\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -764007.875000\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -477053.500000\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -549593.750000\n",
      "    epoch          : 446\n",
      "    loss           : -605898.2259375\n",
      "    val_loss       : -606089.6000610351\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -534645.437500\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -542932.750000\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -484713.468750\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -476404.812500\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -481756.531250\n",
      "    epoch          : 447\n",
      "    loss           : -607864.1425\n",
      "    val_loss       : -606339.7005371094\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -540783.750000\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -685940.125000\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -703106.125000\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -545171.875000\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -762292.187500\n",
      "    epoch          : 448\n",
      "    loss           : -608594.088125\n",
      "    val_loss       : -607326.7416870117\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -537482.375000\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -533975.187500\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -480095.656250\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -536846.125000\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -706446.562500\n",
      "    epoch          : 449\n",
      "    loss           : -607827.8328125\n",
      "    val_loss       : -607064.6906738281\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -538712.312500\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -493770.937500\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -635526.750000\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -753291.875000\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -701089.312500\n",
      "    epoch          : 450\n",
      "    loss           : -607163.4109375\n",
      "    val_loss       : -606441.4953857422\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0717_104644/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -770497.875000\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -762442.562500\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -753761.312500\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -746116.937500\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -687243.750000\n",
      "    epoch          : 451\n",
      "    loss           : -604804.980625\n",
      "    val_loss       : -608903.1394042969\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -483702.625000\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -547695.562500\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -711689.500000\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -692666.750000\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -714392.125000\n",
      "    epoch          : 452\n",
      "    loss           : -609851.486875\n",
      "    val_loss       : -608598.6623413085\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -485314.187500\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -686250.562500\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -472373.718750\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -759530.562500\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -539938.375000\n",
      "    epoch          : 453\n",
      "    loss           : -609420.778125\n",
      "    val_loss       : -607938.9655639648\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -677236.812500\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -488429.906250\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -489030.875000\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -637707.125000\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -710540.562500\n",
      "    epoch          : 454\n",
      "    loss           : -608789.6065625\n",
      "    val_loss       : -606421.821081543\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -686417.875000\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -526805.875000\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -532650.437500\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -467834.750000\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -474516.718750\n",
      "    epoch          : 455\n",
      "    loss           : -604631.6534375\n",
      "    val_loss       : -604576.2634399415\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -751522.125000\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -480069.500000\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -491858.375000\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -469432.781250\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -632300.500000\n",
      "    epoch          : 456\n",
      "    loss           : -607256.0059375\n",
      "    val_loss       : -606846.8078857422\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -536514.750000\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -481653.593750\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -543279.937500\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -635719.437500\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -475416.750000\n",
      "    epoch          : 457\n",
      "    loss           : -609452.22125\n",
      "    val_loss       : -608142.639074707\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -765041.312500\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -716010.687500\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -537303.687500\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -493324.500000\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -477264.156250\n",
      "    epoch          : 458\n",
      "    loss           : -604620.041875\n",
      "    val_loss       : -606891.8362304687\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -712561.875000\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -533556.500000\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -632500.750000\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -493944.656250\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -635920.750000\n",
      "    epoch          : 459\n",
      "    loss           : -607742.8265625\n",
      "    val_loss       : -607815.2395141602\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -545448.562500\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -758459.125000\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -688723.000000\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -529415.375000\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -485208.812500\n",
      "    epoch          : 460\n",
      "    loss           : -610023.6359375\n",
      "    val_loss       : -608462.098071289\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -769494.000000\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -540474.562500\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -484072.062500\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -479216.562500\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -716518.875000\n",
      "    epoch          : 461\n",
      "    loss           : -608815.538125\n",
      "    val_loss       : -609549.7440795898\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -540348.125000\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -542925.625000\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -486955.312500\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -485382.218750\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -486470.750000\n",
      "    epoch          : 462\n",
      "    loss           : -609794.1965625\n",
      "    val_loss       : -608844.6610961914\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -542331.312500\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -540910.500000\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -762227.375000\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -681652.500000\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -704350.500000\n",
      "    epoch          : 463\n",
      "    loss           : -606622.0478125\n",
      "    val_loss       : -605463.6586303711\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -495065.406250\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -683442.312500\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -487167.843750\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -687539.750000\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -690476.375000\n",
      "    epoch          : 464\n",
      "    loss           : -608598.750625\n",
      "    val_loss       : -607160.7538940429\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -531207.187500\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -540740.062500\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -771497.250000\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -712148.875000\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -477846.125000\n",
      "    epoch          : 465\n",
      "    loss           : -609789.6084375\n",
      "    val_loss       : -609376.3401855469\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -490184.906250\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -481285.437500\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -481572.250000\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -492246.375000\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -708200.437500\n",
      "    epoch          : 466\n",
      "    loss           : -605675.86\n",
      "    val_loss       : -602942.377722168\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -643512.062500\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -712785.375000\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -542544.937500\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -692057.500000\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -686007.437500\n",
      "    epoch          : 467\n",
      "    loss           : -608407.11\n",
      "    val_loss       : -609352.0478637696\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -493283.718750\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -678502.062500\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -689174.375000\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -489113.000000\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -764273.562500\n",
      "    epoch          : 468\n",
      "    loss           : -610190.950625\n",
      "    val_loss       : -609545.4273071289\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -636230.812500\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -682903.250000\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -690701.250000\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -710951.187500\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -709644.937500\n",
      "    epoch          : 469\n",
      "    loss           : -609955.31375\n",
      "    val_loss       : -607944.8884155273\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -542543.437500\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -638657.875000\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -545234.250000\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -480699.875000\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -715779.437500\n",
      "    epoch          : 470\n",
      "    loss           : -609875.394375\n",
      "    val_loss       : -609301.0634521485\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -718803.375000\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -495000.031250\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -773452.187500\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -533558.750000\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -702622.562500\n",
      "    epoch          : 471\n",
      "    loss           : -608699.478125\n",
      "    val_loss       : -607737.0887573243\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -531657.375000\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -687697.562500\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -549320.375000\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -540092.562500\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -477628.312500\n",
      "    epoch          : 472\n",
      "    loss           : -607057.64\n",
      "    val_loss       : -605184.006628418\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -685780.125000\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -682637.312500\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -479695.093750\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -486458.718750\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -537901.000000\n",
      "    epoch          : 473\n",
      "    loss           : -607576.79875\n",
      "    val_loss       : -609421.1431640625\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -756154.750000\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -547448.375000\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -636266.625000\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -500423.187500\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -487380.125000\n",
      "    epoch          : 474\n",
      "    loss           : -610353.7003125\n",
      "    val_loss       : -608474.1938232422\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -691550.937500\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -535627.750000\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -748036.750000\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -684105.062500\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -490738.500000\n",
      "    epoch          : 475\n",
      "    loss           : -606438.046875\n",
      "    val_loss       : -605646.7541503906\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -479826.531250\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -480619.125000\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -479420.281250\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -467912.000000\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -519918.937500\n",
      "    epoch          : 476\n",
      "    loss           : -606645.83375\n",
      "    val_loss       : -598950.3242919922\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -636312.812500\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -477697.375000\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -635227.562500\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -641608.750000\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -715173.500000\n",
      "    epoch          : 477\n",
      "    loss           : -607153.4003125\n",
      "    val_loss       : -609137.4763427734\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -758749.562500\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -690997.125000\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -489353.093750\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -474679.625000\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -687290.062500\n",
      "    epoch          : 478\n",
      "    loss           : -610762.78375\n",
      "    val_loss       : -609954.3890380859\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -756359.000000\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -775717.500000\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -750005.125000\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -712109.375000\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -539422.250000\n",
      "    epoch          : 479\n",
      "    loss           : -610652.1759375\n",
      "    val_loss       : -609769.1130249023\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -679527.250000\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -487324.156250\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -692747.875000\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -542143.062500\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -532801.750000\n",
      "    epoch          : 480\n",
      "    loss           : -610556.1571875\n",
      "    val_loss       : -609897.0624389648\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -549692.562500\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -680988.875000\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -711318.062500\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -486502.593750\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -755717.062500\n",
      "    epoch          : 481\n",
      "    loss           : -610164.1903125\n",
      "    val_loss       : -608327.4692382812\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -714890.437500\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -751770.750000\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -752726.812500\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -714602.562500\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -634271.375000\n",
      "    epoch          : 482\n",
      "    loss           : -609710.6046875\n",
      "    val_loss       : -607393.8079467773\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -475499.687500\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -543652.750000\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -488674.281250\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -485515.187500\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -628781.375000\n",
      "    epoch          : 483\n",
      "    loss           : -608229.429375\n",
      "    val_loss       : -607167.7869873047\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -774770.687500\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -765694.812500\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -534202.000000\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -704804.000000\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -628434.125000\n",
      "    epoch          : 484\n",
      "    loss           : -608449.3678125\n",
      "    val_loss       : -606883.7544921875\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -685425.375000\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -693513.125000\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -690442.125000\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -641670.250000\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -707780.562500\n",
      "    epoch          : 485\n",
      "    loss           : -609892.226875\n",
      "    val_loss       : -610312.8448486328\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -494367.687500\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -640959.625000\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -692493.750000\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -768425.437500\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -534250.750000\n",
      "    epoch          : 486\n",
      "    loss           : -611147.133125\n",
      "    val_loss       : -605985.5643432618\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -481323.218750\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -494453.687500\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -472424.687500\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -527029.125000\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -770159.625000\n",
      "    epoch          : 487\n",
      "    loss           : -606999.246875\n",
      "    val_loss       : -609537.6326293945\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -762112.375000\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -538845.625000\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -755858.000000\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -690920.187500\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -483902.125000\n",
      "    epoch          : 488\n",
      "    loss           : -610944.285625\n",
      "    val_loss       : -610443.14140625\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -480995.718750\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -756146.437500\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -471318.562500\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -487613.875000\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -480172.500000\n",
      "    epoch          : 489\n",
      "    loss           : -609151.2165625\n",
      "    val_loss       : -610927.6768310547\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -769901.937500\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -634311.500000\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -551038.500000\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -539659.312500\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -690332.812500\n",
      "    epoch          : 490\n",
      "    loss           : -612091.77625\n",
      "    val_loss       : -610972.3745483399\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -755673.687500\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -715040.375000\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -773577.187500\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -484501.406250\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -686380.937500\n",
      "    epoch          : 491\n",
      "    loss           : -612319.9634375\n",
      "    val_loss       : -610757.9111572265\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -761493.562500\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -536362.500000\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -479779.250000\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -694293.125000\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -640010.062500\n",
      "    epoch          : 492\n",
      "    loss           : -610566.0721875\n",
      "    val_loss       : -608731.0123535156\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -682598.000000\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -494924.593750\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -752600.250000\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -769661.500000\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -684454.625000\n",
      "    epoch          : 493\n",
      "    loss           : -609092.9778125\n",
      "    val_loss       : -609500.2098510743\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -493523.125000\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -757620.687500\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -478402.250000\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -487162.312500\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -687537.000000\n",
      "    epoch          : 494\n",
      "    loss           : -610815.440625\n",
      "    val_loss       : -612266.5537353515\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -540556.937500\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -551327.000000\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -687447.500000\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -488144.906250\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -488133.593750\n",
      "    epoch          : 495\n",
      "    loss           : -612547.8753125\n",
      "    val_loss       : -612157.54296875\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -644337.437500\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -646639.187500\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -487179.281250\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -489739.562500\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -639985.125000\n",
      "    epoch          : 496\n",
      "    loss           : -612320.7340625\n",
      "    val_loss       : -610600.253930664\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -644225.937500\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -543921.437500\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -539426.562500\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -489723.406250\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -641577.875000\n",
      "    epoch          : 497\n",
      "    loss           : -612745.2890625\n",
      "    val_loss       : -610964.5149047852\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -496318.750000\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -487212.218750\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -768604.750000\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -482850.343750\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -723528.312500\n",
      "    epoch          : 498\n",
      "    loss           : -612815.83625\n",
      "    val_loss       : -610969.1477783204\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -715146.937500\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -642587.250000\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -488839.437500\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -778211.937500\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -484623.468750\n",
      "    epoch          : 499\n",
      "    loss           : -612939.789375\n",
      "    val_loss       : -611527.1477539062\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -638413.625000\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -495067.218750\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -771313.625000\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -760132.125000\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -477296.562500\n",
      "    epoch          : 500\n",
      "    loss           : -610622.230625\n",
      "    val_loss       : -607925.0859008789\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0717_104644/checkpoint-epoch500.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAECategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=136, bias=True)\n",
       "        (1): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=136, out_features=136, bias=True)\n",
       "        (4): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=136, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=136, bias=True)\n",
       "        (1): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=136, out_features=136, bias=True)\n",
       "        (4): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=136, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=264, bias=True)\n",
       "        (1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=264, out_features=264, bias=True)\n",
       "        (4): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=264, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=264, bias=True)\n",
       "        (1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=264, out_features=264, bias=True)\n",
       "        (4): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=264, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=400, bias=True)\n",
       "        (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (4): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=400, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_5_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "        (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (4): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=400, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=144, bias=True)\n",
       "        (1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=144, out_features=144, bias=True)\n",
       "        (4): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=144, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=144, bias=True)\n",
       "        (1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=144, out_features=144, bias=True)\n",
       "        (4): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=144, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=272, bias=True)\n",
       "        (1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=272, out_features=272, bias=True)\n",
       "        (4): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=272, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=272, bias=True)\n",
       "        (1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=272, out_features=272, bias=True)\n",
       "        (4): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=272, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_10): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=408, bias=True)\n",
       "        (1): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=408, out_features=408, bias=True)\n",
       "        (4): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=408, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_10_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=408, bias=True)\n",
       "        (1): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=408, out_features=408, bias=True)\n",
       "        (4): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=408, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_11): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_11_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_12): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_12_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_13): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_13_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_14): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_14_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_15): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_15_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_16): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_16_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_17): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_17_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_18): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_18_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_19): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_19_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_20): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_20_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "    (global_element_6): StandardNormal()\n",
       "  )\n",
       "  (guide_embedding): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (guide_confidences): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_distances): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=28, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAScElEQVR4nO3dfZBddX3H8fcnySYhT5AQE0MSgyIUo2CkW3zAcWBQi8x0wE7tmKlO6NDGzqjVKbVa247YsQO1PreWThSG8CCUESlMSytpEKm1UpcHQ2hUIoYkJGYTQsgDELKbb/+4J85l3fO7m3vP3nOT3+c1c2fvPb9z7vnes/vZc+75nQdFBGZ2/JtQdwFm1h0Ou1kmHHazTDjsZplw2M0y4bCbZcJhP85IulLSTW1O+2uSHpa0T9IfV11b1SS9QtJ+SRPrruVY4LBXRNJbJX1f0rOSdkv6b0m/UXddR+nPgPsiYmZEfKXuYlqJiM0RMSMihuuu5VjgsFdA0izgX4G/B+YAC4FPAwfrrKsNS4DHyhp7aQ0qaVKd0x+LHPZqnAEQEbdExHBEPB8R90TEOgBJp0m6V9LTknZJulnSSUcmlrRJ0sckrZN0QNK1kuZL+vdik/o/Jc0uxj1VUkhaKWmbpO2SrigrTNKbii2OPZJ+JOn8kvHuBS4A/qHYND5D0vWSrpF0t6QDwAWSTpR0g6Sdkp6U9JeSJhTvcVmxRfPFYn5PSHpLMXyLpEFJKxK13ifpKkn/W2wh3SlpzojPfbmkzcC9TcMmFeOcIumuYstqo6Q/bHrvKyV9U9JNkvYCl43pN3s8iQg/OnwAs4CngdXAu4DZI9pfDbwDmAK8DLgf+FJT+ybgB8B8GlsFg8BDwBuKae4FPlWMeyoQwC3AdOAsYCfw9qL9SuCm4vnCoq6Lafxjf0fx+mUln+M+4A+aXl8PPAucV0w/FbgBuBOYWdTyU+DyYvzLgCHg94GJwGeAzcBXi8/xTmAfMCMx/6eA1xWf7famz3Lkc99QtJ3QNGxSMc53gX8s6lxWLJcLm5bLIeDS4rOcUPffTdf/Tusu4Hh5AK8pwrG1+IO/C5hfMu6lwMNNrzcBv9f0+nbgmqbXHwb+pXh+5A/8zKb2zwLXFs+bw/5x4MYR8/42sKKkrtHCfkPT64k0vposbRr2ARrf84+E/fGmtrOKWuc3DXsaWJaY/9VNr5cCLxbzPfK5X9XU/suwA4uBYWBmU/tVwPVNy+X+uv9O6nx4M74iEbEhIi6LiEU01kynAF8CkDRP0q2Snio2IW8C5o54ix1Nz58f5fWMEeNvaXr+ZDG/kZYA7yk2qfdI2gO8FVhwFB+teT5zgcnF/JrnvbDp9ci6iYhWn6Vsfk8Cfbx0WW1hdKcAuyNiX6K2smmz4LCPg4j4MY214uuKQVfRWAOdHRGzgPcB6nA2i5uevwLYNso4W2is2U9qekyPiKuPYj7Np0XuorEpvGTEvJ86ivdrZeTnOlTMd7R6mm0D5kiamagt61M8HfYKSDpT0hWSFhWvFwPLaXwPh8b32/3AHkkLgY9VMNu/kjRN0mtpfEf+51HGuQn4LUm/KWmipKmSzj9S59GKRhfXbcDfSJopaQnwJ8V8qvI+SUslTQP+GvhmjKFrLSK2AN8Hrio+59nA5cDNFdZ2THPYq7EPeCPwQLHX+gfAeuDIXvJPA+fQ2Nn1b8C3Kpjnd4GNwFrgcxFxz8gRigBcAnySxs6qLTT+0XTye/8wcAB4Avge8A3gug7eb6QbaWwV/YLGjrajObhnOY3v8duAO2js1FxTYW3HNBU7L+wYIelU4OdAX0QM1VtNtSTdR2Pn4tfrruV45DW7WSYcdrNMeDPeLBNes5tloqsnA0zWlJjK9G7O0iwrL3CAF+PgqMdwdHrm0EXAl2kczvj1VgdrTGU6b9SFnczSzBIeiLWlbW1vxhenO36VxokfS4Hlkpa2+35mNr46+c5+LrAxIp6IiBeBW2kcwGFmPaiTsC/kpScWbOWlJx0AUJx3PSBp4NAxdy0Hs+NHJ2EfbSfAr/TjRcSqiOiPiP4+pnQwOzPrRCdh38pLz1BaxOhnXplZD+gk7D8ETpf0SkmTgffSuGCDmfWgtrveImJI0odoXPlkInBdRJRerNDM6tVRP3tE3A3cXVEtZjaOfLisWSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y0RHt2yWtAnYBwwDQxHRX0VRZla9jsJeuCAidlXwPmY2jrwZb5aJTsMewD2SHpS0crQRJK2UNCBp4BAHO5ydmbWr08348yJim6R5wBpJP46I+5tHiIhVwCqAWZoTHc7PzNrU0Zo9IrYVPweBO4BzqyjKzKrXdtglTZc088hz4J3A+qoKM7NqdbIZPx+4Q9KR9/lGRPxHJVWZWeXaDntEPAG8vsJazGwcuevNLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0yUcWNHS1nEya2P20cbtHuGwhVyWt2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwT7me3JE1q8Sey7Mxk8+HJ5f3wkzZsTk47vGdPet7HaT98q2UeQ0NtvW/LNbuk6yQNSlrfNGyOpDWSHi9+zm5r7mbWNWPZjL8euGjEsE8AayPidGBt8drMeljLsEfE/cDuEYMvAVYXz1cDl1Zcl5lVrN0ddPMjYjtA8XNe2YiSVkoakDRwiINtzs7MOjXue+MjYlVE9EdEfx9Txnt2Zlai3bDvkLQAoPg5WF1JZjYe2g37XcCK4vkK4M5qyjGz8dKyn13SLcD5wFxJW4FPAVcDt0m6HNgMvGc8i8yelG6eWN6XPeGkE9PvPZw+p7xVn+7gr89Mtk9M7KY5ef/89LTDw8n24b17k+21anGe/6SFC0rbdp2/ODntSTf+T1sltQx7RCwvabqwrTmaWS18uKxZJhx2s0w47GaZcNjNMuGwm2XCp7h2wcRZs9IjzDs52Tw8N929NWFfef/W0+fMSU47e326+2rCCy8m2yc9n2xm8Lzyrru+A+luwWknnpBsPzQr/ec77fsbS9sO7z+QnFZ96feeMCv9O9nfvyTZPnlP+XLdeWF6mZ90Y7K5lNfsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1km3M9+RIvTSCctKT/t8MVF6b7sGE5f8rhv885k+8S9fcn2FxaV9+PP2Jbusz04f1qyXUNTk+0HTkkvt/Nf/+PStgd/clZy2r7n0n+eu89ML5d9C8svcz1jW/r02WiRjMOT0p/7mdPTp7jO2lS+nn3LGeXLDGBX6lLTiTOSvWY3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLRW/3sLS6/O2Fyol81cTllAC0qv3QvwDPnzE22z36wvC+87xfPJqclVTfA4fTlnFsZmlb+P3vSc+n37ns23Q8/YajF9HvTd/mZQPkxBvvPTt8O7ODsycn24dOeS7ZP2lh+PnzfgfR6bmhauh9991np5XL263+WbF/380WlbX/78vuS0159UvmFnfVM4rLiyXc1s+OGw26WCYfdLBMOu1kmHHazTDjsZplw2M0y0f1+9sR54zrnNclJD08on/aFeelrjB+Yn+6H73s+fc750NwZpW1b3j49Oe3C/3oh2b79knQf/wvz0n26SjQPz0x/rqlb07VPSpfOgdemR/jTl99T2rbohGeS0z78yvSti7986jeT7TvfXH4MwLU735ac9lUn7Eq2/86sh5PtU9Ld9PzTiW8ubXtb+hICfOb08j76+FH5sQkt1+ySrpM0KGl907ArJT0l6ZHicXGr9zGzeo1lM/564KJRhn8xIpYVj7urLcvMqtYy7BFxP7C7C7WY2TjqZAfdhyStKzbzZ5eNJGmlpAFJA4dIHwttZuOn3bBfA5wGLAO2A58vGzEiVkVEf0T095E+acLMxk9bYY+IHRExHBGHga8B51ZblplVra2wS2o+X/TdwPqycc2sNygi3Q8r6RbgfGAusAP4VPF6GRDAJuADEbG91cxmaU68UeXn4ra6djupWltMqxbnu7eiKYmvIK9+RXLaCTvS+zdb3cu7lWfOKD9com9/+vc7d136PuWD/eXHFwCcvD7dz/700vJO4zV//nfJad/323+UbI+J6d/50LTy6whM/Xm6H314TvpzH5ybPq4jWlxXftrG8mMMnjutdBcYAFPvKe/j/8HQt9l7ePeoM295UE1ELB9l8LWtpjOz3uLDZc0y4bCbZcJhN8uEw26WCYfdLBMtu96q1LLrzdqTugR3tLhM9Xj//lOnNLfoDo2hxP2HbVQPxFr2xuhdb16zm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZ6K1bNlv11OL/eQy3mL7Facct3l+Jy3+3us027mevlNfsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1km3M9+PEids97p+eqtpm/RT588nd796F3lNbtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulomWYZe0WNJ3JG2Q9JikjxTD50haI+nx4mf6PrM2fiLKH2aFsazZh4ArIuI1wJuAD0paCnwCWBsRpwNri9dm1qNahj0itkfEQ8XzfcAGYCFwCbC6GG01cOl4FWlmnTuq7+ySTgXeADwAzI+I7dD4hwDMq7o4M6vOmMMuaQZwO/DRiNh7FNOtlDQgaeAQB9up0cwqMKawS+qjEfSbI+JbxeAdkhYU7QuAwdGmjYhVEdEfEf19TKmiZjNrw1j2xgu4FtgQEV9oaroLWFE8XwHcWX15ZlaVsZzieh7wfuBRSY8Uwz4JXA3cJulyYDPwnvEp0cyq0DLsEfE9oOzi377ZutkxwkfQmWXCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0y0DLukxZK+I2mDpMckfaQYfqWkpyQ9UjwuHv9yzaxdLe/PDgwBV0TEQ5JmAg9KWlO0fTEiPjd+5ZlZVVqGPSK2A9uL5/skbQAWjndhZlato/rOLulU4A3AA8WgD0laJ+k6SbNLplkpaUDSwCEOdlSsmbVvzGGXNAO4HfhoROwFrgFOA5bRWPN/frTpImJVRPRHRH8fUyoo2czaMaawS+qjEfSbI+JbABGxIyKGI+Iw8DXg3PEr08w6NZa98QKuBTZExBeahi9oGu3dwPrqyzOzqoxlb/x5wPuBRyU9Ugz7JLBc0jIggE3AB8alQjOrxFj2xn8P0ChNd1dfjpmNFx9BZ5YJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTKhiOjezKSdwJNNg+YCu7pWwNHp1dp6tS5wbe2qsrYlEfGy0Rq6GvZfmbk0EBH9tRWQ0Ku19Wpd4Nra1a3avBlvlgmH3SwTdYd9Vc3zT+nV2nq1LnBt7epKbbV+Zzez7ql7zW5mXeKwm2WilrBLukjSTyRtlPSJOmooI2mTpEeL21AP1FzLdZIGJa1vGjZH0hpJjxc/R73HXk219cRtvBO3Ga912dV9+/Ouf2eXNBH4KfAOYCvwQ2B5RPxfVwspIWkT0B8RtR+AIeltwH7ghoh4XTHss8DuiLi6+Ec5OyI+3iO1XQnsr/s23sXdihY032YcuBS4jBqXXaKu36ULy62ONfu5wMaIeCIiXgRuBS6poY6eFxH3A7tHDL4EWF08X03jj6XrSmrrCRGxPSIeKp7vA47cZrzWZZeoqyvqCPtCYEvT66301v3eA7hH0oOSVtZdzCjmR8R2aPzxAPNqrmeklrfx7qYRtxnvmWXXzu3PO1VH2Ee7lVQv9f+dFxHnAO8CPlhsrtrYjOk23t0yym3Ge0K7tz/vVB1h3wosbnq9CNhWQx2jiohtxc9B4A5671bUO47cQbf4OVhzPb/US7fxHu024/TAsqvz9ud1hP2HwOmSXilpMvBe4K4a6vgVkqYXO06QNB14J713K+q7gBXF8xXAnTXW8hK9chvvstuMU/Oyq/325xHR9QdwMY098j8D/qKOGkrqehXwo+LxWN21AbfQ2Kw7RGOL6HLgZGAt8Hjxc04P1XYj8CiwjkawFtRU21tpfDVcBzxSPC6ue9kl6urKcvPhsmaZ8BF0Zplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1km/h+FIpvm7sC1uQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
