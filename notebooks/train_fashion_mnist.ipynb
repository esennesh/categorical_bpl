{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f5570f537f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)\n",
    "model.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/54000 (0%)] Loss: 3493.578125\n",
      "Train Epoch: 1 [1408/54000 (3%)] Loss: 1144.795898\n",
      "Train Epoch: 1 [2816/54000 (5%)] Loss: -3728.208984\n",
      "Train Epoch: 1 [4224/54000 (8%)] Loss: -1139.389648\n",
      "Train Epoch: 1 [5632/54000 (10%)] Loss: -22909.580078\n",
      "Train Epoch: 1 [7040/54000 (13%)] Loss: -35041.453125\n",
      "Train Epoch: 1 [8448/54000 (16%)] Loss: -51391.859375\n",
      "Train Epoch: 1 [9856/54000 (18%)] Loss: -74569.429688\n",
      "Train Epoch: 1 [11264/54000 (21%)] Loss: -95702.257812\n",
      "Train Epoch: 1 [12672/54000 (23%)] Loss: -121904.453125\n",
      "Train Epoch: 1 [14080/54000 (26%)] Loss: -147551.843750\n",
      "Train Epoch: 1 [15488/54000 (29%)] Loss: -164828.546875\n",
      "Train Epoch: 1 [16896/54000 (31%)] Loss: -198535.312500\n",
      "Train Epoch: 1 [18304/54000 (34%)] Loss: -199652.453125\n",
      "Train Epoch: 1 [19712/54000 (37%)] Loss: -231548.703125\n",
      "Train Epoch: 1 [21120/54000 (39%)] Loss: -235703.609375\n",
      "Train Epoch: 1 [22528/54000 (42%)] Loss: -270503.187500\n",
      "Train Epoch: 1 [23936/54000 (44%)] Loss: -284566.562500\n",
      "Train Epoch: 1 [25344/54000 (47%)] Loss: -300922.968750\n",
      "Train Epoch: 1 [26752/54000 (50%)] Loss: -326055.718750\n",
      "Train Epoch: 1 [28160/54000 (52%)] Loss: -370599.406250\n",
      "Train Epoch: 1 [29568/54000 (55%)] Loss: -353902.062500\n",
      "Train Epoch: 1 [30976/54000 (57%)] Loss: -382913.156250\n",
      "Train Epoch: 1 [32384/54000 (60%)] Loss: -375629.812500\n",
      "Train Epoch: 1 [33792/54000 (63%)] Loss: -377587.062500\n",
      "Train Epoch: 1 [35200/54000 (65%)] Loss: -426204.062500\n",
      "Train Epoch: 1 [36608/54000 (68%)] Loss: -430131.250000\n",
      "Train Epoch: 1 [38016/54000 (70%)] Loss: -461813.406250\n",
      "Train Epoch: 1 [39424/54000 (73%)] Loss: -417820.187500\n",
      "Train Epoch: 1 [40832/54000 (76%)] Loss: -472265.812500\n",
      "Train Epoch: 1 [42240/54000 (78%)] Loss: -440875.718750\n",
      "Train Epoch: 1 [43648/54000 (81%)] Loss: -498936.531250\n",
      "Train Epoch: 1 [45056/54000 (83%)] Loss: -484373.625000\n",
      "Train Epoch: 1 [46464/54000 (86%)] Loss: -483820.937500\n",
      "Train Epoch: 1 [47872/54000 (89%)] Loss: -501577.812500\n",
      "Train Epoch: 1 [49280/54000 (91%)] Loss: -547478.125000\n",
      "Train Epoch: 1 [50688/54000 (94%)] Loss: -540090.312500\n",
      "Train Epoch: 1 [52096/54000 (96%)] Loss: -518612.687500\n",
      "Train Epoch: 1 [53504/54000 (99%)] Loss: -527320.625000\n",
      "    epoch          : 1\n",
      "    loss           : -289634.67100054846\n",
      "    val_loss       : -554342.1341559228\n",
      "Train Epoch: 2 [0/54000 (0%)] Loss: -587153.125000\n",
      "Train Epoch: 2 [1408/54000 (3%)] Loss: -557314.187500\n",
      "Train Epoch: 2 [2816/54000 (5%)] Loss: -618505.250000\n",
      "Train Epoch: 2 [4224/54000 (8%)] Loss: -564158.875000\n",
      "Train Epoch: 2 [5632/54000 (10%)] Loss: -542869.750000\n",
      "Train Epoch: 2 [7040/54000 (13%)] Loss: -640146.187500\n",
      "Train Epoch: 2 [8448/54000 (16%)] Loss: -556383.625000\n",
      "Train Epoch: 2 [9856/54000 (18%)] Loss: -581089.250000\n",
      "Train Epoch: 2 [11264/54000 (21%)] Loss: -585534.437500\n",
      "Train Epoch: 2 [12672/54000 (23%)] Loss: -608853.125000\n",
      "Train Epoch: 2 [14080/54000 (26%)] Loss: -631041.375000\n",
      "Train Epoch: 2 [15488/54000 (29%)] Loss: -664402.375000\n",
      "Train Epoch: 2 [16896/54000 (31%)] Loss: -642158.812500\n",
      "Train Epoch: 2 [18304/54000 (34%)] Loss: -599609.750000\n",
      "Train Epoch: 2 [19712/54000 (37%)] Loss: -562077.812500\n",
      "Train Epoch: 2 [21120/54000 (39%)] Loss: -653740.125000\n",
      "Train Epoch: 2 [22528/54000 (42%)] Loss: -640977.750000\n",
      "Train Epoch: 2 [23936/54000 (44%)] Loss: -548058.125000\n",
      "Train Epoch: 2 [25344/54000 (47%)] Loss: -628611.562500\n",
      "Train Epoch: 2 [26752/54000 (50%)] Loss: -663363.375000\n",
      "Train Epoch: 2 [28160/54000 (52%)] Loss: -649212.187500\n",
      "Train Epoch: 2 [29568/54000 (55%)] Loss: -685024.000000\n",
      "Train Epoch: 2 [30976/54000 (57%)] Loss: -702416.000000\n",
      "Train Epoch: 2 [32384/54000 (60%)] Loss: -675563.187500\n",
      "Train Epoch: 2 [33792/54000 (63%)] Loss: -693428.562500\n",
      "Train Epoch: 2 [35200/54000 (65%)] Loss: -658581.125000\n",
      "Train Epoch: 2 [36608/54000 (68%)] Loss: -648066.625000\n",
      "Train Epoch: 2 [38016/54000 (70%)] Loss: -681932.750000\n",
      "Train Epoch: 2 [39424/54000 (73%)] Loss: -707805.250000\n",
      "Train Epoch: 2 [40832/54000 (76%)] Loss: -720220.375000\n",
      "Train Epoch: 2 [42240/54000 (78%)] Loss: -680499.937500\n",
      "Train Epoch: 2 [43648/54000 (81%)] Loss: -710628.687500\n",
      "Train Epoch: 2 [45056/54000 (83%)] Loss: -728284.312500\n",
      "Train Epoch: 2 [46464/54000 (86%)] Loss: -684457.187500\n",
      "Train Epoch: 2 [47872/54000 (89%)] Loss: -714944.312500\n",
      "Train Epoch: 2 [49280/54000 (91%)] Loss: -782987.062500\n",
      "Train Epoch: 2 [50688/54000 (94%)] Loss: -759431.500000\n",
      "Train Epoch: 2 [52096/54000 (96%)] Loss: -751515.312500\n",
      "Train Epoch: 2 [53504/54000 (99%)] Loss: -720345.500000\n",
      "    epoch          : 2\n",
      "    loss           : -648526.1842417062\n",
      "    val_loss       : -730964.6203249667\n",
      "Train Epoch: 3 [0/54000 (0%)] Loss: -694617.375000\n",
      "Train Epoch: 3 [1408/54000 (3%)] Loss: -587364.312500\n",
      "Train Epoch: 3 [2816/54000 (5%)] Loss: -716361.437500\n",
      "Train Epoch: 3 [4224/54000 (8%)] Loss: -737942.125000\n",
      "Train Epoch: 3 [5632/54000 (10%)] Loss: -772116.750000\n",
      "Train Epoch: 3 [7040/54000 (13%)] Loss: -783960.812500\n",
      "Train Epoch: 3 [8448/54000 (16%)] Loss: -706445.125000\n",
      "Train Epoch: 3 [9856/54000 (18%)] Loss: -779898.375000\n",
      "Train Epoch: 3 [11264/54000 (21%)] Loss: -773766.250000\n",
      "Train Epoch: 3 [12672/54000 (23%)] Loss: -766994.562500\n",
      "Train Epoch: 3 [14080/54000 (26%)] Loss: -746773.625000\n",
      "Train Epoch: 3 [15488/54000 (29%)] Loss: -845416.625000\n",
      "Train Epoch: 3 [16896/54000 (31%)] Loss: -797069.625000\n",
      "Train Epoch: 3 [18304/54000 (34%)] Loss: -707998.312500\n",
      "Train Epoch: 3 [19712/54000 (37%)] Loss: -756133.312500\n",
      "Train Epoch: 3 [21120/54000 (39%)] Loss: -793598.875000\n",
      "Train Epoch: 3 [22528/54000 (42%)] Loss: -775070.187500\n",
      "Train Epoch: 3 [23936/54000 (44%)] Loss: -784161.250000\n",
      "Train Epoch: 3 [25344/54000 (47%)] Loss: -724526.125000\n",
      "Train Epoch: 3 [26752/54000 (50%)] Loss: -752232.937500\n",
      "Train Epoch: 3 [28160/54000 (52%)] Loss: -806497.750000\n",
      "Train Epoch: 3 [29568/54000 (55%)] Loss: -845335.750000\n",
      "Train Epoch: 3 [30976/54000 (57%)] Loss: -746336.125000\n",
      "Train Epoch: 3 [32384/54000 (60%)] Loss: -783530.125000\n",
      "Train Epoch: 3 [33792/54000 (63%)] Loss: -755632.187500\n",
      "Train Epoch: 3 [35200/54000 (65%)] Loss: -837123.000000\n",
      "Train Epoch: 3 [36608/54000 (68%)] Loss: -801156.687500\n",
      "Train Epoch: 3 [38016/54000 (70%)] Loss: -761150.625000\n",
      "Train Epoch: 3 [39424/54000 (73%)] Loss: -727163.437500\n",
      "Train Epoch: 3 [40832/54000 (76%)] Loss: -772328.000000\n",
      "Train Epoch: 3 [42240/54000 (78%)] Loss: -929401.625000\n",
      "Train Epoch: 3 [43648/54000 (81%)] Loss: -792457.250000\n",
      "Train Epoch: 3 [45056/54000 (83%)] Loss: -799782.937500\n",
      "Train Epoch: 3 [46464/54000 (86%)] Loss: -847936.312500\n",
      "Train Epoch: 3 [47872/54000 (89%)] Loss: -798413.312500\n",
      "Train Epoch: 3 [49280/54000 (91%)] Loss: -862983.125000\n",
      "Train Epoch: 3 [50688/54000 (94%)] Loss: -766272.500000\n",
      "Train Epoch: 3 [52096/54000 (96%)] Loss: -795716.625000\n",
      "Train Epoch: 3 [53504/54000 (99%)] Loss: -789155.250000\n",
      "    epoch          : 3\n",
      "    loss           : -772793.7668838863\n",
      "    val_loss       : -815008.4730640168\n",
      "Train Epoch: 4 [0/54000 (0%)] Loss: -763318.625000\n",
      "Train Epoch: 4 [1408/54000 (3%)] Loss: -827226.687500\n",
      "Train Epoch: 4 [2816/54000 (5%)] Loss: -830686.875000\n",
      "Train Epoch: 4 [4224/54000 (8%)] Loss: -873796.937500\n",
      "Train Epoch: 4 [5632/54000 (10%)] Loss: -835480.187500\n",
      "Train Epoch: 4 [7040/54000 (13%)] Loss: -788318.875000\n",
      "Train Epoch: 4 [8448/54000 (16%)] Loss: -767137.000000\n",
      "Train Epoch: 4 [9856/54000 (18%)] Loss: -810864.687500\n",
      "Train Epoch: 4 [11264/54000 (21%)] Loss: -786165.437500\n",
      "Train Epoch: 4 [12672/54000 (23%)] Loss: -803450.750000\n",
      "Train Epoch: 4 [14080/54000 (26%)] Loss: -810210.812500\n",
      "Train Epoch: 4 [15488/54000 (29%)] Loss: -854149.625000\n",
      "Train Epoch: 4 [16896/54000 (31%)] Loss: -883536.375000\n",
      "Train Epoch: 4 [18304/54000 (34%)] Loss: -774205.250000\n",
      "Train Epoch: 4 [19712/54000 (37%)] Loss: -839110.437500\n",
      "Train Epoch: 4 [21120/54000 (39%)] Loss: -812921.000000\n",
      "Train Epoch: 4 [22528/54000 (42%)] Loss: -793868.812500\n",
      "Train Epoch: 4 [23936/54000 (44%)] Loss: -819784.312500\n",
      "Train Epoch: 4 [25344/54000 (47%)] Loss: -860747.437500\n",
      "Train Epoch: 4 [26752/54000 (50%)] Loss: -832190.187500\n",
      "Train Epoch: 4 [28160/54000 (52%)] Loss: -896594.687500\n",
      "Train Epoch: 4 [29568/54000 (55%)] Loss: -802345.750000\n",
      "Train Epoch: 4 [30976/54000 (57%)] Loss: -865951.625000\n",
      "Train Epoch: 4 [32384/54000 (60%)] Loss: -797481.625000\n",
      "Train Epoch: 4 [33792/54000 (63%)] Loss: -821197.375000\n",
      "Train Epoch: 4 [35200/54000 (65%)] Loss: -900327.062500\n",
      "Train Epoch: 4 [36608/54000 (68%)] Loss: -816029.562500\n",
      "Train Epoch: 4 [38016/54000 (70%)] Loss: -903680.687500\n",
      "Train Epoch: 4 [39424/54000 (73%)] Loss: -860064.250000\n",
      "Train Epoch: 4 [40832/54000 (76%)] Loss: -824318.125000\n",
      "Train Epoch: 4 [42240/54000 (78%)] Loss: -862122.187500\n",
      "Train Epoch: 4 [43648/54000 (81%)] Loss: -878960.875000\n",
      "Train Epoch: 4 [45056/54000 (83%)] Loss: -816693.875000\n",
      "Train Epoch: 4 [46464/54000 (86%)] Loss: -825519.875000\n",
      "Train Epoch: 4 [47872/54000 (89%)] Loss: -822897.500000\n",
      "Train Epoch: 4 [49280/54000 (91%)] Loss: -879133.500000\n",
      "Train Epoch: 4 [50688/54000 (94%)] Loss: -849849.937500\n",
      "Train Epoch: 4 [52096/54000 (96%)] Loss: -844920.937500\n",
      "Train Epoch: 4 [53504/54000 (99%)] Loss: -890599.000000\n",
      "    epoch          : 4\n",
      "    loss           : -833396.2621445497\n",
      "    val_loss       : -858414.1273920587\n",
      "Train Epoch: 5 [0/54000 (0%)] Loss: -874323.687500\n",
      "Train Epoch: 5 [1408/54000 (3%)] Loss: -861335.875000\n",
      "Train Epoch: 5 [2816/54000 (5%)] Loss: -867494.812500\n",
      "Train Epoch: 5 [4224/54000 (8%)] Loss: -870890.937500\n",
      "Train Epoch: 5 [5632/54000 (10%)] Loss: -841329.562500\n",
      "Train Epoch: 5 [7040/54000 (13%)] Loss: -852856.625000\n",
      "Train Epoch: 5 [8448/54000 (16%)] Loss: -857448.750000\n",
      "Train Epoch: 5 [9856/54000 (18%)] Loss: -871644.437500\n",
      "Train Epoch: 5 [11264/54000 (21%)] Loss: -895625.062500\n",
      "Train Epoch: 5 [12672/54000 (23%)] Loss: -943647.625000\n",
      "Train Epoch: 5 [14080/54000 (26%)] Loss: -866276.687500\n",
      "Train Epoch: 5 [15488/54000 (29%)] Loss: -802218.437500\n",
      "Train Epoch: 5 [16896/54000 (31%)] Loss: -893857.000000\n",
      "Train Epoch: 5 [18304/54000 (34%)] Loss: -857595.187500\n",
      "Train Epoch: 5 [19712/54000 (37%)] Loss: -833965.812500\n",
      "Train Epoch: 5 [21120/54000 (39%)] Loss: -860132.625000\n",
      "Train Epoch: 5 [22528/54000 (42%)] Loss: -864275.562500\n",
      "Train Epoch: 5 [23936/54000 (44%)] Loss: -902345.375000\n",
      "Train Epoch: 5 [25344/54000 (47%)] Loss: -834668.687500\n",
      "Train Epoch: 5 [26752/54000 (50%)] Loss: -888896.562500\n",
      "Train Epoch: 5 [28160/54000 (52%)] Loss: -838525.125000\n",
      "Train Epoch: 5 [29568/54000 (55%)] Loss: -772862.937500\n",
      "Train Epoch: 5 [30976/54000 (57%)] Loss: -828147.625000\n",
      "Train Epoch: 5 [32384/54000 (60%)] Loss: -896789.812500\n",
      "Train Epoch: 5 [33792/54000 (63%)] Loss: -895735.312500\n",
      "Train Epoch: 5 [35200/54000 (65%)] Loss: -895713.812500\n",
      "Train Epoch: 5 [36608/54000 (68%)] Loss: -856670.000000\n",
      "Train Epoch: 5 [38016/54000 (70%)] Loss: -913771.875000\n",
      "Train Epoch: 5 [39424/54000 (73%)] Loss: -844786.875000\n",
      "Train Epoch: 5 [40832/54000 (76%)] Loss: -897487.000000\n",
      "Train Epoch: 5 [42240/54000 (78%)] Loss: -799497.312500\n",
      "Train Epoch: 5 [43648/54000 (81%)] Loss: -821279.937500\n",
      "Train Epoch: 5 [45056/54000 (83%)] Loss: -873442.125000\n",
      "Train Epoch: 5 [46464/54000 (86%)] Loss: -875029.375000\n",
      "Train Epoch: 5 [47872/54000 (89%)] Loss: -864812.312500\n",
      "Train Epoch: 5 [49280/54000 (91%)] Loss: -917125.312500\n",
      "Train Epoch: 5 [50688/54000 (94%)] Loss: -890752.187500\n",
      "Train Epoch: 5 [52096/54000 (96%)] Loss: -869215.250000\n",
      "Train Epoch: 5 [53504/54000 (99%)] Loss: -820587.312500\n",
      "    epoch          : 5\n",
      "    loss           : -870275.4721563981\n",
      "    val_loss       : -890098.5429765417\n",
      "Train Epoch: 6 [0/54000 (0%)] Loss: -889001.062500\n",
      "Train Epoch: 6 [1408/54000 (3%)] Loss: -880919.687500\n",
      "Train Epoch: 6 [2816/54000 (5%)] Loss: -862080.437500\n",
      "Train Epoch: 6 [4224/54000 (8%)] Loss: -952553.562500\n",
      "Train Epoch: 6 [5632/54000 (10%)] Loss: -870536.000000\n",
      "Train Epoch: 6 [7040/54000 (13%)] Loss: -847438.937500\n",
      "Train Epoch: 6 [8448/54000 (16%)] Loss: -901824.187500\n",
      "Train Epoch: 6 [9856/54000 (18%)] Loss: -854119.375000\n",
      "Train Epoch: 6 [11264/54000 (21%)] Loss: -827317.875000\n",
      "Train Epoch: 6 [12672/54000 (23%)] Loss: -866763.562500\n",
      "Train Epoch: 6 [14080/54000 (26%)] Loss: -862716.812500\n",
      "Train Epoch: 6 [15488/54000 (29%)] Loss: -912961.000000\n",
      "Train Epoch: 6 [16896/54000 (31%)] Loss: -917696.312500\n",
      "Train Epoch: 6 [18304/54000 (34%)] Loss: -904631.812500\n",
      "Train Epoch: 6 [19712/54000 (37%)] Loss: -921952.312500\n",
      "Train Epoch: 6 [21120/54000 (39%)] Loss: -954634.625000\n",
      "Train Epoch: 6 [22528/54000 (42%)] Loss: -872449.812500\n",
      "Train Epoch: 6 [23936/54000 (44%)] Loss: -907975.937500\n",
      "Train Epoch: 6 [25344/54000 (47%)] Loss: -894555.500000\n",
      "Train Epoch: 6 [26752/54000 (50%)] Loss: -902798.750000\n",
      "Train Epoch: 6 [28160/54000 (52%)] Loss: -859166.875000\n",
      "Train Epoch: 6 [29568/54000 (55%)] Loss: -895485.562500\n",
      "Train Epoch: 6 [30976/54000 (57%)] Loss: -891562.000000\n",
      "Train Epoch: 6 [32384/54000 (60%)] Loss: -930971.000000\n",
      "Train Epoch: 6 [33792/54000 (63%)] Loss: -873436.000000\n",
      "Train Epoch: 6 [35200/54000 (65%)] Loss: -916344.812500\n",
      "Train Epoch: 6 [36608/54000 (68%)] Loss: -900524.937500\n",
      "Train Epoch: 6 [38016/54000 (70%)] Loss: -932765.500000\n",
      "Train Epoch: 6 [39424/54000 (73%)] Loss: -962309.687500\n",
      "Train Epoch: 6 [40832/54000 (76%)] Loss: -874595.250000\n",
      "Train Epoch: 6 [42240/54000 (78%)] Loss: -881096.437500\n",
      "Train Epoch: 6 [43648/54000 (81%)] Loss: -963155.500000\n",
      "Train Epoch: 6 [45056/54000 (83%)] Loss: -921174.875000\n",
      "Train Epoch: 6 [46464/54000 (86%)] Loss: -853360.750000\n",
      "Train Epoch: 6 [47872/54000 (89%)] Loss: -842449.500000\n",
      "Train Epoch: 6 [49280/54000 (91%)] Loss: -870541.125000\n",
      "Train Epoch: 6 [50688/54000 (94%)] Loss: -925188.125000\n",
      "Train Epoch: 6 [52096/54000 (96%)] Loss: -895075.562500\n",
      "Train Epoch: 6 [53504/54000 (99%)] Loss: -914029.000000\n",
      "    epoch          : 6\n",
      "    loss           : -899288.2351895735\n",
      "    val_loss       : -916609.6135253906\n",
      "Train Epoch: 7 [0/54000 (0%)] Loss: -915251.625000\n",
      "Train Epoch: 7 [1408/54000 (3%)] Loss: -904567.312500\n",
      "Train Epoch: 7 [2816/54000 (5%)] Loss: -941098.937500\n",
      "Train Epoch: 7 [4224/54000 (8%)] Loss: -935387.187500\n",
      "Train Epoch: 7 [5632/54000 (10%)] Loss: -932449.812500\n",
      "Train Epoch: 7 [7040/54000 (13%)] Loss: -940262.000000\n",
      "Train Epoch: 7 [8448/54000 (16%)] Loss: -918597.312500\n",
      "Train Epoch: 7 [9856/54000 (18%)] Loss: -853730.312500\n",
      "Train Epoch: 7 [11264/54000 (21%)] Loss: -875264.375000\n",
      "Train Epoch: 7 [12672/54000 (23%)] Loss: -934440.375000\n",
      "Train Epoch: 7 [14080/54000 (26%)] Loss: -844281.250000\n",
      "Train Epoch: 7 [15488/54000 (29%)] Loss: -888833.750000\n",
      "Train Epoch: 7 [16896/54000 (31%)] Loss: -975934.312500\n",
      "Train Epoch: 7 [18304/54000 (34%)] Loss: -896696.750000\n",
      "Train Epoch: 7 [19712/54000 (37%)] Loss: -886755.000000\n",
      "Train Epoch: 7 [21120/54000 (39%)] Loss: -892536.875000\n",
      "Train Epoch: 7 [22528/54000 (42%)] Loss: -914780.937500\n",
      "Train Epoch: 7 [23936/54000 (44%)] Loss: -930449.250000\n",
      "Train Epoch: 7 [25344/54000 (47%)] Loss: -904682.687500\n",
      "Train Epoch: 7 [26752/54000 (50%)] Loss: -974861.187500\n",
      "Train Epoch: 7 [28160/54000 (52%)] Loss: -926885.125000\n",
      "Train Epoch: 7 [29568/54000 (55%)] Loss: -898612.750000\n",
      "Train Epoch: 7 [30976/54000 (57%)] Loss: -916114.000000\n",
      "Train Epoch: 7 [32384/54000 (60%)] Loss: -897845.625000\n",
      "Train Epoch: 7 [33792/54000 (63%)] Loss: -915583.000000\n",
      "Train Epoch: 7 [35200/54000 (65%)] Loss: -969371.562500\n",
      "Train Epoch: 7 [36608/54000 (68%)] Loss: -897134.000000\n",
      "Train Epoch: 7 [38016/54000 (70%)] Loss: -900374.750000\n",
      "Train Epoch: 7 [39424/54000 (73%)] Loss: -974163.375000\n",
      "Train Epoch: 7 [40832/54000 (76%)] Loss: -925153.000000\n",
      "Train Epoch: 7 [42240/54000 (78%)] Loss: -935991.500000\n",
      "Train Epoch: 7 [43648/54000 (81%)] Loss: -918742.750000\n",
      "Train Epoch: 7 [45056/54000 (83%)] Loss: -933186.062500\n",
      "Train Epoch: 7 [46464/54000 (86%)] Loss: -927433.187500\n",
      "Train Epoch: 7 [47872/54000 (89%)] Loss: -964178.812500\n",
      "Train Epoch: 7 [49280/54000 (91%)] Loss: -914278.375000\n",
      "Train Epoch: 7 [50688/54000 (94%)] Loss: -861639.625000\n",
      "Train Epoch: 7 [52096/54000 (96%)] Loss: -936019.812500\n",
      "Train Epoch: 7 [53504/54000 (99%)] Loss: -902855.312500\n",
      "    epoch          : 7\n",
      "    loss           : -921565.5023696682\n",
      "    val_loss       : -935367.2772866107\n",
      "Train Epoch: 8 [0/54000 (0%)] Loss: -976332.000000\n",
      "Train Epoch: 8 [1408/54000 (3%)] Loss: -895406.437500\n",
      "Train Epoch: 8 [2816/54000 (5%)] Loss: -859681.562500\n",
      "Train Epoch: 8 [4224/54000 (8%)] Loss: -959166.625000\n",
      "Train Epoch: 8 [5632/54000 (10%)] Loss: -936105.500000\n",
      "Train Epoch: 8 [7040/54000 (13%)] Loss: -932971.500000\n",
      "Train Epoch: 8 [8448/54000 (16%)] Loss: -939791.812500\n",
      "Train Epoch: 8 [9856/54000 (18%)] Loss: -952194.750000\n",
      "Train Epoch: 8 [11264/54000 (21%)] Loss: -909366.187500\n",
      "Train Epoch: 8 [12672/54000 (23%)] Loss: -984689.625000\n",
      "Train Epoch: 8 [14080/54000 (26%)] Loss: -944310.750000\n",
      "Train Epoch: 8 [15488/54000 (29%)] Loss: -932331.937500\n",
      "Train Epoch: 8 [16896/54000 (31%)] Loss: -876117.812500\n",
      "Train Epoch: 8 [18304/54000 (34%)] Loss: -895506.125000\n",
      "Train Epoch: 8 [19712/54000 (37%)] Loss: -861293.312500\n",
      "Train Epoch: 8 [21120/54000 (39%)] Loss: -954288.562500\n",
      "Train Epoch: 8 [22528/54000 (42%)] Loss: -965892.187500\n",
      "Train Epoch: 8 [23936/54000 (44%)] Loss: -936136.000000\n",
      "Train Epoch: 8 [25344/54000 (47%)] Loss: -925970.125000\n",
      "Train Epoch: 8 [26752/54000 (50%)] Loss: -950259.312500\n",
      "Train Epoch: 8 [28160/54000 (52%)] Loss: -941535.375000\n",
      "Train Epoch: 8 [29568/54000 (55%)] Loss: -935482.250000\n",
      "Train Epoch: 8 [30976/54000 (57%)] Loss: -942908.187500\n",
      "Train Epoch: 8 [32384/54000 (60%)] Loss: -921261.000000\n",
      "Train Epoch: 8 [33792/54000 (63%)] Loss: -922853.437500\n",
      "Train Epoch: 8 [35200/54000 (65%)] Loss: -949872.812500\n",
      "Train Epoch: 8 [36608/54000 (68%)] Loss: -923365.062500\n",
      "Train Epoch: 8 [38016/54000 (70%)] Loss: -908187.562500\n",
      "Train Epoch: 8 [39424/54000 (73%)] Loss: -912505.750000\n",
      "Train Epoch: 8 [40832/54000 (76%)] Loss: -947777.000000\n",
      "Train Epoch: 8 [42240/54000 (78%)] Loss: -873199.562500\n",
      "Train Epoch: 8 [43648/54000 (81%)] Loss: -941669.875000\n",
      "Train Epoch: 8 [45056/54000 (83%)] Loss: -967909.625000\n",
      "Train Epoch: 8 [46464/54000 (86%)] Loss: -945347.437500\n",
      "Train Epoch: 8 [47872/54000 (89%)] Loss: -983171.625000\n",
      "Train Epoch: 8 [49280/54000 (91%)] Loss: -908170.437500\n",
      "Train Epoch: 8 [50688/54000 (94%)] Loss: -949320.812500\n",
      "Train Epoch: 8 [52096/54000 (96%)] Loss: -889717.125000\n",
      "Train Epoch: 8 [53504/54000 (99%)] Loss: -910413.687500\n",
      "    epoch          : 8\n",
      "    loss           : -937820.4247630332\n",
      "    val_loss       : -949666.4871982007\n",
      "Train Epoch: 9 [0/54000 (0%)] Loss: -886967.000000\n",
      "Train Epoch: 9 [1408/54000 (3%)] Loss: -990191.312500\n",
      "Train Epoch: 9 [2816/54000 (5%)] Loss: -872056.625000\n",
      "Train Epoch: 9 [4224/54000 (8%)] Loss: -998637.000000\n",
      "Train Epoch: 9 [5632/54000 (10%)] Loss: -993013.312500\n",
      "Train Epoch: 9 [7040/54000 (13%)] Loss: -918853.062500\n",
      "Train Epoch: 9 [8448/54000 (16%)] Loss: -953822.625000\n",
      "Train Epoch: 9 [9856/54000 (18%)] Loss: -947425.750000\n",
      "Train Epoch: 9 [11264/54000 (21%)] Loss: -920046.375000\n",
      "Train Epoch: 9 [12672/54000 (23%)] Loss: -897717.750000\n",
      "Train Epoch: 9 [14080/54000 (26%)] Loss: -927202.687500\n",
      "Train Epoch: 9 [15488/54000 (29%)] Loss: -1004822.125000\n",
      "Train Epoch: 9 [16896/54000 (31%)] Loss: -965860.187500\n",
      "Train Epoch: 9 [18304/54000 (34%)] Loss: -966510.687500\n",
      "Train Epoch: 9 [19712/54000 (37%)] Loss: -956231.687500\n",
      "Train Epoch: 9 [21120/54000 (39%)] Loss: -970906.375000\n",
      "Train Epoch: 9 [22528/54000 (42%)] Loss: -997457.750000\n",
      "Train Epoch: 9 [23936/54000 (44%)] Loss: -926800.312500\n",
      "Train Epoch: 9 [25344/54000 (47%)] Loss: -911076.625000\n",
      "Train Epoch: 9 [26752/54000 (50%)] Loss: -974618.500000\n",
      "Train Epoch: 9 [28160/54000 (52%)] Loss: -908941.125000\n",
      "Train Epoch: 9 [29568/54000 (55%)] Loss: -950107.687500\n",
      "Train Epoch: 9 [30976/54000 (57%)] Loss: -943691.562500\n",
      "Train Epoch: 9 [32384/54000 (60%)] Loss: -892565.562500\n",
      "Train Epoch: 9 [33792/54000 (63%)] Loss: -946805.125000\n",
      "Train Epoch: 9 [35200/54000 (65%)] Loss: -907026.812500\n",
      "Train Epoch: 9 [36608/54000 (68%)] Loss: -923858.625000\n",
      "Train Epoch: 9 [38016/54000 (70%)] Loss: -969846.500000\n",
      "Train Epoch: 9 [39424/54000 (73%)] Loss: -953471.312500\n",
      "Train Epoch: 9 [40832/54000 (76%)] Loss: -974057.187500\n",
      "Train Epoch: 9 [42240/54000 (78%)] Loss: -871875.812500\n",
      "Train Epoch: 9 [43648/54000 (81%)] Loss: -923319.125000\n",
      "Train Epoch: 9 [45056/54000 (83%)] Loss: -979281.437500\n",
      "Train Epoch: 9 [46464/54000 (86%)] Loss: -975633.312500\n",
      "Train Epoch: 9 [47872/54000 (89%)] Loss: -979103.500000\n",
      "Train Epoch: 9 [49280/54000 (91%)] Loss: -993497.125000\n",
      "Train Epoch: 9 [50688/54000 (94%)] Loss: -954922.937500\n",
      "Train Epoch: 9 [52096/54000 (96%)] Loss: -949417.312500\n",
      "Train Epoch: 9 [53504/54000 (99%)] Loss: -973273.375000\n",
      "    epoch          : 9\n",
      "    loss           : -949843.1861670616\n",
      "    val_loss       : -960774.8717352685\n",
      "Train Epoch: 10 [0/54000 (0%)] Loss: -912519.625000\n",
      "Train Epoch: 10 [1408/54000 (3%)] Loss: -951865.437500\n",
      "Train Epoch: 10 [2816/54000 (5%)] Loss: -994418.687500\n",
      "Train Epoch: 10 [4224/54000 (8%)] Loss: -972658.000000\n",
      "Train Epoch: 10 [5632/54000 (10%)] Loss: -963798.937500\n",
      "Train Epoch: 10 [7040/54000 (13%)] Loss: -927039.187500\n",
      "Train Epoch: 10 [8448/54000 (16%)] Loss: -962045.062500\n",
      "Train Epoch: 10 [9856/54000 (18%)] Loss: -1026034.375000\n",
      "Train Epoch: 10 [11264/54000 (21%)] Loss: -1021436.812500\n",
      "Train Epoch: 10 [12672/54000 (23%)] Loss: -954343.375000\n",
      "Train Epoch: 10 [14080/54000 (26%)] Loss: -946013.250000\n",
      "Train Epoch: 10 [15488/54000 (29%)] Loss: -983402.000000\n",
      "Train Epoch: 10 [16896/54000 (31%)] Loss: -1009067.937500\n",
      "Train Epoch: 10 [18304/54000 (34%)] Loss: -974066.750000\n",
      "Train Epoch: 10 [19712/54000 (37%)] Loss: -1005705.812500\n",
      "Train Epoch: 10 [21120/54000 (39%)] Loss: -963308.625000\n",
      "Train Epoch: 10 [22528/54000 (42%)] Loss: -1005419.750000\n",
      "Train Epoch: 10 [23936/54000 (44%)] Loss: -967919.187500\n",
      "Train Epoch: 10 [25344/54000 (47%)] Loss: -996172.187500\n",
      "Train Epoch: 10 [26752/54000 (50%)] Loss: -957966.125000\n",
      "Train Epoch: 10 [28160/54000 (52%)] Loss: -987419.312500\n",
      "Train Epoch: 10 [29568/54000 (55%)] Loss: -914999.437500\n",
      "Train Epoch: 10 [30976/54000 (57%)] Loss: -963949.062500\n",
      "Train Epoch: 10 [32384/54000 (60%)] Loss: -954650.250000\n",
      "Train Epoch: 10 [33792/54000 (63%)] Loss: -1009628.250000\n",
      "Train Epoch: 10 [35200/54000 (65%)] Loss: -930294.125000\n",
      "Train Epoch: 10 [36608/54000 (68%)] Loss: -956788.562500\n",
      "Train Epoch: 10 [38016/54000 (70%)] Loss: -999786.062500\n",
      "Train Epoch: 10 [39424/54000 (73%)] Loss: -1002951.812500\n",
      "Train Epoch: 10 [40832/54000 (76%)] Loss: -929917.437500\n",
      "Train Epoch: 10 [42240/54000 (78%)] Loss: -964970.500000\n",
      "Train Epoch: 10 [43648/54000 (81%)] Loss: -946218.937500\n",
      "Train Epoch: 10 [45056/54000 (83%)] Loss: -898270.937500\n",
      "Train Epoch: 10 [46464/54000 (86%)] Loss: -981199.687500\n",
      "Train Epoch: 10 [47872/54000 (89%)] Loss: -970284.500000\n",
      "Train Epoch: 10 [49280/54000 (91%)] Loss: -958396.500000\n",
      "Train Epoch: 10 [50688/54000 (94%)] Loss: -992739.187500\n",
      "Train Epoch: 10 [52096/54000 (96%)] Loss: -952252.750000\n",
      "Train Epoch: 10 [53504/54000 (99%)] Loss: -939250.750000\n",
      "    epoch          : 10\n",
      "    loss           : -957409.9058889959\n",
      "    val_loss       : -969089.6708257147\n",
      "Train Epoch: 11 [0/54000 (0%)] Loss: -937443.187500\n",
      "Train Epoch: 11 [1408/54000 (3%)] Loss: -933013.500000\n",
      "Train Epoch: 11 [2816/54000 (5%)] Loss: -936994.312500\n",
      "Train Epoch: 11 [4224/54000 (8%)] Loss: -983304.000000\n",
      "Train Epoch: 11 [5632/54000 (10%)] Loss: -966062.187500\n",
      "Train Epoch: 11 [7040/54000 (13%)] Loss: -993447.312500\n",
      "Train Epoch: 11 [8448/54000 (16%)] Loss: -1021907.187500\n",
      "Train Epoch: 11 [9856/54000 (18%)] Loss: -1015194.312500\n",
      "Train Epoch: 11 [11264/54000 (21%)] Loss: -959153.812500\n",
      "Train Epoch: 11 [12672/54000 (23%)] Loss: -992965.437500\n",
      "Train Epoch: 11 [14080/54000 (26%)] Loss: -994339.375000\n",
      "Train Epoch: 11 [15488/54000 (29%)] Loss: -984107.250000\n",
      "Train Epoch: 11 [16896/54000 (31%)] Loss: -946517.812500\n",
      "Train Epoch: 11 [18304/54000 (34%)] Loss: -972635.125000\n",
      "Train Epoch: 11 [19712/54000 (37%)] Loss: -951398.750000\n",
      "Train Epoch: 11 [21120/54000 (39%)] Loss: -958186.437500\n",
      "Train Epoch: 11 [22528/54000 (42%)] Loss: -969390.875000\n",
      "Train Epoch: 11 [23936/54000 (44%)] Loss: -979985.875000\n",
      "Train Epoch: 11 [25344/54000 (47%)] Loss: -1001048.312500\n",
      "Train Epoch: 11 [26752/54000 (50%)] Loss: -1006471.750000\n",
      "Train Epoch: 11 [28160/54000 (52%)] Loss: -976771.312500\n",
      "Train Epoch: 11 [29568/54000 (55%)] Loss: -922779.625000\n",
      "Train Epoch: 11 [30976/54000 (57%)] Loss: -983496.375000\n",
      "Train Epoch: 11 [32384/54000 (60%)] Loss: -980806.562500\n",
      "Train Epoch: 11 [33792/54000 (63%)] Loss: -970320.500000\n",
      "Train Epoch: 11 [35200/54000 (65%)] Loss: -956916.875000\n",
      "Train Epoch: 11 [36608/54000 (68%)] Loss: -983140.750000\n",
      "Train Epoch: 11 [38016/54000 (70%)] Loss: -1007038.937500\n",
      "Train Epoch: 11 [39424/54000 (73%)] Loss: -972906.500000\n",
      "Train Epoch: 11 [40832/54000 (76%)] Loss: -947228.625000\n",
      "Train Epoch: 11 [42240/54000 (78%)] Loss: -934665.812500\n",
      "Train Epoch: 11 [43648/54000 (81%)] Loss: -942136.562500\n",
      "Train Epoch: 11 [45056/54000 (83%)] Loss: -966426.687500\n",
      "Train Epoch: 11 [46464/54000 (86%)] Loss: -943221.687500\n",
      "Train Epoch: 11 [47872/54000 (89%)] Loss: -947803.687500\n",
      "Train Epoch: 11 [49280/54000 (91%)] Loss: -979781.312500\n",
      "Train Epoch: 11 [50688/54000 (94%)] Loss: -932779.125000\n",
      "Train Epoch: 11 [52096/54000 (96%)] Loss: -949496.875000\n",
      "Train Epoch: 11 [53504/54000 (99%)] Loss: -975712.437500\n",
      "    epoch          : 11\n",
      "    loss           : -968400.1464751185\n",
      "    val_loss       : -977009.8140947057\n",
      "Train Epoch: 12 [0/54000 (0%)] Loss: -994589.750000\n",
      "Train Epoch: 12 [1408/54000 (3%)] Loss: -952708.750000\n",
      "Train Epoch: 12 [2816/54000 (5%)] Loss: -993935.125000\n",
      "Train Epoch: 12 [4224/54000 (8%)] Loss: -1019667.812500\n",
      "Train Epoch: 12 [5632/54000 (10%)] Loss: -991146.375000\n",
      "Train Epoch: 12 [7040/54000 (13%)] Loss: -1017821.125000\n",
      "Train Epoch: 12 [8448/54000 (16%)] Loss: -1011831.125000\n",
      "Train Epoch: 12 [9856/54000 (18%)] Loss: -1003861.812500\n",
      "Train Epoch: 12 [11264/54000 (21%)] Loss: -967508.312500\n",
      "Train Epoch: 12 [12672/54000 (23%)] Loss: -943579.000000\n",
      "Train Epoch: 12 [14080/54000 (26%)] Loss: -965381.937500\n",
      "Train Epoch: 12 [15488/54000 (29%)] Loss: -980110.750000\n",
      "Train Epoch: 12 [16896/54000 (31%)] Loss: -963899.187500\n",
      "Train Epoch: 12 [18304/54000 (34%)] Loss: -986150.125000\n",
      "Train Epoch: 12 [19712/54000 (37%)] Loss: -967106.437500\n",
      "Train Epoch: 12 [21120/54000 (39%)] Loss: -994324.375000\n",
      "Train Epoch: 12 [22528/54000 (42%)] Loss: -984660.187500\n",
      "Train Epoch: 12 [23936/54000 (44%)] Loss: -927191.062500\n",
      "Train Epoch: 12 [25344/54000 (47%)] Loss: -990103.937500\n",
      "Train Epoch: 12 [26752/54000 (50%)] Loss: -991826.875000\n",
      "Train Epoch: 12 [28160/54000 (52%)] Loss: -985654.250000\n",
      "Train Epoch: 12 [29568/54000 (55%)] Loss: -982219.750000\n",
      "Train Epoch: 12 [30976/54000 (57%)] Loss: -955200.250000\n",
      "Train Epoch: 12 [32384/54000 (60%)] Loss: -981393.000000\n",
      "Train Epoch: 12 [33792/54000 (63%)] Loss: -1000154.312500\n",
      "Train Epoch: 12 [35200/54000 (65%)] Loss: -1011029.937500\n",
      "Train Epoch: 12 [36608/54000 (68%)] Loss: -1008361.875000\n",
      "Train Epoch: 12 [38016/54000 (70%)] Loss: -938118.000000\n",
      "Train Epoch: 12 [39424/54000 (73%)] Loss: -976804.000000\n",
      "Train Epoch: 12 [40832/54000 (76%)] Loss: -955547.437500\n",
      "Train Epoch: 12 [42240/54000 (78%)] Loss: -980289.250000\n",
      "Train Epoch: 12 [43648/54000 (81%)] Loss: -940064.375000\n",
      "Train Epoch: 12 [45056/54000 (83%)] Loss: -948534.312500\n",
      "Train Epoch: 12 [46464/54000 (86%)] Loss: -959595.687500\n",
      "Train Epoch: 12 [47872/54000 (89%)] Loss: -966251.187500\n",
      "Train Epoch: 12 [49280/54000 (91%)] Loss: -966805.375000\n",
      "Train Epoch: 12 [50688/54000 (94%)] Loss: -981220.125000\n",
      "Train Epoch: 12 [52096/54000 (96%)] Loss: -973113.687500\n",
      "Train Epoch: 12 [53504/54000 (99%)] Loss: -1034075.250000\n",
      "    epoch          : 12\n",
      "    loss           : -975571.735485782\n",
      "    val_loss       : -983089.0334135015\n",
      "Train Epoch: 13 [0/54000 (0%)] Loss: -962324.437500\n",
      "Train Epoch: 13 [1408/54000 (3%)] Loss: -992842.625000\n",
      "Train Epoch: 13 [2816/54000 (5%)] Loss: -961070.187500\n",
      "Train Epoch: 13 [4224/54000 (8%)] Loss: -970648.375000\n",
      "Train Epoch: 13 [5632/54000 (10%)] Loss: -967444.187500\n",
      "Train Epoch: 13 [7040/54000 (13%)] Loss: -969201.562500\n",
      "Train Epoch: 13 [8448/54000 (16%)] Loss: -960413.062500\n",
      "Train Epoch: 13 [9856/54000 (18%)] Loss: -975396.062500\n",
      "Train Epoch: 13 [11264/54000 (21%)] Loss: -966190.062500\n",
      "Train Epoch: 13 [12672/54000 (23%)] Loss: -950135.187500\n",
      "Train Epoch: 13 [14080/54000 (26%)] Loss: -994825.875000\n",
      "Train Epoch: 13 [15488/54000 (29%)] Loss: -975210.875000\n",
      "Train Epoch: 13 [16896/54000 (31%)] Loss: -960327.250000\n",
      "Train Epoch: 13 [18304/54000 (34%)] Loss: -969360.312500\n",
      "Train Epoch: 13 [19712/54000 (37%)] Loss: -1002762.875000\n",
      "Train Epoch: 13 [21120/54000 (39%)] Loss: -1005068.562500\n",
      "Train Epoch: 13 [22528/54000 (42%)] Loss: -951994.375000\n",
      "Train Epoch: 13 [23936/54000 (44%)] Loss: -974172.062500\n",
      "Train Epoch: 13 [25344/54000 (47%)] Loss: -1042461.687500\n",
      "Train Epoch: 13 [26752/54000 (50%)] Loss: -1008661.562500\n",
      "Train Epoch: 13 [28160/54000 (52%)] Loss: -974171.750000\n",
      "Train Epoch: 13 [29568/54000 (55%)] Loss: -1006037.562500\n",
      "Train Epoch: 13 [30976/54000 (57%)] Loss: -923090.687500\n",
      "Train Epoch: 13 [32384/54000 (60%)] Loss: -991616.500000\n",
      "Train Epoch: 13 [33792/54000 (63%)] Loss: -1032393.187500\n",
      "Train Epoch: 13 [35200/54000 (65%)] Loss: -1008102.375000\n",
      "Train Epoch: 13 [36608/54000 (68%)] Loss: -1014057.437500\n",
      "Train Epoch: 13 [38016/54000 (70%)] Loss: -982388.125000\n",
      "Train Epoch: 13 [39424/54000 (73%)] Loss: -997695.562500\n",
      "Train Epoch: 13 [40832/54000 (76%)] Loss: -998901.875000\n",
      "Train Epoch: 13 [42240/54000 (78%)] Loss: -985527.000000\n",
      "Train Epoch: 13 [43648/54000 (81%)] Loss: -974151.812500\n",
      "Train Epoch: 13 [45056/54000 (83%)] Loss: -972435.000000\n",
      "Train Epoch: 13 [46464/54000 (86%)] Loss: -989935.875000\n",
      "Train Epoch: 13 [47872/54000 (89%)] Loss: -1029018.062500\n",
      "Train Epoch: 13 [49280/54000 (91%)] Loss: -918003.437500\n",
      "Train Epoch: 13 [50688/54000 (94%)] Loss: -996341.875000\n",
      "Train Epoch: 13 [52096/54000 (96%)] Loss: -943554.687500\n",
      "Train Epoch: 13 [53504/54000 (99%)] Loss: -1015756.062500\n",
      "    epoch          : 13\n",
      "    loss           : -982104.8672985781\n",
      "    val_loss       : -990149.1958059757\n",
      "Train Epoch: 14 [0/54000 (0%)] Loss: -989590.125000\n",
      "Train Epoch: 14 [1408/54000 (3%)] Loss: -942028.500000\n",
      "Train Epoch: 14 [2816/54000 (5%)] Loss: -948543.812500\n",
      "Train Epoch: 14 [4224/54000 (8%)] Loss: -951558.375000\n",
      "Train Epoch: 14 [5632/54000 (10%)] Loss: -962968.875000\n",
      "Train Epoch: 14 [7040/54000 (13%)] Loss: -1008352.750000\n",
      "Train Epoch: 14 [8448/54000 (16%)] Loss: -1008032.000000\n",
      "Train Epoch: 14 [9856/54000 (18%)] Loss: -1024909.812500\n",
      "Train Epoch: 14 [11264/54000 (21%)] Loss: -949986.375000\n",
      "Train Epoch: 14 [12672/54000 (23%)] Loss: -999936.437500\n",
      "Train Epoch: 14 [14080/54000 (26%)] Loss: -1022237.187500\n",
      "Train Epoch: 14 [15488/54000 (29%)] Loss: -1008382.125000\n",
      "Train Epoch: 14 [16896/54000 (31%)] Loss: -985912.312500\n",
      "Train Epoch: 14 [18304/54000 (34%)] Loss: -985815.812500\n",
      "Train Epoch: 14 [19712/54000 (37%)] Loss: -1015791.812500\n",
      "Train Epoch: 14 [21120/54000 (39%)] Loss: -1015309.812500\n",
      "Train Epoch: 14 [22528/54000 (42%)] Loss: -963543.562500\n",
      "Train Epoch: 14 [23936/54000 (44%)] Loss: -1020244.062500\n",
      "Train Epoch: 14 [25344/54000 (47%)] Loss: -1002011.437500\n",
      "Train Epoch: 14 [26752/54000 (50%)] Loss: -988170.437500\n",
      "Train Epoch: 14 [28160/54000 (52%)] Loss: -1022581.625000\n",
      "Train Epoch: 14 [29568/54000 (55%)] Loss: -991377.375000\n",
      "Train Epoch: 14 [30976/54000 (57%)] Loss: -997414.687500\n",
      "Train Epoch: 14 [32384/54000 (60%)] Loss: -1001783.562500\n",
      "Train Epoch: 14 [33792/54000 (63%)] Loss: -1042061.812500\n",
      "Train Epoch: 14 [35200/54000 (65%)] Loss: -1076219.500000\n",
      "Train Epoch: 14 [36608/54000 (68%)] Loss: -960259.187500\n",
      "Train Epoch: 14 [38016/54000 (70%)] Loss: -992076.250000\n",
      "Train Epoch: 14 [39424/54000 (73%)] Loss: -966625.812500\n",
      "Train Epoch: 14 [40832/54000 (76%)] Loss: -994258.125000\n",
      "Train Epoch: 14 [42240/54000 (78%)] Loss: -994892.125000\n",
      "Train Epoch: 14 [43648/54000 (81%)] Loss: -1019903.875000\n",
      "Train Epoch: 14 [45056/54000 (83%)] Loss: -971782.625000\n",
      "Train Epoch: 14 [46464/54000 (86%)] Loss: -1006083.937500\n",
      "Train Epoch: 14 [47872/54000 (89%)] Loss: -950267.437500\n",
      "Train Epoch: 14 [49280/54000 (91%)] Loss: -1033059.000000\n",
      "Train Epoch: 14 [50688/54000 (94%)] Loss: -1050610.125000\n",
      "Train Epoch: 14 [52096/54000 (96%)] Loss: -971558.875000\n",
      "Train Epoch: 14 [53504/54000 (99%)] Loss: -1007431.750000\n",
      "    epoch          : 14\n",
      "    loss           : -987504.9986670616\n",
      "    val_loss       : -995023.0127420628\n",
      "Train Epoch: 15 [0/54000 (0%)] Loss: -1020423.062500\n",
      "Train Epoch: 15 [1408/54000 (3%)] Loss: -1005907.937500\n",
      "Train Epoch: 15 [2816/54000 (5%)] Loss: -1049872.875000\n",
      "Train Epoch: 15 [4224/54000 (8%)] Loss: -971025.937500\n",
      "Train Epoch: 15 [5632/54000 (10%)] Loss: -971998.812500\n",
      "Train Epoch: 15 [7040/54000 (13%)] Loss: -1012951.062500\n",
      "Train Epoch: 15 [8448/54000 (16%)] Loss: -938946.062500\n",
      "Train Epoch: 15 [9856/54000 (18%)] Loss: -998785.125000\n",
      "Train Epoch: 15 [11264/54000 (21%)] Loss: -958704.937500\n",
      "Train Epoch: 15 [12672/54000 (23%)] Loss: -999788.000000\n",
      "Train Epoch: 15 [14080/54000 (26%)] Loss: -969918.375000\n",
      "Train Epoch: 15 [15488/54000 (29%)] Loss: -1015814.562500\n",
      "Train Epoch: 15 [16896/54000 (31%)] Loss: -976095.437500\n",
      "Train Epoch: 15 [18304/54000 (34%)] Loss: -998817.687500\n",
      "Train Epoch: 15 [19712/54000 (37%)] Loss: -1003285.625000\n",
      "Train Epoch: 15 [21120/54000 (39%)] Loss: -1023334.500000\n",
      "Train Epoch: 15 [22528/54000 (42%)] Loss: -994412.562500\n",
      "Train Epoch: 15 [23936/54000 (44%)] Loss: -1043207.875000\n",
      "Train Epoch: 15 [25344/54000 (47%)] Loss: -1016569.937500\n",
      "Train Epoch: 15 [26752/54000 (50%)] Loss: -1031183.562500\n",
      "Train Epoch: 15 [28160/54000 (52%)] Loss: -950118.500000\n",
      "Train Epoch: 15 [29568/54000 (55%)] Loss: -981513.625000\n",
      "Train Epoch: 15 [30976/54000 (57%)] Loss: -963049.625000\n",
      "Train Epoch: 15 [32384/54000 (60%)] Loss: -993062.062500\n",
      "Train Epoch: 15 [33792/54000 (63%)] Loss: -959596.187500\n",
      "Train Epoch: 15 [35200/54000 (65%)] Loss: -979522.312500\n",
      "Train Epoch: 15 [36608/54000 (68%)] Loss: -1052028.250000\n",
      "Train Epoch: 15 [38016/54000 (70%)] Loss: -998025.750000\n",
      "Train Epoch: 15 [39424/54000 (73%)] Loss: -1021817.312500\n",
      "Train Epoch: 15 [40832/54000 (76%)] Loss: -943547.312500\n",
      "Train Epoch: 15 [42240/54000 (78%)] Loss: -1036391.812500\n",
      "Train Epoch: 15 [43648/54000 (81%)] Loss: -976419.250000\n",
      "Train Epoch: 15 [45056/54000 (83%)] Loss: -1020543.375000\n",
      "Train Epoch: 15 [46464/54000 (86%)] Loss: -934609.125000\n",
      "Train Epoch: 15 [47872/54000 (89%)] Loss: -984187.500000\n",
      "Train Epoch: 15 [49280/54000 (91%)] Loss: -1011995.125000\n",
      "Train Epoch: 15 [50688/54000 (94%)] Loss: -993364.750000\n",
      "Train Epoch: 15 [52096/54000 (96%)] Loss: -997595.500000\n",
      "Train Epoch: 15 [53504/54000 (99%)] Loss: -974130.312500\n",
      "    epoch          : 15\n",
      "    loss           : -991883.0077014219\n",
      "    val_loss       : -998908.8202605551\n",
      "Train Epoch: 16 [0/54000 (0%)] Loss: -977121.375000\n",
      "Train Epoch: 16 [1408/54000 (3%)] Loss: -1022233.062500\n",
      "Train Epoch: 16 [2816/54000 (5%)] Loss: -978629.562500\n",
      "Train Epoch: 16 [4224/54000 (8%)] Loss: -974625.375000\n",
      "Train Epoch: 16 [5632/54000 (10%)] Loss: -977695.750000\n",
      "Train Epoch: 16 [7040/54000 (13%)] Loss: -976531.625000\n",
      "Train Epoch: 16 [8448/54000 (16%)] Loss: -992871.500000\n",
      "Train Epoch: 16 [9856/54000 (18%)] Loss: -999720.812500\n",
      "Train Epoch: 16 [11264/54000 (21%)] Loss: -1017436.312500\n",
      "Train Epoch: 16 [12672/54000 (23%)] Loss: -949692.125000\n",
      "Train Epoch: 16 [14080/54000 (26%)] Loss: -984981.625000\n",
      "Train Epoch: 16 [15488/54000 (29%)] Loss: -881685.250000\n",
      "Train Epoch: 16 [16896/54000 (31%)] Loss: -983981.062500\n",
      "Train Epoch: 16 [18304/54000 (34%)] Loss: -989628.062500\n",
      "Train Epoch: 16 [19712/54000 (37%)] Loss: -956567.875000\n",
      "Train Epoch: 16 [21120/54000 (39%)] Loss: -998744.625000\n",
      "Train Epoch: 16 [22528/54000 (42%)] Loss: -928025.062500\n",
      "Train Epoch: 16 [23936/54000 (44%)] Loss: -971127.687500\n",
      "Train Epoch: 16 [25344/54000 (47%)] Loss: -1071471.000000\n",
      "Train Epoch: 16 [26752/54000 (50%)] Loss: -990629.562500\n",
      "Train Epoch: 16 [28160/54000 (52%)] Loss: -986882.812500\n",
      "Train Epoch: 16 [29568/54000 (55%)] Loss: -1003450.062500\n",
      "Train Epoch: 16 [30976/54000 (57%)] Loss: -986283.000000\n",
      "Train Epoch: 16 [32384/54000 (60%)] Loss: -979523.062500\n",
      "Train Epoch: 16 [33792/54000 (63%)] Loss: -1002450.937500\n",
      "Train Epoch: 16 [35200/54000 (65%)] Loss: -989732.500000\n",
      "Train Epoch: 16 [36608/54000 (68%)] Loss: -948701.625000\n",
      "Train Epoch: 16 [38016/54000 (70%)] Loss: -999551.000000\n",
      "Train Epoch: 16 [39424/54000 (73%)] Loss: -1003914.875000\n",
      "Train Epoch: 16 [40832/54000 (76%)] Loss: -1015968.250000\n",
      "Train Epoch: 16 [42240/54000 (78%)] Loss: -1001877.875000\n",
      "Train Epoch: 16 [43648/54000 (81%)] Loss: -1003983.375000\n",
      "Train Epoch: 16 [45056/54000 (83%)] Loss: -978936.625000\n",
      "Train Epoch: 16 [46464/54000 (86%)] Loss: -1013333.312500\n",
      "Train Epoch: 16 [47872/54000 (89%)] Loss: -988198.000000\n",
      "Train Epoch: 16 [49280/54000 (91%)] Loss: -1074204.250000\n",
      "Train Epoch: 16 [50688/54000 (94%)] Loss: -957530.000000\n",
      "Train Epoch: 16 [52096/54000 (96%)] Loss: -979596.312500\n",
      "Train Epoch: 16 [53504/54000 (99%)] Loss: -1069541.375000\n",
      "    epoch          : 16\n",
      "    loss           : -996240.7040876778\n",
      "    val_loss       : -1001727.7522440159\n",
      "Train Epoch: 17 [0/54000 (0%)] Loss: -970766.187500\n",
      "Train Epoch: 17 [1408/54000 (3%)] Loss: -1047142.687500\n",
      "Train Epoch: 17 [2816/54000 (5%)] Loss: -1072085.000000\n",
      "Train Epoch: 17 [4224/54000 (8%)] Loss: -1008787.250000\n",
      "Train Epoch: 17 [5632/54000 (10%)] Loss: -966788.937500\n",
      "Train Epoch: 17 [7040/54000 (13%)] Loss: -1020924.500000\n",
      "Train Epoch: 17 [8448/54000 (16%)] Loss: -979521.125000\n",
      "Train Epoch: 17 [9856/54000 (18%)] Loss: -1026706.062500\n",
      "Train Epoch: 17 [11264/54000 (21%)] Loss: -1054073.500000\n",
      "Train Epoch: 17 [12672/54000 (23%)] Loss: -950924.875000\n",
      "Train Epoch: 17 [14080/54000 (26%)] Loss: -987386.625000\n",
      "Train Epoch: 17 [15488/54000 (29%)] Loss: -962735.375000\n",
      "Train Epoch: 17 [16896/54000 (31%)] Loss: -970959.625000\n",
      "Train Epoch: 17 [18304/54000 (34%)] Loss: -1050375.000000\n",
      "Train Epoch: 17 [19712/54000 (37%)] Loss: -1010438.625000\n",
      "Train Epoch: 17 [21120/54000 (39%)] Loss: -993145.500000\n",
      "Train Epoch: 17 [22528/54000 (42%)] Loss: -1010278.562500\n",
      "Train Epoch: 17 [23936/54000 (44%)] Loss: -987702.125000\n",
      "Train Epoch: 17 [25344/54000 (47%)] Loss: -976354.187500\n",
      "Train Epoch: 17 [26752/54000 (50%)] Loss: -1000246.187500\n",
      "Train Epoch: 17 [28160/54000 (52%)] Loss: -1007930.187500\n",
      "Train Epoch: 17 [29568/54000 (55%)] Loss: -1006530.187500\n",
      "Train Epoch: 17 [30976/54000 (57%)] Loss: -1002046.062500\n",
      "Train Epoch: 17 [32384/54000 (60%)] Loss: -1040454.687500\n",
      "Train Epoch: 17 [33792/54000 (63%)] Loss: -1030411.125000\n",
      "Train Epoch: 17 [35200/54000 (65%)] Loss: -961699.750000\n",
      "Train Epoch: 17 [36608/54000 (68%)] Loss: -991394.250000\n",
      "Train Epoch: 17 [38016/54000 (70%)] Loss: -989643.250000\n",
      "Train Epoch: 17 [39424/54000 (73%)] Loss: -1039172.250000\n",
      "Train Epoch: 17 [40832/54000 (76%)] Loss: -981921.187500\n",
      "Train Epoch: 17 [42240/54000 (78%)] Loss: -1016217.437500\n",
      "Train Epoch: 17 [43648/54000 (81%)] Loss: -973222.062500\n",
      "Train Epoch: 17 [45056/54000 (83%)] Loss: -983122.125000\n",
      "Train Epoch: 17 [46464/54000 (86%)] Loss: -1015022.562500\n",
      "Train Epoch: 17 [47872/54000 (89%)] Loss: -947626.625000\n",
      "Train Epoch: 17 [49280/54000 (91%)] Loss: -1051781.375000\n",
      "Train Epoch: 17 [50688/54000 (94%)] Loss: -1020211.062500\n",
      "Train Epoch: 17 [52096/54000 (96%)] Loss: -970140.500000\n",
      "Train Epoch: 17 [53504/54000 (99%)] Loss: -1011567.062500\n",
      "    epoch          : 17\n",
      "    loss           : -1000042.9406101896\n",
      "    val_loss       : -1005980.9678565492\n",
      "Train Epoch: 18 [0/54000 (0%)] Loss: -999947.437500\n",
      "Train Epoch: 18 [1408/54000 (3%)] Loss: -1022403.875000\n",
      "Train Epoch: 18 [2816/54000 (5%)] Loss: -1083426.000000\n",
      "Train Epoch: 18 [4224/54000 (8%)] Loss: -1032812.562500\n",
      "Train Epoch: 18 [5632/54000 (10%)] Loss: -1015264.500000\n",
      "Train Epoch: 18 [7040/54000 (13%)] Loss: -998456.250000\n",
      "Train Epoch: 18 [8448/54000 (16%)] Loss: -1001000.437500\n",
      "Train Epoch: 18 [9856/54000 (18%)] Loss: -1009267.562500\n",
      "Train Epoch: 18 [11264/54000 (21%)] Loss: -1023210.187500\n",
      "Train Epoch: 18 [12672/54000 (23%)] Loss: -996735.625000\n",
      "Train Epoch: 18 [14080/54000 (26%)] Loss: -973659.500000\n",
      "Train Epoch: 18 [15488/54000 (29%)] Loss: -1034905.625000\n",
      "Train Epoch: 18 [16896/54000 (31%)] Loss: -1097759.250000\n",
      "Train Epoch: 18 [18304/54000 (34%)] Loss: -987192.562500\n",
      "Train Epoch: 18 [19712/54000 (37%)] Loss: -1018311.875000\n",
      "Train Epoch: 18 [21120/54000 (39%)] Loss: -1009450.625000\n",
      "Train Epoch: 18 [22528/54000 (42%)] Loss: -1011984.187500\n",
      "Train Epoch: 18 [23936/54000 (44%)] Loss: -1041805.437500\n",
      "Train Epoch: 18 [25344/54000 (47%)] Loss: -1001990.125000\n",
      "Train Epoch: 18 [26752/54000 (50%)] Loss: -967581.250000\n",
      "Train Epoch: 18 [28160/54000 (52%)] Loss: -1000162.750000\n",
      "Train Epoch: 18 [29568/54000 (55%)] Loss: -986255.250000\n",
      "Train Epoch: 18 [30976/54000 (57%)] Loss: -1002713.000000\n",
      "Train Epoch: 18 [32384/54000 (60%)] Loss: -993619.875000\n",
      "Train Epoch: 18 [33792/54000 (63%)] Loss: -1029176.625000\n",
      "Train Epoch: 18 [35200/54000 (65%)] Loss: -965864.375000\n",
      "Train Epoch: 18 [36608/54000 (68%)] Loss: -1038394.687500\n",
      "Train Epoch: 18 [38016/54000 (70%)] Loss: -998168.500000\n",
      "Train Epoch: 18 [39424/54000 (73%)] Loss: -987038.875000\n",
      "Train Epoch: 18 [40832/54000 (76%)] Loss: -981486.375000\n",
      "Train Epoch: 18 [42240/54000 (78%)] Loss: -991745.312500\n",
      "Train Epoch: 18 [43648/54000 (81%)] Loss: -1029884.062500\n",
      "Train Epoch: 18 [45056/54000 (83%)] Loss: -1022217.625000\n",
      "Train Epoch: 18 [46464/54000 (86%)] Loss: -1003675.187500\n",
      "Train Epoch: 18 [47872/54000 (89%)] Loss: -1008451.812500\n",
      "Train Epoch: 18 [49280/54000 (91%)] Loss: -1019867.000000\n",
      "Train Epoch: 18 [50688/54000 (94%)] Loss: -1004287.000000\n",
      "Train Epoch: 18 [52096/54000 (96%)] Loss: -1049660.875000\n",
      "Train Epoch: 18 [53504/54000 (99%)] Loss: -1051713.250000\n",
      "    epoch          : 18\n",
      "    loss           : -1003348.1020438388\n",
      "    val_loss       : -1009499.6073621385\n",
      "Train Epoch: 19 [0/54000 (0%)] Loss: -988497.812500\n",
      "Train Epoch: 19 [1408/54000 (3%)] Loss: -959081.875000\n",
      "Train Epoch: 19 [2816/54000 (5%)] Loss: -1028976.125000\n",
      "Train Epoch: 19 [4224/54000 (8%)] Loss: -1013727.625000\n",
      "Train Epoch: 19 [5632/54000 (10%)] Loss: -1014249.250000\n",
      "Train Epoch: 19 [7040/54000 (13%)] Loss: -1002536.750000\n",
      "Train Epoch: 19 [8448/54000 (16%)] Loss: -954752.437500\n",
      "Train Epoch: 19 [9856/54000 (18%)] Loss: -1014279.687500\n",
      "Train Epoch: 19 [11264/54000 (21%)] Loss: -1015097.375000\n",
      "Train Epoch: 19 [12672/54000 (23%)] Loss: -1064695.500000\n",
      "Train Epoch: 19 [14080/54000 (26%)] Loss: -1037304.250000\n",
      "Train Epoch: 19 [15488/54000 (29%)] Loss: -990563.687500\n",
      "Train Epoch: 19 [16896/54000 (31%)] Loss: -965713.312500\n",
      "Train Epoch: 19 [18304/54000 (34%)] Loss: -1018570.187500\n",
      "Train Epoch: 19 [19712/54000 (37%)] Loss: -1000215.937500\n",
      "Train Epoch: 19 [21120/54000 (39%)] Loss: -1067523.000000\n",
      "Train Epoch: 19 [22528/54000 (42%)] Loss: -966854.750000\n",
      "Train Epoch: 19 [23936/54000 (44%)] Loss: -1051237.500000\n",
      "Train Epoch: 19 [25344/54000 (47%)] Loss: -1064817.250000\n",
      "Train Epoch: 19 [26752/54000 (50%)] Loss: -982563.937500\n",
      "Train Epoch: 19 [28160/54000 (52%)] Loss: -1037024.750000\n",
      "Train Epoch: 19 [29568/54000 (55%)] Loss: -980095.875000\n",
      "Train Epoch: 19 [30976/54000 (57%)] Loss: -966938.687500\n",
      "Train Epoch: 19 [32384/54000 (60%)] Loss: -965569.500000\n",
      "Train Epoch: 19 [33792/54000 (63%)] Loss: -1026426.437500\n",
      "Train Epoch: 19 [35200/54000 (65%)] Loss: -1003061.125000\n",
      "Train Epoch: 19 [36608/54000 (68%)] Loss: -1020228.437500\n",
      "Train Epoch: 19 [38016/54000 (70%)] Loss: -1033509.375000\n",
      "Train Epoch: 19 [39424/54000 (73%)] Loss: -989391.437500\n",
      "Train Epoch: 19 [40832/54000 (76%)] Loss: -998000.812500\n",
      "Train Epoch: 19 [42240/54000 (78%)] Loss: -976879.687500\n",
      "Train Epoch: 19 [43648/54000 (81%)] Loss: -992440.625000\n",
      "Train Epoch: 19 [45056/54000 (83%)] Loss: -944304.062500\n",
      "Train Epoch: 19 [46464/54000 (86%)] Loss: -1029802.125000\n",
      "Train Epoch: 19 [47872/54000 (89%)] Loss: -1011412.812500\n",
      "Train Epoch: 19 [49280/54000 (91%)] Loss: -1048888.500000\n",
      "Train Epoch: 19 [50688/54000 (94%)] Loss: -985857.125000\n",
      "Train Epoch: 19 [52096/54000 (96%)] Loss: -997643.750000\n",
      "Train Epoch: 19 [53504/54000 (99%)] Loss: -1008037.437500\n",
      "    epoch          : 19\n",
      "    loss           : -1006383.7873222749\n",
      "    val_loss       : -1012263.9707265001\n",
      "Train Epoch: 20 [0/54000 (0%)] Loss: -1010677.875000\n",
      "Train Epoch: 20 [1408/54000 (3%)] Loss: -1035603.500000\n",
      "Train Epoch: 20 [2816/54000 (5%)] Loss: -985218.625000\n",
      "Train Epoch: 20 [4224/54000 (8%)] Loss: -1019609.000000\n",
      "Train Epoch: 20 [5632/54000 (10%)] Loss: -1013757.187500\n",
      "Train Epoch: 20 [7040/54000 (13%)] Loss: -1020015.000000\n",
      "Train Epoch: 20 [8448/54000 (16%)] Loss: -971333.750000\n",
      "Train Epoch: 20 [9856/54000 (18%)] Loss: -1018203.187500\n",
      "Train Epoch: 20 [11264/54000 (21%)] Loss: -1006775.437500\n",
      "Train Epoch: 20 [12672/54000 (23%)] Loss: -1018867.500000\n",
      "Train Epoch: 20 [14080/54000 (26%)] Loss: -989435.500000\n",
      "Train Epoch: 20 [15488/54000 (29%)] Loss: -1074993.500000\n",
      "Train Epoch: 20 [16896/54000 (31%)] Loss: -1059747.875000\n",
      "Train Epoch: 20 [18304/54000 (34%)] Loss: -989880.562500\n",
      "Train Epoch: 20 [19712/54000 (37%)] Loss: -983026.687500\n",
      "Train Epoch: 20 [21120/54000 (39%)] Loss: -1024021.812500\n",
      "Train Epoch: 20 [22528/54000 (42%)] Loss: -1057736.625000\n",
      "Train Epoch: 20 [23936/54000 (44%)] Loss: -1046380.562500\n",
      "Train Epoch: 20 [25344/54000 (47%)] Loss: -1047012.250000\n",
      "Train Epoch: 20 [26752/54000 (50%)] Loss: -1089432.500000\n",
      "Train Epoch: 20 [28160/54000 (52%)] Loss: -1026950.437500\n",
      "Train Epoch: 20 [29568/54000 (55%)] Loss: -969915.312500\n",
      "Train Epoch: 20 [30976/54000 (57%)] Loss: -1017788.312500\n",
      "Train Epoch: 20 [32384/54000 (60%)] Loss: -1015228.062500\n",
      "Train Epoch: 20 [33792/54000 (63%)] Loss: -1072884.375000\n",
      "Train Epoch: 20 [35200/54000 (65%)] Loss: -1021939.625000\n",
      "Train Epoch: 20 [36608/54000 (68%)] Loss: -1069294.625000\n",
      "Train Epoch: 20 [38016/54000 (70%)] Loss: -1007542.000000\n",
      "Train Epoch: 20 [39424/54000 (73%)] Loss: -1040513.562500\n",
      "Train Epoch: 20 [40832/54000 (76%)] Loss: -1011197.687500\n",
      "Train Epoch: 20 [42240/54000 (78%)] Loss: -1012162.750000\n",
      "Train Epoch: 20 [43648/54000 (81%)] Loss: -1032218.437500\n",
      "Train Epoch: 20 [45056/54000 (83%)] Loss: -1084796.375000\n",
      "Train Epoch: 20 [46464/54000 (86%)] Loss: -1009719.187500\n",
      "Train Epoch: 20 [47872/54000 (89%)] Loss: -1049438.250000\n",
      "Train Epoch: 20 [49280/54000 (91%)] Loss: -1031245.250000\n",
      "Train Epoch: 20 [50688/54000 (94%)] Loss: -1025552.312500\n",
      "Train Epoch: 20 [52096/54000 (96%)] Loss: -1022277.250000\n",
      "Train Epoch: 20 [53504/54000 (99%)] Loss: -999586.937500\n",
      "    epoch          : 20\n",
      "    loss           : -1009475.3145734597\n",
      "    val_loss       : -1015103.3080820936\n",
      "Train Epoch: 21 [0/54000 (0%)] Loss: -1060532.125000\n",
      "Train Epoch: 21 [1408/54000 (3%)] Loss: -1061615.000000\n",
      "Train Epoch: 21 [2816/54000 (5%)] Loss: -983322.500000\n",
      "Train Epoch: 21 [4224/54000 (8%)] Loss: -1002451.062500\n",
      "Train Epoch: 21 [5632/54000 (10%)] Loss: -983674.937500\n",
      "Train Epoch: 21 [7040/54000 (13%)] Loss: -986631.125000\n",
      "Train Epoch: 21 [8448/54000 (16%)] Loss: -972917.500000\n",
      "Train Epoch: 21 [9856/54000 (18%)] Loss: -1028146.187500\n",
      "Train Epoch: 21 [11264/54000 (21%)] Loss: -993752.937500\n",
      "Train Epoch: 21 [12672/54000 (23%)] Loss: -999078.937500\n",
      "Train Epoch: 21 [14080/54000 (26%)] Loss: -1021105.500000\n",
      "Train Epoch: 21 [15488/54000 (29%)] Loss: -1015281.625000\n",
      "Train Epoch: 21 [16896/54000 (31%)] Loss: -1003434.562500\n",
      "Train Epoch: 21 [18304/54000 (34%)] Loss: -1081257.750000\n",
      "Train Epoch: 21 [19712/54000 (37%)] Loss: -967038.875000\n",
      "Train Epoch: 21 [21120/54000 (39%)] Loss: -981502.312500\n",
      "Train Epoch: 21 [22528/54000 (42%)] Loss: -1037929.062500\n",
      "Train Epoch: 21 [23936/54000 (44%)] Loss: -1071681.500000\n",
      "Train Epoch: 21 [25344/54000 (47%)] Loss: -985304.562500\n",
      "Train Epoch: 21 [26752/54000 (50%)] Loss: -1022244.500000\n",
      "Train Epoch: 21 [28160/54000 (52%)] Loss: -1061649.625000\n",
      "Train Epoch: 21 [29568/54000 (55%)] Loss: -1005922.000000\n",
      "Train Epoch: 21 [30976/54000 (57%)] Loss: -969958.000000\n",
      "Train Epoch: 21 [32384/54000 (60%)] Loss: -998182.500000\n",
      "Train Epoch: 21 [33792/54000 (63%)] Loss: -976749.937500\n",
      "Train Epoch: 21 [35200/54000 (65%)] Loss: -988826.312500\n",
      "Train Epoch: 21 [36608/54000 (68%)] Loss: -988013.812500\n",
      "Train Epoch: 21 [38016/54000 (70%)] Loss: -921130.812500\n",
      "Train Epoch: 21 [39424/54000 (73%)] Loss: -1035072.500000\n",
      "Train Epoch: 21 [40832/54000 (76%)] Loss: -1039943.125000\n",
      "Train Epoch: 21 [42240/54000 (78%)] Loss: -1030397.812500\n",
      "Train Epoch: 21 [43648/54000 (81%)] Loss: -1073847.750000\n",
      "Train Epoch: 21 [45056/54000 (83%)] Loss: -1027511.437500\n",
      "Train Epoch: 21 [46464/54000 (86%)] Loss: -1006552.500000\n",
      "Train Epoch: 21 [47872/54000 (89%)] Loss: -1054732.625000\n",
      "Train Epoch: 21 [49280/54000 (91%)] Loss: -998320.500000\n",
      "Train Epoch: 21 [50688/54000 (94%)] Loss: -1020827.250000\n",
      "Train Epoch: 21 [52096/54000 (96%)] Loss: -995853.750000\n",
      "Train Epoch: 21 [53504/54000 (99%)] Loss: -1044665.687500\n",
      "    epoch          : 21\n",
      "    loss           : -1012276.9081753554\n",
      "    val_loss       : -1018279.1083257147\n",
      "Train Epoch: 22 [0/54000 (0%)] Loss: -987578.000000\n",
      "Train Epoch: 22 [1408/54000 (3%)] Loss: -944604.500000\n",
      "Train Epoch: 22 [2816/54000 (5%)] Loss: -1045096.187500\n",
      "Train Epoch: 22 [4224/54000 (8%)] Loss: -951392.937500\n",
      "Train Epoch: 22 [5632/54000 (10%)] Loss: -962740.812500\n",
      "Train Epoch: 22 [7040/54000 (13%)] Loss: -1040496.062500\n",
      "Train Epoch: 22 [8448/54000 (16%)] Loss: -1034379.250000\n",
      "Train Epoch: 22 [9856/54000 (18%)] Loss: -1022210.375000\n",
      "Train Epoch: 22 [11264/54000 (21%)] Loss: -993059.000000\n",
      "Train Epoch: 22 [12672/54000 (23%)] Loss: -1023495.187500\n",
      "Train Epoch: 22 [14080/54000 (26%)] Loss: -1006541.125000\n",
      "Train Epoch: 22 [15488/54000 (29%)] Loss: -967784.437500\n",
      "Train Epoch: 22 [16896/54000 (31%)] Loss: -1079123.000000\n",
      "Train Epoch: 22 [18304/54000 (34%)] Loss: -988716.875000\n",
      "Train Epoch: 22 [19712/54000 (37%)] Loss: -985463.500000\n",
      "Train Epoch: 22 [21120/54000 (39%)] Loss: -1001412.312500\n",
      "Train Epoch: 22 [22528/54000 (42%)] Loss: -940994.812500\n",
      "Train Epoch: 22 [23936/54000 (44%)] Loss: -1007581.500000\n",
      "Train Epoch: 22 [25344/54000 (47%)] Loss: -1006170.875000\n",
      "Train Epoch: 22 [26752/54000 (50%)] Loss: -1007625.000000\n",
      "Train Epoch: 22 [28160/54000 (52%)] Loss: -983789.250000\n",
      "Train Epoch: 22 [29568/54000 (55%)] Loss: -1019371.562500\n",
      "Train Epoch: 22 [30976/54000 (57%)] Loss: -1031385.250000\n",
      "Train Epoch: 22 [32384/54000 (60%)] Loss: -978803.437500\n",
      "Train Epoch: 22 [33792/54000 (63%)] Loss: -990097.500000\n",
      "Train Epoch: 22 [35200/54000 (65%)] Loss: -1082143.750000\n",
      "Train Epoch: 22 [36608/54000 (68%)] Loss: -1035168.125000\n",
      "Train Epoch: 22 [38016/54000 (70%)] Loss: -1004898.625000\n",
      "Train Epoch: 22 [39424/54000 (73%)] Loss: -937620.000000\n",
      "Train Epoch: 22 [40832/54000 (76%)] Loss: -1001891.000000\n",
      "Train Epoch: 22 [42240/54000 (78%)] Loss: -954162.812500\n",
      "Train Epoch: 22 [43648/54000 (81%)] Loss: -1048446.000000\n",
      "Train Epoch: 22 [45056/54000 (83%)] Loss: -999654.125000\n",
      "Train Epoch: 22 [46464/54000 (86%)] Loss: -1024719.375000\n",
      "Train Epoch: 22 [47872/54000 (89%)] Loss: -1013696.687500\n",
      "Train Epoch: 22 [49280/54000 (91%)] Loss: -984261.187500\n",
      "Train Epoch: 22 [50688/54000 (94%)] Loss: -1014456.062500\n",
      "Train Epoch: 22 [52096/54000 (96%)] Loss: -1051721.875000\n",
      "Train Epoch: 22 [53504/54000 (99%)] Loss: -1004380.812500\n",
      "    epoch          : 22\n",
      "    loss           : -1014824.78125\n",
      "    val_loss       : -1020743.5671698388\n",
      "Train Epoch: 23 [0/54000 (0%)] Loss: -997985.875000\n",
      "Train Epoch: 23 [1408/54000 (3%)] Loss: -1042014.937500\n",
      "Train Epoch: 23 [2816/54000 (5%)] Loss: -1006440.562500\n",
      "Train Epoch: 23 [4224/54000 (8%)] Loss: -1032026.500000\n",
      "Train Epoch: 23 [5632/54000 (10%)] Loss: -1027535.875000\n",
      "Train Epoch: 23 [7040/54000 (13%)] Loss: -995733.875000\n",
      "Train Epoch: 23 [8448/54000 (16%)] Loss: -992132.000000\n",
      "Train Epoch: 23 [9856/54000 (18%)] Loss: -1046309.687500\n",
      "Train Epoch: 23 [11264/54000 (21%)] Loss: -958104.125000\n",
      "Train Epoch: 23 [12672/54000 (23%)] Loss: -997334.125000\n",
      "Train Epoch: 23 [14080/54000 (26%)] Loss: -1026139.062500\n",
      "Train Epoch: 23 [15488/54000 (29%)] Loss: -1037892.000000\n",
      "Train Epoch: 23 [16896/54000 (31%)] Loss: -960730.187500\n",
      "Train Epoch: 23 [18304/54000 (34%)] Loss: -1028363.062500\n",
      "Train Epoch: 23 [19712/54000 (37%)] Loss: -937336.375000\n",
      "Train Epoch: 23 [21120/54000 (39%)] Loss: -1015089.375000\n",
      "Train Epoch: 23 [22528/54000 (42%)] Loss: -992178.250000\n",
      "Train Epoch: 23 [23936/54000 (44%)] Loss: -1021031.937500\n",
      "Train Epoch: 23 [25344/54000 (47%)] Loss: -987655.562500\n",
      "Train Epoch: 23 [26752/54000 (50%)] Loss: -1082775.500000\n",
      "Train Epoch: 23 [28160/54000 (52%)] Loss: -1046965.062500\n",
      "Train Epoch: 23 [29568/54000 (55%)] Loss: -977541.500000\n",
      "Train Epoch: 23 [30976/54000 (57%)] Loss: -1061445.500000\n",
      "Train Epoch: 23 [32384/54000 (60%)] Loss: -1032161.250000\n",
      "Train Epoch: 23 [33792/54000 (63%)] Loss: -1009574.812500\n",
      "Train Epoch: 23 [35200/54000 (65%)] Loss: -1053752.125000\n",
      "Train Epoch: 23 [36608/54000 (68%)] Loss: -1007672.000000\n",
      "Train Epoch: 23 [38016/54000 (70%)] Loss: -965089.812500\n",
      "Train Epoch: 23 [39424/54000 (73%)] Loss: -993231.562500\n",
      "Train Epoch: 23 [40832/54000 (76%)] Loss: -1071968.125000\n",
      "Train Epoch: 23 [42240/54000 (78%)] Loss: -986673.437500\n",
      "Train Epoch: 23 [43648/54000 (81%)] Loss: -1002820.937500\n",
      "Train Epoch: 23 [45056/54000 (83%)] Loss: -1001933.750000\n",
      "Train Epoch: 23 [46464/54000 (86%)] Loss: -1017853.937500\n",
      "Train Epoch: 23 [47872/54000 (89%)] Loss: -998507.875000\n",
      "Train Epoch: 23 [49280/54000 (91%)] Loss: -956647.000000\n",
      "Train Epoch: 23 [50688/54000 (94%)] Loss: -1016320.250000\n",
      "Train Epoch: 23 [52096/54000 (96%)] Loss: -1064864.750000\n",
      "Train Epoch: 23 [53504/54000 (99%)] Loss: -1042556.687500\n",
      "    epoch          : 23\n",
      "    loss           : -1017397.2819905214\n",
      "    val_loss       : -1022680.424734562\n",
      "Train Epoch: 24 [0/54000 (0%)] Loss: -1030990.312500\n",
      "Train Epoch: 24 [1408/54000 (3%)] Loss: -994048.875000\n",
      "Train Epoch: 24 [2816/54000 (5%)] Loss: -964659.375000\n",
      "Train Epoch: 24 [4224/54000 (8%)] Loss: -1016140.000000\n",
      "Train Epoch: 24 [5632/54000 (10%)] Loss: -1030272.375000\n",
      "Train Epoch: 24 [7040/54000 (13%)] Loss: -970672.125000\n",
      "Train Epoch: 24 [8448/54000 (16%)] Loss: -1052436.750000\n",
      "Train Epoch: 24 [9856/54000 (18%)] Loss: -994217.812500\n",
      "Train Epoch: 24 [11264/54000 (21%)] Loss: -967049.687500\n",
      "Train Epoch: 24 [12672/54000 (23%)] Loss: -1040557.375000\n",
      "Train Epoch: 24 [14080/54000 (26%)] Loss: -1058052.875000\n",
      "Train Epoch: 24 [15488/54000 (29%)] Loss: -1017673.187500\n",
      "Train Epoch: 24 [16896/54000 (31%)] Loss: -1003717.875000\n",
      "Train Epoch: 24 [18304/54000 (34%)] Loss: -1106314.375000\n",
      "Train Epoch: 24 [19712/54000 (37%)] Loss: -995633.937500\n",
      "Train Epoch: 24 [21120/54000 (39%)] Loss: -1016826.187500\n",
      "Train Epoch: 24 [22528/54000 (42%)] Loss: -1017726.062500\n",
      "Train Epoch: 24 [23936/54000 (44%)] Loss: -1016975.062500\n",
      "Train Epoch: 24 [25344/54000 (47%)] Loss: -989548.437500\n",
      "Train Epoch: 24 [26752/54000 (50%)] Loss: -987550.312500\n",
      "Train Epoch: 24 [28160/54000 (52%)] Loss: -1037069.812500\n",
      "Train Epoch: 24 [29568/54000 (55%)] Loss: -1053049.375000\n",
      "Train Epoch: 24 [30976/54000 (57%)] Loss: -1001451.250000\n",
      "Train Epoch: 24 [32384/54000 (60%)] Loss: -977771.187500\n",
      "Train Epoch: 24 [33792/54000 (63%)] Loss: -1030837.500000\n",
      "Train Epoch: 24 [35200/54000 (65%)] Loss: -1081440.875000\n",
      "Train Epoch: 24 [36608/54000 (68%)] Loss: -980577.812500\n",
      "Train Epoch: 24 [38016/54000 (70%)] Loss: -1019875.500000\n",
      "Train Epoch: 24 [39424/54000 (73%)] Loss: -1040980.375000\n",
      "Train Epoch: 24 [40832/54000 (76%)] Loss: -1013373.062500\n",
      "Train Epoch: 24 [42240/54000 (78%)] Loss: -1035747.187500\n",
      "Train Epoch: 24 [43648/54000 (81%)] Loss: -1088780.875000\n",
      "Train Epoch: 24 [45056/54000 (83%)] Loss: -1015107.625000\n",
      "Train Epoch: 24 [46464/54000 (86%)] Loss: -1072044.125000\n",
      "Train Epoch: 24 [47872/54000 (89%)] Loss: -977611.000000\n",
      "Train Epoch: 24 [49280/54000 (91%)] Loss: -1042629.937500\n",
      "Train Epoch: 24 [50688/54000 (94%)] Loss: -1002742.437500\n",
      "Train Epoch: 24 [52096/54000 (96%)] Loss: -1013833.437500\n",
      "Train Epoch: 24 [53504/54000 (99%)] Loss: -1041485.250000\n",
      "    epoch          : 24\n",
      "    loss           : -1019569.2782879147\n",
      "    val_loss       : -1025115.9112003407\n",
      "Train Epoch: 25 [0/54000 (0%)] Loss: -1019510.875000\n",
      "Train Epoch: 25 [1408/54000 (3%)] Loss: -1047380.000000\n",
      "Train Epoch: 25 [2816/54000 (5%)] Loss: -1022595.125000\n",
      "Train Epoch: 25 [4224/54000 (8%)] Loss: -1042813.125000\n",
      "Train Epoch: 25 [5632/54000 (10%)] Loss: -997695.687500\n",
      "Train Epoch: 25 [7040/54000 (13%)] Loss: -983623.312500\n",
      "Train Epoch: 25 [8448/54000 (16%)] Loss: -1079934.375000\n",
      "Train Epoch: 25 [9856/54000 (18%)] Loss: -997066.062500\n",
      "Train Epoch: 25 [11264/54000 (21%)] Loss: -993114.687500\n",
      "Train Epoch: 25 [12672/54000 (23%)] Loss: -1037515.437500\n",
      "Train Epoch: 25 [14080/54000 (26%)] Loss: -1045887.562500\n",
      "Train Epoch: 25 [15488/54000 (29%)] Loss: -1015675.000000\n",
      "Train Epoch: 25 [16896/54000 (31%)] Loss: -1031803.250000\n",
      "Train Epoch: 25 [18304/54000 (34%)] Loss: -1002661.312500\n",
      "Train Epoch: 25 [19712/54000 (37%)] Loss: -1061881.625000\n",
      "Train Epoch: 25 [21120/54000 (39%)] Loss: -1042333.750000\n",
      "Train Epoch: 25 [22528/54000 (42%)] Loss: -1044293.812500\n",
      "Train Epoch: 25 [23936/54000 (44%)] Loss: -1014143.312500\n",
      "Train Epoch: 25 [25344/54000 (47%)] Loss: -1030019.500000\n",
      "Train Epoch: 25 [26752/54000 (50%)] Loss: -984955.125000\n",
      "Train Epoch: 25 [28160/54000 (52%)] Loss: -1032854.312500\n",
      "Train Epoch: 25 [29568/54000 (55%)] Loss: -1006197.500000\n",
      "Train Epoch: 25 [30976/54000 (57%)] Loss: -1026786.125000\n",
      "Train Epoch: 25 [32384/54000 (60%)] Loss: -1027659.437500\n",
      "Train Epoch: 25 [33792/54000 (63%)] Loss: -1011852.812500\n",
      "Train Epoch: 25 [35200/54000 (65%)] Loss: -988987.562500\n",
      "Train Epoch: 25 [36608/54000 (68%)] Loss: -1055179.250000\n",
      "Train Epoch: 25 [38016/54000 (70%)] Loss: -1023518.500000\n",
      "Train Epoch: 25 [39424/54000 (73%)] Loss: -1011820.187500\n",
      "Train Epoch: 25 [40832/54000 (76%)] Loss: -1035290.062500\n",
      "Train Epoch: 25 [42240/54000 (78%)] Loss: -1016823.750000\n",
      "Train Epoch: 25 [43648/54000 (81%)] Loss: -1045266.562500\n",
      "Train Epoch: 25 [45056/54000 (83%)] Loss: -1016180.812500\n",
      "Train Epoch: 25 [46464/54000 (86%)] Loss: -1018764.312500\n",
      "Train Epoch: 25 [47872/54000 (89%)] Loss: -1071150.250000\n",
      "Train Epoch: 25 [49280/54000 (91%)] Loss: -1026115.000000\n",
      "Train Epoch: 25 [50688/54000 (94%)] Loss: -1040453.125000\n",
      "Train Epoch: 25 [52096/54000 (96%)] Loss: -1002908.562500\n",
      "Train Epoch: 25 [53504/54000 (99%)] Loss: -1022391.000000\n",
      "    epoch          : 25\n",
      "    loss           : -1022063.7495556872\n",
      "    val_loss       : -1027018.0458257147\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0427_111527/checkpoint-epoch25.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 26 [0/54000 (0%)] Loss: -1013449.250000\n",
      "Train Epoch: 26 [1408/54000 (3%)] Loss: -1067016.875000\n",
      "Train Epoch: 26 [2816/54000 (5%)] Loss: -944984.562500\n",
      "Train Epoch: 26 [4224/54000 (8%)] Loss: -1027038.937500\n",
      "Train Epoch: 26 [5632/54000 (10%)] Loss: -975155.937500\n",
      "Train Epoch: 26 [7040/54000 (13%)] Loss: -1091355.375000\n",
      "Train Epoch: 26 [8448/54000 (16%)] Loss: -1016535.000000\n",
      "Train Epoch: 26 [9856/54000 (18%)] Loss: -1028776.812500\n",
      "Train Epoch: 26 [11264/54000 (21%)] Loss: -994230.437500\n",
      "Train Epoch: 26 [12672/54000 (23%)] Loss: -986004.937500\n",
      "Train Epoch: 26 [14080/54000 (26%)] Loss: -1007485.750000\n",
      "Train Epoch: 26 [15488/54000 (29%)] Loss: -1025521.750000\n",
      "Train Epoch: 26 [16896/54000 (31%)] Loss: -1030150.625000\n",
      "Train Epoch: 26 [18304/54000 (34%)] Loss: -1000734.750000\n",
      "Train Epoch: 26 [19712/54000 (37%)] Loss: -955843.625000\n",
      "Train Epoch: 26 [21120/54000 (39%)] Loss: -1061661.625000\n",
      "Train Epoch: 26 [22528/54000 (42%)] Loss: -1028148.812500\n",
      "Train Epoch: 26 [23936/54000 (44%)] Loss: -1022690.937500\n",
      "Train Epoch: 26 [25344/54000 (47%)] Loss: -1032536.437500\n",
      "Train Epoch: 26 [26752/54000 (50%)] Loss: -1017075.812500\n",
      "Train Epoch: 26 [28160/54000 (52%)] Loss: -1016122.812500\n",
      "Train Epoch: 26 [29568/54000 (55%)] Loss: -1047365.562500\n",
      "Train Epoch: 26 [30976/54000 (57%)] Loss: -1032815.437500\n",
      "Train Epoch: 26 [32384/54000 (60%)] Loss: -1006221.062500\n",
      "Train Epoch: 26 [33792/54000 (63%)] Loss: -1067288.625000\n",
      "Train Epoch: 26 [35200/54000 (65%)] Loss: -1002503.687500\n",
      "Train Epoch: 26 [36608/54000 (68%)] Loss: -1020001.000000\n",
      "Train Epoch: 26 [38016/54000 (70%)] Loss: -943787.312500\n",
      "Train Epoch: 26 [39424/54000 (73%)] Loss: -1027048.375000\n",
      "Train Epoch: 26 [40832/54000 (76%)] Loss: -1012593.250000\n",
      "Train Epoch: 26 [42240/54000 (78%)] Loss: -988369.562500\n",
      "Train Epoch: 26 [43648/54000 (81%)] Loss: -1074213.125000\n",
      "Train Epoch: 26 [45056/54000 (83%)] Loss: -1045682.312500\n",
      "Train Epoch: 26 [46464/54000 (86%)] Loss: -1026895.812500\n",
      "Train Epoch: 26 [47872/54000 (89%)] Loss: -969720.812500\n",
      "Train Epoch: 26 [49280/54000 (91%)] Loss: -1050057.375000\n",
      "Train Epoch: 26 [50688/54000 (94%)] Loss: -1005806.250000\n",
      "Train Epoch: 26 [52096/54000 (96%)] Loss: -1049395.125000\n",
      "Train Epoch: 26 [53504/54000 (99%)] Loss: -1037193.875000\n",
      "    epoch          : 26\n",
      "    loss           : -1024145.1167061612\n",
      "    val_loss       : -1029951.5155289021\n",
      "Train Epoch: 27 [0/54000 (0%)] Loss: -1004928.375000\n",
      "Train Epoch: 27 [1408/54000 (3%)] Loss: -1000534.125000\n",
      "Train Epoch: 27 [2816/54000 (5%)] Loss: -1057131.500000\n",
      "Train Epoch: 27 [4224/54000 (8%)] Loss: -1055848.375000\n",
      "Train Epoch: 27 [5632/54000 (10%)] Loss: -1057396.875000\n",
      "Train Epoch: 27 [7040/54000 (13%)] Loss: -1014406.312500\n",
      "Train Epoch: 27 [8448/54000 (16%)] Loss: -1038537.750000\n",
      "Train Epoch: 27 [9856/54000 (18%)] Loss: -1001296.000000\n",
      "Train Epoch: 27 [11264/54000 (21%)] Loss: -1013714.937500\n",
      "Train Epoch: 27 [12672/54000 (23%)] Loss: -1098363.625000\n",
      "Train Epoch: 27 [14080/54000 (26%)] Loss: -1019558.812500\n",
      "Train Epoch: 27 [15488/54000 (29%)] Loss: -1011841.312500\n",
      "Train Epoch: 27 [16896/54000 (31%)] Loss: -1053778.750000\n",
      "Train Epoch: 27 [18304/54000 (34%)] Loss: -1031833.062500\n",
      "Train Epoch: 27 [19712/54000 (37%)] Loss: -1016475.812500\n",
      "Train Epoch: 27 [21120/54000 (39%)] Loss: -1005842.312500\n",
      "Train Epoch: 27 [22528/54000 (42%)] Loss: -1021436.062500\n",
      "Train Epoch: 27 [23936/54000 (44%)] Loss: -1055512.375000\n",
      "Train Epoch: 27 [25344/54000 (47%)] Loss: -952579.875000\n",
      "Train Epoch: 27 [26752/54000 (50%)] Loss: -1004809.375000\n",
      "Train Epoch: 27 [28160/54000 (52%)] Loss: -988621.125000\n",
      "Train Epoch: 27 [29568/54000 (55%)] Loss: -1029652.937500\n",
      "Train Epoch: 27 [30976/54000 (57%)] Loss: -1090570.000000\n",
      "Train Epoch: 27 [32384/54000 (60%)] Loss: -1050953.625000\n",
      "Train Epoch: 27 [33792/54000 (63%)] Loss: -1058221.750000\n",
      "Train Epoch: 27 [35200/54000 (65%)] Loss: -1031896.687500\n",
      "Train Epoch: 27 [36608/54000 (68%)] Loss: -990197.250000\n",
      "Train Epoch: 27 [38016/54000 (70%)] Loss: -1033212.187500\n",
      "Train Epoch: 27 [39424/54000 (73%)] Loss: -1063171.250000\n",
      "Train Epoch: 27 [40832/54000 (76%)] Loss: -1004685.812500\n",
      "Train Epoch: 27 [42240/54000 (78%)] Loss: -1029833.625000\n",
      "Train Epoch: 27 [43648/54000 (81%)] Loss: -997869.687500\n",
      "Train Epoch: 27 [45056/54000 (83%)] Loss: -1059127.750000\n",
      "Train Epoch: 27 [46464/54000 (86%)] Loss: -1078390.875000\n",
      "Train Epoch: 27 [47872/54000 (89%)] Loss: -1036833.437500\n",
      "Train Epoch: 27 [49280/54000 (91%)] Loss: -949057.562500\n",
      "Train Epoch: 27 [50688/54000 (94%)] Loss: -1053311.125000\n",
      "Train Epoch: 27 [52096/54000 (96%)] Loss: -1044523.937500\n",
      "Train Epoch: 27 [53504/54000 (99%)] Loss: -1051035.250000\n",
      "    epoch          : 27\n",
      "    loss           : -1026419.4154324644\n",
      "    val_loss       : -1031960.4037410655\n",
      "Train Epoch: 28 [0/54000 (0%)] Loss: -1066585.625000\n",
      "Train Epoch: 28 [1408/54000 (3%)] Loss: -963450.500000\n",
      "Train Epoch: 28 [2816/54000 (5%)] Loss: -1013843.375000\n",
      "Train Epoch: 28 [4224/54000 (8%)] Loss: -1005322.562500\n",
      "Train Epoch: 28 [5632/54000 (10%)] Loss: -1010524.062500\n",
      "Train Epoch: 28 [7040/54000 (13%)] Loss: -1040324.562500\n",
      "Train Epoch: 28 [8448/54000 (16%)] Loss: -1049868.375000\n",
      "Train Epoch: 28 [9856/54000 (18%)] Loss: -1078299.875000\n",
      "Train Epoch: 28 [11264/54000 (21%)] Loss: -1016100.812500\n",
      "Train Epoch: 28 [12672/54000 (23%)] Loss: -1005917.000000\n",
      "Train Epoch: 28 [14080/54000 (26%)] Loss: -1077001.875000\n",
      "Train Epoch: 28 [15488/54000 (29%)] Loss: -997499.000000\n",
      "Train Epoch: 28 [16896/54000 (31%)] Loss: -1014212.625000\n",
      "Train Epoch: 28 [18304/54000 (34%)] Loss: -1018361.875000\n",
      "Train Epoch: 28 [19712/54000 (37%)] Loss: -1034281.812500\n",
      "Train Epoch: 28 [21120/54000 (39%)] Loss: -1035181.937500\n",
      "Train Epoch: 28 [22528/54000 (42%)] Loss: -1006349.875000\n",
      "Train Epoch: 28 [23936/54000 (44%)] Loss: -1028971.187500\n",
      "Train Epoch: 28 [25344/54000 (47%)] Loss: -1010641.125000\n",
      "Train Epoch: 28 [26752/54000 (50%)] Loss: -1037604.687500\n",
      "Train Epoch: 28 [28160/54000 (52%)] Loss: -971545.875000\n",
      "Train Epoch: 28 [29568/54000 (55%)] Loss: -1021984.375000\n",
      "Train Epoch: 28 [30976/54000 (57%)] Loss: -1016957.687500\n",
      "Train Epoch: 28 [32384/54000 (60%)] Loss: -1033012.187500\n",
      "Train Epoch: 28 [33792/54000 (63%)] Loss: -1047708.312500\n",
      "Train Epoch: 28 [35200/54000 (65%)] Loss: -1013527.812500\n",
      "Train Epoch: 28 [36608/54000 (68%)] Loss: -1073737.000000\n",
      "Train Epoch: 28 [38016/54000 (70%)] Loss: -992440.875000\n",
      "Train Epoch: 28 [39424/54000 (73%)] Loss: -1030570.000000\n",
      "Train Epoch: 28 [40832/54000 (76%)] Loss: -1034330.000000\n",
      "Train Epoch: 28 [42240/54000 (78%)] Loss: -1069691.250000\n",
      "Train Epoch: 28 [43648/54000 (81%)] Loss: -994584.687500\n",
      "Train Epoch: 28 [45056/54000 (83%)] Loss: -1001731.562500\n",
      "Train Epoch: 28 [46464/54000 (86%)] Loss: -1041031.875000\n",
      "Train Epoch: 28 [47872/54000 (89%)] Loss: -1025979.000000\n",
      "Train Epoch: 28 [49280/54000 (91%)] Loss: -1043203.375000\n",
      "Train Epoch: 28 [50688/54000 (94%)] Loss: -1015851.375000\n",
      "Train Epoch: 28 [52096/54000 (96%)] Loss: -1055893.625000\n",
      "Train Epoch: 28 [53504/54000 (99%)] Loss: -1045977.187500\n",
      "    epoch          : 28\n",
      "    loss           : -1028459.2837677725\n",
      "    val_loss       : -1033198.7071870845\n",
      "Train Epoch: 29 [0/54000 (0%)] Loss: -1051245.500000\n",
      "Train Epoch: 29 [1408/54000 (3%)] Loss: -981534.062500\n",
      "Train Epoch: 29 [2816/54000 (5%)] Loss: -1036494.000000\n",
      "Train Epoch: 29 [4224/54000 (8%)] Loss: -1042599.000000\n",
      "Train Epoch: 29 [5632/54000 (10%)] Loss: -1043419.625000\n",
      "Train Epoch: 29 [7040/54000 (13%)] Loss: -1001538.250000\n",
      "Train Epoch: 29 [8448/54000 (16%)] Loss: -1004408.375000\n",
      "Train Epoch: 29 [9856/54000 (18%)] Loss: -1071414.375000\n",
      "Train Epoch: 29 [11264/54000 (21%)] Loss: -1103810.125000\n",
      "Train Epoch: 29 [12672/54000 (23%)] Loss: -955414.375000\n",
      "Train Epoch: 29 [14080/54000 (26%)] Loss: -1020210.375000\n",
      "Train Epoch: 29 [15488/54000 (29%)] Loss: -1043487.625000\n",
      "Train Epoch: 29 [16896/54000 (31%)] Loss: -1043319.625000\n",
      "Train Epoch: 29 [18304/54000 (34%)] Loss: -1018886.062500\n",
      "Train Epoch: 29 [19712/54000 (37%)] Loss: -1031945.375000\n",
      "Train Epoch: 29 [21120/54000 (39%)] Loss: -1001812.062500\n",
      "Train Epoch: 29 [22528/54000 (42%)] Loss: -1045264.187500\n",
      "Train Epoch: 29 [23936/54000 (44%)] Loss: -1032503.562500\n",
      "Train Epoch: 29 [25344/54000 (47%)] Loss: -1046738.750000\n",
      "Train Epoch: 29 [26752/54000 (50%)] Loss: -1090875.875000\n",
      "Train Epoch: 29 [28160/54000 (52%)] Loss: -1090177.125000\n",
      "Train Epoch: 29 [29568/54000 (55%)] Loss: -1023933.312500\n",
      "Train Epoch: 29 [30976/54000 (57%)] Loss: -1014684.125000\n",
      "Train Epoch: 29 [32384/54000 (60%)] Loss: -1010249.062500\n",
      "Train Epoch: 29 [33792/54000 (63%)] Loss: -1074168.500000\n",
      "Train Epoch: 29 [35200/54000 (65%)] Loss: -1012214.125000\n",
      "Train Epoch: 29 [36608/54000 (68%)] Loss: -1098507.375000\n",
      "Train Epoch: 29 [38016/54000 (70%)] Loss: -1033531.375000\n",
      "Train Epoch: 29 [39424/54000 (73%)] Loss: -990281.062500\n",
      "Train Epoch: 29 [40832/54000 (76%)] Loss: -1067190.250000\n",
      "Train Epoch: 29 [42240/54000 (78%)] Loss: -1000912.000000\n",
      "Train Epoch: 29 [43648/54000 (81%)] Loss: -996133.000000\n",
      "Train Epoch: 29 [45056/54000 (83%)] Loss: -1015180.062500\n",
      "Train Epoch: 29 [46464/54000 (86%)] Loss: -1005998.062500\n",
      "Train Epoch: 29 [47872/54000 (89%)] Loss: -1000443.062500\n",
      "Train Epoch: 29 [49280/54000 (91%)] Loss: -1037737.187500\n",
      "Train Epoch: 29 [50688/54000 (94%)] Loss: -979119.312500\n",
      "Train Epoch: 29 [52096/54000 (96%)] Loss: -1026718.750000\n",
      "Train Epoch: 29 [53504/54000 (99%)] Loss: -1009912.437500\n",
      "    epoch          : 29\n",
      "    loss           : -1030285.3624111374\n",
      "    val_loss       : -1035767.3195541058\n",
      "Train Epoch: 30 [0/54000 (0%)] Loss: -1019019.937500\n",
      "Train Epoch: 30 [1408/54000 (3%)] Loss: -1060342.250000\n",
      "Train Epoch: 30 [2816/54000 (5%)] Loss: -1051237.875000\n",
      "Train Epoch: 30 [4224/54000 (8%)] Loss: -982791.312500\n",
      "Train Epoch: 30 [5632/54000 (10%)] Loss: -1057137.875000\n",
      "Train Epoch: 30 [7040/54000 (13%)] Loss: -1004321.812500\n",
      "Train Epoch: 30 [8448/54000 (16%)] Loss: -1019698.500000\n",
      "Train Epoch: 30 [9856/54000 (18%)] Loss: -1047284.437500\n",
      "Train Epoch: 30 [11264/54000 (21%)] Loss: -1023487.500000\n",
      "Train Epoch: 30 [12672/54000 (23%)] Loss: -1084422.125000\n",
      "Train Epoch: 30 [14080/54000 (26%)] Loss: -985999.312500\n",
      "Train Epoch: 30 [15488/54000 (29%)] Loss: -1011846.500000\n",
      "Train Epoch: 30 [16896/54000 (31%)] Loss: -992520.187500\n",
      "Train Epoch: 30 [18304/54000 (34%)] Loss: -1025069.500000\n",
      "Train Epoch: 30 [19712/54000 (37%)] Loss: -1004860.750000\n",
      "Train Epoch: 30 [21120/54000 (39%)] Loss: -990471.062500\n",
      "Train Epoch: 30 [22528/54000 (42%)] Loss: -1039498.437500\n",
      "Train Epoch: 30 [23936/54000 (44%)] Loss: -1078339.750000\n",
      "Train Epoch: 30 [25344/54000 (47%)] Loss: -1033943.000000\n",
      "Train Epoch: 30 [26752/54000 (50%)] Loss: -1051022.500000\n",
      "Train Epoch: 30 [28160/54000 (52%)] Loss: -991605.000000\n",
      "Train Epoch: 30 [29568/54000 (55%)] Loss: -1092396.750000\n",
      "Train Epoch: 30 [30976/54000 (57%)] Loss: -1085131.750000\n",
      "Train Epoch: 30 [32384/54000 (60%)] Loss: -1084805.250000\n",
      "Train Epoch: 30 [33792/54000 (63%)] Loss: -994986.437500\n",
      "Train Epoch: 30 [35200/54000 (65%)] Loss: -1011188.875000\n",
      "Train Epoch: 30 [36608/54000 (68%)] Loss: -1041524.937500\n",
      "Train Epoch: 30 [38016/54000 (70%)] Loss: -1047733.625000\n",
      "Train Epoch: 30 [39424/54000 (73%)] Loss: -1015418.562500\n",
      "Train Epoch: 30 [40832/54000 (76%)] Loss: -1045042.312500\n",
      "Train Epoch: 30 [42240/54000 (78%)] Loss: -1059474.625000\n",
      "Train Epoch: 30 [43648/54000 (81%)] Loss: -1006447.312500\n",
      "Train Epoch: 30 [45056/54000 (83%)] Loss: -1014615.437500\n",
      "Train Epoch: 30 [46464/54000 (86%)] Loss: -1006742.437500\n",
      "Train Epoch: 30 [47872/54000 (89%)] Loss: -987527.625000\n",
      "Train Epoch: 30 [49280/54000 (91%)] Loss: -1036460.875000\n",
      "Train Epoch: 30 [50688/54000 (94%)] Loss: -1085176.125000\n",
      "Train Epoch: 30 [52096/54000 (96%)] Loss: -1066111.000000\n",
      "Train Epoch: 30 [53504/54000 (99%)] Loss: -988977.687500\n",
      "    epoch          : 30\n",
      "    loss           : -1032207.4142476303\n",
      "    val_loss       : -1037782.1206548163\n",
      "Train Epoch: 31 [0/54000 (0%)] Loss: -994998.062500\n",
      "Train Epoch: 31 [1408/54000 (3%)] Loss: -1066302.000000\n",
      "Train Epoch: 31 [2816/54000 (5%)] Loss: -1022665.562500\n",
      "Train Epoch: 31 [4224/54000 (8%)] Loss: -1020669.812500\n",
      "Train Epoch: 31 [5632/54000 (10%)] Loss: -1041663.250000\n",
      "Train Epoch: 31 [7040/54000 (13%)] Loss: -1016994.937500\n",
      "Train Epoch: 31 [8448/54000 (16%)] Loss: -1052633.125000\n",
      "Train Epoch: 31 [9856/54000 (18%)] Loss: -1046699.250000\n",
      "Train Epoch: 31 [11264/54000 (21%)] Loss: -1053814.500000\n",
      "Train Epoch: 31 [12672/54000 (23%)] Loss: -977186.187500\n",
      "Train Epoch: 31 [14080/54000 (26%)] Loss: -1027176.375000\n",
      "Train Epoch: 31 [15488/54000 (29%)] Loss: -1014846.312500\n",
      "Train Epoch: 31 [16896/54000 (31%)] Loss: -1034496.125000\n",
      "Train Epoch: 31 [18304/54000 (34%)] Loss: -1016200.562500\n",
      "Train Epoch: 31 [19712/54000 (37%)] Loss: -1071336.750000\n",
      "Train Epoch: 31 [21120/54000 (39%)] Loss: -1044361.437500\n",
      "Train Epoch: 31 [22528/54000 (42%)] Loss: -1042197.562500\n",
      "Train Epoch: 31 [23936/54000 (44%)] Loss: -1058862.750000\n",
      "Train Epoch: 31 [25344/54000 (47%)] Loss: -1016150.437500\n",
      "Train Epoch: 31 [26752/54000 (50%)] Loss: -997620.375000\n",
      "Train Epoch: 31 [28160/54000 (52%)] Loss: -965174.250000\n",
      "Train Epoch: 31 [29568/54000 (55%)] Loss: -1054634.875000\n",
      "Train Epoch: 31 [30976/54000 (57%)] Loss: -1019568.187500\n",
      "Train Epoch: 31 [32384/54000 (60%)] Loss: -1007174.500000\n",
      "Train Epoch: 31 [33792/54000 (63%)] Loss: -1008408.500000\n",
      "Train Epoch: 31 [35200/54000 (65%)] Loss: -988522.937500\n",
      "Train Epoch: 31 [36608/54000 (68%)] Loss: -1031098.562500\n",
      "Train Epoch: 31 [38016/54000 (70%)] Loss: -1011288.000000\n",
      "Train Epoch: 31 [39424/54000 (73%)] Loss: -1018904.500000\n",
      "Train Epoch: 31 [40832/54000 (76%)] Loss: -1041929.062500\n",
      "Train Epoch: 31 [42240/54000 (78%)] Loss: -1077444.750000\n",
      "Train Epoch: 31 [43648/54000 (81%)] Loss: -994227.125000\n",
      "Train Epoch: 31 [45056/54000 (83%)] Loss: -1029008.125000\n",
      "Train Epoch: 31 [46464/54000 (86%)] Loss: -1087915.125000\n",
      "Train Epoch: 31 [47872/54000 (89%)] Loss: -1027249.250000\n",
      "Train Epoch: 31 [49280/54000 (91%)] Loss: -1077632.000000\n",
      "Train Epoch: 31 [50688/54000 (94%)] Loss: -985532.750000\n",
      "Train Epoch: 31 [52096/54000 (96%)] Loss: -1031200.187500\n",
      "Train Epoch: 31 [53504/54000 (99%)] Loss: -1063914.125000\n",
      "    epoch          : 31\n",
      "    loss           : -1034027.1100414692\n",
      "    val_loss       : -1038620.7165787067\n",
      "Train Epoch: 32 [0/54000 (0%)] Loss: -1032099.625000\n",
      "Train Epoch: 32 [1408/54000 (3%)] Loss: -1074690.500000\n",
      "Train Epoch: 32 [2816/54000 (5%)] Loss: -1036422.250000\n",
      "Train Epoch: 32 [4224/54000 (8%)] Loss: -1015347.687500\n",
      "Train Epoch: 32 [5632/54000 (10%)] Loss: -1067321.750000\n",
      "Train Epoch: 32 [7040/54000 (13%)] Loss: -1095531.000000\n",
      "Train Epoch: 32 [8448/54000 (16%)] Loss: -995005.875000\n",
      "Train Epoch: 32 [9856/54000 (18%)] Loss: -1016086.250000\n",
      "Train Epoch: 32 [11264/54000 (21%)] Loss: -958559.562500\n",
      "Train Epoch: 32 [12672/54000 (23%)] Loss: -1009776.875000\n",
      "Train Epoch: 32 [14080/54000 (26%)] Loss: -1090164.750000\n",
      "Train Epoch: 32 [15488/54000 (29%)] Loss: -996310.875000\n",
      "Train Epoch: 32 [16896/54000 (31%)] Loss: -1044678.125000\n",
      "Train Epoch: 32 [18304/54000 (34%)] Loss: -1007282.375000\n",
      "Train Epoch: 32 [19712/54000 (37%)] Loss: -1029512.000000\n",
      "Train Epoch: 32 [21120/54000 (39%)] Loss: -1024511.000000\n",
      "Train Epoch: 32 [22528/54000 (42%)] Loss: -1038382.437500\n",
      "Train Epoch: 32 [23936/54000 (44%)] Loss: -1097154.625000\n",
      "Train Epoch: 32 [25344/54000 (47%)] Loss: -1036531.625000\n",
      "Train Epoch: 32 [26752/54000 (50%)] Loss: -1033398.375000\n",
      "Train Epoch: 32 [28160/54000 (52%)] Loss: -1031022.625000\n",
      "Train Epoch: 32 [29568/54000 (55%)] Loss: -1024630.125000\n",
      "Train Epoch: 32 [30976/54000 (57%)] Loss: -981352.875000\n",
      "Train Epoch: 32 [32384/54000 (60%)] Loss: -1025058.687500\n",
      "Train Epoch: 32 [33792/54000 (63%)] Loss: -1041047.375000\n",
      "Train Epoch: 32 [35200/54000 (65%)] Loss: -1023265.812500\n",
      "Train Epoch: 32 [36608/54000 (68%)] Loss: -1051589.625000\n",
      "Train Epoch: 32 [38016/54000 (70%)] Loss: -1071579.375000\n",
      "Train Epoch: 32 [39424/54000 (73%)] Loss: -1057041.875000\n",
      "Train Epoch: 32 [40832/54000 (76%)] Loss: -1054966.625000\n",
      "Train Epoch: 32 [42240/54000 (78%)] Loss: -1025727.437500\n",
      "Train Epoch: 32 [43648/54000 (81%)] Loss: -1032216.875000\n",
      "Train Epoch: 32 [45056/54000 (83%)] Loss: -1010365.187500\n",
      "Train Epoch: 32 [46464/54000 (86%)] Loss: -1051888.375000\n",
      "Train Epoch: 32 [47872/54000 (89%)] Loss: -1061157.750000\n",
      "Train Epoch: 32 [49280/54000 (91%)] Loss: -992720.500000\n",
      "Train Epoch: 32 [50688/54000 (94%)] Loss: -1088545.000000\n",
      "Train Epoch: 32 [52096/54000 (96%)] Loss: -1021266.750000\n",
      "Train Epoch: 32 [53504/54000 (99%)] Loss: -1094111.875000\n",
      "    epoch          : 32\n",
      "    loss           : -1035612.4431279621\n",
      "    val_loss       : -1040418.3425163106\n",
      "Train Epoch: 33 [0/54000 (0%)] Loss: -1083152.375000\n",
      "Train Epoch: 33 [1408/54000 (3%)] Loss: -1018951.625000\n",
      "Train Epoch: 33 [2816/54000 (5%)] Loss: -1065982.500000\n",
      "Train Epoch: 33 [4224/54000 (8%)] Loss: -1050245.250000\n",
      "Train Epoch: 33 [5632/54000 (10%)] Loss: -1004236.625000\n",
      "Train Epoch: 33 [7040/54000 (13%)] Loss: -1022678.937500\n",
      "Train Epoch: 33 [8448/54000 (16%)] Loss: -991606.312500\n",
      "Train Epoch: 33 [9856/54000 (18%)] Loss: -1049964.125000\n",
      "Train Epoch: 33 [11264/54000 (21%)] Loss: -1091576.875000\n",
      "Train Epoch: 33 [12672/54000 (23%)] Loss: -1085466.125000\n",
      "Train Epoch: 33 [14080/54000 (26%)] Loss: -1058069.000000\n",
      "Train Epoch: 33 [15488/54000 (29%)] Loss: -1029330.062500\n",
      "Train Epoch: 33 [16896/54000 (31%)] Loss: -997539.750000\n",
      "Train Epoch: 33 [18304/54000 (34%)] Loss: -1007195.812500\n",
      "Train Epoch: 33 [19712/54000 (37%)] Loss: -1056687.875000\n",
      "Train Epoch: 33 [21120/54000 (39%)] Loss: -1014156.312500\n",
      "Train Epoch: 33 [22528/54000 (42%)] Loss: -957925.812500\n",
      "Train Epoch: 33 [23936/54000 (44%)] Loss: -1073163.750000\n",
      "Train Epoch: 33 [25344/54000 (47%)] Loss: -1028950.812500\n",
      "Train Epoch: 33 [26752/54000 (50%)] Loss: -1030495.812500\n",
      "Train Epoch: 33 [28160/54000 (52%)] Loss: -1078379.875000\n",
      "Train Epoch: 33 [29568/54000 (55%)] Loss: -1032267.500000\n",
      "Train Epoch: 33 [30976/54000 (57%)] Loss: -1078630.625000\n",
      "Train Epoch: 33 [32384/54000 (60%)] Loss: -1031225.750000\n",
      "Train Epoch: 33 [33792/54000 (63%)] Loss: -1044850.875000\n",
      "Train Epoch: 33 [35200/54000 (65%)] Loss: -1024828.500000\n",
      "Train Epoch: 33 [36608/54000 (68%)] Loss: -1053537.750000\n",
      "Train Epoch: 33 [38016/54000 (70%)] Loss: -1027167.375000\n",
      "Train Epoch: 33 [39424/54000 (73%)] Loss: -1052934.875000\n",
      "Train Epoch: 33 [40832/54000 (76%)] Loss: -985686.562500\n",
      "Train Epoch: 33 [42240/54000 (78%)] Loss: -987241.687500\n",
      "Train Epoch: 33 [43648/54000 (81%)] Loss: -1044415.687500\n",
      "Train Epoch: 33 [45056/54000 (83%)] Loss: -978545.562500\n",
      "Train Epoch: 33 [46464/54000 (86%)] Loss: -1052981.875000\n",
      "Train Epoch: 33 [47872/54000 (89%)] Loss: -1015972.375000\n",
      "Train Epoch: 33 [49280/54000 (91%)] Loss: -1021916.812500\n",
      "Train Epoch: 33 [50688/54000 (94%)] Loss: -1079780.000000\n",
      "Train Epoch: 33 [52096/54000 (96%)] Loss: -1068571.250000\n",
      "Train Epoch: 33 [53504/54000 (99%)] Loss: -1056358.375000\n",
      "    epoch          : 33\n",
      "    loss           : -1037299.4087677725\n",
      "    val_loss       : -1041655.4932601604\n",
      "Train Epoch: 34 [0/54000 (0%)] Loss: -1062852.875000\n",
      "Train Epoch: 34 [1408/54000 (3%)] Loss: -1067775.750000\n",
      "Train Epoch: 34 [2816/54000 (5%)] Loss: -1061251.375000\n",
      "Train Epoch: 34 [4224/54000 (8%)] Loss: -939564.875000\n",
      "Train Epoch: 34 [5632/54000 (10%)] Loss: -1021187.875000\n",
      "Train Epoch: 34 [7040/54000 (13%)] Loss: -999973.375000\n",
      "Train Epoch: 34 [8448/54000 (16%)] Loss: -1040667.187500\n",
      "Train Epoch: 34 [9856/54000 (18%)] Loss: -1070405.375000\n",
      "Train Epoch: 34 [11264/54000 (21%)] Loss: -978620.125000\n",
      "Train Epoch: 34 [12672/54000 (23%)] Loss: -983500.937500\n",
      "Train Epoch: 34 [14080/54000 (26%)] Loss: -1086764.000000\n",
      "Train Epoch: 34 [15488/54000 (29%)] Loss: -1043571.562500\n",
      "Train Epoch: 34 [16896/54000 (31%)] Loss: -1043116.375000\n",
      "Train Epoch: 34 [18304/54000 (34%)] Loss: -1055807.625000\n",
      "Train Epoch: 34 [19712/54000 (37%)] Loss: -1053848.875000\n",
      "Train Epoch: 34 [21120/54000 (39%)] Loss: -1026348.937500\n",
      "Train Epoch: 34 [22528/54000 (42%)] Loss: -1011834.125000\n",
      "Train Epoch: 34 [23936/54000 (44%)] Loss: -1044012.187500\n",
      "Train Epoch: 34 [25344/54000 (47%)] Loss: -1082341.250000\n",
      "Train Epoch: 34 [26752/54000 (50%)] Loss: -1002872.875000\n",
      "Train Epoch: 34 [28160/54000 (52%)] Loss: -1000305.875000\n",
      "Train Epoch: 34 [29568/54000 (55%)] Loss: -980185.812500\n",
      "Train Epoch: 34 [30976/54000 (57%)] Loss: -1034252.312500\n",
      "Train Epoch: 34 [32384/54000 (60%)] Loss: -1031218.500000\n",
      "Train Epoch: 34 [33792/54000 (63%)] Loss: -1056475.375000\n",
      "Train Epoch: 34 [35200/54000 (65%)] Loss: -1039066.437500\n",
      "Train Epoch: 34 [36608/54000 (68%)] Loss: -1080295.625000\n",
      "Train Epoch: 34 [38016/54000 (70%)] Loss: -1048736.625000\n",
      "Train Epoch: 34 [39424/54000 (73%)] Loss: -1069324.500000\n",
      "Train Epoch: 34 [40832/54000 (76%)] Loss: -1064668.125000\n",
      "Train Epoch: 34 [42240/54000 (78%)] Loss: -1045568.375000\n",
      "Train Epoch: 34 [43648/54000 (81%)] Loss: -1043845.812500\n",
      "Train Epoch: 34 [45056/54000 (83%)] Loss: -1003721.562500\n",
      "Train Epoch: 34 [46464/54000 (86%)] Loss: -1068023.125000\n",
      "Train Epoch: 34 [47872/54000 (89%)] Loss: -1040317.437500\n",
      "Train Epoch: 34 [49280/54000 (91%)] Loss: -1057149.875000\n",
      "Train Epoch: 34 [50688/54000 (94%)] Loss: -1058399.250000\n",
      "Train Epoch: 34 [52096/54000 (96%)] Loss: -1105780.125000\n",
      "Train Epoch: 34 [53504/54000 (99%)] Loss: -1130386.875000\n",
      "    epoch          : 34\n",
      "    loss           : -1038764.8693720379\n",
      "    val_loss       : -1043699.1387939453\n",
      "Train Epoch: 35 [0/54000 (0%)] Loss: -1028107.000000\n",
      "Train Epoch: 35 [1408/54000 (3%)] Loss: -1013788.187500\n",
      "Train Epoch: 35 [2816/54000 (5%)] Loss: -1052425.125000\n",
      "Train Epoch: 35 [4224/54000 (8%)] Loss: -998585.125000\n",
      "Train Epoch: 35 [5632/54000 (10%)] Loss: -996782.625000\n",
      "Train Epoch: 35 [7040/54000 (13%)] Loss: -1079066.250000\n",
      "Train Epoch: 35 [8448/54000 (16%)] Loss: -1023562.375000\n",
      "Train Epoch: 35 [9856/54000 (18%)] Loss: -1005921.937500\n",
      "Train Epoch: 35 [11264/54000 (21%)] Loss: -1084006.125000\n",
      "Train Epoch: 35 [12672/54000 (23%)] Loss: -1013316.250000\n",
      "Train Epoch: 35 [14080/54000 (26%)] Loss: -1053010.500000\n",
      "Train Epoch: 35 [15488/54000 (29%)] Loss: -1068222.250000\n",
      "Train Epoch: 35 [16896/54000 (31%)] Loss: -1010945.562500\n",
      "Train Epoch: 35 [18304/54000 (34%)] Loss: -1053725.000000\n",
      "Train Epoch: 35 [19712/54000 (37%)] Loss: -989704.125000\n",
      "Train Epoch: 35 [21120/54000 (39%)] Loss: -1023388.000000\n",
      "Train Epoch: 35 [22528/54000 (42%)] Loss: -1083034.125000\n",
      "Train Epoch: 35 [23936/54000 (44%)] Loss: -1032377.312500\n",
      "Train Epoch: 35 [25344/54000 (47%)] Loss: -1009440.625000\n",
      "Train Epoch: 35 [26752/54000 (50%)] Loss: -1086610.500000\n",
      "Train Epoch: 35 [28160/54000 (52%)] Loss: -1044997.000000\n",
      "Train Epoch: 35 [29568/54000 (55%)] Loss: -1051278.750000\n",
      "Train Epoch: 35 [30976/54000 (57%)] Loss: -1017196.125000\n",
      "Train Epoch: 35 [32384/54000 (60%)] Loss: -1056453.375000\n",
      "Train Epoch: 35 [33792/54000 (63%)] Loss: -1060405.875000\n",
      "Train Epoch: 35 [35200/54000 (65%)] Loss: -1005285.687500\n",
      "Train Epoch: 35 [36608/54000 (68%)] Loss: -1058357.750000\n",
      "Train Epoch: 35 [38016/54000 (70%)] Loss: -1041188.187500\n",
      "Train Epoch: 35 [39424/54000 (73%)] Loss: -1067494.625000\n",
      "Train Epoch: 35 [40832/54000 (76%)] Loss: -970781.750000\n",
      "Train Epoch: 35 [42240/54000 (78%)] Loss: -1062233.375000\n",
      "Train Epoch: 35 [43648/54000 (81%)] Loss: -1157991.500000\n",
      "Train Epoch: 35 [45056/54000 (83%)] Loss: -1064065.000000\n",
      "Train Epoch: 35 [46464/54000 (86%)] Loss: -1035584.812500\n",
      "Train Epoch: 35 [47872/54000 (89%)] Loss: -1026486.750000\n",
      "Train Epoch: 35 [49280/54000 (91%)] Loss: -1065925.375000\n",
      "Train Epoch: 35 [50688/54000 (94%)] Loss: -1076110.625000\n",
      "Train Epoch: 35 [52096/54000 (96%)] Loss: -1089862.125000\n",
      "Train Epoch: 35 [53504/54000 (99%)] Loss: -1065283.625000\n",
      "    epoch          : 35\n",
      "    loss           : -1040394.9173578199\n",
      "    val_loss       : -1044807.2387903092\n",
      "Train Epoch: 36 [0/54000 (0%)] Loss: -1101998.625000\n",
      "Train Epoch: 36 [1408/54000 (3%)] Loss: -1023488.750000\n",
      "Train Epoch: 36 [2816/54000 (5%)] Loss: -978254.875000\n",
      "Train Epoch: 36 [4224/54000 (8%)] Loss: -1049199.125000\n",
      "Train Epoch: 36 [5632/54000 (10%)] Loss: -1015286.312500\n",
      "Train Epoch: 36 [7040/54000 (13%)] Loss: -994994.687500\n",
      "Train Epoch: 36 [8448/54000 (16%)] Loss: -1007986.750000\n",
      "Train Epoch: 36 [9856/54000 (18%)] Loss: -1083216.000000\n",
      "Train Epoch: 36 [11264/54000 (21%)] Loss: -1104673.000000\n",
      "Train Epoch: 36 [12672/54000 (23%)] Loss: -1013821.562500\n",
      "Train Epoch: 36 [14080/54000 (26%)] Loss: -988068.000000\n",
      "Train Epoch: 36 [15488/54000 (29%)] Loss: -1048320.375000\n",
      "Train Epoch: 36 [16896/54000 (31%)] Loss: -1047457.187500\n",
      "Train Epoch: 36 [18304/54000 (34%)] Loss: -1022397.062500\n",
      "Train Epoch: 36 [19712/54000 (37%)] Loss: -1097906.875000\n",
      "Train Epoch: 36 [21120/54000 (39%)] Loss: -1079570.125000\n",
      "Train Epoch: 36 [22528/54000 (42%)] Loss: -1017379.437500\n",
      "Train Epoch: 36 [23936/54000 (44%)] Loss: -1044118.812500\n",
      "Train Epoch: 36 [25344/54000 (47%)] Loss: -1071645.375000\n",
      "Train Epoch: 36 [26752/54000 (50%)] Loss: -1036629.062500\n",
      "Train Epoch: 36 [28160/54000 (52%)] Loss: -1034580.125000\n",
      "Train Epoch: 36 [29568/54000 (55%)] Loss: -1090168.875000\n",
      "Train Epoch: 36 [30976/54000 (57%)] Loss: -1127870.750000\n",
      "Train Epoch: 36 [32384/54000 (60%)] Loss: -985594.062500\n",
      "Train Epoch: 36 [33792/54000 (63%)] Loss: -1078017.875000\n",
      "Train Epoch: 36 [35200/54000 (65%)] Loss: -1033322.687500\n",
      "Train Epoch: 36 [36608/54000 (68%)] Loss: -1057167.875000\n",
      "Train Epoch: 36 [38016/54000 (70%)] Loss: -1097083.000000\n",
      "Train Epoch: 36 [39424/54000 (73%)] Loss: -976982.812500\n",
      "Train Epoch: 36 [40832/54000 (76%)] Loss: -1025049.062500\n",
      "Train Epoch: 36 [42240/54000 (78%)] Loss: -952404.375000\n",
      "Train Epoch: 36 [43648/54000 (81%)] Loss: -1020257.875000\n",
      "Train Epoch: 36 [45056/54000 (83%)] Loss: -1018185.250000\n",
      "Train Epoch: 36 [46464/54000 (86%)] Loss: -1064504.500000\n",
      "Train Epoch: 36 [47872/54000 (89%)] Loss: -1034695.687500\n",
      "Train Epoch: 36 [49280/54000 (91%)] Loss: -1075463.625000\n",
      "Train Epoch: 36 [50688/54000 (94%)] Loss: -1063288.125000\n",
      "Train Epoch: 36 [52096/54000 (96%)] Loss: -1040147.812500\n",
      "Train Epoch: 36 [53504/54000 (99%)] Loss: -1110028.750000\n",
      "    epoch          : 36\n",
      "    loss           : -1041893.5595379147\n",
      "    val_loss       : -1046744.456254675\n",
      "Train Epoch: 37 [0/54000 (0%)] Loss: -1009381.250000\n",
      "Train Epoch: 37 [1408/54000 (3%)] Loss: -1113769.625000\n",
      "Train Epoch: 37 [2816/54000 (5%)] Loss: -1048440.750000\n",
      "Train Epoch: 37 [4224/54000 (8%)] Loss: -1031857.312500\n",
      "Train Epoch: 37 [5632/54000 (10%)] Loss: -1092115.125000\n",
      "Train Epoch: 37 [7040/54000 (13%)] Loss: -1027009.812500\n",
      "Train Epoch: 37 [8448/54000 (16%)] Loss: -1047581.812500\n",
      "Train Epoch: 37 [9856/54000 (18%)] Loss: -1037806.000000\n",
      "Train Epoch: 37 [11264/54000 (21%)] Loss: -979016.562500\n",
      "Train Epoch: 37 [12672/54000 (23%)] Loss: -1042821.625000\n",
      "Train Epoch: 37 [14080/54000 (26%)] Loss: -1054100.375000\n",
      "Train Epoch: 37 [15488/54000 (29%)] Loss: -1070612.625000\n",
      "Train Epoch: 37 [16896/54000 (31%)] Loss: -1001260.562500\n",
      "Train Epoch: 37 [18304/54000 (34%)] Loss: -1070457.500000\n",
      "Train Epoch: 37 [19712/54000 (37%)] Loss: -966961.687500\n",
      "Train Epoch: 37 [21120/54000 (39%)] Loss: -988487.562500\n",
      "Train Epoch: 37 [22528/54000 (42%)] Loss: -1035421.000000\n",
      "Train Epoch: 37 [23936/54000 (44%)] Loss: -1015803.375000\n",
      "Train Epoch: 37 [25344/54000 (47%)] Loss: -1081277.875000\n",
      "Train Epoch: 37 [26752/54000 (50%)] Loss: -1047807.125000\n",
      "Train Epoch: 37 [28160/54000 (52%)] Loss: -1133972.000000\n",
      "Train Epoch: 37 [29568/54000 (55%)] Loss: -1056380.375000\n",
      "Train Epoch: 37 [30976/54000 (57%)] Loss: -1076412.500000\n",
      "Train Epoch: 37 [32384/54000 (60%)] Loss: -978378.312500\n",
      "Train Epoch: 37 [33792/54000 (63%)] Loss: -978832.187500\n",
      "Train Epoch: 37 [35200/54000 (65%)] Loss: -1075907.000000\n",
      "Train Epoch: 37 [36608/54000 (68%)] Loss: -1017542.937500\n",
      "Train Epoch: 37 [38016/54000 (70%)] Loss: -1049120.750000\n",
      "Train Epoch: 37 [39424/54000 (73%)] Loss: -1038581.125000\n",
      "Train Epoch: 37 [40832/54000 (76%)] Loss: -1084701.125000\n",
      "Train Epoch: 37 [42240/54000 (78%)] Loss: -1005339.062500\n",
      "Train Epoch: 37 [43648/54000 (81%)] Loss: -1066185.750000\n",
      "Train Epoch: 37 [45056/54000 (83%)] Loss: -1049311.375000\n",
      "Train Epoch: 37 [46464/54000 (86%)] Loss: -1115799.750000\n",
      "Train Epoch: 37 [47872/54000 (89%)] Loss: -1045147.187500\n",
      "Train Epoch: 37 [49280/54000 (91%)] Loss: -1049314.875000\n",
      "Train Epoch: 37 [50688/54000 (94%)] Loss: -1048130.437500\n",
      "Train Epoch: 37 [52096/54000 (96%)] Loss: -1034867.500000\n",
      "Train Epoch: 37 [53504/54000 (99%)] Loss: -1062880.125000\n",
      "    epoch          : 37\n",
      "    loss           : -1043213.6626184834\n",
      "    val_loss       : -1048060.405743538\n",
      "Train Epoch: 38 [0/54000 (0%)] Loss: -985471.375000\n",
      "Train Epoch: 38 [1408/54000 (3%)] Loss: -979842.750000\n",
      "Train Epoch: 38 [2816/54000 (5%)] Loss: -1043205.562500\n",
      "Train Epoch: 38 [4224/54000 (8%)] Loss: -985019.500000\n",
      "Train Epoch: 38 [5632/54000 (10%)] Loss: -1071870.500000\n",
      "Train Epoch: 38 [7040/54000 (13%)] Loss: -1036189.062500\n",
      "Train Epoch: 38 [8448/54000 (16%)] Loss: -1023635.875000\n",
      "Train Epoch: 38 [9856/54000 (18%)] Loss: -1022413.750000\n",
      "Train Epoch: 38 [11264/54000 (21%)] Loss: -1006265.250000\n",
      "Train Epoch: 38 [12672/54000 (23%)] Loss: -1022093.375000\n",
      "Train Epoch: 38 [14080/54000 (26%)] Loss: -1000375.687500\n",
      "Train Epoch: 38 [15488/54000 (29%)] Loss: -1131709.125000\n",
      "Train Epoch: 38 [16896/54000 (31%)] Loss: -1098658.625000\n",
      "Train Epoch: 38 [18304/54000 (34%)] Loss: -1126951.375000\n",
      "Train Epoch: 38 [19712/54000 (37%)] Loss: -1070566.125000\n",
      "Train Epoch: 38 [21120/54000 (39%)] Loss: -1038151.062500\n",
      "Train Epoch: 38 [22528/54000 (42%)] Loss: -1081499.375000\n",
      "Train Epoch: 38 [23936/54000 (44%)] Loss: -1034277.125000\n",
      "Train Epoch: 38 [25344/54000 (47%)] Loss: -1064056.250000\n",
      "Train Epoch: 38 [26752/54000 (50%)] Loss: -981407.687500\n",
      "Train Epoch: 38 [28160/54000 (52%)] Loss: -1045195.750000\n",
      "Train Epoch: 38 [29568/54000 (55%)] Loss: -1020378.875000\n",
      "Train Epoch: 38 [30976/54000 (57%)] Loss: -1051441.125000\n",
      "Train Epoch: 38 [32384/54000 (60%)] Loss: -1000745.312500\n",
      "Train Epoch: 38 [33792/54000 (63%)] Loss: -1028670.687500\n",
      "Train Epoch: 38 [35200/54000 (65%)] Loss: -1011642.937500\n",
      "Train Epoch: 38 [36608/54000 (68%)] Loss: -1038279.000000\n",
      "Train Epoch: 38 [38016/54000 (70%)] Loss: -1030631.687500\n",
      "Train Epoch: 38 [39424/54000 (73%)] Loss: -1073314.875000\n",
      "Train Epoch: 38 [40832/54000 (76%)] Loss: -1086328.750000\n",
      "Train Epoch: 38 [42240/54000 (78%)] Loss: -1114274.875000\n",
      "Train Epoch: 38 [43648/54000 (81%)] Loss: -972001.125000\n",
      "Train Epoch: 38 [45056/54000 (83%)] Loss: -1058961.000000\n",
      "Train Epoch: 38 [46464/54000 (86%)] Loss: -1042789.812500\n",
      "Train Epoch: 38 [47872/54000 (89%)] Loss: -1028198.812500\n",
      "Train Epoch: 38 [49280/54000 (91%)] Loss: -995351.937500\n",
      "Train Epoch: 38 [50688/54000 (94%)] Loss: -996026.812500\n",
      "Train Epoch: 38 [52096/54000 (96%)] Loss: -1107604.875000\n",
      "Train Epoch: 38 [53504/54000 (99%)] Loss: -1018092.750000\n",
      "    epoch          : 38\n",
      "    loss           : -1044630.2380035545\n",
      "    val_loss       : -1049121.9609375\n",
      "Train Epoch: 39 [0/54000 (0%)] Loss: -1053677.000000\n",
      "Train Epoch: 39 [1408/54000 (3%)] Loss: -1085028.125000\n",
      "Train Epoch: 39 [2816/54000 (5%)] Loss: -1013971.875000\n",
      "Train Epoch: 39 [4224/54000 (8%)] Loss: -1019223.000000\n",
      "Train Epoch: 39 [5632/54000 (10%)] Loss: -1013561.437500\n",
      "Train Epoch: 39 [7040/54000 (13%)] Loss: -1077563.750000\n",
      "Train Epoch: 39 [8448/54000 (16%)] Loss: -1105470.000000\n",
      "Train Epoch: 39 [9856/54000 (18%)] Loss: -1064654.375000\n",
      "Train Epoch: 39 [11264/54000 (21%)] Loss: -1020853.625000\n",
      "Train Epoch: 39 [12672/54000 (23%)] Loss: -1047596.812500\n",
      "Train Epoch: 39 [14080/54000 (26%)] Loss: -1030679.375000\n",
      "Train Epoch: 39 [15488/54000 (29%)] Loss: -1088109.375000\n",
      "Train Epoch: 39 [16896/54000 (31%)] Loss: -1080801.125000\n",
      "Train Epoch: 39 [18304/54000 (34%)] Loss: -1115301.500000\n",
      "Train Epoch: 39 [19712/54000 (37%)] Loss: -1076485.125000\n",
      "Train Epoch: 39 [21120/54000 (39%)] Loss: -1090407.625000\n",
      "Train Epoch: 39 [22528/54000 (42%)] Loss: -1012677.625000\n",
      "Train Epoch: 39 [23936/54000 (44%)] Loss: -1055872.750000\n",
      "Train Epoch: 39 [25344/54000 (47%)] Loss: -1101090.250000\n",
      "Train Epoch: 39 [26752/54000 (50%)] Loss: -1062866.500000\n",
      "Train Epoch: 39 [28160/54000 (52%)] Loss: -1055482.625000\n",
      "Train Epoch: 39 [29568/54000 (55%)] Loss: -1040158.375000\n",
      "Train Epoch: 39 [30976/54000 (57%)] Loss: -1034917.250000\n",
      "Train Epoch: 39 [32384/54000 (60%)] Loss: -1008178.562500\n",
      "Train Epoch: 39 [33792/54000 (63%)] Loss: -1032214.250000\n",
      "Train Epoch: 39 [35200/54000 (65%)] Loss: -1020322.187500\n",
      "Train Epoch: 39 [36608/54000 (68%)] Loss: -1084496.125000\n",
      "Train Epoch: 39 [38016/54000 (70%)] Loss: -1030961.125000\n",
      "Train Epoch: 39 [39424/54000 (73%)] Loss: -1026844.625000\n",
      "Train Epoch: 39 [40832/54000 (76%)] Loss: -1000559.375000\n",
      "Train Epoch: 39 [42240/54000 (78%)] Loss: -1041001.125000\n",
      "Train Epoch: 39 [43648/54000 (81%)] Loss: -1081370.250000\n",
      "Train Epoch: 39 [45056/54000 (83%)] Loss: -1051766.750000\n",
      "Train Epoch: 39 [46464/54000 (86%)] Loss: -1054073.625000\n",
      "Train Epoch: 39 [47872/54000 (89%)] Loss: -1032721.000000\n",
      "Train Epoch: 39 [49280/54000 (91%)] Loss: -1079156.375000\n",
      "Train Epoch: 39 [50688/54000 (94%)] Loss: -1070803.125000\n",
      "Train Epoch: 39 [52096/54000 (96%)] Loss: -1046776.375000\n",
      "Train Epoch: 39 [53504/54000 (99%)] Loss: -1053964.125000\n",
      "    epoch          : 39\n",
      "    loss           : -1045991.858264218\n",
      "    val_loss       : -1050619.128690679\n",
      "Train Epoch: 40 [0/54000 (0%)] Loss: -1023697.812500\n",
      "Train Epoch: 40 [1408/54000 (3%)] Loss: -1053387.875000\n",
      "Train Epoch: 40 [2816/54000 (5%)] Loss: -1037400.375000\n",
      "Train Epoch: 40 [4224/54000 (8%)] Loss: -1089638.500000\n",
      "Train Epoch: 40 [5632/54000 (10%)] Loss: -1093334.500000\n",
      "Train Epoch: 40 [7040/54000 (13%)] Loss: -1072521.750000\n",
      "Train Epoch: 40 [8448/54000 (16%)] Loss: -1074202.875000\n",
      "Train Epoch: 40 [9856/54000 (18%)] Loss: -1024694.000000\n",
      "Train Epoch: 40 [11264/54000 (21%)] Loss: -999686.875000\n",
      "Train Epoch: 40 [12672/54000 (23%)] Loss: -1120366.375000\n",
      "Train Epoch: 40 [14080/54000 (26%)] Loss: -1031231.312500\n",
      "Train Epoch: 40 [15488/54000 (29%)] Loss: -1044947.375000\n",
      "Train Epoch: 40 [16896/54000 (31%)] Loss: -1068539.625000\n",
      "Train Epoch: 40 [18304/54000 (34%)] Loss: -1110303.500000\n",
      "Train Epoch: 40 [19712/54000 (37%)] Loss: -1039737.500000\n",
      "Train Epoch: 40 [21120/54000 (39%)] Loss: -973721.000000\n",
      "Train Epoch: 40 [22528/54000 (42%)] Loss: -963032.750000\n",
      "Train Epoch: 40 [23936/54000 (44%)] Loss: -1055265.625000\n",
      "Train Epoch: 40 [25344/54000 (47%)] Loss: -1068623.375000\n",
      "Train Epoch: 40 [26752/54000 (50%)] Loss: -1032948.375000\n",
      "Train Epoch: 40 [28160/54000 (52%)] Loss: -1101952.125000\n",
      "Train Epoch: 40 [29568/54000 (55%)] Loss: -1089568.125000\n",
      "Train Epoch: 40 [30976/54000 (57%)] Loss: -1051049.000000\n",
      "Train Epoch: 40 [32384/54000 (60%)] Loss: -1040714.125000\n",
      "Train Epoch: 40 [33792/54000 (63%)] Loss: -1022580.750000\n",
      "Train Epoch: 40 [35200/54000 (65%)] Loss: -1046988.000000\n",
      "Train Epoch: 40 [36608/54000 (68%)] Loss: -1037832.687500\n",
      "Train Epoch: 40 [38016/54000 (70%)] Loss: -1084420.875000\n",
      "Train Epoch: 40 [39424/54000 (73%)] Loss: -1033397.437500\n",
      "Train Epoch: 40 [40832/54000 (76%)] Loss: -1108964.750000\n",
      "Train Epoch: 40 [42240/54000 (78%)] Loss: -1035759.375000\n",
      "Train Epoch: 40 [43648/54000 (81%)] Loss: -1048631.250000\n",
      "Train Epoch: 40 [45056/54000 (83%)] Loss: -1051312.875000\n",
      "Train Epoch: 40 [46464/54000 (86%)] Loss: -1067558.875000\n",
      "Train Epoch: 40 [47872/54000 (89%)] Loss: -1074955.250000\n",
      "Train Epoch: 40 [49280/54000 (91%)] Loss: -1061341.375000\n",
      "Train Epoch: 40 [50688/54000 (94%)] Loss: -1012742.500000\n",
      "Train Epoch: 40 [52096/54000 (96%)] Loss: -1107814.375000\n",
      "Train Epoch: 40 [53504/54000 (99%)] Loss: -1029500.500000\n",
      "    epoch          : 40\n",
      "    loss           : -1047298.512292654\n",
      "    val_loss       : -1051470.6745111994\n",
      "Train Epoch: 41 [0/54000 (0%)] Loss: -1016951.375000\n",
      "Train Epoch: 41 [1408/54000 (3%)] Loss: -1025257.125000\n",
      "Train Epoch: 41 [2816/54000 (5%)] Loss: -1044589.687500\n",
      "Train Epoch: 41 [4224/54000 (8%)] Loss: -1056094.375000\n",
      "Train Epoch: 41 [5632/54000 (10%)] Loss: -1014760.562500\n",
      "Train Epoch: 41 [7040/54000 (13%)] Loss: -1073428.250000\n",
      "Train Epoch: 41 [8448/54000 (16%)] Loss: -1099365.500000\n",
      "Train Epoch: 41 [9856/54000 (18%)] Loss: -1050898.375000\n",
      "Train Epoch: 41 [11264/54000 (21%)] Loss: -1064490.500000\n",
      "Train Epoch: 41 [12672/54000 (23%)] Loss: -1069694.000000\n",
      "Train Epoch: 41 [14080/54000 (26%)] Loss: -999218.875000\n",
      "Train Epoch: 41 [15488/54000 (29%)] Loss: -1033206.562500\n",
      "Train Epoch: 41 [16896/54000 (31%)] Loss: -1046112.062500\n",
      "Train Epoch: 41 [18304/54000 (34%)] Loss: -1061055.000000\n",
      "Train Epoch: 41 [19712/54000 (37%)] Loss: -1054442.750000\n",
      "Train Epoch: 41 [21120/54000 (39%)] Loss: -980549.625000\n",
      "Train Epoch: 41 [22528/54000 (42%)] Loss: -1018930.437500\n",
      "Train Epoch: 41 [23936/54000 (44%)] Loss: -980602.500000\n",
      "Train Epoch: 41 [25344/54000 (47%)] Loss: -1052304.375000\n",
      "Train Epoch: 41 [26752/54000 (50%)] Loss: -1069479.625000\n",
      "Train Epoch: 41 [28160/54000 (52%)] Loss: -1108682.875000\n",
      "Train Epoch: 41 [29568/54000 (55%)] Loss: -1059728.250000\n",
      "Train Epoch: 41 [30976/54000 (57%)] Loss: -1053618.000000\n",
      "Train Epoch: 41 [32384/54000 (60%)] Loss: -1048584.125000\n",
      "Train Epoch: 41 [33792/54000 (63%)] Loss: -1077612.750000\n",
      "Train Epoch: 41 [35200/54000 (65%)] Loss: -1077752.750000\n",
      "Train Epoch: 41 [36608/54000 (68%)] Loss: -1068514.125000\n",
      "Train Epoch: 41 [38016/54000 (70%)] Loss: -1034323.625000\n",
      "Train Epoch: 41 [39424/54000 (73%)] Loss: -1018203.437500\n",
      "Train Epoch: 41 [40832/54000 (76%)] Loss: -1059306.125000\n",
      "Train Epoch: 41 [42240/54000 (78%)] Loss: -1044322.750000\n",
      "Train Epoch: 41 [43648/54000 (81%)] Loss: -1055745.250000\n",
      "Train Epoch: 41 [45056/54000 (83%)] Loss: -1031129.625000\n",
      "Train Epoch: 41 [46464/54000 (86%)] Loss: -1043574.187500\n",
      "Train Epoch: 41 [47872/54000 (89%)] Loss: -1068536.125000\n",
      "Train Epoch: 41 [49280/54000 (91%)] Loss: -1070121.750000\n",
      "Train Epoch: 41 [50688/54000 (94%)] Loss: -1081827.250000\n",
      "Train Epoch: 41 [52096/54000 (96%)] Loss: -1114374.625000\n",
      "Train Epoch: 41 [53504/54000 (99%)] Loss: -1092658.375000\n",
      "    epoch          : 41\n",
      "    loss           : -1048531.0924170616\n",
      "    val_loss       : -1052773.4871072972\n",
      "Train Epoch: 42 [0/54000 (0%)] Loss: -1036077.000000\n",
      "Train Epoch: 42 [1408/54000 (3%)] Loss: -1133426.500000\n",
      "Train Epoch: 42 [2816/54000 (5%)] Loss: -1041884.125000\n",
      "Train Epoch: 42 [4224/54000 (8%)] Loss: -1070299.000000\n",
      "Train Epoch: 42 [5632/54000 (10%)] Loss: -1084314.625000\n",
      "Train Epoch: 42 [7040/54000 (13%)] Loss: -1101478.875000\n",
      "Train Epoch: 42 [8448/54000 (16%)] Loss: -1018747.625000\n",
      "Train Epoch: 42 [9856/54000 (18%)] Loss: -1082350.875000\n",
      "Train Epoch: 42 [11264/54000 (21%)] Loss: -1067834.625000\n",
      "Train Epoch: 42 [12672/54000 (23%)] Loss: -1060021.625000\n",
      "Train Epoch: 42 [14080/54000 (26%)] Loss: -1066952.375000\n",
      "Train Epoch: 42 [15488/54000 (29%)] Loss: -1045893.875000\n",
      "Train Epoch: 42 [16896/54000 (31%)] Loss: -976455.375000\n",
      "Train Epoch: 42 [18304/54000 (34%)] Loss: -1008076.250000\n",
      "Train Epoch: 42 [19712/54000 (37%)] Loss: -997908.875000\n",
      "Train Epoch: 42 [21120/54000 (39%)] Loss: -1039090.937500\n",
      "Train Epoch: 42 [22528/54000 (42%)] Loss: -1048934.750000\n",
      "Train Epoch: 42 [23936/54000 (44%)] Loss: -1027484.187500\n",
      "Train Epoch: 42 [25344/54000 (47%)] Loss: -1009519.250000\n",
      "Train Epoch: 42 [26752/54000 (50%)] Loss: -1012824.125000\n",
      "Train Epoch: 42 [28160/54000 (52%)] Loss: -1076128.750000\n",
      "Train Epoch: 42 [29568/54000 (55%)] Loss: -1109355.625000\n",
      "Train Epoch: 42 [30976/54000 (57%)] Loss: -1049879.750000\n",
      "Train Epoch: 42 [32384/54000 (60%)] Loss: -1057574.625000\n",
      "Train Epoch: 42 [33792/54000 (63%)] Loss: -1076783.250000\n",
      "Train Epoch: 42 [35200/54000 (65%)] Loss: -1030029.250000\n",
      "Train Epoch: 42 [36608/54000 (68%)] Loss: -1063921.875000\n",
      "Train Epoch: 42 [38016/54000 (70%)] Loss: -1095517.875000\n",
      "Train Epoch: 42 [39424/54000 (73%)] Loss: -1021430.687500\n",
      "Train Epoch: 42 [40832/54000 (76%)] Loss: -1062882.000000\n",
      "Train Epoch: 42 [42240/54000 (78%)] Loss: -1004280.312500\n",
      "Train Epoch: 42 [43648/54000 (81%)] Loss: -1059315.250000\n",
      "Train Epoch: 42 [45056/54000 (83%)] Loss: -1044295.312500\n",
      "Train Epoch: 42 [46464/54000 (86%)] Loss: -1077183.625000\n",
      "Train Epoch: 42 [47872/54000 (89%)] Loss: -1071990.625000\n",
      "Train Epoch: 42 [49280/54000 (91%)] Loss: -1041459.312500\n",
      "Train Epoch: 42 [50688/54000 (94%)] Loss: -986343.625000\n",
      "Train Epoch: 42 [52096/54000 (96%)] Loss: -1060970.250000\n",
      "Train Epoch: 42 [53504/54000 (99%)] Loss: -1049874.750000\n",
      "    epoch          : 42\n",
      "    loss           : -1049625.6709123224\n",
      "    val_loss       : -1053897.9889876994\n",
      "Train Epoch: 43 [0/54000 (0%)] Loss: -1080934.875000\n",
      "Train Epoch: 43 [1408/54000 (3%)] Loss: -1040558.750000\n",
      "Train Epoch: 43 [2816/54000 (5%)] Loss: -1067083.000000\n",
      "Train Epoch: 43 [4224/54000 (8%)] Loss: -1038821.062500\n",
      "Train Epoch: 43 [5632/54000 (10%)] Loss: -1004190.625000\n",
      "Train Epoch: 43 [7040/54000 (13%)] Loss: -1054096.750000\n",
      "Train Epoch: 43 [8448/54000 (16%)] Loss: -1051170.875000\n",
      "Train Epoch: 43 [9856/54000 (18%)] Loss: -1040005.375000\n",
      "Train Epoch: 43 [11264/54000 (21%)] Loss: -1028549.312500\n",
      "Train Epoch: 43 [12672/54000 (23%)] Loss: -1060750.750000\n",
      "Train Epoch: 43 [14080/54000 (26%)] Loss: -1069732.250000\n",
      "Train Epoch: 43 [15488/54000 (29%)] Loss: -1084476.500000\n",
      "Train Epoch: 43 [16896/54000 (31%)] Loss: -1134952.125000\n",
      "Train Epoch: 43 [18304/54000 (34%)] Loss: -1014392.625000\n",
      "Train Epoch: 43 [19712/54000 (37%)] Loss: -1096527.375000\n",
      "Train Epoch: 43 [21120/54000 (39%)] Loss: -1036348.562500\n",
      "Train Epoch: 43 [22528/54000 (42%)] Loss: -1082418.000000\n",
      "Train Epoch: 43 [23936/54000 (44%)] Loss: -1065624.750000\n",
      "Train Epoch: 43 [25344/54000 (47%)] Loss: -1058418.000000\n",
      "Train Epoch: 43 [26752/54000 (50%)] Loss: -1072374.375000\n",
      "Train Epoch: 43 [28160/54000 (52%)] Loss: -1056195.500000\n",
      "Train Epoch: 43 [29568/54000 (55%)] Loss: -1069945.500000\n",
      "Train Epoch: 43 [30976/54000 (57%)] Loss: -1068004.875000\n",
      "Train Epoch: 43 [32384/54000 (60%)] Loss: -1077823.250000\n",
      "Train Epoch: 43 [33792/54000 (63%)] Loss: -1032486.562500\n",
      "Train Epoch: 43 [35200/54000 (65%)] Loss: -1065031.875000\n",
      "Train Epoch: 43 [36608/54000 (68%)] Loss: -1094715.250000\n",
      "Train Epoch: 43 [38016/54000 (70%)] Loss: -1077275.250000\n",
      "Train Epoch: 43 [39424/54000 (73%)] Loss: -1019058.312500\n",
      "Train Epoch: 43 [40832/54000 (76%)] Loss: -998883.875000\n",
      "Train Epoch: 43 [42240/54000 (78%)] Loss: -1077314.375000\n",
      "Train Epoch: 43 [43648/54000 (81%)] Loss: -1099017.250000\n",
      "Train Epoch: 43 [45056/54000 (83%)] Loss: -1072016.625000\n",
      "Train Epoch: 43 [46464/54000 (86%)] Loss: -1076760.500000\n",
      "Train Epoch: 43 [47872/54000 (89%)] Loss: -1035006.125000\n",
      "Train Epoch: 43 [49280/54000 (91%)] Loss: -1061001.500000\n",
      "Train Epoch: 43 [50688/54000 (94%)] Loss: -1068579.125000\n",
      "Train Epoch: 43 [52096/54000 (96%)] Loss: -1067199.875000\n",
      "Train Epoch: 43 [53504/54000 (99%)] Loss: -1071624.125000\n",
      "    epoch          : 43\n",
      "    loss           : -1050870.5525770143\n",
      "    val_loss       : -1055083.5236920295\n",
      "Train Epoch: 44 [0/54000 (0%)] Loss: -1035253.562500\n",
      "Train Epoch: 44 [1408/54000 (3%)] Loss: -1113162.500000\n",
      "Train Epoch: 44 [2816/54000 (5%)] Loss: -1088003.875000\n",
      "Train Epoch: 44 [4224/54000 (8%)] Loss: -1060858.375000\n",
      "Train Epoch: 44 [5632/54000 (10%)] Loss: -1003019.750000\n",
      "Train Epoch: 44 [7040/54000 (13%)] Loss: -1060218.750000\n",
      "Train Epoch: 44 [8448/54000 (16%)] Loss: -1077298.125000\n",
      "Train Epoch: 44 [9856/54000 (18%)] Loss: -1100077.250000\n",
      "Train Epoch: 44 [11264/54000 (21%)] Loss: -1103952.375000\n",
      "Train Epoch: 44 [12672/54000 (23%)] Loss: -1132360.000000\n",
      "Train Epoch: 44 [14080/54000 (26%)] Loss: -1059615.750000\n",
      "Train Epoch: 44 [15488/54000 (29%)] Loss: -1063043.375000\n",
      "Train Epoch: 44 [16896/54000 (31%)] Loss: -1113694.500000\n",
      "Train Epoch: 44 [18304/54000 (34%)] Loss: -976417.437500\n",
      "Train Epoch: 44 [19712/54000 (37%)] Loss: -1038094.687500\n",
      "Train Epoch: 44 [21120/54000 (39%)] Loss: -1043215.250000\n",
      "Train Epoch: 44 [22528/54000 (42%)] Loss: -1068528.875000\n",
      "Train Epoch: 44 [23936/54000 (44%)] Loss: -1071500.750000\n",
      "Train Epoch: 44 [25344/54000 (47%)] Loss: -986984.437500\n",
      "Train Epoch: 44 [26752/54000 (50%)] Loss: -1059468.125000\n",
      "Train Epoch: 44 [28160/54000 (52%)] Loss: -1058181.875000\n",
      "Train Epoch: 44 [29568/54000 (55%)] Loss: -1047485.312500\n",
      "Train Epoch: 44 [30976/54000 (57%)] Loss: -1090100.000000\n",
      "Train Epoch: 44 [32384/54000 (60%)] Loss: -994519.562500\n",
      "Train Epoch: 44 [33792/54000 (63%)] Loss: -1022654.312500\n",
      "Train Epoch: 44 [35200/54000 (65%)] Loss: -1079838.375000\n",
      "Train Epoch: 44 [36608/54000 (68%)] Loss: -1054406.500000\n",
      "Train Epoch: 44 [38016/54000 (70%)] Loss: -1101184.750000\n",
      "Train Epoch: 44 [39424/54000 (73%)] Loss: -1037895.312500\n",
      "Train Epoch: 44 [40832/54000 (76%)] Loss: -1065672.750000\n",
      "Train Epoch: 44 [42240/54000 (78%)] Loss: -1093705.125000\n",
      "Train Epoch: 44 [43648/54000 (81%)] Loss: -1083912.500000\n",
      "Train Epoch: 44 [45056/54000 (83%)] Loss: -1019449.687500\n",
      "Train Epoch: 44 [46464/54000 (86%)] Loss: -1006182.000000\n",
      "Train Epoch: 44 [47872/54000 (89%)] Loss: -1054956.625000\n",
      "Train Epoch: 44 [49280/54000 (91%)] Loss: -1083807.500000\n",
      "Train Epoch: 44 [50688/54000 (94%)] Loss: -1023876.750000\n",
      "Train Epoch: 44 [52096/54000 (96%)] Loss: -1028496.562500\n",
      "Train Epoch: 44 [53504/54000 (99%)] Loss: -1080012.000000\n",
      "    epoch          : 44\n",
      "    loss           : -1051848.2954680095\n",
      "    val_loss       : -1056097.0993262758\n",
      "Train Epoch: 45 [0/54000 (0%)] Loss: -1046587.625000\n",
      "Train Epoch: 45 [1408/54000 (3%)] Loss: -1022706.312500\n",
      "Train Epoch: 45 [2816/54000 (5%)] Loss: -1004847.125000\n",
      "Train Epoch: 45 [4224/54000 (8%)] Loss: -1057229.875000\n",
      "Train Epoch: 45 [5632/54000 (10%)] Loss: -1102276.125000\n",
      "Train Epoch: 45 [7040/54000 (13%)] Loss: -1066374.875000\n",
      "Train Epoch: 45 [8448/54000 (16%)] Loss: -1012932.187500\n",
      "Train Epoch: 45 [9856/54000 (18%)] Loss: -1048683.750000\n",
      "Train Epoch: 45 [11264/54000 (21%)] Loss: -1075250.500000\n",
      "Train Epoch: 45 [12672/54000 (23%)] Loss: -1088073.375000\n",
      "Train Epoch: 45 [14080/54000 (26%)] Loss: -1031549.625000\n",
      "Train Epoch: 45 [15488/54000 (29%)] Loss: -1073414.875000\n",
      "Train Epoch: 45 [16896/54000 (31%)] Loss: -1058721.625000\n",
      "Train Epoch: 45 [18304/54000 (34%)] Loss: -1053314.250000\n",
      "Train Epoch: 45 [19712/54000 (37%)] Loss: -1006259.750000\n",
      "Train Epoch: 45 [21120/54000 (39%)] Loss: -1090486.250000\n",
      "Train Epoch: 45 [22528/54000 (42%)] Loss: -1051335.500000\n",
      "Train Epoch: 45 [23936/54000 (44%)] Loss: -1068684.250000\n",
      "Train Epoch: 45 [25344/54000 (47%)] Loss: -999815.687500\n",
      "Train Epoch: 45 [26752/54000 (50%)] Loss: -1047462.187500\n",
      "Train Epoch: 45 [28160/54000 (52%)] Loss: -1018603.125000\n",
      "Train Epoch: 45 [29568/54000 (55%)] Loss: -1086244.750000\n",
      "Train Epoch: 45 [30976/54000 (57%)] Loss: -1047917.250000\n",
      "Train Epoch: 45 [32384/54000 (60%)] Loss: -1075913.250000\n",
      "Train Epoch: 45 [33792/54000 (63%)] Loss: -1096027.625000\n",
      "Train Epoch: 45 [35200/54000 (65%)] Loss: -1021908.250000\n",
      "Train Epoch: 45 [36608/54000 (68%)] Loss: -1080342.875000\n",
      "Train Epoch: 45 [38016/54000 (70%)] Loss: -1041295.812500\n",
      "Train Epoch: 45 [39424/54000 (73%)] Loss: -1065033.625000\n",
      "Train Epoch: 45 [40832/54000 (76%)] Loss: -1044200.062500\n",
      "Train Epoch: 45 [42240/54000 (78%)] Loss: -997742.375000\n",
      "Train Epoch: 45 [43648/54000 (81%)] Loss: -1063108.625000\n",
      "Train Epoch: 45 [45056/54000 (83%)] Loss: -1023441.625000\n",
      "Train Epoch: 45 [46464/54000 (86%)] Loss: -1032480.500000\n",
      "Train Epoch: 45 [47872/54000 (89%)] Loss: -1035065.812500\n",
      "Train Epoch: 45 [49280/54000 (91%)] Loss: -1074306.875000\n",
      "Train Epoch: 45 [50688/54000 (94%)] Loss: -1091655.125000\n",
      "Train Epoch: 45 [52096/54000 (96%)] Loss: -1079470.750000\n",
      "Train Epoch: 45 [53504/54000 (99%)] Loss: -1084786.750000\n",
      "    epoch          : 45\n",
      "    loss           : -1052918.7664395734\n",
      "    val_loss       : -1056731.9593168218\n",
      "Train Epoch: 46 [0/54000 (0%)] Loss: -1126171.500000\n",
      "Train Epoch: 46 [1408/54000 (3%)] Loss: -986001.187500\n",
      "Train Epoch: 46 [2816/54000 (5%)] Loss: -1046227.250000\n",
      "Train Epoch: 46 [4224/54000 (8%)] Loss: -1016142.250000\n",
      "Train Epoch: 46 [5632/54000 (10%)] Loss: -1029547.375000\n",
      "Train Epoch: 46 [7040/54000 (13%)] Loss: -1041423.687500\n",
      "Train Epoch: 46 [8448/54000 (16%)] Loss: -1025500.875000\n",
      "Train Epoch: 46 [9856/54000 (18%)] Loss: -1015106.687500\n",
      "Train Epoch: 46 [11264/54000 (21%)] Loss: -1089720.875000\n",
      "Train Epoch: 46 [12672/54000 (23%)] Loss: -1040851.375000\n",
      "Train Epoch: 46 [14080/54000 (26%)] Loss: -1072313.875000\n",
      "Train Epoch: 46 [15488/54000 (29%)] Loss: -1030197.500000\n",
      "Train Epoch: 46 [16896/54000 (31%)] Loss: -1018041.812500\n",
      "Train Epoch: 46 [18304/54000 (34%)] Loss: -1068560.375000\n",
      "Train Epoch: 46 [19712/54000 (37%)] Loss: -1064857.125000\n",
      "Train Epoch: 46 [21120/54000 (39%)] Loss: -1021789.250000\n",
      "Train Epoch: 46 [22528/54000 (42%)] Loss: -1011549.750000\n",
      "Train Epoch: 46 [23936/54000 (44%)] Loss: -1046214.812500\n",
      "Train Epoch: 46 [25344/54000 (47%)] Loss: -1085702.875000\n",
      "Train Epoch: 46 [26752/54000 (50%)] Loss: -1064874.625000\n",
      "Train Epoch: 46 [28160/54000 (52%)] Loss: -1040438.500000\n",
      "Train Epoch: 46 [29568/54000 (55%)] Loss: -1093985.875000\n",
      "Train Epoch: 46 [30976/54000 (57%)] Loss: -1055768.750000\n",
      "Train Epoch: 46 [32384/54000 (60%)] Loss: -1074926.875000\n",
      "Train Epoch: 46 [33792/54000 (63%)] Loss: -1085943.500000\n",
      "Train Epoch: 46 [35200/54000 (65%)] Loss: -1008336.500000\n",
      "Train Epoch: 46 [36608/54000 (68%)] Loss: -1062356.750000\n",
      "Train Epoch: 46 [38016/54000 (70%)] Loss: -1082864.375000\n",
      "Train Epoch: 46 [39424/54000 (73%)] Loss: -1064423.375000\n",
      "Train Epoch: 46 [40832/54000 (76%)] Loss: -1016439.312500\n",
      "Train Epoch: 46 [42240/54000 (78%)] Loss: -1073335.750000\n",
      "Train Epoch: 46 [43648/54000 (81%)] Loss: -1031865.812500\n",
      "Train Epoch: 46 [45056/54000 (83%)] Loss: -993715.500000\n",
      "Train Epoch: 46 [46464/54000 (86%)] Loss: -1055171.500000\n",
      "Train Epoch: 46 [47872/54000 (89%)] Loss: -1005544.625000\n",
      "Train Epoch: 46 [49280/54000 (91%)] Loss: -1103632.000000\n",
      "Train Epoch: 46 [50688/54000 (94%)] Loss: -1046211.937500\n",
      "Train Epoch: 46 [52096/54000 (96%)] Loss: -1089317.875000\n",
      "Train Epoch: 46 [53504/54000 (99%)] Loss: -1015844.500000\n",
      "    epoch          : 46\n",
      "    loss           : -1053786.1731338862\n",
      "    val_loss       : -1057820.8795919216\n",
      "Train Epoch: 47 [0/54000 (0%)] Loss: -1028937.375000\n",
      "Train Epoch: 47 [1408/54000 (3%)] Loss: -1079011.875000\n",
      "Train Epoch: 47 [2816/54000 (5%)] Loss: -1070386.375000\n",
      "Train Epoch: 47 [4224/54000 (8%)] Loss: -1057927.750000\n",
      "Train Epoch: 47 [5632/54000 (10%)] Loss: -1034244.375000\n",
      "Train Epoch: 47 [7040/54000 (13%)] Loss: -1096144.750000\n",
      "Train Epoch: 47 [8448/54000 (16%)] Loss: -1026031.312500\n",
      "Train Epoch: 47 [9856/54000 (18%)] Loss: -1017699.125000\n",
      "Train Epoch: 47 [11264/54000 (21%)] Loss: -1024819.687500\n",
      "Train Epoch: 47 [12672/54000 (23%)] Loss: -1064655.750000\n",
      "Train Epoch: 47 [14080/54000 (26%)] Loss: -1036139.125000\n",
      "Train Epoch: 47 [15488/54000 (29%)] Loss: -1036045.500000\n",
      "Train Epoch: 47 [16896/54000 (31%)] Loss: -1024407.437500\n",
      "Train Epoch: 47 [18304/54000 (34%)] Loss: -1048108.375000\n",
      "Train Epoch: 47 [19712/54000 (37%)] Loss: -1109387.125000\n",
      "Train Epoch: 47 [21120/54000 (39%)] Loss: -1035701.000000\n",
      "Train Epoch: 47 [22528/54000 (42%)] Loss: -1050169.375000\n",
      "Train Epoch: 47 [23936/54000 (44%)] Loss: -1065594.125000\n",
      "Train Epoch: 47 [25344/54000 (47%)] Loss: -1031781.250000\n",
      "Train Epoch: 47 [26752/54000 (50%)] Loss: -1047204.437500\n",
      "Train Epoch: 47 [28160/54000 (52%)] Loss: -1056755.500000\n",
      "Train Epoch: 47 [29568/54000 (55%)] Loss: -1034152.500000\n",
      "Train Epoch: 47 [30976/54000 (57%)] Loss: -1063816.750000\n",
      "Train Epoch: 47 [32384/54000 (60%)] Loss: -1061863.375000\n",
      "Train Epoch: 47 [33792/54000 (63%)] Loss: -1030961.937500\n",
      "Train Epoch: 47 [35200/54000 (65%)] Loss: -1072537.000000\n",
      "Train Epoch: 47 [36608/54000 (68%)] Loss: -1081467.375000\n",
      "Train Epoch: 47 [38016/54000 (70%)] Loss: -1076317.375000\n",
      "Train Epoch: 47 [39424/54000 (73%)] Loss: -1072320.000000\n",
      "Train Epoch: 47 [40832/54000 (76%)] Loss: -1097915.500000\n",
      "Train Epoch: 47 [42240/54000 (78%)] Loss: -1062764.000000\n",
      "Train Epoch: 47 [43648/54000 (81%)] Loss: -1054042.250000\n",
      "Train Epoch: 47 [45056/54000 (83%)] Loss: -1101717.750000\n",
      "Train Epoch: 47 [46464/54000 (86%)] Loss: -1074956.625000\n",
      "Train Epoch: 47 [47872/54000 (89%)] Loss: -1100472.375000\n",
      "Train Epoch: 47 [49280/54000 (91%)] Loss: -1064308.875000\n",
      "Train Epoch: 47 [50688/54000 (94%)] Loss: -1086727.625000\n",
      "Train Epoch: 47 [52096/54000 (96%)] Loss: -1062923.625000\n",
      "Train Epoch: 47 [53504/54000 (99%)] Loss: -1025998.687500\n",
      "    epoch          : 47\n",
      "    loss           : -1055015.0228080568\n",
      "    val_loss       : -1059068.048017786\n",
      "Train Epoch: 48 [0/54000 (0%)] Loss: -1042805.562500\n",
      "Train Epoch: 48 [1408/54000 (3%)] Loss: -1080900.250000\n",
      "Train Epoch: 48 [2816/54000 (5%)] Loss: -1077798.500000\n",
      "Train Epoch: 48 [4224/54000 (8%)] Loss: -1097859.375000\n",
      "Train Epoch: 48 [5632/54000 (10%)] Loss: -1051249.375000\n",
      "Train Epoch: 48 [7040/54000 (13%)] Loss: -1027485.125000\n",
      "Train Epoch: 48 [8448/54000 (16%)] Loss: -1022849.187500\n",
      "Train Epoch: 48 [9856/54000 (18%)] Loss: -1063670.625000\n",
      "Train Epoch: 48 [11264/54000 (21%)] Loss: -1039315.125000\n",
      "Train Epoch: 48 [12672/54000 (23%)] Loss: -1053827.875000\n",
      "Train Epoch: 48 [14080/54000 (26%)] Loss: -1125017.875000\n",
      "Train Epoch: 48 [15488/54000 (29%)] Loss: -1065432.375000\n",
      "Train Epoch: 48 [16896/54000 (31%)] Loss: -1035189.875000\n",
      "Train Epoch: 48 [18304/54000 (34%)] Loss: -999724.812500\n",
      "Train Epoch: 48 [19712/54000 (37%)] Loss: -1053448.625000\n",
      "Train Epoch: 48 [21120/54000 (39%)] Loss: -1045947.312500\n",
      "Train Epoch: 48 [22528/54000 (42%)] Loss: -995179.437500\n",
      "Train Epoch: 48 [23936/54000 (44%)] Loss: -1048147.312500\n",
      "Train Epoch: 48 [25344/54000 (47%)] Loss: -1078898.625000\n",
      "Train Epoch: 48 [26752/54000 (50%)] Loss: -1053447.750000\n",
      "Train Epoch: 48 [28160/54000 (52%)] Loss: -1053254.500000\n",
      "Train Epoch: 48 [29568/54000 (55%)] Loss: -1037919.375000\n",
      "Train Epoch: 48 [30976/54000 (57%)] Loss: -1026030.562500\n",
      "Train Epoch: 48 [32384/54000 (60%)] Loss: -1041251.312500\n",
      "Train Epoch: 48 [33792/54000 (63%)] Loss: -1064470.125000\n",
      "Train Epoch: 48 [35200/54000 (65%)] Loss: -1076826.250000\n",
      "Train Epoch: 48 [36608/54000 (68%)] Loss: -1080799.750000\n",
      "Train Epoch: 48 [38016/54000 (70%)] Loss: -972890.500000\n",
      "Train Epoch: 48 [39424/54000 (73%)] Loss: -1074249.750000\n",
      "Train Epoch: 48 [40832/54000 (76%)] Loss: -1024779.125000\n",
      "Train Epoch: 48 [42240/54000 (78%)] Loss: -1024571.187500\n",
      "Train Epoch: 48 [43648/54000 (81%)] Loss: -1127293.125000\n",
      "Train Epoch: 48 [45056/54000 (83%)] Loss: -1048348.750000\n",
      "Train Epoch: 48 [46464/54000 (86%)] Loss: -1017807.750000\n",
      "Train Epoch: 48 [47872/54000 (89%)] Loss: -1053224.375000\n",
      "Train Epoch: 48 [49280/54000 (91%)] Loss: -1085307.125000\n",
      "Train Epoch: 48 [50688/54000 (94%)] Loss: -1126378.250000\n",
      "Train Epoch: 48 [52096/54000 (96%)] Loss: -1069910.750000\n",
      "Train Epoch: 48 [53504/54000 (99%)] Loss: -1002782.437500\n",
      "    epoch          : 48\n",
      "    loss           : -1055779.7280805688\n",
      "    val_loss       : -1059883.8357556933\n",
      "Train Epoch: 49 [0/54000 (0%)] Loss: -1047341.750000\n",
      "Train Epoch: 49 [1408/54000 (3%)] Loss: -1033704.250000\n",
      "Train Epoch: 49 [2816/54000 (5%)] Loss: -1103248.000000\n",
      "Train Epoch: 49 [4224/54000 (8%)] Loss: -1033050.437500\n",
      "Train Epoch: 49 [5632/54000 (10%)] Loss: -1084048.875000\n",
      "Train Epoch: 49 [7040/54000 (13%)] Loss: -1058215.750000\n",
      "Train Epoch: 49 [8448/54000 (16%)] Loss: -1031660.875000\n",
      "Train Epoch: 49 [9856/54000 (18%)] Loss: -1090739.625000\n",
      "Train Epoch: 49 [11264/54000 (21%)] Loss: -1028273.875000\n",
      "Train Epoch: 49 [12672/54000 (23%)] Loss: -1046399.937500\n",
      "Train Epoch: 49 [14080/54000 (26%)] Loss: -1055197.500000\n",
      "Train Epoch: 49 [15488/54000 (29%)] Loss: -1032966.312500\n",
      "Train Epoch: 49 [16896/54000 (31%)] Loss: -1025450.875000\n",
      "Train Epoch: 49 [18304/54000 (34%)] Loss: -1091248.500000\n",
      "Train Epoch: 49 [19712/54000 (37%)] Loss: -1048744.000000\n",
      "Train Epoch: 49 [21120/54000 (39%)] Loss: -1112143.000000\n",
      "Train Epoch: 49 [22528/54000 (42%)] Loss: -1102040.250000\n",
      "Train Epoch: 49 [23936/54000 (44%)] Loss: -1015761.625000\n",
      "Train Epoch: 49 [25344/54000 (47%)] Loss: -1072016.625000\n",
      "Train Epoch: 49 [26752/54000 (50%)] Loss: -1077134.750000\n",
      "Train Epoch: 49 [28160/54000 (52%)] Loss: -1065823.625000\n",
      "Train Epoch: 49 [29568/54000 (55%)] Loss: -1041697.687500\n",
      "Train Epoch: 49 [30976/54000 (57%)] Loss: -1050516.875000\n",
      "Train Epoch: 49 [32384/54000 (60%)] Loss: -1081480.500000\n",
      "Train Epoch: 49 [33792/54000 (63%)] Loss: -1065014.500000\n",
      "Train Epoch: 49 [35200/54000 (65%)] Loss: -1050204.375000\n",
      "Train Epoch: 49 [36608/54000 (68%)] Loss: -1067426.375000\n",
      "Train Epoch: 49 [38016/54000 (70%)] Loss: -1079984.000000\n",
      "Train Epoch: 49 [39424/54000 (73%)] Loss: -1060969.875000\n",
      "Train Epoch: 49 [40832/54000 (76%)] Loss: -1028347.812500\n",
      "Train Epoch: 49 [42240/54000 (78%)] Loss: -1017965.875000\n",
      "Train Epoch: 49 [43648/54000 (81%)] Loss: -1062559.875000\n",
      "Train Epoch: 49 [45056/54000 (83%)] Loss: -1097033.000000\n",
      "Train Epoch: 49 [46464/54000 (86%)] Loss: -1053586.125000\n",
      "Train Epoch: 49 [47872/54000 (89%)] Loss: -1039146.937500\n",
      "Train Epoch: 49 [49280/54000 (91%)] Loss: -1063160.750000\n",
      "Train Epoch: 49 [50688/54000 (94%)] Loss: -1026524.812500\n",
      "Train Epoch: 49 [52096/54000 (96%)] Loss: -1013412.812500\n",
      "Train Epoch: 49 [53504/54000 (99%)] Loss: -1022810.500000\n",
      "    epoch          : 49\n",
      "    loss           : -1056797.7012736967\n",
      "    val_loss       : -1060837.2671547749\n",
      "Train Epoch: 50 [0/54000 (0%)] Loss: -1064058.500000\n",
      "Train Epoch: 50 [1408/54000 (3%)] Loss: -1044681.062500\n",
      "Train Epoch: 50 [2816/54000 (5%)] Loss: -1072174.500000\n",
      "Train Epoch: 50 [4224/54000 (8%)] Loss: -1094609.125000\n",
      "Train Epoch: 50 [5632/54000 (10%)] Loss: -1011558.437500\n",
      "Train Epoch: 50 [7040/54000 (13%)] Loss: -1025494.625000\n",
      "Train Epoch: 50 [8448/54000 (16%)] Loss: -1063845.250000\n",
      "Train Epoch: 50 [9856/54000 (18%)] Loss: -1060850.000000\n",
      "Train Epoch: 50 [11264/54000 (21%)] Loss: -1107303.875000\n",
      "Train Epoch: 50 [12672/54000 (23%)] Loss: -1003018.250000\n",
      "Train Epoch: 50 [14080/54000 (26%)] Loss: -993810.562500\n",
      "Train Epoch: 50 [15488/54000 (29%)] Loss: -1049350.000000\n",
      "Train Epoch: 50 [16896/54000 (31%)] Loss: -1067172.250000\n",
      "Train Epoch: 50 [18304/54000 (34%)] Loss: -1134226.625000\n",
      "Train Epoch: 50 [19712/54000 (37%)] Loss: -1066554.875000\n",
      "Train Epoch: 50 [21120/54000 (39%)] Loss: -1063178.625000\n",
      "Train Epoch: 50 [22528/54000 (42%)] Loss: -1032900.187500\n",
      "Train Epoch: 50 [23936/54000 (44%)] Loss: -1064748.125000\n",
      "Train Epoch: 50 [25344/54000 (47%)] Loss: -1069773.500000\n",
      "Train Epoch: 50 [26752/54000 (50%)] Loss: -1014316.812500\n",
      "Train Epoch: 50 [28160/54000 (52%)] Loss: -967513.437500\n",
      "Train Epoch: 50 [29568/54000 (55%)] Loss: -1093895.250000\n",
      "Train Epoch: 50 [30976/54000 (57%)] Loss: -1059081.500000\n",
      "Train Epoch: 50 [32384/54000 (60%)] Loss: -1044625.437500\n",
      "Train Epoch: 50 [33792/54000 (63%)] Loss: -1021685.562500\n",
      "Train Epoch: 50 [35200/54000 (65%)] Loss: -1071782.250000\n",
      "Train Epoch: 50 [36608/54000 (68%)] Loss: -1118800.750000\n",
      "Train Epoch: 50 [38016/54000 (70%)] Loss: -1073722.750000\n",
      "Train Epoch: 50 [39424/54000 (73%)] Loss: -1018339.250000\n",
      "Train Epoch: 50 [40832/54000 (76%)] Loss: -1098492.000000\n",
      "Train Epoch: 50 [42240/54000 (78%)] Loss: -1089699.500000\n",
      "Train Epoch: 50 [43648/54000 (81%)] Loss: -1051459.750000\n",
      "Train Epoch: 50 [45056/54000 (83%)] Loss: -1026238.375000\n",
      "Train Epoch: 50 [46464/54000 (86%)] Loss: -1056385.750000\n",
      "Train Epoch: 50 [47872/54000 (89%)] Loss: -1089091.000000\n",
      "Train Epoch: 50 [49280/54000 (91%)] Loss: -1032102.500000\n",
      "Train Epoch: 50 [50688/54000 (94%)] Loss: -1049684.625000\n",
      "Train Epoch: 50 [52096/54000 (96%)] Loss: -1082446.000000\n",
      "Train Epoch: 50 [53504/54000 (99%)] Loss: -1071442.125000\n",
      "    epoch          : 50\n",
      "    loss           : -1057633.6355154028\n",
      "    val_loss       : -1061759.0209311626\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0427_111527/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [0/54000 (0%)] Loss: -1088852.125000\n",
      "Train Epoch: 51 [1408/54000 (3%)] Loss: -1041644.500000\n",
      "Train Epoch: 51 [2816/54000 (5%)] Loss: -1055284.375000\n",
      "Train Epoch: 51 [4224/54000 (8%)] Loss: -1091624.000000\n",
      "Train Epoch: 51 [5632/54000 (10%)] Loss: -1083717.625000\n",
      "Train Epoch: 51 [7040/54000 (13%)] Loss: -1109617.000000\n",
      "Train Epoch: 51 [8448/54000 (16%)] Loss: -1070543.125000\n",
      "Train Epoch: 51 [9856/54000 (18%)] Loss: -1009877.062500\n",
      "Train Epoch: 51 [11264/54000 (21%)] Loss: -1041979.000000\n",
      "Train Epoch: 51 [12672/54000 (23%)] Loss: -1074988.000000\n",
      "Train Epoch: 51 [14080/54000 (26%)] Loss: -1039201.312500\n",
      "Train Epoch: 51 [15488/54000 (29%)] Loss: -1075866.625000\n",
      "Train Epoch: 51 [16896/54000 (31%)] Loss: -967558.500000\n",
      "Train Epoch: 51 [18304/54000 (34%)] Loss: -1037042.125000\n",
      "Train Epoch: 51 [19712/54000 (37%)] Loss: -1034934.562500\n",
      "Train Epoch: 51 [21120/54000 (39%)] Loss: -1027201.500000\n",
      "Train Epoch: 51 [22528/54000 (42%)] Loss: -1053734.125000\n",
      "Train Epoch: 51 [23936/54000 (44%)] Loss: -1080184.625000\n",
      "Train Epoch: 51 [25344/54000 (47%)] Loss: -1047620.625000\n",
      "Train Epoch: 51 [26752/54000 (50%)] Loss: -1112097.500000\n",
      "Train Epoch: 51 [28160/54000 (52%)] Loss: -1102155.125000\n",
      "Train Epoch: 51 [29568/54000 (55%)] Loss: -1007242.312500\n",
      "Train Epoch: 51 [30976/54000 (57%)] Loss: -1046956.375000\n",
      "Train Epoch: 51 [32384/54000 (60%)] Loss: -1118990.750000\n",
      "Train Epoch: 51 [33792/54000 (63%)] Loss: -1000461.000000\n",
      "Train Epoch: 51 [35200/54000 (65%)] Loss: -1063944.875000\n",
      "Train Epoch: 51 [36608/54000 (68%)] Loss: -1026032.187500\n",
      "Train Epoch: 51 [38016/54000 (70%)] Loss: -1052429.500000\n",
      "Train Epoch: 51 [39424/54000 (73%)] Loss: -1102051.375000\n",
      "Train Epoch: 51 [40832/54000 (76%)] Loss: -1043926.187500\n",
      "Train Epoch: 51 [42240/54000 (78%)] Loss: -1100657.375000\n",
      "Train Epoch: 51 [43648/54000 (81%)] Loss: -1076822.625000\n",
      "Train Epoch: 51 [45056/54000 (83%)] Loss: -1079912.000000\n",
      "Train Epoch: 51 [46464/54000 (86%)] Loss: -1039195.687500\n",
      "Train Epoch: 51 [47872/54000 (89%)] Loss: -1005335.750000\n",
      "Train Epoch: 51 [49280/54000 (91%)] Loss: -1079076.875000\n",
      "Train Epoch: 51 [50688/54000 (94%)] Loss: -1092702.625000\n",
      "Train Epoch: 51 [52096/54000 (96%)] Loss: -1058312.375000\n",
      "Train Epoch: 51 [53504/54000 (99%)] Loss: -1047228.625000\n",
      "    epoch          : 51\n",
      "    loss           : -1058460.4878554503\n",
      "    val_loss       : -1062079.7387721285\n",
      "Train Epoch: 52 [0/54000 (0%)] Loss: -1143116.625000\n",
      "Train Epoch: 52 [1408/54000 (3%)] Loss: -1023949.000000\n",
      "Train Epoch: 52 [2816/54000 (5%)] Loss: -1108946.125000\n",
      "Train Epoch: 52 [4224/54000 (8%)] Loss: -1129084.750000\n",
      "Train Epoch: 52 [5632/54000 (10%)] Loss: -1086629.750000\n",
      "Train Epoch: 52 [7040/54000 (13%)] Loss: -1057771.375000\n",
      "Train Epoch: 52 [8448/54000 (16%)] Loss: -1101143.375000\n",
      "Train Epoch: 52 [9856/54000 (18%)] Loss: -1050724.875000\n",
      "Train Epoch: 52 [11264/54000 (21%)] Loss: -1085633.875000\n",
      "Train Epoch: 52 [12672/54000 (23%)] Loss: -1074674.250000\n",
      "Train Epoch: 52 [14080/54000 (26%)] Loss: -1119072.875000\n",
      "Train Epoch: 52 [15488/54000 (29%)] Loss: -1074679.875000\n",
      "Train Epoch: 52 [16896/54000 (31%)] Loss: -1061995.625000\n",
      "Train Epoch: 52 [18304/54000 (34%)] Loss: -989891.562500\n",
      "Train Epoch: 52 [19712/54000 (37%)] Loss: -1028437.437500\n",
      "Train Epoch: 52 [21120/54000 (39%)] Loss: -1058049.000000\n",
      "Train Epoch: 52 [22528/54000 (42%)] Loss: -1037435.812500\n",
      "Train Epoch: 52 [23936/54000 (44%)] Loss: -1008399.937500\n",
      "Train Epoch: 52 [25344/54000 (47%)] Loss: -1089259.500000\n",
      "Train Epoch: 52 [26752/54000 (50%)] Loss: -1033649.125000\n",
      "Train Epoch: 52 [28160/54000 (52%)] Loss: -1047401.125000\n",
      "Train Epoch: 52 [29568/54000 (55%)] Loss: -1051212.750000\n",
      "Train Epoch: 52 [30976/54000 (57%)] Loss: -1109023.000000\n",
      "Train Epoch: 52 [32384/54000 (60%)] Loss: -1036265.625000\n",
      "Train Epoch: 52 [33792/54000 (63%)] Loss: -1089296.125000\n",
      "Train Epoch: 52 [35200/54000 (65%)] Loss: -1116579.250000\n",
      "Train Epoch: 52 [36608/54000 (68%)] Loss: -1044115.187500\n",
      "Train Epoch: 52 [38016/54000 (70%)] Loss: -1068322.375000\n",
      "Train Epoch: 52 [39424/54000 (73%)] Loss: -1057412.000000\n",
      "Train Epoch: 52 [40832/54000 (76%)] Loss: -1059869.375000\n",
      "Train Epoch: 52 [42240/54000 (78%)] Loss: -1042391.625000\n",
      "Train Epoch: 52 [43648/54000 (81%)] Loss: -1083974.500000\n",
      "Train Epoch: 52 [45056/54000 (83%)] Loss: -1052912.375000\n",
      "Train Epoch: 52 [46464/54000 (86%)] Loss: -1082931.375000\n",
      "Train Epoch: 52 [47872/54000 (89%)] Loss: -1033168.937500\n",
      "Train Epoch: 52 [49280/54000 (91%)] Loss: -1080871.125000\n",
      "Train Epoch: 52 [50688/54000 (94%)] Loss: -1067250.875000\n",
      "Train Epoch: 52 [52096/54000 (96%)] Loss: -1029401.375000\n",
      "Train Epoch: 52 [53504/54000 (99%)] Loss: -1082051.375000\n",
      "    epoch          : 52\n",
      "    loss           : -1059363.0710900475\n",
      "    val_loss       : -1063178.7368631565\n",
      "Train Epoch: 53 [0/54000 (0%)] Loss: -1071797.875000\n",
      "Train Epoch: 53 [1408/54000 (3%)] Loss: -1082271.875000\n",
      "Train Epoch: 53 [2816/54000 (5%)] Loss: -1086446.125000\n",
      "Train Epoch: 53 [4224/54000 (8%)] Loss: -1096272.000000\n",
      "Train Epoch: 53 [5632/54000 (10%)] Loss: -1048751.375000\n",
      "Train Epoch: 53 [7040/54000 (13%)] Loss: -1074653.750000\n",
      "Train Epoch: 53 [8448/54000 (16%)] Loss: -1011494.375000\n",
      "Train Epoch: 53 [9856/54000 (18%)] Loss: -1045294.875000\n",
      "Train Epoch: 53 [11264/54000 (21%)] Loss: -1045067.312500\n",
      "Train Epoch: 53 [12672/54000 (23%)] Loss: -1034715.375000\n",
      "Train Epoch: 53 [14080/54000 (26%)] Loss: -1016034.125000\n",
      "Train Epoch: 53 [15488/54000 (29%)] Loss: -1037609.500000\n",
      "Train Epoch: 53 [16896/54000 (31%)] Loss: -1064139.000000\n",
      "Train Epoch: 53 [18304/54000 (34%)] Loss: -1099977.250000\n",
      "Train Epoch: 53 [19712/54000 (37%)] Loss: -1091769.125000\n",
      "Train Epoch: 53 [21120/54000 (39%)] Loss: -1068189.375000\n",
      "Train Epoch: 53 [22528/54000 (42%)] Loss: -1020915.312500\n",
      "Train Epoch: 53 [23936/54000 (44%)] Loss: -1050922.500000\n",
      "Train Epoch: 53 [25344/54000 (47%)] Loss: -1027176.312500\n",
      "Train Epoch: 53 [26752/54000 (50%)] Loss: -1084589.875000\n",
      "Train Epoch: 53 [28160/54000 (52%)] Loss: -1057162.750000\n",
      "Train Epoch: 53 [29568/54000 (55%)] Loss: -1056067.875000\n",
      "Train Epoch: 53 [30976/54000 (57%)] Loss: -1022565.187500\n",
      "Train Epoch: 53 [32384/54000 (60%)] Loss: -1046620.312500\n",
      "Train Epoch: 53 [33792/54000 (63%)] Loss: -1027014.125000\n",
      "Train Epoch: 53 [35200/54000 (65%)] Loss: -1044233.625000\n",
      "Train Epoch: 53 [36608/54000 (68%)] Loss: -1037850.750000\n",
      "Train Epoch: 53 [38016/54000 (70%)] Loss: -1060608.000000\n",
      "Train Epoch: 53 [39424/54000 (73%)] Loss: -1107712.000000\n",
      "Train Epoch: 53 [40832/54000 (76%)] Loss: -1085888.000000\n",
      "Train Epoch: 53 [42240/54000 (78%)] Loss: -1036557.375000\n",
      "Train Epoch: 53 [43648/54000 (81%)] Loss: -1021910.250000\n",
      "Train Epoch: 53 [45056/54000 (83%)] Loss: -997417.312500\n",
      "Train Epoch: 53 [46464/54000 (86%)] Loss: -1085389.375000\n",
      "Train Epoch: 53 [47872/54000 (89%)] Loss: -1074516.125000\n",
      "Train Epoch: 53 [49280/54000 (91%)] Loss: -997200.625000\n",
      "Train Epoch: 53 [50688/54000 (94%)] Loss: -1048140.937500\n",
      "Train Epoch: 53 [52096/54000 (96%)] Loss: -1111271.875000\n",
      "Train Epoch: 53 [53504/54000 (99%)] Loss: -1060850.250000\n",
      "    epoch          : 53\n",
      "    loss           : -1060031.4813388626\n",
      "    val_loss       : -1064131.1956839054\n",
      "Train Epoch: 54 [0/54000 (0%)] Loss: -1031562.750000\n",
      "Train Epoch: 54 [1408/54000 (3%)] Loss: -1056121.250000\n",
      "Train Epoch: 54 [2816/54000 (5%)] Loss: -1076306.875000\n",
      "Train Epoch: 54 [4224/54000 (8%)] Loss: -1085486.875000\n",
      "Train Epoch: 54 [5632/54000 (10%)] Loss: -1065698.875000\n",
      "Train Epoch: 54 [7040/54000 (13%)] Loss: -1018393.687500\n",
      "Train Epoch: 54 [8448/54000 (16%)] Loss: -1077709.625000\n",
      "Train Epoch: 54 [9856/54000 (18%)] Loss: -969387.500000\n",
      "Train Epoch: 54 [11264/54000 (21%)] Loss: -1016089.625000\n",
      "Train Epoch: 54 [12672/54000 (23%)] Loss: -1110098.125000\n",
      "Train Epoch: 54 [14080/54000 (26%)] Loss: -1072424.500000\n",
      "Train Epoch: 54 [15488/54000 (29%)] Loss: -1074318.125000\n",
      "Train Epoch: 54 [16896/54000 (31%)] Loss: -1084915.250000\n",
      "Train Epoch: 54 [18304/54000 (34%)] Loss: -1010127.250000\n",
      "Train Epoch: 54 [19712/54000 (37%)] Loss: -1108270.375000\n",
      "Train Epoch: 54 [21120/54000 (39%)] Loss: -1062955.500000\n",
      "Train Epoch: 54 [22528/54000 (42%)] Loss: -1064695.750000\n",
      "Train Epoch: 54 [23936/54000 (44%)] Loss: -1020032.937500\n",
      "Train Epoch: 54 [25344/54000 (47%)] Loss: -1054858.250000\n",
      "Train Epoch: 54 [26752/54000 (50%)] Loss: -1077424.500000\n",
      "Train Epoch: 54 [28160/54000 (52%)] Loss: -972845.500000\n",
      "Train Epoch: 54 [29568/54000 (55%)] Loss: -1064683.625000\n",
      "Train Epoch: 54 [30976/54000 (57%)] Loss: -1100618.000000\n",
      "Train Epoch: 54 [32384/54000 (60%)] Loss: -994953.437500\n",
      "Train Epoch: 54 [33792/54000 (63%)] Loss: -1084144.125000\n",
      "Train Epoch: 54 [35200/54000 (65%)] Loss: -1016368.562500\n",
      "Train Epoch: 54 [36608/54000 (68%)] Loss: -1084775.750000\n",
      "Train Epoch: 54 [38016/54000 (70%)] Loss: -1087218.500000\n",
      "Train Epoch: 54 [39424/54000 (73%)] Loss: -1024513.000000\n",
      "Train Epoch: 54 [40832/54000 (76%)] Loss: -1100163.625000\n",
      "Train Epoch: 54 [42240/54000 (78%)] Loss: -1092843.250000\n",
      "Train Epoch: 54 [43648/54000 (81%)] Loss: -1081075.000000\n",
      "Train Epoch: 54 [45056/54000 (83%)] Loss: -1028453.187500\n",
      "Train Epoch: 54 [46464/54000 (86%)] Loss: -1064997.375000\n",
      "Train Epoch: 54 [47872/54000 (89%)] Loss: -1066433.875000\n",
      "Train Epoch: 54 [49280/54000 (91%)] Loss: -1045670.000000\n",
      "Train Epoch: 54 [50688/54000 (94%)] Loss: -1065032.500000\n",
      "Train Epoch: 54 [52096/54000 (96%)] Loss: -1081146.500000\n",
      "Train Epoch: 54 [53504/54000 (99%)] Loss: -1129022.000000\n",
      "    epoch          : 54\n",
      "    loss           : -1060883.9571978673\n",
      "    val_loss       : -1064557.3007007355\n",
      "Train Epoch: 55 [0/54000 (0%)] Loss: -1040082.875000\n",
      "Train Epoch: 55 [1408/54000 (3%)] Loss: -1073393.125000\n",
      "Train Epoch: 55 [2816/54000 (5%)] Loss: -1119705.500000\n",
      "Train Epoch: 55 [4224/54000 (8%)] Loss: -1015753.375000\n",
      "Train Epoch: 55 [5632/54000 (10%)] Loss: -1073538.500000\n",
      "Train Epoch: 55 [7040/54000 (13%)] Loss: -1099484.500000\n",
      "Train Epoch: 55 [8448/54000 (16%)] Loss: -1062571.875000\n",
      "Train Epoch: 55 [9856/54000 (18%)] Loss: -1070628.375000\n",
      "Train Epoch: 55 [11264/54000 (21%)] Loss: -1061001.000000\n",
      "Train Epoch: 55 [12672/54000 (23%)] Loss: -1006918.062500\n",
      "Train Epoch: 55 [14080/54000 (26%)] Loss: -1060716.000000\n",
      "Train Epoch: 55 [15488/54000 (29%)] Loss: -1076519.500000\n",
      "Train Epoch: 55 [16896/54000 (31%)] Loss: -1090200.750000\n",
      "Train Epoch: 55 [18304/54000 (34%)] Loss: -1052245.625000\n",
      "Train Epoch: 55 [19712/54000 (37%)] Loss: -1095837.000000\n",
      "Train Epoch: 55 [21120/54000 (39%)] Loss: -1072352.375000\n",
      "Train Epoch: 55 [22528/54000 (42%)] Loss: -1065544.125000\n",
      "Train Epoch: 55 [23936/54000 (44%)] Loss: -1083748.625000\n",
      "Train Epoch: 55 [25344/54000 (47%)] Loss: -1082883.625000\n",
      "Train Epoch: 55 [26752/54000 (50%)] Loss: -1102054.375000\n",
      "Train Epoch: 55 [28160/54000 (52%)] Loss: -1083753.750000\n",
      "Train Epoch: 55 [29568/54000 (55%)] Loss: -1070190.750000\n",
      "Train Epoch: 55 [30976/54000 (57%)] Loss: -1098530.500000\n",
      "Train Epoch: 55 [32384/54000 (60%)] Loss: -1060258.500000\n",
      "Train Epoch: 55 [33792/54000 (63%)] Loss: -1076038.625000\n",
      "Train Epoch: 55 [35200/54000 (65%)] Loss: -1044331.187500\n",
      "Train Epoch: 55 [36608/54000 (68%)] Loss: -1071101.625000\n",
      "Train Epoch: 55 [38016/54000 (70%)] Loss: -1054189.000000\n",
      "Train Epoch: 55 [39424/54000 (73%)] Loss: -1052898.125000\n",
      "Train Epoch: 55 [40832/54000 (76%)] Loss: -1034823.000000\n",
      "Train Epoch: 55 [42240/54000 (78%)] Loss: -1086617.250000\n",
      "Train Epoch: 55 [43648/54000 (81%)] Loss: -1045976.750000\n",
      "Train Epoch: 55 [45056/54000 (83%)] Loss: -1069853.000000\n",
      "Train Epoch: 55 [46464/54000 (86%)] Loss: -1063523.000000\n",
      "Train Epoch: 55 [47872/54000 (89%)] Loss: -1143286.750000\n",
      "Train Epoch: 55 [49280/54000 (91%)] Loss: -1077982.375000\n",
      "Train Epoch: 55 [50688/54000 (94%)] Loss: -1039737.000000\n",
      "Train Epoch: 55 [52096/54000 (96%)] Loss: -1070014.000000\n",
      "Train Epoch: 55 [53504/54000 (99%)] Loss: -1052725.125000\n",
      "    epoch          : 55\n",
      "    loss           : -1061742.9506812796\n",
      "    val_loss       : -1065191.9575506982\n",
      "Train Epoch: 56 [0/54000 (0%)] Loss: -1103126.375000\n",
      "Train Epoch: 56 [1408/54000 (3%)] Loss: -1035701.437500\n",
      "Train Epoch: 56 [2816/54000 (5%)] Loss: -1079050.750000\n",
      "Train Epoch: 56 [4224/54000 (8%)] Loss: -1063340.875000\n",
      "Train Epoch: 56 [5632/54000 (10%)] Loss: -1075461.125000\n",
      "Train Epoch: 56 [7040/54000 (13%)] Loss: -1075490.750000\n",
      "Train Epoch: 56 [8448/54000 (16%)] Loss: -1032158.937500\n",
      "Train Epoch: 56 [9856/54000 (18%)] Loss: -1033081.687500\n",
      "Train Epoch: 56 [11264/54000 (21%)] Loss: -987827.812500\n",
      "Train Epoch: 56 [12672/54000 (23%)] Loss: -1116739.125000\n",
      "Train Epoch: 56 [14080/54000 (26%)] Loss: -1049487.750000\n",
      "Train Epoch: 56 [15488/54000 (29%)] Loss: -1101669.750000\n",
      "Train Epoch: 56 [16896/54000 (31%)] Loss: -1073455.125000\n",
      "Train Epoch: 56 [18304/54000 (34%)] Loss: -1095733.750000\n",
      "Train Epoch: 56 [19712/54000 (37%)] Loss: -1076014.875000\n",
      "Train Epoch: 56 [21120/54000 (39%)] Loss: -1025662.562500\n",
      "Train Epoch: 56 [22528/54000 (42%)] Loss: -1019724.812500\n",
      "Train Epoch: 56 [23936/54000 (44%)] Loss: -1091370.875000\n",
      "Train Epoch: 56 [25344/54000 (47%)] Loss: -1087425.125000\n",
      "Train Epoch: 56 [26752/54000 (50%)] Loss: -1058176.125000\n",
      "Train Epoch: 56 [28160/54000 (52%)] Loss: -1078450.750000\n",
      "Train Epoch: 56 [29568/54000 (55%)] Loss: -1106377.500000\n",
      "Train Epoch: 56 [30976/54000 (57%)] Loss: -986248.437500\n",
      "Train Epoch: 56 [32384/54000 (60%)] Loss: -1075962.500000\n",
      "Train Epoch: 56 [33792/54000 (63%)] Loss: -1019090.000000\n",
      "Train Epoch: 56 [35200/54000 (65%)] Loss: -1128619.375000\n",
      "Train Epoch: 56 [36608/54000 (68%)] Loss: -1083116.875000\n",
      "Train Epoch: 56 [38016/54000 (70%)] Loss: -1022303.562500\n",
      "Train Epoch: 56 [39424/54000 (73%)] Loss: -1098437.750000\n",
      "Train Epoch: 56 [40832/54000 (76%)] Loss: -1011096.375000\n",
      "Train Epoch: 56 [42240/54000 (78%)] Loss: -1063942.625000\n",
      "Train Epoch: 56 [43648/54000 (81%)] Loss: -1063916.500000\n",
      "Train Epoch: 56 [45056/54000 (83%)] Loss: -1051423.125000\n",
      "Train Epoch: 56 [46464/54000 (86%)] Loss: -1048491.000000\n",
      "Train Epoch: 56 [47872/54000 (89%)] Loss: -1081616.750000\n",
      "Train Epoch: 56 [49280/54000 (91%)] Loss: -1038246.875000\n",
      "Train Epoch: 56 [50688/54000 (94%)] Loss: -1088519.375000\n",
      "Train Epoch: 56 [52096/54000 (96%)] Loss: -1038502.687500\n",
      "Train Epoch: 56 [53504/54000 (99%)] Loss: -1029892.500000\n",
      "    epoch          : 56\n",
      "    loss           : -1062572.2708827015\n",
      "    val_loss       : -1066458.1237766996\n",
      "Train Epoch: 57 [0/54000 (0%)] Loss: -1071180.125000\n",
      "Train Epoch: 57 [1408/54000 (3%)] Loss: -1129097.500000\n",
      "Train Epoch: 57 [2816/54000 (5%)] Loss: -1081833.375000\n",
      "Train Epoch: 57 [4224/54000 (8%)] Loss: -1066767.250000\n",
      "Train Epoch: 57 [5632/54000 (10%)] Loss: -1047364.500000\n",
      "Train Epoch: 57 [7040/54000 (13%)] Loss: -1000963.562500\n",
      "Train Epoch: 57 [8448/54000 (16%)] Loss: -1035578.937500\n",
      "Train Epoch: 57 [9856/54000 (18%)] Loss: -1070364.750000\n",
      "Train Epoch: 57 [11264/54000 (21%)] Loss: -1062208.125000\n",
      "Train Epoch: 57 [12672/54000 (23%)] Loss: -1095301.875000\n",
      "Train Epoch: 57 [14080/54000 (26%)] Loss: -1031189.062500\n",
      "Train Epoch: 57 [15488/54000 (29%)] Loss: -1055100.000000\n",
      "Train Epoch: 57 [16896/54000 (31%)] Loss: -1050471.000000\n",
      "Train Epoch: 57 [18304/54000 (34%)] Loss: -1067109.875000\n",
      "Train Epoch: 57 [19712/54000 (37%)] Loss: -1029215.437500\n",
      "Train Epoch: 57 [21120/54000 (39%)] Loss: -1103449.500000\n",
      "Train Epoch: 57 [22528/54000 (42%)] Loss: -1091969.250000\n",
      "Train Epoch: 57 [23936/54000 (44%)] Loss: -1118990.750000\n",
      "Train Epoch: 57 [25344/54000 (47%)] Loss: -1057869.250000\n",
      "Train Epoch: 57 [26752/54000 (50%)] Loss: -1066979.625000\n",
      "Train Epoch: 57 [28160/54000 (52%)] Loss: -1018598.125000\n",
      "Train Epoch: 57 [29568/54000 (55%)] Loss: -1043115.625000\n",
      "Train Epoch: 57 [30976/54000 (57%)] Loss: -1041091.375000\n",
      "Train Epoch: 57 [32384/54000 (60%)] Loss: -1014540.062500\n",
      "Train Epoch: 57 [33792/54000 (63%)] Loss: -1034893.812500\n",
      "Train Epoch: 57 [35200/54000 (65%)] Loss: -1111663.250000\n",
      "Train Epoch: 57 [36608/54000 (68%)] Loss: -1042030.500000\n",
      "Train Epoch: 57 [38016/54000 (70%)] Loss: -1025581.750000\n",
      "Train Epoch: 57 [39424/54000 (73%)] Loss: -1051104.875000\n",
      "Train Epoch: 57 [40832/54000 (76%)] Loss: -1095248.500000\n",
      "Train Epoch: 57 [42240/54000 (78%)] Loss: -1079452.000000\n",
      "Train Epoch: 57 [43648/54000 (81%)] Loss: -1072331.375000\n",
      "Train Epoch: 57 [45056/54000 (83%)] Loss: -1084781.750000\n",
      "Train Epoch: 57 [46464/54000 (86%)] Loss: -1051299.625000\n",
      "Train Epoch: 57 [47872/54000 (89%)] Loss: -1083362.750000\n",
      "Train Epoch: 57 [49280/54000 (91%)] Loss: -1027883.062500\n",
      "Train Epoch: 57 [50688/54000 (94%)] Loss: -1138271.125000\n",
      "Train Epoch: 57 [52096/54000 (96%)] Loss: -1049911.125000\n",
      "Train Epoch: 57 [53504/54000 (99%)] Loss: -1022834.375000\n",
      "    epoch          : 57\n",
      "    loss           : -1063113.507257109\n",
      "    val_loss       : -1066646.4743652344\n",
      "Train Epoch: 58 [0/54000 (0%)] Loss: -1052059.750000\n",
      "Train Epoch: 58 [1408/54000 (3%)] Loss: -1056393.625000\n",
      "Train Epoch: 58 [2816/54000 (5%)] Loss: -1041580.062500\n",
      "Train Epoch: 58 [4224/54000 (8%)] Loss: -1029536.125000\n",
      "Train Epoch: 58 [5632/54000 (10%)] Loss: -1044036.625000\n",
      "Train Epoch: 58 [7040/54000 (13%)] Loss: -1056646.375000\n",
      "Train Epoch: 58 [8448/54000 (16%)] Loss: -1072443.500000\n",
      "Train Epoch: 58 [9856/54000 (18%)] Loss: -1098908.875000\n",
      "Train Epoch: 58 [11264/54000 (21%)] Loss: -1066868.000000\n",
      "Train Epoch: 58 [12672/54000 (23%)] Loss: -1032658.687500\n",
      "Train Epoch: 58 [14080/54000 (26%)] Loss: -1010125.687500\n",
      "Train Epoch: 58 [15488/54000 (29%)] Loss: -1028934.875000\n",
      "Train Epoch: 58 [16896/54000 (31%)] Loss: -1034019.187500\n",
      "Train Epoch: 58 [18304/54000 (34%)] Loss: -1064396.250000\n",
      "Train Epoch: 58 [19712/54000 (37%)] Loss: -1100059.125000\n",
      "Train Epoch: 58 [21120/54000 (39%)] Loss: -1076546.125000\n",
      "Train Epoch: 58 [22528/54000 (42%)] Loss: -1118017.750000\n",
      "Train Epoch: 58 [23936/54000 (44%)] Loss: -1096820.875000\n",
      "Train Epoch: 58 [25344/54000 (47%)] Loss: -1031701.062500\n",
      "Train Epoch: 58 [26752/54000 (50%)] Loss: -1040843.250000\n",
      "Train Epoch: 58 [28160/54000 (52%)] Loss: -1082975.000000\n",
      "Train Epoch: 58 [29568/54000 (55%)] Loss: -1037279.562500\n",
      "Train Epoch: 58 [30976/54000 (57%)] Loss: -1020635.250000\n",
      "Train Epoch: 58 [32384/54000 (60%)] Loss: -1067767.250000\n",
      "Train Epoch: 58 [33792/54000 (63%)] Loss: -1084020.750000\n",
      "Train Epoch: 58 [35200/54000 (65%)] Loss: -1056813.875000\n",
      "Train Epoch: 58 [36608/54000 (68%)] Loss: -1106246.000000\n",
      "Train Epoch: 58 [38016/54000 (70%)] Loss: -990582.687500\n",
      "Train Epoch: 58 [39424/54000 (73%)] Loss: -1085983.000000\n",
      "Train Epoch: 58 [40832/54000 (76%)] Loss: -1052622.875000\n",
      "Train Epoch: 58 [42240/54000 (78%)] Loss: -1109006.875000\n",
      "Train Epoch: 58 [43648/54000 (81%)] Loss: -1065117.375000\n",
      "Train Epoch: 58 [45056/54000 (83%)] Loss: -1084540.375000\n",
      "Train Epoch: 58 [46464/54000 (86%)] Loss: -1050155.875000\n",
      "Train Epoch: 58 [47872/54000 (89%)] Loss: -1115697.250000\n",
      "Train Epoch: 58 [49280/54000 (91%)] Loss: -1033989.750000\n",
      "Train Epoch: 58 [50688/54000 (94%)] Loss: -1053351.000000\n",
      "Train Epoch: 58 [52096/54000 (96%)] Loss: -1044537.312500\n",
      "Train Epoch: 58 [53504/54000 (99%)] Loss: -1114096.375000\n",
      "    epoch          : 58\n",
      "    loss           : -1063904.0359893364\n",
      "    val_loss       : -1067445.262861536\n",
      "Train Epoch: 59 [0/54000 (0%)] Loss: -1061925.125000\n",
      "Train Epoch: 59 [1408/54000 (3%)] Loss: -1101521.750000\n",
      "Train Epoch: 59 [2816/54000 (5%)] Loss: -1090519.375000\n",
      "Train Epoch: 59 [4224/54000 (8%)] Loss: -1072122.125000\n",
      "Train Epoch: 59 [5632/54000 (10%)] Loss: -1030927.812500\n",
      "Train Epoch: 59 [7040/54000 (13%)] Loss: -1046939.062500\n",
      "Train Epoch: 59 [8448/54000 (16%)] Loss: -1079704.375000\n",
      "Train Epoch: 59 [9856/54000 (18%)] Loss: -1009733.812500\n",
      "Train Epoch: 59 [11264/54000 (21%)] Loss: -1113501.375000\n",
      "Train Epoch: 59 [12672/54000 (23%)] Loss: -1089477.500000\n",
      "Train Epoch: 59 [14080/54000 (26%)] Loss: -1035638.625000\n",
      "Train Epoch: 59 [15488/54000 (29%)] Loss: -1077785.625000\n",
      "Train Epoch: 59 [16896/54000 (31%)] Loss: -1120804.250000\n",
      "Train Epoch: 59 [18304/54000 (34%)] Loss: -1149338.500000\n",
      "Train Epoch: 59 [19712/54000 (37%)] Loss: -1081514.375000\n",
      "Train Epoch: 59 [21120/54000 (39%)] Loss: -1027553.000000\n",
      "Train Epoch: 59 [22528/54000 (42%)] Loss: -1064489.875000\n",
      "Train Epoch: 59 [23936/54000 (44%)] Loss: -1047237.250000\n",
      "Train Epoch: 59 [25344/54000 (47%)] Loss: -1018398.687500\n",
      "Train Epoch: 59 [26752/54000 (50%)] Loss: -1104476.375000\n",
      "Train Epoch: 59 [28160/54000 (52%)] Loss: -1084884.250000\n",
      "Train Epoch: 59 [29568/54000 (55%)] Loss: -1043266.687500\n",
      "Train Epoch: 59 [30976/54000 (57%)] Loss: -1078032.500000\n",
      "Train Epoch: 59 [32384/54000 (60%)] Loss: -1058633.750000\n",
      "Train Epoch: 59 [33792/54000 (63%)] Loss: -1072766.000000\n",
      "Train Epoch: 59 [35200/54000 (65%)] Loss: -1138725.750000\n",
      "Train Epoch: 59 [36608/54000 (68%)] Loss: -991846.437500\n",
      "Train Epoch: 59 [38016/54000 (70%)] Loss: -1101125.625000\n",
      "Train Epoch: 59 [39424/54000 (73%)] Loss: -1016371.500000\n",
      "Train Epoch: 59 [40832/54000 (76%)] Loss: -1089616.250000\n",
      "Train Epoch: 59 [42240/54000 (78%)] Loss: -1036876.437500\n",
      "Train Epoch: 59 [43648/54000 (81%)] Loss: -1074046.875000\n",
      "Train Epoch: 59 [45056/54000 (83%)] Loss: -1071326.875000\n",
      "Train Epoch: 59 [46464/54000 (86%)] Loss: -1089909.375000\n",
      "Train Epoch: 59 [47872/54000 (89%)] Loss: -1013412.437500\n",
      "Train Epoch: 59 [49280/54000 (91%)] Loss: -1101963.625000\n",
      "Train Epoch: 59 [50688/54000 (94%)] Loss: -1068639.000000\n",
      "Train Epoch: 59 [52096/54000 (96%)] Loss: -1024796.312500\n",
      "Train Epoch: 59 [53504/54000 (99%)] Loss: -1011050.062500\n",
      "    epoch          : 59\n",
      "    loss           : -1064474.9454976304\n",
      "    val_loss       : -1068058.7848991232\n",
      "Train Epoch: 60 [0/54000 (0%)] Loss: -1092242.375000\n",
      "Train Epoch: 60 [1408/54000 (3%)] Loss: -1088270.250000\n",
      "Train Epoch: 60 [2816/54000 (5%)] Loss: -1104724.375000\n",
      "Train Epoch: 60 [4224/54000 (8%)] Loss: -1102701.000000\n",
      "Train Epoch: 60 [5632/54000 (10%)] Loss: -1050431.375000\n",
      "Train Epoch: 60 [7040/54000 (13%)] Loss: -1080930.250000\n",
      "Train Epoch: 60 [8448/54000 (16%)] Loss: -1062142.625000\n",
      "Train Epoch: 60 [9856/54000 (18%)] Loss: -1089683.875000\n",
      "Train Epoch: 60 [11264/54000 (21%)] Loss: -1082556.875000\n",
      "Train Epoch: 60 [12672/54000 (23%)] Loss: -1058824.375000\n",
      "Train Epoch: 60 [14080/54000 (26%)] Loss: -1101555.875000\n",
      "Train Epoch: 60 [15488/54000 (29%)] Loss: -1066425.000000\n",
      "Train Epoch: 60 [16896/54000 (31%)] Loss: -1101735.000000\n",
      "Train Epoch: 60 [18304/54000 (34%)] Loss: -982175.062500\n",
      "Train Epoch: 60 [19712/54000 (37%)] Loss: -1083542.875000\n",
      "Train Epoch: 60 [21120/54000 (39%)] Loss: -1040298.812500\n",
      "Train Epoch: 60 [22528/54000 (42%)] Loss: -1099283.375000\n",
      "Train Epoch: 60 [23936/54000 (44%)] Loss: -1030610.062500\n",
      "Train Epoch: 60 [25344/54000 (47%)] Loss: -1084649.125000\n",
      "Train Epoch: 60 [26752/54000 (50%)] Loss: -1095024.375000\n",
      "Train Epoch: 60 [28160/54000 (52%)] Loss: -1044227.625000\n",
      "Train Epoch: 60 [29568/54000 (55%)] Loss: -1032074.625000\n",
      "Train Epoch: 60 [30976/54000 (57%)] Loss: -1048785.750000\n",
      "Train Epoch: 60 [32384/54000 (60%)] Loss: -1096522.250000\n",
      "Train Epoch: 60 [33792/54000 (63%)] Loss: -1058621.375000\n",
      "Train Epoch: 60 [35200/54000 (65%)] Loss: -1036636.187500\n",
      "Train Epoch: 60 [36608/54000 (68%)] Loss: -1070615.000000\n",
      "Train Epoch: 60 [38016/54000 (70%)] Loss: -1095036.625000\n",
      "Train Epoch: 60 [39424/54000 (73%)] Loss: -1041952.875000\n",
      "Train Epoch: 60 [40832/54000 (76%)] Loss: -1058144.750000\n",
      "Train Epoch: 60 [42240/54000 (78%)] Loss: -1006628.750000\n",
      "Train Epoch: 60 [43648/54000 (81%)] Loss: -1067144.625000\n",
      "Train Epoch: 60 [45056/54000 (83%)] Loss: -1040252.125000\n",
      "Train Epoch: 60 [46464/54000 (86%)] Loss: -1068169.250000\n",
      "Train Epoch: 60 [47872/54000 (89%)] Loss: -1041473.125000\n",
      "Train Epoch: 60 [49280/54000 (91%)] Loss: -1046348.687500\n",
      "Train Epoch: 60 [50688/54000 (94%)] Loss: -1083921.250000\n",
      "Train Epoch: 60 [52096/54000 (96%)] Loss: -1069569.000000\n",
      "Train Epoch: 60 [53504/54000 (99%)] Loss: -1103698.125000\n",
      "    epoch          : 60\n",
      "    loss           : -1065083.2591824646\n",
      "    val_loss       : -1068684.7299648854\n",
      "Train Epoch: 61 [0/54000 (0%)] Loss: -1105753.625000\n",
      "Train Epoch: 61 [1408/54000 (3%)] Loss: -1066410.250000\n",
      "Train Epoch: 61 [2816/54000 (5%)] Loss: -1058587.000000\n",
      "Train Epoch: 61 [4224/54000 (8%)] Loss: -1028289.437500\n",
      "Train Epoch: 61 [5632/54000 (10%)] Loss: -1034816.250000\n",
      "Train Epoch: 61 [7040/54000 (13%)] Loss: -1042886.062500\n",
      "Train Epoch: 61 [8448/54000 (16%)] Loss: -1000335.625000\n",
      "Train Epoch: 61 [9856/54000 (18%)] Loss: -1089188.500000\n",
      "Train Epoch: 61 [11264/54000 (21%)] Loss: -1047741.250000\n",
      "Train Epoch: 61 [12672/54000 (23%)] Loss: -1037324.250000\n",
      "Train Epoch: 61 [14080/54000 (26%)] Loss: -1035324.625000\n",
      "Train Epoch: 61 [15488/54000 (29%)] Loss: -1100369.750000\n",
      "Train Epoch: 61 [16896/54000 (31%)] Loss: -1050076.625000\n",
      "Train Epoch: 61 [18304/54000 (34%)] Loss: -1054784.375000\n",
      "Train Epoch: 61 [19712/54000 (37%)] Loss: -1056741.625000\n",
      "Train Epoch: 61 [21120/54000 (39%)] Loss: -1064806.250000\n",
      "Train Epoch: 61 [22528/54000 (42%)] Loss: -1102127.250000\n",
      "Train Epoch: 61 [23936/54000 (44%)] Loss: -1074140.250000\n",
      "Train Epoch: 61 [25344/54000 (47%)] Loss: -1103118.375000\n",
      "Train Epoch: 61 [26752/54000 (50%)] Loss: -1051844.375000\n",
      "Train Epoch: 61 [28160/54000 (52%)] Loss: -1112128.250000\n",
      "Train Epoch: 61 [29568/54000 (55%)] Loss: -1100351.125000\n",
      "Train Epoch: 61 [30976/54000 (57%)] Loss: -1078277.625000\n",
      "Train Epoch: 61 [32384/54000 (60%)] Loss: -993926.625000\n",
      "Train Epoch: 61 [33792/54000 (63%)] Loss: -1044556.250000\n",
      "Train Epoch: 61 [35200/54000 (65%)] Loss: -1095451.125000\n",
      "Train Epoch: 61 [36608/54000 (68%)] Loss: -1055854.000000\n",
      "Train Epoch: 61 [38016/54000 (70%)] Loss: -1085099.750000\n",
      "Train Epoch: 61 [39424/54000 (73%)] Loss: -1040434.125000\n",
      "Train Epoch: 61 [40832/54000 (76%)] Loss: -1032417.437500\n",
      "Train Epoch: 61 [42240/54000 (78%)] Loss: -1050451.125000\n",
      "Train Epoch: 61 [43648/54000 (81%)] Loss: -1074746.500000\n",
      "Train Epoch: 61 [45056/54000 (83%)] Loss: -1026000.062500\n",
      "Train Epoch: 61 [46464/54000 (86%)] Loss: -1079122.125000\n",
      "Train Epoch: 61 [47872/54000 (89%)] Loss: -1108262.875000\n",
      "Train Epoch: 61 [49280/54000 (91%)] Loss: -1069508.625000\n",
      "Train Epoch: 61 [50688/54000 (94%)] Loss: -1011659.687500\n",
      "Train Epoch: 61 [52096/54000 (96%)] Loss: -1050054.625000\n",
      "Train Epoch: 61 [53504/54000 (99%)] Loss: -1045370.000000\n",
      "    epoch          : 61\n",
      "    loss           : -1065697.8037618482\n",
      "    val_loss       : -1069168.5368678316\n",
      "Train Epoch: 62 [0/54000 (0%)] Loss: -1085246.000000\n",
      "Train Epoch: 62 [1408/54000 (3%)] Loss: -1001989.000000\n",
      "Train Epoch: 62 [2816/54000 (5%)] Loss: -1065427.000000\n",
      "Train Epoch: 62 [4224/54000 (8%)] Loss: -1047414.187500\n",
      "Train Epoch: 62 [5632/54000 (10%)] Loss: -1098490.375000\n",
      "Train Epoch: 62 [7040/54000 (13%)] Loss: -1050242.125000\n",
      "Train Epoch: 62 [8448/54000 (16%)] Loss: -1063572.625000\n",
      "Train Epoch: 62 [9856/54000 (18%)] Loss: -1118451.000000\n",
      "Train Epoch: 62 [11264/54000 (21%)] Loss: -1075558.625000\n",
      "Train Epoch: 62 [12672/54000 (23%)] Loss: -1035937.250000\n",
      "Train Epoch: 62 [14080/54000 (26%)] Loss: -1087512.750000\n",
      "Train Epoch: 62 [15488/54000 (29%)] Loss: -1078201.875000\n",
      "Train Epoch: 62 [16896/54000 (31%)] Loss: -1137388.500000\n",
      "Train Epoch: 62 [18304/54000 (34%)] Loss: -1057405.250000\n",
      "Train Epoch: 62 [19712/54000 (37%)] Loss: -1042770.000000\n",
      "Train Epoch: 62 [21120/54000 (39%)] Loss: -1031361.062500\n",
      "Train Epoch: 62 [22528/54000 (42%)] Loss: -1094232.125000\n",
      "Train Epoch: 62 [23936/54000 (44%)] Loss: -1043892.437500\n",
      "Train Epoch: 62 [25344/54000 (47%)] Loss: -1034204.125000\n",
      "Train Epoch: 62 [26752/54000 (50%)] Loss: -1061624.000000\n",
      "Train Epoch: 62 [28160/54000 (52%)] Loss: -1089872.750000\n",
      "Train Epoch: 62 [29568/54000 (55%)] Loss: -1086324.750000\n",
      "Train Epoch: 62 [30976/54000 (57%)] Loss: -1032482.000000\n",
      "Train Epoch: 62 [32384/54000 (60%)] Loss: -1060611.375000\n",
      "Train Epoch: 62 [33792/54000 (63%)] Loss: -1075624.875000\n",
      "Train Epoch: 62 [35200/54000 (65%)] Loss: -1077204.125000\n",
      "Train Epoch: 62 [36608/54000 (68%)] Loss: -1006466.062500\n",
      "Train Epoch: 62 [38016/54000 (70%)] Loss: -1097390.500000\n",
      "Train Epoch: 62 [39424/54000 (73%)] Loss: -1029059.375000\n",
      "Train Epoch: 62 [40832/54000 (76%)] Loss: -1077571.750000\n",
      "Train Epoch: 62 [42240/54000 (78%)] Loss: -1072539.875000\n",
      "Train Epoch: 62 [43648/54000 (81%)] Loss: -1059521.250000\n",
      "Train Epoch: 62 [45056/54000 (83%)] Loss: -1022929.750000\n",
      "Train Epoch: 62 [46464/54000 (86%)] Loss: -1048771.250000\n",
      "Train Epoch: 62 [47872/54000 (89%)] Loss: -1048405.500000\n",
      "Train Epoch: 62 [49280/54000 (91%)] Loss: -1062887.750000\n",
      "Train Epoch: 62 [50688/54000 (94%)] Loss: -1107151.250000\n",
      "Train Epoch: 62 [52096/54000 (96%)] Loss: -1060708.500000\n",
      "Train Epoch: 62 [53504/54000 (99%)] Loss: -1078208.250000\n",
      "    epoch          : 62\n",
      "    loss           : -1066322.7585900475\n",
      "    val_loss       : -1069897.7457794838\n",
      "Train Epoch: 63 [0/54000 (0%)] Loss: -1087580.875000\n",
      "Train Epoch: 63 [1408/54000 (3%)] Loss: -1080017.500000\n",
      "Train Epoch: 63 [2816/54000 (5%)] Loss: -1048471.437500\n",
      "Train Epoch: 63 [4224/54000 (8%)] Loss: -1032118.625000\n",
      "Train Epoch: 63 [5632/54000 (10%)] Loss: -1070621.750000\n",
      "Train Epoch: 63 [7040/54000 (13%)] Loss: -1056926.500000\n",
      "Train Epoch: 63 [8448/54000 (16%)] Loss: -1093136.875000\n",
      "Train Epoch: 63 [9856/54000 (18%)] Loss: -1064164.750000\n",
      "Train Epoch: 63 [11264/54000 (21%)] Loss: -1069597.375000\n",
      "Train Epoch: 63 [12672/54000 (23%)] Loss: -1065359.625000\n",
      "Train Epoch: 63 [14080/54000 (26%)] Loss: -1088640.000000\n",
      "Train Epoch: 63 [15488/54000 (29%)] Loss: -1033477.250000\n",
      "Train Epoch: 63 [16896/54000 (31%)] Loss: -1094974.500000\n",
      "Train Epoch: 63 [18304/54000 (34%)] Loss: -1055316.375000\n",
      "Train Epoch: 63 [19712/54000 (37%)] Loss: -1030886.187500\n",
      "Train Epoch: 63 [21120/54000 (39%)] Loss: -1079914.250000\n",
      "Train Epoch: 63 [22528/54000 (42%)] Loss: -1058372.000000\n",
      "Train Epoch: 63 [23936/54000 (44%)] Loss: -1081724.875000\n",
      "Train Epoch: 63 [25344/54000 (47%)] Loss: -1073072.750000\n",
      "Train Epoch: 63 [26752/54000 (50%)] Loss: -1048990.625000\n",
      "Train Epoch: 63 [28160/54000 (52%)] Loss: -1079151.375000\n",
      "Train Epoch: 63 [29568/54000 (55%)] Loss: -1128604.375000\n",
      "Train Epoch: 63 [30976/54000 (57%)] Loss: -1056130.375000\n",
      "Train Epoch: 63 [32384/54000 (60%)] Loss: -1038421.375000\n",
      "Train Epoch: 63 [33792/54000 (63%)] Loss: -1080604.375000\n",
      "Train Epoch: 63 [35200/54000 (65%)] Loss: -1093282.625000\n",
      "Train Epoch: 63 [36608/54000 (68%)] Loss: -1005662.312500\n",
      "Train Epoch: 63 [38016/54000 (70%)] Loss: -1128522.750000\n",
      "Train Epoch: 63 [39424/54000 (73%)] Loss: -1034250.437500\n",
      "Train Epoch: 63 [40832/54000 (76%)] Loss: -1073015.500000\n",
      "Train Epoch: 63 [42240/54000 (78%)] Loss: -1059730.625000\n",
      "Train Epoch: 63 [43648/54000 (81%)] Loss: -1057790.125000\n",
      "Train Epoch: 63 [45056/54000 (83%)] Loss: -1078850.375000\n",
      "Train Epoch: 63 [46464/54000 (86%)] Loss: -1044299.312500\n",
      "Train Epoch: 63 [47872/54000 (89%)] Loss: -1015790.125000\n",
      "Train Epoch: 63 [49280/54000 (91%)] Loss: -1022105.500000\n",
      "Train Epoch: 63 [50688/54000 (94%)] Loss: -1112749.125000\n",
      "Train Epoch: 63 [52096/54000 (96%)] Loss: -1099037.625000\n",
      "Train Epoch: 63 [53504/54000 (99%)] Loss: -1056189.125000\n",
      "    epoch          : 63\n",
      "    loss           : -1067014.948163507\n",
      "    val_loss       : -1070271.9330379404\n",
      "Train Epoch: 64 [0/54000 (0%)] Loss: -1113084.625000\n",
      "Train Epoch: 64 [1408/54000 (3%)] Loss: -1032928.000000\n",
      "Train Epoch: 64 [2816/54000 (5%)] Loss: -1084391.625000\n",
      "Train Epoch: 64 [4224/54000 (8%)] Loss: -1066944.875000\n",
      "Train Epoch: 64 [5632/54000 (10%)] Loss: -1110191.000000\n",
      "Train Epoch: 64 [7040/54000 (13%)] Loss: -1041585.187500\n",
      "Train Epoch: 64 [8448/54000 (16%)] Loss: -1060357.750000\n",
      "Train Epoch: 64 [9856/54000 (18%)] Loss: -1104892.250000\n",
      "Train Epoch: 64 [11264/54000 (21%)] Loss: -1068041.375000\n",
      "Train Epoch: 64 [12672/54000 (23%)] Loss: -1063619.375000\n",
      "Train Epoch: 64 [14080/54000 (26%)] Loss: -1072192.375000\n",
      "Train Epoch: 64 [15488/54000 (29%)] Loss: -1080471.500000\n",
      "Train Epoch: 64 [16896/54000 (31%)] Loss: -1050930.000000\n",
      "Train Epoch: 64 [18304/54000 (34%)] Loss: -1132077.000000\n",
      "Train Epoch: 64 [19712/54000 (37%)] Loss: -1142122.375000\n",
      "Train Epoch: 64 [21120/54000 (39%)] Loss: -984160.187500\n",
      "Train Epoch: 64 [22528/54000 (42%)] Loss: -1050265.750000\n",
      "Train Epoch: 64 [23936/54000 (44%)] Loss: -1127880.375000\n",
      "Train Epoch: 64 [25344/54000 (47%)] Loss: -1070203.500000\n",
      "Train Epoch: 64 [26752/54000 (50%)] Loss: -1128369.500000\n",
      "Train Epoch: 64 [28160/54000 (52%)] Loss: -1023481.500000\n",
      "Train Epoch: 64 [29568/54000 (55%)] Loss: -1061339.500000\n",
      "Train Epoch: 64 [30976/54000 (57%)] Loss: -1046796.500000\n",
      "Train Epoch: 64 [32384/54000 (60%)] Loss: -1091429.500000\n",
      "Train Epoch: 64 [33792/54000 (63%)] Loss: -1039836.500000\n",
      "Train Epoch: 64 [35200/54000 (65%)] Loss: -1019999.937500\n",
      "Train Epoch: 64 [36608/54000 (68%)] Loss: -1058177.625000\n",
      "Train Epoch: 64 [38016/54000 (70%)] Loss: -1048627.500000\n",
      "Train Epoch: 64 [39424/54000 (73%)] Loss: -1043936.250000\n",
      "Train Epoch: 64 [40832/54000 (76%)] Loss: -1040663.375000\n",
      "Train Epoch: 64 [42240/54000 (78%)] Loss: -1051989.375000\n",
      "Train Epoch: 64 [43648/54000 (81%)] Loss: -1008296.750000\n",
      "Train Epoch: 64 [45056/54000 (83%)] Loss: -1091692.500000\n",
      "Train Epoch: 64 [46464/54000 (86%)] Loss: -1055889.375000\n",
      "Train Epoch: 64 [47872/54000 (89%)] Loss: -1067864.375000\n",
      "Train Epoch: 64 [49280/54000 (91%)] Loss: -1024472.312500\n",
      "Train Epoch: 64 [50688/54000 (94%)] Loss: -1028628.437500\n",
      "Train Epoch: 64 [52096/54000 (96%)] Loss: -1112788.000000\n",
      "Train Epoch: 64 [53504/54000 (99%)] Loss: -1035016.750000\n",
      "    epoch          : 64\n",
      "    loss           : -1067577.892328199\n",
      "    val_loss       : -1071497.6135903217\n",
      "Train Epoch: 65 [0/54000 (0%)] Loss: -1017342.562500\n",
      "Train Epoch: 65 [1408/54000 (3%)] Loss: -1050818.375000\n",
      "Train Epoch: 65 [2816/54000 (5%)] Loss: -1067019.875000\n",
      "Train Epoch: 65 [4224/54000 (8%)] Loss: -1053124.250000\n",
      "Train Epoch: 65 [5632/54000 (10%)] Loss: -1062517.625000\n",
      "Train Epoch: 65 [7040/54000 (13%)] Loss: -1086438.125000\n",
      "Train Epoch: 65 [8448/54000 (16%)] Loss: -1015609.687500\n",
      "Train Epoch: 65 [9856/54000 (18%)] Loss: -1085029.875000\n",
      "Train Epoch: 65 [11264/54000 (21%)] Loss: -1062142.500000\n",
      "Train Epoch: 65 [12672/54000 (23%)] Loss: -1052785.125000\n",
      "Train Epoch: 65 [14080/54000 (26%)] Loss: -1076101.000000\n",
      "Train Epoch: 65 [15488/54000 (29%)] Loss: -1086578.000000\n",
      "Train Epoch: 65 [16896/54000 (31%)] Loss: -1119247.875000\n",
      "Train Epoch: 65 [18304/54000 (34%)] Loss: -1116669.625000\n",
      "Train Epoch: 65 [19712/54000 (37%)] Loss: -1049818.500000\n",
      "Train Epoch: 65 [21120/54000 (39%)] Loss: -1039730.250000\n",
      "Train Epoch: 65 [22528/54000 (42%)] Loss: -1078691.375000\n",
      "Train Epoch: 65 [23936/54000 (44%)] Loss: -1111572.500000\n",
      "Train Epoch: 65 [25344/54000 (47%)] Loss: -1023790.875000\n",
      "Train Epoch: 65 [26752/54000 (50%)] Loss: -1118138.500000\n",
      "Train Epoch: 65 [28160/54000 (52%)] Loss: -1045818.562500\n",
      "Train Epoch: 65 [29568/54000 (55%)] Loss: -996876.625000\n",
      "Train Epoch: 65 [30976/54000 (57%)] Loss: -1038897.687500\n",
      "Train Epoch: 65 [32384/54000 (60%)] Loss: -989929.875000\n",
      "Train Epoch: 65 [33792/54000 (63%)] Loss: -1111845.125000\n",
      "Train Epoch: 65 [35200/54000 (65%)] Loss: -1101232.250000\n",
      "Train Epoch: 65 [36608/54000 (68%)] Loss: -1047578.687500\n",
      "Train Epoch: 65 [38016/54000 (70%)] Loss: -1080229.250000\n",
      "Train Epoch: 65 [39424/54000 (73%)] Loss: -1029021.187500\n",
      "Train Epoch: 65 [40832/54000 (76%)] Loss: -1065390.750000\n",
      "Train Epoch: 65 [42240/54000 (78%)] Loss: -1081656.750000\n",
      "Train Epoch: 65 [43648/54000 (81%)] Loss: -1090603.250000\n",
      "Train Epoch: 65 [45056/54000 (83%)] Loss: -1082721.500000\n",
      "Train Epoch: 65 [46464/54000 (86%)] Loss: -1091810.250000\n",
      "Train Epoch: 65 [47872/54000 (89%)] Loss: -1040473.562500\n",
      "Train Epoch: 65 [49280/54000 (91%)] Loss: -1089079.875000\n",
      "Train Epoch: 65 [50688/54000 (94%)] Loss: -1097585.000000\n",
      "Train Epoch: 65 [52096/54000 (96%)] Loss: -1082483.500000\n",
      "Train Epoch: 65 [53504/54000 (99%)] Loss: -1094274.375000\n",
      "    epoch          : 65\n",
      "    loss           : -1068050.4597156397\n",
      "    val_loss       : -1070992.9067954205\n",
      "Train Epoch: 66 [0/54000 (0%)] Loss: -1095189.500000\n",
      "Train Epoch: 66 [1408/54000 (3%)] Loss: -1041681.812500\n",
      "Train Epoch: 66 [2816/54000 (5%)] Loss: -1079939.125000\n",
      "Train Epoch: 66 [4224/54000 (8%)] Loss: -1093097.875000\n",
      "Train Epoch: 66 [5632/54000 (10%)] Loss: -1018869.312500\n",
      "Train Epoch: 66 [7040/54000 (13%)] Loss: -1114317.625000\n",
      "Train Epoch: 66 [8448/54000 (16%)] Loss: -1119099.625000\n",
      "Train Epoch: 66 [9856/54000 (18%)] Loss: -1080577.375000\n",
      "Train Epoch: 66 [11264/54000 (21%)] Loss: -1085832.500000\n",
      "Train Epoch: 66 [12672/54000 (23%)] Loss: -1087712.875000\n",
      "Train Epoch: 66 [14080/54000 (26%)] Loss: -1117176.125000\n",
      "Train Epoch: 66 [15488/54000 (29%)] Loss: -1114459.500000\n",
      "Train Epoch: 66 [16896/54000 (31%)] Loss: -1033233.750000\n",
      "Train Epoch: 66 [18304/54000 (34%)] Loss: -1046803.000000\n",
      "Train Epoch: 66 [19712/54000 (37%)] Loss: -1108358.875000\n",
      "Train Epoch: 66 [21120/54000 (39%)] Loss: -1128449.125000\n",
      "Train Epoch: 66 [22528/54000 (42%)] Loss: -1134800.250000\n",
      "Train Epoch: 66 [23936/54000 (44%)] Loss: -1020345.062500\n",
      "Train Epoch: 66 [25344/54000 (47%)] Loss: -1158672.125000\n",
      "Train Epoch: 66 [26752/54000 (50%)] Loss: -1103802.625000\n",
      "Train Epoch: 66 [28160/54000 (52%)] Loss: -1015103.562500\n",
      "Train Epoch: 66 [29568/54000 (55%)] Loss: -1047436.937500\n",
      "Train Epoch: 66 [30976/54000 (57%)] Loss: -1112585.000000\n",
      "Train Epoch: 66 [32384/54000 (60%)] Loss: -1025798.812500\n",
      "Train Epoch: 66 [33792/54000 (63%)] Loss: -1081848.125000\n",
      "Train Epoch: 66 [35200/54000 (65%)] Loss: -1064454.250000\n",
      "Train Epoch: 66 [36608/54000 (68%)] Loss: -1096717.625000\n",
      "Train Epoch: 66 [38016/54000 (70%)] Loss: -1043122.812500\n",
      "Train Epoch: 66 [39424/54000 (73%)] Loss: -1075357.375000\n",
      "Train Epoch: 66 [40832/54000 (76%)] Loss: -1096624.750000\n",
      "Train Epoch: 66 [42240/54000 (78%)] Loss: -1105808.625000\n",
      "Train Epoch: 66 [43648/54000 (81%)] Loss: -1114540.000000\n",
      "Train Epoch: 66 [45056/54000 (83%)] Loss: -1075072.625000\n",
      "Train Epoch: 66 [46464/54000 (86%)] Loss: -1062822.375000\n",
      "Train Epoch: 66 [47872/54000 (89%)] Loss: -1022534.500000\n",
      "Train Epoch: 66 [49280/54000 (91%)] Loss: -1048088.500000\n",
      "Train Epoch: 66 [50688/54000 (94%)] Loss: -1018713.562500\n",
      "Train Epoch: 66 [52096/54000 (96%)] Loss: -1061274.625000\n",
      "Train Epoch: 66 [53504/54000 (99%)] Loss: -1056362.875000\n",
      "    epoch          : 66\n",
      "    loss           : -1068559.9175059241\n",
      "    val_loss       : -1071811.851302776\n",
      "Train Epoch: 67 [0/54000 (0%)] Loss: -1079660.875000\n",
      "Train Epoch: 67 [1408/54000 (3%)] Loss: -1064162.625000\n",
      "Train Epoch: 67 [2816/54000 (5%)] Loss: -1151636.625000\n",
      "Train Epoch: 67 [4224/54000 (8%)] Loss: -1014159.312500\n",
      "Train Epoch: 67 [5632/54000 (10%)] Loss: -1009271.125000\n",
      "Train Epoch: 67 [7040/54000 (13%)] Loss: -1066372.500000\n",
      "Train Epoch: 67 [8448/54000 (16%)] Loss: -1039384.250000\n",
      "Train Epoch: 67 [9856/54000 (18%)] Loss: -1070313.750000\n",
      "Train Epoch: 67 [11264/54000 (21%)] Loss: -1046123.562500\n",
      "Train Epoch: 67 [12672/54000 (23%)] Loss: -1036032.875000\n",
      "Train Epoch: 67 [14080/54000 (26%)] Loss: -1076290.000000\n",
      "Train Epoch: 67 [15488/54000 (29%)] Loss: -1021449.062500\n",
      "Train Epoch: 67 [16896/54000 (31%)] Loss: -1010168.312500\n",
      "Train Epoch: 67 [18304/54000 (34%)] Loss: -1023233.250000\n",
      "Train Epoch: 67 [19712/54000 (37%)] Loss: -1100929.875000\n",
      "Train Epoch: 67 [21120/54000 (39%)] Loss: -1114746.500000\n",
      "Train Epoch: 67 [22528/54000 (42%)] Loss: -1063541.875000\n",
      "Train Epoch: 67 [23936/54000 (44%)] Loss: -1093702.875000\n",
      "Train Epoch: 67 [25344/54000 (47%)] Loss: -1060515.875000\n",
      "Train Epoch: 67 [26752/54000 (50%)] Loss: -1089395.875000\n",
      "Train Epoch: 67 [28160/54000 (52%)] Loss: -1089254.625000\n",
      "Train Epoch: 67 [29568/54000 (55%)] Loss: -1085060.750000\n",
      "Train Epoch: 67 [30976/54000 (57%)] Loss: -1046592.750000\n",
      "Train Epoch: 67 [32384/54000 (60%)] Loss: -1061063.625000\n",
      "Train Epoch: 67 [33792/54000 (63%)] Loss: -1083638.625000\n",
      "Train Epoch: 67 [35200/54000 (65%)] Loss: -1052557.375000\n",
      "Train Epoch: 67 [36608/54000 (68%)] Loss: -1078887.750000\n",
      "Train Epoch: 67 [38016/54000 (70%)] Loss: -1048553.250000\n",
      "Train Epoch: 67 [39424/54000 (73%)] Loss: -1114448.875000\n",
      "Train Epoch: 67 [40832/54000 (76%)] Loss: -1052166.625000\n",
      "Train Epoch: 67 [42240/54000 (78%)] Loss: -1087260.750000\n",
      "Train Epoch: 67 [43648/54000 (81%)] Loss: -1078934.500000\n",
      "Train Epoch: 67 [45056/54000 (83%)] Loss: -1044275.000000\n",
      "Train Epoch: 67 [46464/54000 (86%)] Loss: -1024406.437500\n",
      "Train Epoch: 67 [47872/54000 (89%)] Loss: -1008330.812500\n",
      "Train Epoch: 67 [49280/54000 (91%)] Loss: -1062633.625000\n",
      "Train Epoch: 67 [50688/54000 (94%)] Loss: -1110517.875000\n",
      "Train Epoch: 67 [52096/54000 (96%)] Loss: -1074409.000000\n",
      "Train Epoch: 67 [53504/54000 (99%)] Loss: -1029078.687500\n",
      "    epoch          : 67\n",
      "    loss           : -1069226.5873815166\n",
      "    val_loss       : -1072482.8470666765\n",
      "Train Epoch: 68 [0/54000 (0%)] Loss: -1085968.250000\n",
      "Train Epoch: 68 [1408/54000 (3%)] Loss: -1108584.500000\n",
      "Train Epoch: 68 [2816/54000 (5%)] Loss: -1033929.625000\n",
      "Train Epoch: 68 [4224/54000 (8%)] Loss: -1060664.625000\n",
      "Train Epoch: 68 [5632/54000 (10%)] Loss: -1093717.750000\n",
      "Train Epoch: 68 [7040/54000 (13%)] Loss: -1103333.250000\n",
      "Train Epoch: 68 [8448/54000 (16%)] Loss: -1049242.125000\n",
      "Train Epoch: 68 [9856/54000 (18%)] Loss: -1109712.250000\n",
      "Train Epoch: 68 [11264/54000 (21%)] Loss: -992210.625000\n",
      "Train Epoch: 68 [12672/54000 (23%)] Loss: -1026705.625000\n",
      "Train Epoch: 68 [14080/54000 (26%)] Loss: -1045818.812500\n",
      "Train Epoch: 68 [15488/54000 (29%)] Loss: -1085722.875000\n",
      "Train Epoch: 68 [16896/54000 (31%)] Loss: -1061009.875000\n",
      "Train Epoch: 68 [18304/54000 (34%)] Loss: -1124834.875000\n",
      "Train Epoch: 68 [19712/54000 (37%)] Loss: -1133727.750000\n",
      "Train Epoch: 68 [21120/54000 (39%)] Loss: -1078829.250000\n",
      "Train Epoch: 68 [22528/54000 (42%)] Loss: -1049819.250000\n",
      "Train Epoch: 68 [23936/54000 (44%)] Loss: -1107564.875000\n",
      "Train Epoch: 68 [25344/54000 (47%)] Loss: -1051363.375000\n",
      "Train Epoch: 68 [26752/54000 (50%)] Loss: -1040724.750000\n",
      "Train Epoch: 68 [28160/54000 (52%)] Loss: -1078955.000000\n",
      "Train Epoch: 68 [29568/54000 (55%)] Loss: -1064540.500000\n",
      "Train Epoch: 68 [30976/54000 (57%)] Loss: -1092438.750000\n",
      "Train Epoch: 68 [32384/54000 (60%)] Loss: -1119671.125000\n",
      "Train Epoch: 68 [33792/54000 (63%)] Loss: -1078836.000000\n",
      "Train Epoch: 68 [35200/54000 (65%)] Loss: -1051485.750000\n",
      "Train Epoch: 68 [36608/54000 (68%)] Loss: -1066411.750000\n",
      "Train Epoch: 68 [38016/54000 (70%)] Loss: -1060453.750000\n",
      "Train Epoch: 68 [39424/54000 (73%)] Loss: -1099552.500000\n",
      "Train Epoch: 68 [40832/54000 (76%)] Loss: -1077397.250000\n",
      "Train Epoch: 68 [42240/54000 (78%)] Loss: -1048424.125000\n",
      "Train Epoch: 68 [43648/54000 (81%)] Loss: -1022383.937500\n",
      "Train Epoch: 68 [45056/54000 (83%)] Loss: -1035019.562500\n",
      "Train Epoch: 68 [46464/54000 (86%)] Loss: -1066803.375000\n",
      "Train Epoch: 68 [47872/54000 (89%)] Loss: -1074838.500000\n",
      "Train Epoch: 68 [49280/54000 (91%)] Loss: -1022715.562500\n",
      "Train Epoch: 68 [50688/54000 (94%)] Loss: -1029856.312500\n",
      "Train Epoch: 68 [52096/54000 (96%)] Loss: -1061978.500000\n",
      "Train Epoch: 68 [53504/54000 (99%)] Loss: -1059965.750000\n",
      "    epoch          : 68\n",
      "    loss           : -1069729.0438388626\n",
      "    val_loss       : -1073164.6241974526\n",
      "Train Epoch: 69 [0/54000 (0%)] Loss: -1081520.875000\n",
      "Train Epoch: 69 [1408/54000 (3%)] Loss: -1065356.125000\n",
      "Train Epoch: 69 [2816/54000 (5%)] Loss: -1010716.000000\n",
      "Train Epoch: 69 [4224/54000 (8%)] Loss: -1016838.562500\n",
      "Train Epoch: 69 [5632/54000 (10%)] Loss: -1065480.875000\n",
      "Train Epoch: 69 [7040/54000 (13%)] Loss: -1082134.625000\n",
      "Train Epoch: 69 [8448/54000 (16%)] Loss: -1138625.375000\n",
      "Train Epoch: 69 [9856/54000 (18%)] Loss: -1098137.375000\n",
      "Train Epoch: 69 [11264/54000 (21%)] Loss: -1070196.000000\n",
      "Train Epoch: 69 [12672/54000 (23%)] Loss: -1114959.375000\n",
      "Train Epoch: 69 [14080/54000 (26%)] Loss: -1052539.500000\n",
      "Train Epoch: 69 [15488/54000 (29%)] Loss: -1049317.250000\n",
      "Train Epoch: 69 [16896/54000 (31%)] Loss: -1055782.375000\n",
      "Train Epoch: 69 [18304/54000 (34%)] Loss: -1035609.875000\n",
      "Train Epoch: 69 [19712/54000 (37%)] Loss: -1137744.625000\n",
      "Train Epoch: 69 [21120/54000 (39%)] Loss: -1056359.750000\n",
      "Train Epoch: 69 [22528/54000 (42%)] Loss: -1043913.062500\n",
      "Train Epoch: 69 [23936/54000 (44%)] Loss: -1064805.125000\n",
      "Train Epoch: 69 [25344/54000 (47%)] Loss: -1027701.500000\n",
      "Train Epoch: 69 [26752/54000 (50%)] Loss: -1141491.500000\n",
      "Train Epoch: 69 [28160/54000 (52%)] Loss: -1097933.625000\n",
      "Train Epoch: 69 [29568/54000 (55%)] Loss: -1047834.750000\n",
      "Train Epoch: 69 [30976/54000 (57%)] Loss: -1093771.375000\n",
      "Train Epoch: 69 [32384/54000 (60%)] Loss: -1054506.875000\n",
      "Train Epoch: 69 [33792/54000 (63%)] Loss: -962603.000000\n",
      "Train Epoch: 69 [35200/54000 (65%)] Loss: -1106882.000000\n",
      "Train Epoch: 69 [36608/54000 (68%)] Loss: -1033726.500000\n",
      "Train Epoch: 69 [38016/54000 (70%)] Loss: -1075045.000000\n",
      "Train Epoch: 69 [39424/54000 (73%)] Loss: -1084236.250000\n",
      "Train Epoch: 69 [40832/54000 (76%)] Loss: -1044706.312500\n",
      "Train Epoch: 69 [42240/54000 (78%)] Loss: -1073772.375000\n",
      "Train Epoch: 69 [43648/54000 (81%)] Loss: -1105153.000000\n",
      "Train Epoch: 69 [45056/54000 (83%)] Loss: -1101574.125000\n",
      "Train Epoch: 69 [46464/54000 (86%)] Loss: -986301.562500\n",
      "Train Epoch: 69 [47872/54000 (89%)] Loss: -1122117.625000\n",
      "Train Epoch: 69 [49280/54000 (91%)] Loss: -1052881.625000\n",
      "Train Epoch: 69 [50688/54000 (94%)] Loss: -1119033.250000\n",
      "Train Epoch: 69 [52096/54000 (96%)] Loss: -1055411.000000\n",
      "Train Epoch: 69 [53504/54000 (99%)] Loss: -1060654.625000\n",
      "    epoch          : 69\n",
      "    loss           : -1070202.4880035545\n",
      "    val_loss       : -1073635.535945811\n",
      "Train Epoch: 70 [0/54000 (0%)] Loss: -1028957.187500\n",
      "Train Epoch: 70 [1408/54000 (3%)] Loss: -1014035.562500\n",
      "Train Epoch: 70 [2816/54000 (5%)] Loss: -1102127.250000\n",
      "Train Epoch: 70 [4224/54000 (8%)] Loss: -1040430.062500\n",
      "Train Epoch: 70 [5632/54000 (10%)] Loss: -1135686.500000\n",
      "Train Epoch: 70 [7040/54000 (13%)] Loss: -1036980.750000\n",
      "Train Epoch: 70 [8448/54000 (16%)] Loss: -1062978.500000\n",
      "Train Epoch: 70 [9856/54000 (18%)] Loss: -1091484.250000\n",
      "Train Epoch: 70 [11264/54000 (21%)] Loss: -1145663.250000\n",
      "Train Epoch: 70 [12672/54000 (23%)] Loss: -1131671.625000\n",
      "Train Epoch: 70 [14080/54000 (26%)] Loss: -1052031.750000\n",
      "Train Epoch: 70 [15488/54000 (29%)] Loss: -1124217.250000\n",
      "Train Epoch: 70 [16896/54000 (31%)] Loss: -1088832.125000\n",
      "Train Epoch: 70 [18304/54000 (34%)] Loss: -1093247.000000\n",
      "Train Epoch: 70 [19712/54000 (37%)] Loss: -1097573.750000\n",
      "Train Epoch: 70 [21120/54000 (39%)] Loss: -1036157.687500\n",
      "Train Epoch: 70 [22528/54000 (42%)] Loss: -1073383.000000\n",
      "Train Epoch: 70 [23936/54000 (44%)] Loss: -1092852.875000\n",
      "Train Epoch: 70 [25344/54000 (47%)] Loss: -1047537.312500\n",
      "Train Epoch: 70 [26752/54000 (50%)] Loss: -1065758.000000\n",
      "Train Epoch: 70 [28160/54000 (52%)] Loss: -1076988.375000\n",
      "Train Epoch: 70 [29568/54000 (55%)] Loss: -1029273.625000\n",
      "Train Epoch: 70 [30976/54000 (57%)] Loss: -1071301.375000\n",
      "Train Epoch: 70 [32384/54000 (60%)] Loss: -1050287.500000\n",
      "Train Epoch: 70 [33792/54000 (63%)] Loss: -1028690.500000\n",
      "Train Epoch: 70 [35200/54000 (65%)] Loss: -1035209.687500\n",
      "Train Epoch: 70 [36608/54000 (68%)] Loss: -1078815.125000\n",
      "Train Epoch: 70 [38016/54000 (70%)] Loss: -1105858.125000\n",
      "Train Epoch: 70 [39424/54000 (73%)] Loss: -1096087.625000\n",
      "Train Epoch: 70 [40832/54000 (76%)] Loss: -1073058.750000\n",
      "Train Epoch: 70 [42240/54000 (78%)] Loss: -1054187.500000\n",
      "Train Epoch: 70 [43648/54000 (81%)] Loss: -1065240.500000\n",
      "Train Epoch: 70 [45056/54000 (83%)] Loss: -1009405.687500\n",
      "Train Epoch: 70 [46464/54000 (86%)] Loss: -1068201.750000\n",
      "Train Epoch: 70 [47872/54000 (89%)] Loss: -1052817.625000\n",
      "Train Epoch: 70 [49280/54000 (91%)] Loss: -1057759.625000\n",
      "Train Epoch: 70 [50688/54000 (94%)] Loss: -1106743.625000\n",
      "Train Epoch: 70 [52096/54000 (96%)] Loss: -1123385.500000\n",
      "Train Epoch: 70 [53504/54000 (99%)] Loss: -1092805.250000\n",
      "    epoch          : 70\n",
      "    loss           : -1070609.1793542653\n",
      "    val_loss       : -1074024.9334560963\n",
      "Train Epoch: 71 [0/54000 (0%)] Loss: -1099838.375000\n",
      "Train Epoch: 71 [1408/54000 (3%)] Loss: -1067571.875000\n",
      "Train Epoch: 71 [2816/54000 (5%)] Loss: -1096270.750000\n",
      "Train Epoch: 71 [4224/54000 (8%)] Loss: -1059931.500000\n",
      "Train Epoch: 71 [5632/54000 (10%)] Loss: -1054883.750000\n",
      "Train Epoch: 71 [7040/54000 (13%)] Loss: -1072192.625000\n",
      "Train Epoch: 71 [8448/54000 (16%)] Loss: -1089495.125000\n",
      "Train Epoch: 71 [9856/54000 (18%)] Loss: -1036697.437500\n",
      "Train Epoch: 71 [11264/54000 (21%)] Loss: -1091279.375000\n",
      "Train Epoch: 71 [12672/54000 (23%)] Loss: -1065249.875000\n",
      "Train Epoch: 71 [14080/54000 (26%)] Loss: -1079587.250000\n",
      "Train Epoch: 71 [15488/54000 (29%)] Loss: -1118123.500000\n",
      "Train Epoch: 71 [16896/54000 (31%)] Loss: -1103886.625000\n",
      "Train Epoch: 71 [18304/54000 (34%)] Loss: -1112359.000000\n",
      "Train Epoch: 71 [19712/54000 (37%)] Loss: -1110342.375000\n",
      "Train Epoch: 71 [21120/54000 (39%)] Loss: -1103963.000000\n",
      "Train Epoch: 71 [22528/54000 (42%)] Loss: -1023730.687500\n",
      "Train Epoch: 71 [23936/54000 (44%)] Loss: -1072529.750000\n",
      "Train Epoch: 71 [25344/54000 (47%)] Loss: -1032282.312500\n",
      "Train Epoch: 71 [26752/54000 (50%)] Loss: -1082847.375000\n",
      "Train Epoch: 71 [28160/54000 (52%)] Loss: -987912.125000\n",
      "Train Epoch: 71 [29568/54000 (55%)] Loss: -1149900.250000\n",
      "Train Epoch: 71 [30976/54000 (57%)] Loss: -1096354.750000\n",
      "Train Epoch: 71 [32384/54000 (60%)] Loss: -1094721.625000\n",
      "Train Epoch: 71 [33792/54000 (63%)] Loss: -1063400.375000\n",
      "Train Epoch: 71 [35200/54000 (65%)] Loss: -1095793.750000\n",
      "Train Epoch: 71 [36608/54000 (68%)] Loss: -1049304.125000\n",
      "Train Epoch: 71 [38016/54000 (70%)] Loss: -1030943.375000\n",
      "Train Epoch: 71 [39424/54000 (73%)] Loss: -1073259.500000\n",
      "Train Epoch: 71 [40832/54000 (76%)] Loss: -1041845.750000\n",
      "Train Epoch: 71 [42240/54000 (78%)] Loss: -1079656.625000\n",
      "Train Epoch: 71 [43648/54000 (81%)] Loss: -1054506.875000\n",
      "Train Epoch: 71 [45056/54000 (83%)] Loss: -1043767.562500\n",
      "Train Epoch: 71 [46464/54000 (86%)] Loss: -1070504.125000\n",
      "Train Epoch: 71 [47872/54000 (89%)] Loss: -1084285.875000\n",
      "Train Epoch: 71 [49280/54000 (91%)] Loss: -1002929.750000\n",
      "Train Epoch: 71 [50688/54000 (94%)] Loss: -1102682.000000\n",
      "Train Epoch: 71 [52096/54000 (96%)] Loss: -1013695.500000\n",
      "Train Epoch: 71 [53504/54000 (99%)] Loss: -1085288.250000\n",
      "    epoch          : 71\n",
      "    loss           : -1071112.100414692\n",
      "    val_loss       : -1074709.4771702543\n",
      "Train Epoch: 72 [0/54000 (0%)] Loss: -1009932.062500\n",
      "Train Epoch: 72 [1408/54000 (3%)] Loss: -1058386.375000\n",
      "Train Epoch: 72 [2816/54000 (5%)] Loss: -1123673.250000\n",
      "Train Epoch: 72 [4224/54000 (8%)] Loss: -1055039.125000\n",
      "Train Epoch: 72 [5632/54000 (10%)] Loss: -1081410.125000\n",
      "Train Epoch: 72 [7040/54000 (13%)] Loss: -1091903.125000\n",
      "Train Epoch: 72 [8448/54000 (16%)] Loss: -1079816.125000\n",
      "Train Epoch: 72 [9856/54000 (18%)] Loss: -1021242.062500\n",
      "Train Epoch: 72 [11264/54000 (21%)] Loss: -1048025.187500\n",
      "Train Epoch: 72 [12672/54000 (23%)] Loss: -1137025.250000\n",
      "Train Epoch: 72 [14080/54000 (26%)] Loss: -1098240.375000\n",
      "Train Epoch: 72 [15488/54000 (29%)] Loss: -1048907.750000\n",
      "Train Epoch: 72 [16896/54000 (31%)] Loss: -1093687.625000\n",
      "Train Epoch: 72 [18304/54000 (34%)] Loss: -1122900.000000\n",
      "Train Epoch: 72 [19712/54000 (37%)] Loss: -1060567.250000\n",
      "Train Epoch: 72 [21120/54000 (39%)] Loss: -1064000.750000\n",
      "Train Epoch: 72 [22528/54000 (42%)] Loss: -1062284.125000\n",
      "Train Epoch: 72 [23936/54000 (44%)] Loss: -1031013.500000\n",
      "Train Epoch: 72 [25344/54000 (47%)] Loss: -1049856.500000\n",
      "Train Epoch: 72 [26752/54000 (50%)] Loss: -994112.062500\n",
      "Train Epoch: 72 [28160/54000 (52%)] Loss: -1074386.750000\n",
      "Train Epoch: 72 [29568/54000 (55%)] Loss: -1042563.250000\n",
      "Train Epoch: 72 [30976/54000 (57%)] Loss: -1054044.625000\n",
      "Train Epoch: 72 [32384/54000 (60%)] Loss: -1111009.875000\n",
      "Train Epoch: 72 [33792/54000 (63%)] Loss: -1063066.875000\n",
      "Train Epoch: 72 [35200/54000 (65%)] Loss: -1074413.375000\n",
      "Train Epoch: 72 [36608/54000 (68%)] Loss: -1132134.250000\n",
      "Train Epoch: 72 [38016/54000 (70%)] Loss: -994058.437500\n",
      "Train Epoch: 72 [39424/54000 (73%)] Loss: -1124407.750000\n",
      "Train Epoch: 72 [40832/54000 (76%)] Loss: -1126428.625000\n",
      "Train Epoch: 72 [42240/54000 (78%)] Loss: -1075617.625000\n",
      "Train Epoch: 72 [43648/54000 (81%)] Loss: -1090732.625000\n",
      "Train Epoch: 72 [45056/54000 (83%)] Loss: -1097836.875000\n",
      "Train Epoch: 72 [46464/54000 (86%)] Loss: -1120604.500000\n",
      "Train Epoch: 72 [47872/54000 (89%)] Loss: -1100504.500000\n",
      "Train Epoch: 72 [49280/54000 (91%)] Loss: -1106791.625000\n",
      "Train Epoch: 72 [50688/54000 (94%)] Loss: -1065590.250000\n",
      "Train Epoch: 72 [52096/54000 (96%)] Loss: -1093081.125000\n",
      "Train Epoch: 72 [53504/54000 (99%)] Loss: -1069222.000000\n",
      "    epoch          : 72\n",
      "    loss           : -1071730.883293839\n",
      "    val_loss       : -1074768.273058303\n",
      "Train Epoch: 73 [0/54000 (0%)] Loss: -1036702.687500\n",
      "Train Epoch: 73 [1408/54000 (3%)] Loss: -1031205.125000\n",
      "Train Epoch: 73 [2816/54000 (5%)] Loss: -1139738.875000\n",
      "Train Epoch: 73 [4224/54000 (8%)] Loss: -1084433.500000\n",
      "Train Epoch: 73 [5632/54000 (10%)] Loss: -1088657.625000\n",
      "Train Epoch: 73 [7040/54000 (13%)] Loss: -1036092.875000\n",
      "Train Epoch: 73 [8448/54000 (16%)] Loss: -1039800.937500\n",
      "Train Epoch: 73 [9856/54000 (18%)] Loss: -1047130.312500\n",
      "Train Epoch: 73 [11264/54000 (21%)] Loss: -1098540.750000\n",
      "Train Epoch: 73 [12672/54000 (23%)] Loss: -1095054.625000\n",
      "Train Epoch: 73 [14080/54000 (26%)] Loss: -1083546.375000\n",
      "Train Epoch: 73 [15488/54000 (29%)] Loss: -1087271.375000\n",
      "Train Epoch: 73 [16896/54000 (31%)] Loss: -1013898.750000\n",
      "Train Epoch: 73 [18304/54000 (34%)] Loss: -1078179.625000\n",
      "Train Epoch: 73 [19712/54000 (37%)] Loss: -1070267.375000\n",
      "Train Epoch: 73 [21120/54000 (39%)] Loss: -1052692.875000\n",
      "Train Epoch: 73 [22528/54000 (42%)] Loss: -1090358.500000\n",
      "Train Epoch: 73 [23936/54000 (44%)] Loss: -1089578.000000\n",
      "Train Epoch: 73 [25344/54000 (47%)] Loss: -1130911.375000\n",
      "Train Epoch: 73 [26752/54000 (50%)] Loss: -1042382.687500\n",
      "Train Epoch: 73 [28160/54000 (52%)] Loss: -1013675.750000\n",
      "Train Epoch: 73 [29568/54000 (55%)] Loss: -1108413.250000\n",
      "Train Epoch: 73 [30976/54000 (57%)] Loss: -1117357.875000\n",
      "Train Epoch: 73 [32384/54000 (60%)] Loss: -1122362.750000\n",
      "Train Epoch: 73 [33792/54000 (63%)] Loss: -1146184.000000\n",
      "Train Epoch: 73 [35200/54000 (65%)] Loss: -1048500.500000\n",
      "Train Epoch: 73 [36608/54000 (68%)] Loss: -1014915.500000\n",
      "Train Epoch: 73 [38016/54000 (70%)] Loss: -1054326.875000\n",
      "Train Epoch: 73 [39424/54000 (73%)] Loss: -1077113.875000\n",
      "Train Epoch: 73 [40832/54000 (76%)] Loss: -1101086.625000\n",
      "Train Epoch: 73 [42240/54000 (78%)] Loss: -1043824.937500\n",
      "Train Epoch: 73 [43648/54000 (81%)] Loss: -1082496.000000\n",
      "Train Epoch: 73 [45056/54000 (83%)] Loss: -1041807.312500\n",
      "Train Epoch: 73 [46464/54000 (86%)] Loss: -1095962.000000\n",
      "Train Epoch: 73 [47872/54000 (89%)] Loss: -1108606.000000\n",
      "Train Epoch: 73 [49280/54000 (91%)] Loss: -1056855.500000\n",
      "Train Epoch: 73 [50688/54000 (94%)] Loss: -1097971.000000\n",
      "Train Epoch: 73 [52096/54000 (96%)] Loss: -1044892.312500\n",
      "Train Epoch: 73 [53504/54000 (99%)] Loss: -1053596.000000\n",
      "    epoch          : 73\n",
      "    loss           : -1072083.4164691942\n",
      "    val_loss       : -1075289.771484375\n",
      "Train Epoch: 74 [0/54000 (0%)] Loss: -1118595.625000\n",
      "Train Epoch: 74 [1408/54000 (3%)] Loss: -1077088.750000\n",
      "Train Epoch: 74 [2816/54000 (5%)] Loss: -1041912.937500\n",
      "Train Epoch: 74 [4224/54000 (8%)] Loss: -1070397.500000\n",
      "Train Epoch: 74 [5632/54000 (10%)] Loss: -1021651.812500\n",
      "Train Epoch: 74 [7040/54000 (13%)] Loss: -1066285.375000\n",
      "Train Epoch: 74 [8448/54000 (16%)] Loss: -1089840.375000\n",
      "Train Epoch: 74 [9856/54000 (18%)] Loss: -1126885.750000\n",
      "Train Epoch: 74 [11264/54000 (21%)] Loss: -1040735.625000\n",
      "Train Epoch: 74 [12672/54000 (23%)] Loss: -1027011.125000\n",
      "Train Epoch: 74 [14080/54000 (26%)] Loss: -1061692.750000\n",
      "Train Epoch: 74 [15488/54000 (29%)] Loss: -1081222.000000\n",
      "Train Epoch: 74 [16896/54000 (31%)] Loss: -1055218.875000\n",
      "Train Epoch: 74 [18304/54000 (34%)] Loss: -1123223.375000\n",
      "Train Epoch: 74 [19712/54000 (37%)] Loss: -1050815.750000\n",
      "Train Epoch: 74 [21120/54000 (39%)] Loss: -1111124.375000\n",
      "Train Epoch: 74 [22528/54000 (42%)] Loss: -1095864.125000\n",
      "Train Epoch: 74 [23936/54000 (44%)] Loss: -1062720.500000\n",
      "Train Epoch: 74 [25344/54000 (47%)] Loss: -1085394.000000\n",
      "Train Epoch: 74 [26752/54000 (50%)] Loss: -1068345.750000\n",
      "Train Epoch: 74 [28160/54000 (52%)] Loss: -1095435.250000\n",
      "Train Epoch: 74 [29568/54000 (55%)] Loss: -1111013.875000\n",
      "Train Epoch: 74 [30976/54000 (57%)] Loss: -1025915.437500\n",
      "Train Epoch: 74 [32384/54000 (60%)] Loss: -1060567.875000\n",
      "Train Epoch: 74 [33792/54000 (63%)] Loss: -1092294.250000\n",
      "Train Epoch: 74 [35200/54000 (65%)] Loss: -1090464.000000\n",
      "Train Epoch: 74 [36608/54000 (68%)] Loss: -1109270.375000\n",
      "Train Epoch: 74 [38016/54000 (70%)] Loss: -1083519.500000\n",
      "Train Epoch: 74 [39424/54000 (73%)] Loss: -1074321.625000\n",
      "Train Epoch: 74 [40832/54000 (76%)] Loss: -1071433.000000\n",
      "Train Epoch: 74 [42240/54000 (78%)] Loss: -1105580.375000\n",
      "Train Epoch: 74 [43648/54000 (81%)] Loss: -1121644.625000\n",
      "Train Epoch: 74 [45056/54000 (83%)] Loss: -1078035.500000\n",
      "Train Epoch: 74 [46464/54000 (86%)] Loss: -1022337.187500\n",
      "Train Epoch: 74 [47872/54000 (89%)] Loss: -1052335.125000\n",
      "Train Epoch: 74 [49280/54000 (91%)] Loss: -1011748.250000\n",
      "Train Epoch: 74 [50688/54000 (94%)] Loss: -1106973.500000\n",
      "Train Epoch: 74 [52096/54000 (96%)] Loss: -1020546.187500\n",
      "Train Epoch: 74 [53504/54000 (99%)] Loss: -1042924.812500\n",
      "    epoch          : 74\n",
      "    loss           : -1072677.987114929\n",
      "    val_loss       : -1075752.6005210064\n",
      "Train Epoch: 75 [0/54000 (0%)] Loss: -1127633.250000\n",
      "Train Epoch: 75 [1408/54000 (3%)] Loss: -1075998.000000\n",
      "Train Epoch: 75 [2816/54000 (5%)] Loss: -1079618.625000\n",
      "Train Epoch: 75 [4224/54000 (8%)] Loss: -1092135.125000\n",
      "Train Epoch: 75 [5632/54000 (10%)] Loss: -1087709.625000\n",
      "Train Epoch: 75 [7040/54000 (13%)] Loss: -1128438.000000\n",
      "Train Epoch: 75 [8448/54000 (16%)] Loss: -1068096.375000\n",
      "Train Epoch: 75 [9856/54000 (18%)] Loss: -1028558.125000\n",
      "Train Epoch: 75 [11264/54000 (21%)] Loss: -1113393.625000\n",
      "Train Epoch: 75 [12672/54000 (23%)] Loss: -992621.812500\n",
      "Train Epoch: 75 [14080/54000 (26%)] Loss: -1142951.500000\n",
      "Train Epoch: 75 [15488/54000 (29%)] Loss: -1059310.500000\n",
      "Train Epoch: 75 [16896/54000 (31%)] Loss: -1065988.625000\n",
      "Train Epoch: 75 [18304/54000 (34%)] Loss: -1110917.125000\n",
      "Train Epoch: 75 [19712/54000 (37%)] Loss: -1119264.375000\n",
      "Train Epoch: 75 [21120/54000 (39%)] Loss: -1046238.625000\n",
      "Train Epoch: 75 [22528/54000 (42%)] Loss: -1103401.125000\n",
      "Train Epoch: 75 [23936/54000 (44%)] Loss: -1106048.625000\n",
      "Train Epoch: 75 [25344/54000 (47%)] Loss: -1010265.375000\n",
      "Train Epoch: 75 [26752/54000 (50%)] Loss: -1093786.625000\n",
      "Train Epoch: 75 [28160/54000 (52%)] Loss: -1059178.875000\n",
      "Train Epoch: 75 [29568/54000 (55%)] Loss: -1061621.625000\n",
      "Train Epoch: 75 [30976/54000 (57%)] Loss: -1014162.812500\n",
      "Train Epoch: 75 [32384/54000 (60%)] Loss: -1117955.375000\n",
      "Train Epoch: 75 [33792/54000 (63%)] Loss: -1090480.250000\n",
      "Train Epoch: 75 [35200/54000 (65%)] Loss: -1078292.125000\n",
      "Train Epoch: 75 [36608/54000 (68%)] Loss: -1077549.250000\n",
      "Train Epoch: 75 [38016/54000 (70%)] Loss: -1074384.125000\n",
      "Train Epoch: 75 [39424/54000 (73%)] Loss: -1089343.000000\n",
      "Train Epoch: 75 [40832/54000 (76%)] Loss: -1088248.500000\n",
      "Train Epoch: 75 [42240/54000 (78%)] Loss: -1031224.812500\n",
      "Train Epoch: 75 [43648/54000 (81%)] Loss: -1037135.250000\n",
      "Train Epoch: 75 [45056/54000 (83%)] Loss: -1044051.687500\n",
      "Train Epoch: 75 [46464/54000 (86%)] Loss: -1124903.250000\n",
      "Train Epoch: 75 [47872/54000 (89%)] Loss: -1053444.625000\n",
      "Train Epoch: 75 [49280/54000 (91%)] Loss: -1067292.000000\n",
      "Train Epoch: 75 [50688/54000 (94%)] Loss: -1057695.250000\n",
      "Train Epoch: 75 [52096/54000 (96%)] Loss: -1013226.500000\n",
      "Train Epoch: 75 [53504/54000 (99%)] Loss: -1069965.875000\n",
      "    epoch          : 75\n",
      "    loss           : -1072962.4880035545\n",
      "    val_loss       : -1076192.4190517995\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0427_111527/checkpoint-epoch75.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 76 [0/54000 (0%)] Loss: -1044662.937500\n",
      "Train Epoch: 76 [1408/54000 (3%)] Loss: -1116374.875000\n",
      "Train Epoch: 76 [2816/54000 (5%)] Loss: -1062974.125000\n",
      "Train Epoch: 76 [4224/54000 (8%)] Loss: -1096641.125000\n",
      "Train Epoch: 76 [5632/54000 (10%)] Loss: -1079975.750000\n",
      "Train Epoch: 76 [7040/54000 (13%)] Loss: -1163166.000000\n",
      "Train Epoch: 76 [8448/54000 (16%)] Loss: -1020595.125000\n",
      "Train Epoch: 76 [9856/54000 (18%)] Loss: -1042840.875000\n",
      "Train Epoch: 76 [11264/54000 (21%)] Loss: -1067157.375000\n",
      "Train Epoch: 76 [12672/54000 (23%)] Loss: -1050415.125000\n",
      "Train Epoch: 76 [14080/54000 (26%)] Loss: -1064412.875000\n",
      "Train Epoch: 76 [15488/54000 (29%)] Loss: -1040213.750000\n",
      "Train Epoch: 76 [16896/54000 (31%)] Loss: -1059106.750000\n",
      "Train Epoch: 76 [18304/54000 (34%)] Loss: -1036064.812500\n",
      "Train Epoch: 76 [19712/54000 (37%)] Loss: -1034875.687500\n",
      "Train Epoch: 76 [21120/54000 (39%)] Loss: -1063363.625000\n",
      "Train Epoch: 76 [22528/54000 (42%)] Loss: -1029511.625000\n",
      "Train Epoch: 76 [23936/54000 (44%)] Loss: -1104645.750000\n",
      "Train Epoch: 76 [25344/54000 (47%)] Loss: -1071677.750000\n",
      "Train Epoch: 76 [26752/54000 (50%)] Loss: -1084603.000000\n",
      "Train Epoch: 76 [28160/54000 (52%)] Loss: -1071281.250000\n",
      "Train Epoch: 76 [29568/54000 (55%)] Loss: -1060490.000000\n",
      "Train Epoch: 76 [30976/54000 (57%)] Loss: -1050659.875000\n",
      "Train Epoch: 76 [32384/54000 (60%)] Loss: -1120069.375000\n",
      "Train Epoch: 76 [33792/54000 (63%)] Loss: -1100326.875000\n",
      "Train Epoch: 76 [35200/54000 (65%)] Loss: -1036659.562500\n",
      "Train Epoch: 76 [36608/54000 (68%)] Loss: -1042478.812500\n",
      "Train Epoch: 76 [38016/54000 (70%)] Loss: -1104727.500000\n",
      "Train Epoch: 76 [39424/54000 (73%)] Loss: -1037918.625000\n",
      "Train Epoch: 76 [40832/54000 (76%)] Loss: -1007153.062500\n",
      "Train Epoch: 76 [42240/54000 (78%)] Loss: -1088068.000000\n",
      "Train Epoch: 76 [43648/54000 (81%)] Loss: -1118104.375000\n",
      "Train Epoch: 76 [45056/54000 (83%)] Loss: -1114836.875000\n",
      "Train Epoch: 76 [46464/54000 (86%)] Loss: -1088812.000000\n",
      "Train Epoch: 76 [47872/54000 (89%)] Loss: -1022783.875000\n",
      "Train Epoch: 76 [49280/54000 (91%)] Loss: -1088877.250000\n",
      "Train Epoch: 76 [50688/54000 (94%)] Loss: -1075209.125000\n",
      "Train Epoch: 76 [52096/54000 (96%)] Loss: -1102755.125000\n",
      "Train Epoch: 76 [53504/54000 (99%)] Loss: -1077341.375000\n",
      "    epoch          : 76\n",
      "    loss           : -1073553.7228969194\n",
      "    val_loss       : -1076619.8836566033\n",
      "Train Epoch: 77 [0/54000 (0%)] Loss: -1041847.312500\n",
      "Train Epoch: 77 [1408/54000 (3%)] Loss: -1080721.125000\n",
      "Train Epoch: 77 [2816/54000 (5%)] Loss: -1039808.562500\n",
      "Train Epoch: 77 [4224/54000 (8%)] Loss: -1021904.125000\n",
      "Train Epoch: 77 [5632/54000 (10%)] Loss: -1089061.750000\n",
      "Train Epoch: 77 [7040/54000 (13%)] Loss: -1049274.625000\n",
      "Train Epoch: 77 [8448/54000 (16%)] Loss: -1104794.250000\n",
      "Train Epoch: 77 [9856/54000 (18%)] Loss: -1092500.000000\n",
      "Train Epoch: 77 [11264/54000 (21%)] Loss: -1037136.625000\n",
      "Train Epoch: 77 [12672/54000 (23%)] Loss: -990011.812500\n",
      "Train Epoch: 77 [14080/54000 (26%)] Loss: -1066963.750000\n",
      "Train Epoch: 77 [15488/54000 (29%)] Loss: -1087240.625000\n",
      "Train Epoch: 77 [16896/54000 (31%)] Loss: -1085320.375000\n",
      "Train Epoch: 77 [18304/54000 (34%)] Loss: -1011145.437500\n",
      "Train Epoch: 77 [19712/54000 (37%)] Loss: -1055542.750000\n",
      "Train Epoch: 77 [21120/54000 (39%)] Loss: -1072713.250000\n",
      "Train Epoch: 77 [22528/54000 (42%)] Loss: -1122718.125000\n",
      "Train Epoch: 77 [23936/54000 (44%)] Loss: -1121150.000000\n",
      "Train Epoch: 77 [25344/54000 (47%)] Loss: -1019569.812500\n",
      "Train Epoch: 77 [26752/54000 (50%)] Loss: -1132510.125000\n",
      "Train Epoch: 77 [28160/54000 (52%)] Loss: -1077636.625000\n",
      "Train Epoch: 77 [29568/54000 (55%)] Loss: -1083298.250000\n",
      "Train Epoch: 77 [30976/54000 (57%)] Loss: -1117638.750000\n",
      "Train Epoch: 77 [32384/54000 (60%)] Loss: -1083475.125000\n",
      "Train Epoch: 77 [33792/54000 (63%)] Loss: -1133705.125000\n",
      "Train Epoch: 77 [35200/54000 (65%)] Loss: -1081892.375000\n",
      "Train Epoch: 77 [36608/54000 (68%)] Loss: -1121317.375000\n",
      "Train Epoch: 77 [38016/54000 (70%)] Loss: -1098463.875000\n",
      "Train Epoch: 77 [39424/54000 (73%)] Loss: -1058559.500000\n",
      "Train Epoch: 77 [40832/54000 (76%)] Loss: -1074081.000000\n",
      "Train Epoch: 77 [42240/54000 (78%)] Loss: -1073371.750000\n",
      "Train Epoch: 77 [43648/54000 (81%)] Loss: -1059421.875000\n",
      "Train Epoch: 77 [45056/54000 (83%)] Loss: -1080074.625000\n",
      "Train Epoch: 77 [46464/54000 (86%)] Loss: -1066022.375000\n",
      "Train Epoch: 77 [47872/54000 (89%)] Loss: -1042113.000000\n",
      "Train Epoch: 77 [49280/54000 (91%)] Loss: -1072668.625000\n",
      "Train Epoch: 77 [50688/54000 (94%)] Loss: -1131226.625000\n",
      "Train Epoch: 77 [52096/54000 (96%)] Loss: -1093074.500000\n",
      "Train Epoch: 77 [53504/54000 (99%)] Loss: -1102455.375000\n",
      "    epoch          : 77\n",
      "    loss           : -1073923.1977191942\n",
      "    val_loss       : -1076660.1028195645\n",
      "Train Epoch: 78 [0/54000 (0%)] Loss: -1086525.000000\n",
      "Train Epoch: 78 [1408/54000 (3%)] Loss: -1072627.000000\n",
      "Train Epoch: 78 [2816/54000 (5%)] Loss: -1072775.500000\n",
      "Train Epoch: 78 [4224/54000 (8%)] Loss: -1085301.750000\n",
      "Train Epoch: 78 [5632/54000 (10%)] Loss: -1087133.875000\n",
      "Train Epoch: 78 [7040/54000 (13%)] Loss: -1079084.000000\n",
      "Train Epoch: 78 [8448/54000 (16%)] Loss: -1035723.062500\n",
      "Train Epoch: 78 [9856/54000 (18%)] Loss: -1058069.375000\n",
      "Train Epoch: 78 [11264/54000 (21%)] Loss: -1090854.625000\n",
      "Train Epoch: 78 [12672/54000 (23%)] Loss: -1112392.375000\n",
      "Train Epoch: 78 [14080/54000 (26%)] Loss: -1089363.625000\n",
      "Train Epoch: 78 [15488/54000 (29%)] Loss: -1103869.000000\n",
      "Train Epoch: 78 [16896/54000 (31%)] Loss: -1066905.875000\n",
      "Train Epoch: 78 [18304/54000 (34%)] Loss: -989514.875000\n",
      "Train Epoch: 78 [19712/54000 (37%)] Loss: -1084806.875000\n",
      "Train Epoch: 78 [21120/54000 (39%)] Loss: -1059877.125000\n",
      "Train Epoch: 78 [22528/54000 (42%)] Loss: -1085004.125000\n",
      "Train Epoch: 78 [23936/54000 (44%)] Loss: -1078429.500000\n",
      "Train Epoch: 78 [25344/54000 (47%)] Loss: -1102835.500000\n",
      "Train Epoch: 78 [26752/54000 (50%)] Loss: -1093484.500000\n",
      "Train Epoch: 78 [28160/54000 (52%)] Loss: -1081488.750000\n",
      "Train Epoch: 78 [29568/54000 (55%)] Loss: -1074348.000000\n",
      "Train Epoch: 78 [30976/54000 (57%)] Loss: -1095678.125000\n",
      "Train Epoch: 78 [32384/54000 (60%)] Loss: -1053894.875000\n",
      "Train Epoch: 78 [33792/54000 (63%)] Loss: -1095067.000000\n",
      "Train Epoch: 78 [35200/54000 (65%)] Loss: -1018469.375000\n",
      "Train Epoch: 78 [36608/54000 (68%)] Loss: -1089938.375000\n",
      "Train Epoch: 78 [38016/54000 (70%)] Loss: -1093454.125000\n",
      "Train Epoch: 78 [39424/54000 (73%)] Loss: -1002423.812500\n",
      "Train Epoch: 78 [40832/54000 (76%)] Loss: -1079162.125000\n",
      "Train Epoch: 78 [42240/54000 (78%)] Loss: -1099254.250000\n",
      "Train Epoch: 78 [43648/54000 (81%)] Loss: -1097885.000000\n",
      "Train Epoch: 78 [45056/54000 (83%)] Loss: -1106859.000000\n",
      "Train Epoch: 78 [46464/54000 (86%)] Loss: -1068531.250000\n",
      "Train Epoch: 78 [47872/54000 (89%)] Loss: -1108726.875000\n",
      "Train Epoch: 78 [49280/54000 (91%)] Loss: -1054664.500000\n",
      "Train Epoch: 78 [50688/54000 (94%)] Loss: -1072678.000000\n",
      "Train Epoch: 78 [52096/54000 (96%)] Loss: -1076888.125000\n",
      "Train Epoch: 78 [53504/54000 (99%)] Loss: -1047238.437500\n",
      "    epoch          : 78\n",
      "    loss           : -1074356.8585604266\n",
      "    val_loss       : -1077134.2761775889\n",
      "Train Epoch: 79 [0/54000 (0%)] Loss: -1095659.375000\n",
      "Train Epoch: 79 [1408/54000 (3%)] Loss: -1100912.500000\n",
      "Train Epoch: 79 [2816/54000 (5%)] Loss: -1034430.687500\n",
      "Train Epoch: 79 [4224/54000 (8%)] Loss: -1021706.062500\n",
      "Train Epoch: 79 [5632/54000 (10%)] Loss: -1025483.312500\n",
      "Train Epoch: 79 [7040/54000 (13%)] Loss: -1094432.375000\n",
      "Train Epoch: 79 [8448/54000 (16%)] Loss: -1092913.500000\n",
      "Train Epoch: 79 [9856/54000 (18%)] Loss: -1105120.750000\n",
      "Train Epoch: 79 [11264/54000 (21%)] Loss: -1090755.125000\n",
      "Train Epoch: 79 [12672/54000 (23%)] Loss: -1139316.250000\n",
      "Train Epoch: 79 [14080/54000 (26%)] Loss: -1080184.375000\n",
      "Train Epoch: 79 [15488/54000 (29%)] Loss: -1045518.812500\n",
      "Train Epoch: 79 [16896/54000 (31%)] Loss: -1066404.500000\n",
      "Train Epoch: 79 [18304/54000 (34%)] Loss: -1062870.875000\n",
      "Train Epoch: 79 [19712/54000 (37%)] Loss: -1092218.000000\n",
      "Train Epoch: 79 [21120/54000 (39%)] Loss: -1076932.000000\n",
      "Train Epoch: 79 [22528/54000 (42%)] Loss: -1080356.750000\n",
      "Train Epoch: 79 [23936/54000 (44%)] Loss: -1087761.000000\n",
      "Train Epoch: 79 [25344/54000 (47%)] Loss: -1041413.500000\n",
      "Train Epoch: 79 [26752/54000 (50%)] Loss: -1084237.000000\n",
      "Train Epoch: 79 [28160/54000 (52%)] Loss: -1139588.000000\n",
      "Train Epoch: 79 [29568/54000 (55%)] Loss: -1065930.625000\n",
      "Train Epoch: 79 [30976/54000 (57%)] Loss: -977495.312500\n",
      "Train Epoch: 79 [32384/54000 (60%)] Loss: -1134176.375000\n",
      "Train Epoch: 79 [33792/54000 (63%)] Loss: -1153092.875000\n",
      "Train Epoch: 79 [35200/54000 (65%)] Loss: -1089005.750000\n",
      "Train Epoch: 79 [36608/54000 (68%)] Loss: -1072804.875000\n",
      "Train Epoch: 79 [38016/54000 (70%)] Loss: -1084089.250000\n",
      "Train Epoch: 79 [39424/54000 (73%)] Loss: -1024010.937500\n",
      "Train Epoch: 79 [40832/54000 (76%)] Loss: -1061813.375000\n",
      "Train Epoch: 79 [42240/54000 (78%)] Loss: -1095269.500000\n",
      "Train Epoch: 79 [43648/54000 (81%)] Loss: -1126073.750000\n",
      "Train Epoch: 79 [45056/54000 (83%)] Loss: -1043985.250000\n",
      "Train Epoch: 79 [46464/54000 (86%)] Loss: -1127943.875000\n",
      "Train Epoch: 79 [47872/54000 (89%)] Loss: -1051691.500000\n",
      "Train Epoch: 79 [49280/54000 (91%)] Loss: -1104271.125000\n",
      "Train Epoch: 79 [50688/54000 (94%)] Loss: -1073702.250000\n",
      "Train Epoch: 79 [52096/54000 (96%)] Loss: -1087957.500000\n",
      "Train Epoch: 79 [53504/54000 (99%)] Loss: -1076556.625000\n",
      "    epoch          : 79\n",
      "    loss           : -1074634.042950237\n",
      "    val_loss       : -1077718.3188684343\n",
      "Train Epoch: 80 [0/54000 (0%)] Loss: -993596.187500\n",
      "Train Epoch: 80 [1408/54000 (3%)] Loss: -1105289.750000\n",
      "Train Epoch: 80 [2816/54000 (5%)] Loss: -1095845.500000\n",
      "Train Epoch: 80 [4224/54000 (8%)] Loss: -1081540.875000\n",
      "Train Epoch: 80 [5632/54000 (10%)] Loss: -1040970.687500\n",
      "Train Epoch: 80 [7040/54000 (13%)] Loss: -1069591.625000\n",
      "Train Epoch: 80 [8448/54000 (16%)] Loss: -1100249.750000\n",
      "Train Epoch: 80 [9856/54000 (18%)] Loss: -1043803.937500\n",
      "Train Epoch: 80 [11264/54000 (21%)] Loss: -1104252.375000\n",
      "Train Epoch: 80 [12672/54000 (23%)] Loss: -1060910.625000\n",
      "Train Epoch: 80 [14080/54000 (26%)] Loss: -1041864.062500\n",
      "Train Epoch: 80 [15488/54000 (29%)] Loss: -1031232.875000\n",
      "Train Epoch: 80 [16896/54000 (31%)] Loss: -1084097.625000\n",
      "Train Epoch: 80 [18304/54000 (34%)] Loss: -1077174.875000\n",
      "Train Epoch: 80 [19712/54000 (37%)] Loss: -1039559.625000\n",
      "Train Epoch: 80 [21120/54000 (39%)] Loss: -1101632.625000\n",
      "Train Epoch: 80 [22528/54000 (42%)] Loss: -1063226.500000\n",
      "Train Epoch: 80 [23936/54000 (44%)] Loss: -1078561.375000\n",
      "Train Epoch: 80 [25344/54000 (47%)] Loss: -1131761.000000\n",
      "Train Epoch: 80 [26752/54000 (50%)] Loss: -1091408.125000\n",
      "Train Epoch: 80 [28160/54000 (52%)] Loss: -1065167.500000\n",
      "Train Epoch: 80 [29568/54000 (55%)] Loss: -1056818.625000\n",
      "Train Epoch: 80 [30976/54000 (57%)] Loss: -1082641.000000\n",
      "Train Epoch: 80 [32384/54000 (60%)] Loss: -1025211.312500\n",
      "Train Epoch: 80 [33792/54000 (63%)] Loss: -1073771.000000\n",
      "Train Epoch: 80 [35200/54000 (65%)] Loss: -1074110.375000\n",
      "Train Epoch: 80 [36608/54000 (68%)] Loss: -1113128.125000\n",
      "Train Epoch: 80 [38016/54000 (70%)] Loss: -1119235.750000\n",
      "Train Epoch: 80 [39424/54000 (73%)] Loss: -1047164.312500\n",
      "Train Epoch: 80 [40832/54000 (76%)] Loss: -1055129.000000\n",
      "Train Epoch: 80 [42240/54000 (78%)] Loss: -1115153.375000\n",
      "Train Epoch: 80 [43648/54000 (81%)] Loss: -1071930.375000\n",
      "Train Epoch: 80 [45056/54000 (83%)] Loss: -1085965.125000\n",
      "Train Epoch: 80 [46464/54000 (86%)] Loss: -1130894.250000\n",
      "Train Epoch: 80 [47872/54000 (89%)] Loss: -1105805.500000\n",
      "Train Epoch: 80 [49280/54000 (91%)] Loss: -1076835.875000\n",
      "Train Epoch: 80 [50688/54000 (94%)] Loss: -1079785.625000\n",
      "Train Epoch: 80 [52096/54000 (96%)] Loss: -986429.125000\n",
      "Train Epoch: 80 [53504/54000 (99%)] Loss: -1052935.625000\n",
      "    epoch          : 80\n",
      "    loss           : -1074933.3964751186\n",
      "    val_loss       : -1078365.07657185\n",
      "Train Epoch: 81 [0/54000 (0%)] Loss: -1026462.625000\n",
      "Train Epoch: 81 [1408/54000 (3%)] Loss: -1113907.375000\n",
      "Train Epoch: 81 [2816/54000 (5%)] Loss: -1062470.250000\n",
      "Train Epoch: 81 [4224/54000 (8%)] Loss: -1002743.187500\n",
      "Train Epoch: 81 [5632/54000 (10%)] Loss: -1085011.000000\n",
      "Train Epoch: 81 [7040/54000 (13%)] Loss: -1057804.750000\n",
      "Train Epoch: 81 [8448/54000 (16%)] Loss: -1052938.375000\n",
      "Train Epoch: 81 [9856/54000 (18%)] Loss: -1112930.375000\n",
      "Train Epoch: 81 [11264/54000 (21%)] Loss: -1038629.625000\n",
      "Train Epoch: 81 [12672/54000 (23%)] Loss: -1107296.750000\n",
      "Train Epoch: 81 [14080/54000 (26%)] Loss: -1077984.875000\n",
      "Train Epoch: 81 [15488/54000 (29%)] Loss: -1097060.250000\n",
      "Train Epoch: 81 [16896/54000 (31%)] Loss: -1086386.750000\n",
      "Train Epoch: 81 [18304/54000 (34%)] Loss: -1057212.875000\n",
      "Train Epoch: 81 [19712/54000 (37%)] Loss: -1138822.500000\n",
      "Train Epoch: 81 [21120/54000 (39%)] Loss: -1076540.000000\n",
      "Train Epoch: 81 [22528/54000 (42%)] Loss: -1094003.000000\n",
      "Train Epoch: 81 [23936/54000 (44%)] Loss: -1073354.250000\n",
      "Train Epoch: 81 [25344/54000 (47%)] Loss: -1068972.875000\n",
      "Train Epoch: 81 [26752/54000 (50%)] Loss: -1095662.625000\n",
      "Train Epoch: 81 [28160/54000 (52%)] Loss: -1033383.000000\n",
      "Train Epoch: 81 [29568/54000 (55%)] Loss: -1078267.750000\n",
      "Train Epoch: 81 [30976/54000 (57%)] Loss: -1068822.750000\n",
      "Train Epoch: 81 [32384/54000 (60%)] Loss: -1071102.750000\n",
      "Train Epoch: 81 [33792/54000 (63%)] Loss: -1044693.437500\n",
      "Train Epoch: 81 [35200/54000 (65%)] Loss: -1085327.125000\n",
      "Train Epoch: 81 [36608/54000 (68%)] Loss: -1092200.375000\n",
      "Train Epoch: 81 [38016/54000 (70%)] Loss: -1025410.500000\n",
      "Train Epoch: 81 [39424/54000 (73%)] Loss: -1062499.250000\n",
      "Train Epoch: 81 [40832/54000 (76%)] Loss: -1081923.875000\n",
      "Train Epoch: 81 [42240/54000 (78%)] Loss: -1104616.750000\n",
      "Train Epoch: 81 [43648/54000 (81%)] Loss: -1087551.375000\n",
      "Train Epoch: 81 [45056/54000 (83%)] Loss: -1038928.937500\n",
      "Train Epoch: 81 [46464/54000 (86%)] Loss: -1044659.062500\n",
      "Train Epoch: 81 [47872/54000 (89%)] Loss: -1086841.750000\n",
      "Train Epoch: 81 [49280/54000 (91%)] Loss: -1124020.000000\n",
      "Train Epoch: 81 [50688/54000 (94%)] Loss: -1118891.250000\n",
      "Train Epoch: 81 [52096/54000 (96%)] Loss: -1051065.250000\n",
      "Train Epoch: 81 [53504/54000 (99%)] Loss: -1119508.500000\n",
      "    epoch          : 81\n",
      "    loss           : -1075599.5470971563\n",
      "    val_loss       : -1078336.2596019988\n",
      "Train Epoch: 82 [0/54000 (0%)] Loss: -1076730.875000\n",
      "Train Epoch: 82 [1408/54000 (3%)] Loss: -1044869.375000\n",
      "Train Epoch: 82 [2816/54000 (5%)] Loss: -1067567.125000\n",
      "Train Epoch: 82 [4224/54000 (8%)] Loss: -1101346.000000\n",
      "Train Epoch: 82 [5632/54000 (10%)] Loss: -1039583.812500\n",
      "Train Epoch: 82 [7040/54000 (13%)] Loss: -1055574.500000\n",
      "Train Epoch: 82 [8448/54000 (16%)] Loss: -1023570.937500\n",
      "Train Epoch: 82 [9856/54000 (18%)] Loss: -1113719.750000\n",
      "Train Epoch: 82 [11264/54000 (21%)] Loss: -1107713.375000\n",
      "Train Epoch: 82 [12672/54000 (23%)] Loss: -1109108.750000\n",
      "Train Epoch: 82 [14080/54000 (26%)] Loss: -1065958.750000\n",
      "Train Epoch: 82 [15488/54000 (29%)] Loss: -1043116.187500\n",
      "Train Epoch: 82 [16896/54000 (31%)] Loss: -1041039.375000\n",
      "Train Epoch: 82 [18304/54000 (34%)] Loss: -1100223.000000\n",
      "Train Epoch: 82 [19712/54000 (37%)] Loss: -1041751.750000\n",
      "Train Epoch: 82 [21120/54000 (39%)] Loss: -1060037.875000\n",
      "Train Epoch: 82 [22528/54000 (42%)] Loss: -1072034.875000\n",
      "Train Epoch: 82 [23936/54000 (44%)] Loss: -1099839.625000\n",
      "Train Epoch: 82 [25344/54000 (47%)] Loss: -1013323.687500\n",
      "Train Epoch: 82 [26752/54000 (50%)] Loss: -1080587.375000\n",
      "Train Epoch: 82 [28160/54000 (52%)] Loss: -1072531.625000\n",
      "Train Epoch: 82 [29568/54000 (55%)] Loss: -1133283.500000\n",
      "Train Epoch: 82 [30976/54000 (57%)] Loss: -1078577.000000\n",
      "Train Epoch: 82 [32384/54000 (60%)] Loss: -1077771.500000\n",
      "Train Epoch: 82 [33792/54000 (63%)] Loss: -1042002.250000\n",
      "Train Epoch: 82 [35200/54000 (65%)] Loss: -1087493.500000\n",
      "Train Epoch: 82 [36608/54000 (68%)] Loss: -1097102.375000\n",
      "Train Epoch: 82 [38016/54000 (70%)] Loss: -1043122.937500\n",
      "Train Epoch: 82 [39424/54000 (73%)] Loss: -1058753.875000\n",
      "Train Epoch: 82 [40832/54000 (76%)] Loss: -1128317.000000\n",
      "Train Epoch: 82 [42240/54000 (78%)] Loss: -1069114.625000\n",
      "Train Epoch: 82 [43648/54000 (81%)] Loss: -1063929.000000\n",
      "Train Epoch: 82 [45056/54000 (83%)] Loss: -1076116.625000\n",
      "Train Epoch: 82 [46464/54000 (86%)] Loss: -1073087.875000\n",
      "Train Epoch: 82 [47872/54000 (89%)] Loss: -1074816.000000\n",
      "Train Epoch: 82 [49280/54000 (91%)] Loss: -1115730.000000\n",
      "Train Epoch: 82 [50688/54000 (94%)] Loss: -1049862.375000\n",
      "Train Epoch: 82 [52096/54000 (96%)] Loss: -1083438.750000\n",
      "Train Epoch: 82 [53504/54000 (99%)] Loss: -1055868.875000\n",
      "    epoch          : 82\n",
      "    loss           : -1075801.4069905214\n",
      "    val_loss       : -1078852.9990442155\n",
      "Train Epoch: 83 [0/54000 (0%)] Loss: -1027700.250000\n",
      "Train Epoch: 83 [1408/54000 (3%)] Loss: -1078500.000000\n",
      "Train Epoch: 83 [2816/54000 (5%)] Loss: -1064637.875000\n",
      "Train Epoch: 83 [4224/54000 (8%)] Loss: -1019402.000000\n",
      "Train Epoch: 83 [5632/54000 (10%)] Loss: -1083370.875000\n",
      "Train Epoch: 83 [7040/54000 (13%)] Loss: -1087239.000000\n",
      "Train Epoch: 83 [8448/54000 (16%)] Loss: -1029408.125000\n",
      "Train Epoch: 83 [9856/54000 (18%)] Loss: -1058936.000000\n",
      "Train Epoch: 83 [11264/54000 (21%)] Loss: -1140288.625000\n",
      "Train Epoch: 83 [12672/54000 (23%)] Loss: -1073509.125000\n",
      "Train Epoch: 83 [14080/54000 (26%)] Loss: -1049935.250000\n",
      "Train Epoch: 83 [15488/54000 (29%)] Loss: -1140841.625000\n",
      "Train Epoch: 83 [16896/54000 (31%)] Loss: -1041837.750000\n",
      "Train Epoch: 83 [18304/54000 (34%)] Loss: -1012553.812500\n",
      "Train Epoch: 83 [19712/54000 (37%)] Loss: -1103749.750000\n",
      "Train Epoch: 83 [21120/54000 (39%)] Loss: -1062619.750000\n",
      "Train Epoch: 83 [22528/54000 (42%)] Loss: -1057429.875000\n",
      "Train Epoch: 83 [23936/54000 (44%)] Loss: -1043382.250000\n",
      "Train Epoch: 83 [25344/54000 (47%)] Loss: -1061902.375000\n",
      "Train Epoch: 83 [26752/54000 (50%)] Loss: -1048543.937500\n",
      "Train Epoch: 83 [28160/54000 (52%)] Loss: -1092782.875000\n",
      "Train Epoch: 83 [29568/54000 (55%)] Loss: -1068002.250000\n",
      "Train Epoch: 83 [30976/54000 (57%)] Loss: -1069165.875000\n",
      "Train Epoch: 83 [32384/54000 (60%)] Loss: -1100683.250000\n",
      "Train Epoch: 83 [33792/54000 (63%)] Loss: -1138422.875000\n",
      "Train Epoch: 83 [35200/54000 (65%)] Loss: -1058167.000000\n",
      "Train Epoch: 83 [36608/54000 (68%)] Loss: -1076296.500000\n",
      "Train Epoch: 83 [38016/54000 (70%)] Loss: -1038732.937500\n",
      "Train Epoch: 83 [39424/54000 (73%)] Loss: -1045713.437500\n",
      "Train Epoch: 83 [40832/54000 (76%)] Loss: -1112652.500000\n",
      "Train Epoch: 83 [42240/54000 (78%)] Loss: -1051236.250000\n",
      "Train Epoch: 83 [43648/54000 (81%)] Loss: -1083927.125000\n",
      "Train Epoch: 83 [45056/54000 (83%)] Loss: -1072903.125000\n",
      "Train Epoch: 83 [46464/54000 (86%)] Loss: -1040511.062500\n",
      "Train Epoch: 83 [47872/54000 (89%)] Loss: -1040256.687500\n",
      "Train Epoch: 83 [49280/54000 (91%)] Loss: -1101714.000000\n",
      "Train Epoch: 83 [50688/54000 (94%)] Loss: -1086956.625000\n",
      "Train Epoch: 83 [52096/54000 (96%)] Loss: -1078655.250000\n",
      "Train Epoch: 83 [53504/54000 (99%)] Loss: -1090960.375000\n",
      "    epoch          : 83\n",
      "    loss           : -1076206.1627665877\n",
      "    val_loss       : -1078937.8325532954\n",
      "Train Epoch: 84 [0/54000 (0%)] Loss: -1070795.500000\n",
      "Train Epoch: 84 [1408/54000 (3%)] Loss: -1059921.000000\n",
      "Train Epoch: 84 [2816/54000 (5%)] Loss: -1081883.625000\n",
      "Train Epoch: 84 [4224/54000 (8%)] Loss: -1110669.625000\n",
      "Train Epoch: 84 [5632/54000 (10%)] Loss: -1098560.625000\n",
      "Train Epoch: 84 [7040/54000 (13%)] Loss: -1050786.250000\n",
      "Train Epoch: 84 [8448/54000 (16%)] Loss: -1082639.125000\n",
      "Train Epoch: 84 [9856/54000 (18%)] Loss: -1140037.500000\n",
      "Train Epoch: 84 [11264/54000 (21%)] Loss: -1048650.250000\n",
      "Train Epoch: 84 [12672/54000 (23%)] Loss: -1046219.312500\n",
      "Train Epoch: 84 [14080/54000 (26%)] Loss: -1035109.562500\n",
      "Train Epoch: 84 [15488/54000 (29%)] Loss: -1097719.625000\n",
      "Train Epoch: 84 [16896/54000 (31%)] Loss: -1043843.812500\n",
      "Train Epoch: 84 [18304/54000 (34%)] Loss: -1069790.625000\n",
      "Train Epoch: 84 [19712/54000 (37%)] Loss: -1120025.000000\n",
      "Train Epoch: 84 [21120/54000 (39%)] Loss: -1081396.625000\n",
      "Train Epoch: 84 [22528/54000 (42%)] Loss: -1070139.750000\n",
      "Train Epoch: 84 [23936/54000 (44%)] Loss: -1070223.250000\n",
      "Train Epoch: 84 [25344/54000 (47%)] Loss: -1101065.875000\n",
      "Train Epoch: 84 [26752/54000 (50%)] Loss: -1049861.500000\n",
      "Train Epoch: 84 [28160/54000 (52%)] Loss: -1025638.625000\n",
      "Train Epoch: 84 [29568/54000 (55%)] Loss: -1108005.625000\n",
      "Train Epoch: 84 [30976/54000 (57%)] Loss: -1113969.375000\n",
      "Train Epoch: 84 [32384/54000 (60%)] Loss: -1097643.750000\n",
      "Train Epoch: 84 [33792/54000 (63%)] Loss: -1121641.125000\n",
      "Train Epoch: 84 [35200/54000 (65%)] Loss: -1050348.750000\n",
      "Train Epoch: 84 [36608/54000 (68%)] Loss: -1020764.375000\n",
      "Train Epoch: 84 [38016/54000 (70%)] Loss: -1071510.250000\n",
      "Train Epoch: 84 [39424/54000 (73%)] Loss: -1032806.062500\n",
      "Train Epoch: 84 [40832/54000 (76%)] Loss: -1082181.500000\n",
      "Train Epoch: 84 [42240/54000 (78%)] Loss: -1044486.500000\n",
      "Train Epoch: 84 [43648/54000 (81%)] Loss: -1048967.250000\n",
      "Train Epoch: 84 [45056/54000 (83%)] Loss: -1058394.500000\n",
      "Train Epoch: 84 [46464/54000 (86%)] Loss: -1036371.375000\n",
      "Train Epoch: 84 [47872/54000 (89%)] Loss: -1046887.562500\n",
      "Train Epoch: 84 [49280/54000 (91%)] Loss: -1082435.250000\n",
      "Train Epoch: 84 [50688/54000 (94%)] Loss: -1026543.750000\n",
      "Train Epoch: 84 [52096/54000 (96%)] Loss: -1086948.875000\n",
      "Train Epoch: 84 [53504/54000 (99%)] Loss: -1054605.875000\n",
      "    epoch          : 84\n",
      "    loss           : -1076506.6870556872\n",
      "    val_loss       : -1079465.6290490984\n",
      "Train Epoch: 85 [0/54000 (0%)] Loss: -1007105.250000\n",
      "Train Epoch: 85 [1408/54000 (3%)] Loss: -1118423.125000\n",
      "Train Epoch: 85 [2816/54000 (5%)] Loss: -1058621.625000\n",
      "Train Epoch: 85 [4224/54000 (8%)] Loss: -1121508.500000\n",
      "Train Epoch: 85 [5632/54000 (10%)] Loss: -1109042.875000\n",
      "Train Epoch: 85 [7040/54000 (13%)] Loss: -1048381.000000\n",
      "Train Epoch: 85 [8448/54000 (16%)] Loss: -1140424.500000\n",
      "Train Epoch: 85 [9856/54000 (18%)] Loss: -989482.437500\n",
      "Train Epoch: 85 [11264/54000 (21%)] Loss: -1113022.875000\n",
      "Train Epoch: 85 [12672/54000 (23%)] Loss: -1057558.125000\n",
      "Train Epoch: 85 [14080/54000 (26%)] Loss: -1046997.750000\n",
      "Train Epoch: 85 [15488/54000 (29%)] Loss: -1080816.875000\n",
      "Train Epoch: 85 [16896/54000 (31%)] Loss: -1080837.125000\n",
      "Train Epoch: 85 [18304/54000 (34%)] Loss: -1043378.500000\n",
      "Train Epoch: 85 [19712/54000 (37%)] Loss: -1056884.375000\n",
      "Train Epoch: 85 [21120/54000 (39%)] Loss: -1106248.875000\n",
      "Train Epoch: 85 [22528/54000 (42%)] Loss: -1075290.875000\n",
      "Train Epoch: 85 [23936/54000 (44%)] Loss: -1083485.375000\n",
      "Train Epoch: 85 [25344/54000 (47%)] Loss: -1030732.687500\n",
      "Train Epoch: 85 [26752/54000 (50%)] Loss: -1142482.500000\n",
      "Train Epoch: 85 [28160/54000 (52%)] Loss: -1132096.125000\n",
      "Train Epoch: 85 [29568/54000 (55%)] Loss: -1104775.750000\n",
      "Train Epoch: 85 [30976/54000 (57%)] Loss: -1103425.000000\n",
      "Train Epoch: 85 [32384/54000 (60%)] Loss: -1018029.625000\n",
      "Train Epoch: 85 [33792/54000 (63%)] Loss: -1089232.875000\n",
      "Train Epoch: 85 [35200/54000 (65%)] Loss: -1109255.125000\n",
      "Train Epoch: 85 [36608/54000 (68%)] Loss: -1089986.875000\n",
      "Train Epoch: 85 [38016/54000 (70%)] Loss: -1078038.750000\n",
      "Train Epoch: 85 [39424/54000 (73%)] Loss: -1083976.625000\n",
      "Train Epoch: 85 [40832/54000 (76%)] Loss: -1107327.625000\n",
      "Train Epoch: 85 [42240/54000 (78%)] Loss: -1072587.750000\n",
      "Train Epoch: 85 [43648/54000 (81%)] Loss: -1103150.125000\n",
      "Train Epoch: 85 [45056/54000 (83%)] Loss: -1141419.500000\n",
      "Train Epoch: 85 [46464/54000 (86%)] Loss: -1088169.625000\n",
      "Train Epoch: 85 [47872/54000 (89%)] Loss: -1109985.750000\n",
      "Train Epoch: 85 [49280/54000 (91%)] Loss: -1125715.500000\n",
      "Train Epoch: 85 [50688/54000 (94%)] Loss: -1054954.250000\n",
      "Train Epoch: 85 [52096/54000 (96%)] Loss: -1117889.875000\n",
      "Train Epoch: 85 [53504/54000 (99%)] Loss: -1098059.750000\n",
      "    epoch          : 85\n",
      "    loss           : -1076953.5142180095\n",
      "    val_loss       : -1079910.6922815202\n",
      "Train Epoch: 86 [0/54000 (0%)] Loss: -1063243.625000\n",
      "Train Epoch: 86 [1408/54000 (3%)] Loss: -1062002.250000\n",
      "Train Epoch: 86 [2816/54000 (5%)] Loss: -1087204.125000\n",
      "Train Epoch: 86 [4224/54000 (8%)] Loss: -1027876.375000\n",
      "Train Epoch: 86 [5632/54000 (10%)] Loss: -1057008.625000\n",
      "Train Epoch: 86 [7040/54000 (13%)] Loss: -1085174.250000\n",
      "Train Epoch: 86 [8448/54000 (16%)] Loss: -1020344.000000\n",
      "Train Epoch: 86 [9856/54000 (18%)] Loss: -1112563.875000\n",
      "Train Epoch: 86 [11264/54000 (21%)] Loss: -1061989.000000\n",
      "Train Epoch: 86 [12672/54000 (23%)] Loss: -1085406.000000\n",
      "Train Epoch: 86 [14080/54000 (26%)] Loss: -1040892.625000\n",
      "Train Epoch: 86 [15488/54000 (29%)] Loss: -1099815.625000\n",
      "Train Epoch: 86 [16896/54000 (31%)] Loss: -1030494.750000\n",
      "Train Epoch: 86 [18304/54000 (34%)] Loss: -1082713.125000\n",
      "Train Epoch: 86 [19712/54000 (37%)] Loss: -1054721.625000\n",
      "Train Epoch: 86 [21120/54000 (39%)] Loss: -1059517.375000\n",
      "Train Epoch: 86 [22528/54000 (42%)] Loss: -1052530.250000\n",
      "Train Epoch: 86 [23936/54000 (44%)] Loss: -1137685.750000\n",
      "Train Epoch: 86 [25344/54000 (47%)] Loss: -1092918.250000\n",
      "Train Epoch: 86 [26752/54000 (50%)] Loss: -1049113.750000\n",
      "Train Epoch: 86 [28160/54000 (52%)] Loss: -1115701.125000\n",
      "Train Epoch: 86 [29568/54000 (55%)] Loss: -1055102.500000\n",
      "Train Epoch: 86 [30976/54000 (57%)] Loss: -1127215.250000\n",
      "Train Epoch: 86 [32384/54000 (60%)] Loss: -1109334.500000\n",
      "Train Epoch: 86 [33792/54000 (63%)] Loss: -1156783.250000\n",
      "Train Epoch: 86 [35200/54000 (65%)] Loss: -1091712.375000\n",
      "Train Epoch: 86 [36608/54000 (68%)] Loss: -1069847.625000\n",
      "Train Epoch: 86 [38016/54000 (70%)] Loss: -1097057.500000\n",
      "Train Epoch: 86 [39424/54000 (73%)] Loss: -1086703.375000\n",
      "Train Epoch: 86 [40832/54000 (76%)] Loss: -1112312.500000\n",
      "Train Epoch: 86 [42240/54000 (78%)] Loss: -1076261.000000\n",
      "Train Epoch: 86 [43648/54000 (81%)] Loss: -1077939.250000\n",
      "Train Epoch: 86 [45056/54000 (83%)] Loss: -1094661.250000\n",
      "Train Epoch: 86 [46464/54000 (86%)] Loss: -1123734.000000\n",
      "Train Epoch: 86 [47872/54000 (89%)] Loss: -1076859.500000\n",
      "Train Epoch: 86 [49280/54000 (91%)] Loss: -1096056.750000\n",
      "Train Epoch: 86 [50688/54000 (94%)] Loss: -1128572.750000\n",
      "Train Epoch: 86 [52096/54000 (96%)] Loss: -1069310.250000\n",
      "Train Epoch: 86 [53504/54000 (99%)] Loss: -1077636.500000\n",
      "    epoch          : 86\n",
      "    loss           : -1077224.45867891\n",
      "    val_loss       : -1080244.7530881192\n",
      "Train Epoch: 87 [0/54000 (0%)] Loss: -1081298.125000\n",
      "Train Epoch: 87 [1408/54000 (3%)] Loss: -1058024.500000\n",
      "Train Epoch: 87 [2816/54000 (5%)] Loss: -1043704.375000\n",
      "Train Epoch: 87 [4224/54000 (8%)] Loss: -1078417.750000\n",
      "Train Epoch: 87 [5632/54000 (10%)] Loss: -1085819.000000\n",
      "Train Epoch: 87 [7040/54000 (13%)] Loss: -1028432.062500\n",
      "Train Epoch: 87 [8448/54000 (16%)] Loss: -1160816.500000\n",
      "Train Epoch: 87 [9856/54000 (18%)] Loss: -1079495.375000\n",
      "Train Epoch: 87 [11264/54000 (21%)] Loss: -1062977.625000\n",
      "Train Epoch: 87 [12672/54000 (23%)] Loss: -1065778.625000\n",
      "Train Epoch: 87 [14080/54000 (26%)] Loss: -1077019.000000\n",
      "Train Epoch: 87 [15488/54000 (29%)] Loss: -1077030.625000\n",
      "Train Epoch: 87 [16896/54000 (31%)] Loss: -1111390.250000\n",
      "Train Epoch: 87 [18304/54000 (34%)] Loss: -1077862.750000\n",
      "Train Epoch: 87 [19712/54000 (37%)] Loss: -1075471.500000\n",
      "Train Epoch: 87 [21120/54000 (39%)] Loss: -1056258.000000\n",
      "Train Epoch: 87 [22528/54000 (42%)] Loss: -1041339.687500\n",
      "Train Epoch: 87 [23936/54000 (44%)] Loss: -1086249.375000\n",
      "Train Epoch: 87 [25344/54000 (47%)] Loss: -1064510.250000\n",
      "Train Epoch: 87 [26752/54000 (50%)] Loss: -1124517.000000\n",
      "Train Epoch: 87 [28160/54000 (52%)] Loss: -1068773.000000\n",
      "Train Epoch: 87 [29568/54000 (55%)] Loss: -1114187.250000\n",
      "Train Epoch: 87 [30976/54000 (57%)] Loss: -1084521.625000\n",
      "Train Epoch: 87 [32384/54000 (60%)] Loss: -1050016.250000\n",
      "Train Epoch: 87 [33792/54000 (63%)] Loss: -1060930.125000\n",
      "Train Epoch: 87 [35200/54000 (65%)] Loss: -1053472.125000\n",
      "Train Epoch: 87 [36608/54000 (68%)] Loss: -1068272.875000\n",
      "Train Epoch: 87 [38016/54000 (70%)] Loss: -1088310.125000\n",
      "Train Epoch: 87 [39424/54000 (73%)] Loss: -1077309.625000\n",
      "Train Epoch: 87 [40832/54000 (76%)] Loss: -1046648.062500\n",
      "Train Epoch: 87 [42240/54000 (78%)] Loss: -1093556.750000\n",
      "Train Epoch: 87 [43648/54000 (81%)] Loss: -1129742.000000\n",
      "Train Epoch: 87 [45056/54000 (83%)] Loss: -1117403.250000\n",
      "Train Epoch: 87 [46464/54000 (86%)] Loss: -1123429.250000\n",
      "Train Epoch: 87 [47872/54000 (89%)] Loss: -1039145.375000\n",
      "Train Epoch: 87 [49280/54000 (91%)] Loss: -1067845.875000\n",
      "Train Epoch: 87 [50688/54000 (94%)] Loss: -1162875.625000\n",
      "Train Epoch: 87 [52096/54000 (96%)] Loss: -1168253.500000\n",
      "Train Epoch: 87 [53504/54000 (99%)] Loss: -1085998.375000\n",
      "    epoch          : 87\n",
      "    loss           : -1077370.7446682465\n",
      "    val_loss       : -1080411.896419444\n",
      "Train Epoch: 88 [0/54000 (0%)] Loss: -1104334.500000\n",
      "Train Epoch: 88 [1408/54000 (3%)] Loss: -1003215.500000\n",
      "Train Epoch: 88 [2816/54000 (5%)] Loss: -1048075.250000\n",
      "Train Epoch: 88 [4224/54000 (8%)] Loss: -1098957.000000\n",
      "Train Epoch: 88 [5632/54000 (10%)] Loss: -1112329.500000\n",
      "Train Epoch: 88 [7040/54000 (13%)] Loss: -1090691.375000\n",
      "Train Epoch: 88 [8448/54000 (16%)] Loss: -1125830.500000\n",
      "Train Epoch: 88 [9856/54000 (18%)] Loss: -1083531.000000\n",
      "Train Epoch: 88 [11264/54000 (21%)] Loss: -1042591.937500\n",
      "Train Epoch: 88 [12672/54000 (23%)] Loss: -1061055.875000\n",
      "Train Epoch: 88 [14080/54000 (26%)] Loss: -1090882.750000\n",
      "Train Epoch: 88 [15488/54000 (29%)] Loss: -1117795.250000\n",
      "Train Epoch: 88 [16896/54000 (31%)] Loss: -1089138.625000\n",
      "Train Epoch: 88 [18304/54000 (34%)] Loss: -1088233.250000\n",
      "Train Epoch: 88 [19712/54000 (37%)] Loss: -1026997.437500\n",
      "Train Epoch: 88 [21120/54000 (39%)] Loss: -1084756.375000\n",
      "Train Epoch: 88 [22528/54000 (42%)] Loss: -1077910.000000\n",
      "Train Epoch: 88 [23936/54000 (44%)] Loss: -1090792.250000\n",
      "Train Epoch: 88 [25344/54000 (47%)] Loss: -1051077.750000\n",
      "Train Epoch: 88 [26752/54000 (50%)] Loss: -1038424.437500\n",
      "Train Epoch: 88 [28160/54000 (52%)] Loss: -1042667.125000\n",
      "Train Epoch: 88 [29568/54000 (55%)] Loss: -1123358.750000\n",
      "Train Epoch: 88 [30976/54000 (57%)] Loss: -1118939.000000\n",
      "Train Epoch: 88 [32384/54000 (60%)] Loss: -1089693.875000\n",
      "Train Epoch: 88 [33792/54000 (63%)] Loss: -1054976.750000\n",
      "Train Epoch: 88 [35200/54000 (65%)] Loss: -1053495.125000\n",
      "Train Epoch: 88 [36608/54000 (68%)] Loss: -1151845.625000\n",
      "Train Epoch: 88 [38016/54000 (70%)] Loss: -1091669.750000\n",
      "Train Epoch: 88 [39424/54000 (73%)] Loss: -1081402.375000\n",
      "Train Epoch: 88 [40832/54000 (76%)] Loss: -1093135.125000\n",
      "Train Epoch: 88 [42240/54000 (78%)] Loss: -1107245.750000\n",
      "Train Epoch: 88 [43648/54000 (81%)] Loss: -1102562.875000\n",
      "Train Epoch: 88 [45056/54000 (83%)] Loss: -1090723.625000\n",
      "Train Epoch: 88 [46464/54000 (86%)] Loss: -1056011.500000\n",
      "Train Epoch: 88 [47872/54000 (89%)] Loss: -1081856.500000\n",
      "Train Epoch: 88 [49280/54000 (91%)] Loss: -1096739.750000\n",
      "Train Epoch: 88 [50688/54000 (94%)] Loss: -1032117.000000\n",
      "Train Epoch: 88 [52096/54000 (96%)] Loss: -1052475.750000\n",
      "Train Epoch: 88 [53504/54000 (99%)] Loss: -1043184.625000\n",
      "    epoch          : 88\n",
      "    loss           : -1077790.8204976304\n",
      "    val_loss       : -1080507.6536345787\n",
      "Train Epoch: 89 [0/54000 (0%)] Loss: -1077753.250000\n",
      "Train Epoch: 89 [1408/54000 (3%)] Loss: -1015001.312500\n",
      "Train Epoch: 89 [2816/54000 (5%)] Loss: -1044054.312500\n",
      "Train Epoch: 89 [4224/54000 (8%)] Loss: -1039692.750000\n",
      "Train Epoch: 89 [5632/54000 (10%)] Loss: -1052564.500000\n",
      "Train Epoch: 89 [7040/54000 (13%)] Loss: -1112983.750000\n",
      "Train Epoch: 89 [8448/54000 (16%)] Loss: -1095212.875000\n",
      "Train Epoch: 89 [9856/54000 (18%)] Loss: -1061381.750000\n",
      "Train Epoch: 89 [11264/54000 (21%)] Loss: -1087368.750000\n",
      "Train Epoch: 89 [12672/54000 (23%)] Loss: -1118692.500000\n",
      "Train Epoch: 89 [14080/54000 (26%)] Loss: -1032050.312500\n",
      "Train Epoch: 89 [15488/54000 (29%)] Loss: -1096125.625000\n",
      "Train Epoch: 89 [16896/54000 (31%)] Loss: -1058755.750000\n",
      "Train Epoch: 89 [18304/54000 (34%)] Loss: -1076494.250000\n",
      "Train Epoch: 89 [19712/54000 (37%)] Loss: -1047227.312500\n",
      "Train Epoch: 89 [21120/54000 (39%)] Loss: -1095734.625000\n",
      "Train Epoch: 89 [22528/54000 (42%)] Loss: -1100285.000000\n",
      "Train Epoch: 89 [23936/54000 (44%)] Loss: -1080556.750000\n",
      "Train Epoch: 89 [25344/54000 (47%)] Loss: -1045007.375000\n",
      "Train Epoch: 89 [26752/54000 (50%)] Loss: -1059445.875000\n",
      "Train Epoch: 89 [28160/54000 (52%)] Loss: -1121778.500000\n",
      "Train Epoch: 89 [29568/54000 (55%)] Loss: -1060756.000000\n",
      "Train Epoch: 89 [30976/54000 (57%)] Loss: -1013064.125000\n",
      "Train Epoch: 89 [32384/54000 (60%)] Loss: -1068777.375000\n",
      "Train Epoch: 89 [33792/54000 (63%)] Loss: -1044419.750000\n",
      "Train Epoch: 89 [35200/54000 (65%)] Loss: -1063777.750000\n",
      "Train Epoch: 89 [36608/54000 (68%)] Loss: -1018119.750000\n",
      "Train Epoch: 89 [38016/54000 (70%)] Loss: -1121112.875000\n",
      "Train Epoch: 89 [39424/54000 (73%)] Loss: -1075545.375000\n",
      "Train Epoch: 89 [40832/54000 (76%)] Loss: -1152417.625000\n",
      "Train Epoch: 89 [42240/54000 (78%)] Loss: -1059359.375000\n",
      "Train Epoch: 89 [43648/54000 (81%)] Loss: -1065284.375000\n",
      "Train Epoch: 89 [45056/54000 (83%)] Loss: -1061324.250000\n",
      "Train Epoch: 89 [46464/54000 (86%)] Loss: -1060071.750000\n",
      "Train Epoch: 89 [47872/54000 (89%)] Loss: -1068640.000000\n",
      "Train Epoch: 89 [49280/54000 (91%)] Loss: -1120107.125000\n",
      "Train Epoch: 89 [50688/54000 (94%)] Loss: -1098324.375000\n",
      "Train Epoch: 89 [52096/54000 (96%)] Loss: -1109402.500000\n",
      "Train Epoch: 89 [53504/54000 (99%)] Loss: -1035293.812500\n",
      "    epoch          : 89\n",
      "    loss           : -1078180.5549466824\n",
      "    val_loss       : -1080716.2819226854\n",
      "Train Epoch: 90 [0/54000 (0%)] Loss: -1090037.875000\n",
      "Train Epoch: 90 [1408/54000 (3%)] Loss: -1085873.000000\n",
      "Train Epoch: 90 [2816/54000 (5%)] Loss: -1100643.750000\n",
      "Train Epoch: 90 [4224/54000 (8%)] Loss: -1093465.750000\n",
      "Train Epoch: 90 [5632/54000 (10%)] Loss: -1115076.250000\n",
      "Train Epoch: 90 [7040/54000 (13%)] Loss: -1022731.250000\n",
      "Train Epoch: 90 [8448/54000 (16%)] Loss: -1099076.250000\n",
      "Train Epoch: 90 [9856/54000 (18%)] Loss: -1051567.125000\n",
      "Train Epoch: 90 [11264/54000 (21%)] Loss: -1068188.000000\n",
      "Train Epoch: 90 [12672/54000 (23%)] Loss: -1067898.250000\n",
      "Train Epoch: 90 [14080/54000 (26%)] Loss: -1046861.625000\n",
      "Train Epoch: 90 [15488/54000 (29%)] Loss: -1039378.250000\n",
      "Train Epoch: 90 [16896/54000 (31%)] Loss: -1123826.125000\n",
      "Train Epoch: 90 [18304/54000 (34%)] Loss: -1094052.000000\n",
      "Train Epoch: 90 [19712/54000 (37%)] Loss: -1035799.437500\n",
      "Train Epoch: 90 [21120/54000 (39%)] Loss: -1110419.000000\n",
      "Train Epoch: 90 [22528/54000 (42%)] Loss: -1081745.125000\n",
      "Train Epoch: 90 [23936/54000 (44%)] Loss: -1084753.875000\n",
      "Train Epoch: 90 [25344/54000 (47%)] Loss: -1030262.937500\n",
      "Train Epoch: 90 [26752/54000 (50%)] Loss: -1150963.125000\n",
      "Train Epoch: 90 [28160/54000 (52%)] Loss: -1037837.750000\n",
      "Train Epoch: 90 [29568/54000 (55%)] Loss: -1132054.500000\n",
      "Train Epoch: 90 [30976/54000 (57%)] Loss: -1050851.375000\n",
      "Train Epoch: 90 [32384/54000 (60%)] Loss: -1091477.250000\n",
      "Train Epoch: 90 [33792/54000 (63%)] Loss: -1068492.125000\n",
      "Train Epoch: 90 [35200/54000 (65%)] Loss: -1128542.750000\n",
      "Train Epoch: 90 [36608/54000 (68%)] Loss: -1107179.625000\n",
      "Train Epoch: 90 [38016/54000 (70%)] Loss: -1036053.750000\n",
      "Train Epoch: 90 [39424/54000 (73%)] Loss: -979138.187500\n",
      "Train Epoch: 90 [40832/54000 (76%)] Loss: -1028197.125000\n",
      "Train Epoch: 90 [42240/54000 (78%)] Loss: -1046808.812500\n",
      "Train Epoch: 90 [43648/54000 (81%)] Loss: -1132235.375000\n",
      "Train Epoch: 90 [45056/54000 (83%)] Loss: -1095573.500000\n",
      "Train Epoch: 90 [46464/54000 (86%)] Loss: -1076028.750000\n",
      "Train Epoch: 90 [47872/54000 (89%)] Loss: -1073761.250000\n",
      "Train Epoch: 90 [49280/54000 (91%)] Loss: -1078757.625000\n",
      "Train Epoch: 90 [50688/54000 (94%)] Loss: -1055444.750000\n",
      "Train Epoch: 90 [52096/54000 (96%)] Loss: -1108141.250000\n",
      "Train Epoch: 90 [53504/54000 (99%)] Loss: -1088978.875000\n",
      "    epoch          : 90\n",
      "    loss           : -1078567.7471860189\n",
      "    val_loss       : -1081355.2124387051\n",
      "Train Epoch: 91 [0/54000 (0%)] Loss: -1065638.375000\n",
      "Train Epoch: 91 [1408/54000 (3%)] Loss: -1087222.875000\n",
      "Train Epoch: 91 [2816/54000 (5%)] Loss: -1086494.625000\n",
      "Train Epoch: 91 [4224/54000 (8%)] Loss: -1121780.750000\n",
      "Train Epoch: 91 [5632/54000 (10%)] Loss: -1057235.125000\n",
      "Train Epoch: 91 [7040/54000 (13%)] Loss: -1111189.000000\n",
      "Train Epoch: 91 [8448/54000 (16%)] Loss: -1111327.625000\n",
      "Train Epoch: 91 [9856/54000 (18%)] Loss: -1067756.250000\n",
      "Train Epoch: 91 [11264/54000 (21%)] Loss: -1036044.625000\n",
      "Train Epoch: 91 [12672/54000 (23%)] Loss: -1077656.375000\n",
      "Train Epoch: 91 [14080/54000 (26%)] Loss: -993910.812500\n",
      "Train Epoch: 91 [15488/54000 (29%)] Loss: -1091608.000000\n",
      "Train Epoch: 91 [16896/54000 (31%)] Loss: -1082978.125000\n",
      "Train Epoch: 91 [18304/54000 (34%)] Loss: -1107647.125000\n",
      "Train Epoch: 91 [19712/54000 (37%)] Loss: -1075001.875000\n",
      "Train Epoch: 91 [21120/54000 (39%)] Loss: -1066782.000000\n",
      "Train Epoch: 91 [22528/54000 (42%)] Loss: -1087003.125000\n",
      "Train Epoch: 91 [23936/54000 (44%)] Loss: -1083526.750000\n",
      "Train Epoch: 91 [25344/54000 (47%)] Loss: -1098969.375000\n",
      "Train Epoch: 91 [26752/54000 (50%)] Loss: -1127513.125000\n",
      "Train Epoch: 91 [28160/54000 (52%)] Loss: -1084511.625000\n",
      "Train Epoch: 91 [29568/54000 (55%)] Loss: -1100310.375000\n",
      "Train Epoch: 91 [30976/54000 (57%)] Loss: -1072975.750000\n",
      "Train Epoch: 91 [32384/54000 (60%)] Loss: -1062300.125000\n",
      "Train Epoch: 91 [33792/54000 (63%)] Loss: -1071811.875000\n",
      "Train Epoch: 91 [35200/54000 (65%)] Loss: -1154591.250000\n",
      "Train Epoch: 91 [36608/54000 (68%)] Loss: -1084096.250000\n",
      "Train Epoch: 91 [38016/54000 (70%)] Loss: -1079293.875000\n",
      "Train Epoch: 91 [39424/54000 (73%)] Loss: -1116600.250000\n",
      "Train Epoch: 91 [40832/54000 (76%)] Loss: -1112764.625000\n",
      "Train Epoch: 91 [42240/54000 (78%)] Loss: -1047064.500000\n",
      "Train Epoch: 91 [43648/54000 (81%)] Loss: -1133854.875000\n",
      "Train Epoch: 91 [45056/54000 (83%)] Loss: -1102556.000000\n",
      "Train Epoch: 91 [46464/54000 (86%)] Loss: -1095960.000000\n",
      "Train Epoch: 91 [47872/54000 (89%)] Loss: -1030145.625000\n",
      "Train Epoch: 91 [49280/54000 (91%)] Loss: -1051619.000000\n",
      "Train Epoch: 91 [50688/54000 (94%)] Loss: -1074508.625000\n",
      "Train Epoch: 91 [52096/54000 (96%)] Loss: -1074791.750000\n",
      "Train Epoch: 91 [53504/54000 (99%)] Loss: -1069754.000000\n",
      "    epoch          : 91\n",
      "    loss           : -1078805.651806872\n",
      "    val_loss       : -1081519.797771048\n",
      "Train Epoch: 92 [0/54000 (0%)] Loss: -1071503.625000\n",
      "Train Epoch: 92 [1408/54000 (3%)] Loss: -1080340.000000\n",
      "Train Epoch: 92 [2816/54000 (5%)] Loss: -1118211.125000\n",
      "Train Epoch: 92 [4224/54000 (8%)] Loss: -1056613.875000\n",
      "Train Epoch: 92 [5632/54000 (10%)] Loss: -1034359.062500\n",
      "Train Epoch: 92 [7040/54000 (13%)] Loss: -1059595.250000\n",
      "Train Epoch: 92 [8448/54000 (16%)] Loss: -1125910.500000\n",
      "Train Epoch: 92 [9856/54000 (18%)] Loss: -1034387.125000\n",
      "Train Epoch: 92 [11264/54000 (21%)] Loss: -1063981.375000\n",
      "Train Epoch: 92 [12672/54000 (23%)] Loss: -1047065.500000\n",
      "Train Epoch: 92 [14080/54000 (26%)] Loss: -1139083.500000\n",
      "Train Epoch: 92 [15488/54000 (29%)] Loss: -1091787.875000\n",
      "Train Epoch: 92 [16896/54000 (31%)] Loss: -1041235.937500\n",
      "Train Epoch: 92 [18304/54000 (34%)] Loss: -1086292.375000\n",
      "Train Epoch: 92 [19712/54000 (37%)] Loss: -1125370.500000\n",
      "Train Epoch: 92 [21120/54000 (39%)] Loss: -1065033.625000\n",
      "Train Epoch: 92 [22528/54000 (42%)] Loss: -1120272.125000\n",
      "Train Epoch: 92 [23936/54000 (44%)] Loss: -1106226.750000\n",
      "Train Epoch: 92 [25344/54000 (47%)] Loss: -1110285.750000\n",
      "Train Epoch: 92 [26752/54000 (50%)] Loss: -1063187.375000\n",
      "Train Epoch: 92 [28160/54000 (52%)] Loss: -1084141.125000\n",
      "Train Epoch: 92 [29568/54000 (55%)] Loss: -1097514.750000\n",
      "Train Epoch: 92 [30976/54000 (57%)] Loss: -1047595.187500\n",
      "Train Epoch: 92 [32384/54000 (60%)] Loss: -1121394.750000\n",
      "Train Epoch: 92 [33792/54000 (63%)] Loss: -1067665.000000\n",
      "Train Epoch: 92 [35200/54000 (65%)] Loss: -1129175.875000\n",
      "Train Epoch: 92 [36608/54000 (68%)] Loss: -1099717.000000\n",
      "Train Epoch: 92 [38016/54000 (70%)] Loss: -1038897.250000\n",
      "Train Epoch: 92 [39424/54000 (73%)] Loss: -1115701.875000\n",
      "Train Epoch: 92 [40832/54000 (76%)] Loss: -1035422.062500\n",
      "Train Epoch: 92 [42240/54000 (78%)] Loss: -1097783.125000\n",
      "Train Epoch: 92 [43648/54000 (81%)] Loss: -1118047.375000\n",
      "Train Epoch: 92 [45056/54000 (83%)] Loss: -1076955.750000\n",
      "Train Epoch: 92 [46464/54000 (86%)] Loss: -1061041.125000\n",
      "Train Epoch: 92 [47872/54000 (89%)] Loss: -1043908.937500\n",
      "Train Epoch: 92 [49280/54000 (91%)] Loss: -1113268.625000\n",
      "Train Epoch: 92 [50688/54000 (94%)] Loss: -1040445.312500\n",
      "Train Epoch: 92 [52096/54000 (96%)] Loss: -1047149.687500\n",
      "Train Epoch: 92 [53504/54000 (99%)] Loss: -1115736.625000\n",
      "    epoch          : 92\n",
      "    loss           : -1079098.6603969194\n",
      "    val_loss       : -1081656.312941531\n",
      "Train Epoch: 93 [0/54000 (0%)] Loss: -1084253.500000\n",
      "Train Epoch: 93 [1408/54000 (3%)] Loss: -1093702.750000\n",
      "Train Epoch: 93 [2816/54000 (5%)] Loss: -1050111.250000\n",
      "Train Epoch: 93 [4224/54000 (8%)] Loss: -1057549.000000\n",
      "Train Epoch: 93 [5632/54000 (10%)] Loss: -1103146.250000\n",
      "Train Epoch: 93 [7040/54000 (13%)] Loss: -1111988.875000\n",
      "Train Epoch: 93 [8448/54000 (16%)] Loss: -1050906.375000\n",
      "Train Epoch: 93 [9856/54000 (18%)] Loss: -1137545.750000\n",
      "Train Epoch: 93 [11264/54000 (21%)] Loss: -1092582.000000\n",
      "Train Epoch: 93 [12672/54000 (23%)] Loss: -1131245.250000\n",
      "Train Epoch: 93 [14080/54000 (26%)] Loss: -1078927.500000\n",
      "Train Epoch: 93 [15488/54000 (29%)] Loss: -1069674.750000\n",
      "Train Epoch: 93 [16896/54000 (31%)] Loss: -1099775.500000\n",
      "Train Epoch: 93 [18304/54000 (34%)] Loss: -1067765.000000\n",
      "Train Epoch: 93 [19712/54000 (37%)] Loss: -1101930.750000\n",
      "Train Epoch: 93 [21120/54000 (39%)] Loss: -1057459.625000\n",
      "Train Epoch: 93 [22528/54000 (42%)] Loss: -1123665.750000\n",
      "Train Epoch: 93 [23936/54000 (44%)] Loss: -1062737.500000\n",
      "Train Epoch: 93 [25344/54000 (47%)] Loss: -1009230.687500\n",
      "Train Epoch: 93 [26752/54000 (50%)] Loss: -1061819.375000\n",
      "Train Epoch: 93 [28160/54000 (52%)] Loss: -1074633.250000\n",
      "Train Epoch: 93 [29568/54000 (55%)] Loss: -1069826.250000\n",
      "Train Epoch: 93 [30976/54000 (57%)] Loss: -1027281.750000\n",
      "Train Epoch: 93 [32384/54000 (60%)] Loss: -1105790.500000\n",
      "Train Epoch: 93 [33792/54000 (63%)] Loss: -1055649.875000\n",
      "Train Epoch: 93 [35200/54000 (65%)] Loss: -1081868.500000\n",
      "Train Epoch: 93 [36608/54000 (68%)] Loss: -1117717.500000\n",
      "Train Epoch: 93 [38016/54000 (70%)] Loss: -1084900.500000\n",
      "Train Epoch: 93 [39424/54000 (73%)] Loss: -1124541.750000\n",
      "Train Epoch: 93 [40832/54000 (76%)] Loss: -1032391.937500\n",
      "Train Epoch: 93 [42240/54000 (78%)] Loss: -1140036.750000\n",
      "Train Epoch: 93 [43648/54000 (81%)] Loss: -1124960.375000\n",
      "Train Epoch: 93 [45056/54000 (83%)] Loss: -1069933.250000\n",
      "Train Epoch: 93 [46464/54000 (86%)] Loss: -1107054.750000\n",
      "Train Epoch: 93 [47872/54000 (89%)] Loss: -1018517.125000\n",
      "Train Epoch: 93 [49280/54000 (91%)] Loss: -1090678.875000\n",
      "Train Epoch: 93 [50688/54000 (94%)] Loss: -1095619.875000\n",
      "Train Epoch: 93 [52096/54000 (96%)] Loss: -1108833.375000\n",
      "Train Epoch: 93 [53504/54000 (99%)] Loss: -1098684.000000\n",
      "    epoch          : 93\n",
      "    loss           : -1079401.4059537915\n",
      "    val_loss       : -1082010.8844046085\n",
      "Train Epoch: 94 [0/54000 (0%)] Loss: -1050986.375000\n",
      "Train Epoch: 94 [1408/54000 (3%)] Loss: -1060934.875000\n",
      "Train Epoch: 94 [2816/54000 (5%)] Loss: -1058281.625000\n",
      "Train Epoch: 94 [4224/54000 (8%)] Loss: -1094470.875000\n",
      "Train Epoch: 94 [5632/54000 (10%)] Loss: -1073247.125000\n",
      "Train Epoch: 94 [7040/54000 (13%)] Loss: -1055486.375000\n",
      "Train Epoch: 94 [8448/54000 (16%)] Loss: -1094444.250000\n",
      "Train Epoch: 94 [9856/54000 (18%)] Loss: -1056492.125000\n",
      "Train Epoch: 94 [11264/54000 (21%)] Loss: -1097390.125000\n",
      "Train Epoch: 94 [12672/54000 (23%)] Loss: -1086692.875000\n",
      "Train Epoch: 94 [14080/54000 (26%)] Loss: -1075656.250000\n",
      "Train Epoch: 94 [15488/54000 (29%)] Loss: -1066721.750000\n",
      "Train Epoch: 94 [16896/54000 (31%)] Loss: -1030648.187500\n",
      "Train Epoch: 94 [18304/54000 (34%)] Loss: -1082433.875000\n",
      "Train Epoch: 94 [19712/54000 (37%)] Loss: -1020868.687500\n",
      "Train Epoch: 94 [21120/54000 (39%)] Loss: -1122538.875000\n",
      "Train Epoch: 94 [22528/54000 (42%)] Loss: -1089175.375000\n",
      "Train Epoch: 94 [23936/54000 (44%)] Loss: -1083750.000000\n",
      "Train Epoch: 94 [25344/54000 (47%)] Loss: -1077674.000000\n",
      "Train Epoch: 94 [26752/54000 (50%)] Loss: -1085736.125000\n",
      "Train Epoch: 94 [28160/54000 (52%)] Loss: -1136891.625000\n",
      "Train Epoch: 94 [29568/54000 (55%)] Loss: -1089364.000000\n",
      "Train Epoch: 94 [30976/54000 (57%)] Loss: -1056956.500000\n",
      "Train Epoch: 94 [32384/54000 (60%)] Loss: -1049601.125000\n",
      "Train Epoch: 94 [33792/54000 (63%)] Loss: -1110386.000000\n",
      "Train Epoch: 94 [35200/54000 (65%)] Loss: -1102937.875000\n",
      "Train Epoch: 94 [36608/54000 (68%)] Loss: -1055327.125000\n",
      "Train Epoch: 94 [38016/54000 (70%)] Loss: -1113849.000000\n",
      "Train Epoch: 94 [39424/54000 (73%)] Loss: -1114186.875000\n",
      "Train Epoch: 94 [40832/54000 (76%)] Loss: -1100008.375000\n",
      "Train Epoch: 94 [42240/54000 (78%)] Loss: -1089952.125000\n",
      "Train Epoch: 94 [43648/54000 (81%)] Loss: -1007134.812500\n",
      "Train Epoch: 94 [45056/54000 (83%)] Loss: -1096427.625000\n",
      "Train Epoch: 94 [46464/54000 (86%)] Loss: -1091709.625000\n",
      "Train Epoch: 94 [47872/54000 (89%)] Loss: -1122146.500000\n",
      "Train Epoch: 94 [49280/54000 (91%)] Loss: -1092028.875000\n",
      "Train Epoch: 94 [50688/54000 (94%)] Loss: -1086825.250000\n",
      "Train Epoch: 94 [52096/54000 (96%)] Loss: -1062142.125000\n",
      "Train Epoch: 94 [53504/54000 (99%)] Loss: -1084269.875000\n",
      "    epoch          : 94\n",
      "    loss           : -1079529.1550651658\n",
      "    val_loss       : -1082251.7131010015\n",
      "Train Epoch: 95 [0/54000 (0%)] Loss: -1063326.500000\n",
      "Train Epoch: 95 [1408/54000 (3%)] Loss: -1070411.000000\n",
      "Train Epoch: 95 [2816/54000 (5%)] Loss: -1040061.062500\n",
      "Train Epoch: 95 [4224/54000 (8%)] Loss: -1108588.875000\n",
      "Train Epoch: 95 [5632/54000 (10%)] Loss: -1086723.625000\n",
      "Train Epoch: 95 [7040/54000 (13%)] Loss: -1121955.000000\n",
      "Train Epoch: 95 [8448/54000 (16%)] Loss: -1087863.375000\n",
      "Train Epoch: 95 [9856/54000 (18%)] Loss: -1050195.750000\n",
      "Train Epoch: 95 [11264/54000 (21%)] Loss: -1090731.750000\n",
      "Train Epoch: 95 [12672/54000 (23%)] Loss: -1026649.250000\n",
      "Train Epoch: 95 [14080/54000 (26%)] Loss: -1065443.875000\n",
      "Train Epoch: 95 [15488/54000 (29%)] Loss: -1075261.625000\n",
      "Train Epoch: 95 [16896/54000 (31%)] Loss: -1120663.875000\n",
      "Train Epoch: 95 [18304/54000 (34%)] Loss: -1100248.125000\n",
      "Train Epoch: 95 [19712/54000 (37%)] Loss: -1156100.875000\n",
      "Train Epoch: 95 [21120/54000 (39%)] Loss: -1040584.687500\n",
      "Train Epoch: 95 [22528/54000 (42%)] Loss: -1127610.875000\n",
      "Train Epoch: 95 [23936/54000 (44%)] Loss: -1073936.375000\n",
      "Train Epoch: 95 [25344/54000 (47%)] Loss: -1085395.500000\n",
      "Train Epoch: 95 [26752/54000 (50%)] Loss: -1062338.750000\n",
      "Train Epoch: 95 [28160/54000 (52%)] Loss: -1097060.875000\n",
      "Train Epoch: 95 [29568/54000 (55%)] Loss: -1090861.000000\n",
      "Train Epoch: 95 [30976/54000 (57%)] Loss: -1086848.000000\n",
      "Train Epoch: 95 [32384/54000 (60%)] Loss: -1064963.125000\n",
      "Train Epoch: 95 [33792/54000 (63%)] Loss: -1095218.875000\n",
      "Train Epoch: 95 [35200/54000 (65%)] Loss: -1132944.500000\n",
      "Train Epoch: 95 [36608/54000 (68%)] Loss: -1108895.750000\n",
      "Train Epoch: 95 [38016/54000 (70%)] Loss: -1138313.000000\n",
      "Train Epoch: 95 [39424/54000 (73%)] Loss: -1062757.875000\n",
      "Train Epoch: 95 [40832/54000 (76%)] Loss: -1106332.375000\n",
      "Train Epoch: 95 [42240/54000 (78%)] Loss: -1107783.250000\n",
      "Train Epoch: 95 [43648/54000 (81%)] Loss: -1060547.125000\n",
      "Train Epoch: 95 [45056/54000 (83%)] Loss: -1059338.500000\n",
      "Train Epoch: 95 [46464/54000 (86%)] Loss: -1086198.375000\n",
      "Train Epoch: 95 [47872/54000 (89%)] Loss: -1069207.500000\n",
      "Train Epoch: 95 [49280/54000 (91%)] Loss: -1130776.500000\n",
      "Train Epoch: 95 [50688/54000 (94%)] Loss: -1115209.500000\n",
      "Train Epoch: 95 [52096/54000 (96%)] Loss: -1119979.125000\n",
      "Train Epoch: 95 [53504/54000 (99%)] Loss: -1085601.250000\n",
      "    epoch          : 95\n",
      "    loss           : -1079922.3321978673\n",
      "    val_loss       : -1082618.2734089303\n",
      "Train Epoch: 96 [0/54000 (0%)] Loss: -1119129.625000\n",
      "Train Epoch: 96 [1408/54000 (3%)] Loss: -1098562.500000\n",
      "Train Epoch: 96 [2816/54000 (5%)] Loss: -1105114.250000\n",
      "Train Epoch: 96 [4224/54000 (8%)] Loss: -1079655.625000\n",
      "Train Epoch: 96 [5632/54000 (10%)] Loss: -1100238.625000\n",
      "Train Epoch: 96 [7040/54000 (13%)] Loss: -1070829.625000\n",
      "Train Epoch: 96 [8448/54000 (16%)] Loss: -1047290.500000\n",
      "Train Epoch: 96 [9856/54000 (18%)] Loss: -1106255.375000\n",
      "Train Epoch: 96 [11264/54000 (21%)] Loss: -1101054.375000\n",
      "Train Epoch: 96 [12672/54000 (23%)] Loss: -1010450.312500\n",
      "Train Epoch: 96 [14080/54000 (26%)] Loss: -1067267.000000\n",
      "Train Epoch: 96 [15488/54000 (29%)] Loss: -1063336.250000\n",
      "Train Epoch: 96 [16896/54000 (31%)] Loss: -1112816.500000\n",
      "Train Epoch: 96 [18304/54000 (34%)] Loss: -1062346.000000\n",
      "Train Epoch: 96 [19712/54000 (37%)] Loss: -1069592.375000\n",
      "Train Epoch: 96 [21120/54000 (39%)] Loss: -1099583.625000\n",
      "Train Epoch: 96 [22528/54000 (42%)] Loss: -1102599.125000\n",
      "Train Epoch: 96 [23936/54000 (44%)] Loss: -1105792.125000\n",
      "Train Epoch: 96 [25344/54000 (47%)] Loss: -1028971.812500\n",
      "Train Epoch: 96 [26752/54000 (50%)] Loss: -1103364.000000\n",
      "Train Epoch: 96 [28160/54000 (52%)] Loss: -1050800.500000\n",
      "Train Epoch: 96 [29568/54000 (55%)] Loss: -1057352.750000\n",
      "Train Epoch: 96 [30976/54000 (57%)] Loss: -1058256.500000\n",
      "Train Epoch: 96 [32384/54000 (60%)] Loss: -1109229.125000\n",
      "Train Epoch: 96 [33792/54000 (63%)] Loss: -995153.500000\n",
      "Train Epoch: 96 [35200/54000 (65%)] Loss: -1057266.500000\n",
      "Train Epoch: 96 [36608/54000 (68%)] Loss: -1096204.250000\n",
      "Train Epoch: 96 [38016/54000 (70%)] Loss: -1133326.875000\n",
      "Train Epoch: 96 [39424/54000 (73%)] Loss: -1088808.500000\n",
      "Train Epoch: 96 [40832/54000 (76%)] Loss: -1099085.500000\n",
      "Train Epoch: 96 [42240/54000 (78%)] Loss: -1050448.375000\n",
      "Train Epoch: 96 [43648/54000 (81%)] Loss: -1110667.625000\n",
      "Train Epoch: 96 [45056/54000 (83%)] Loss: -1124684.875000\n",
      "Train Epoch: 96 [46464/54000 (86%)] Loss: -1050479.375000\n",
      "Train Epoch: 96 [47872/54000 (89%)] Loss: -1079738.375000\n",
      "Train Epoch: 96 [49280/54000 (91%)] Loss: -1089595.875000\n",
      "Train Epoch: 96 [50688/54000 (94%)] Loss: -1140350.625000\n",
      "Train Epoch: 96 [52096/54000 (96%)] Loss: -1115388.625000\n",
      "Train Epoch: 96 [53504/54000 (99%)] Loss: -1061228.875000\n",
      "    epoch          : 96\n",
      "    loss           : -1080192.3462677726\n",
      "    val_loss       : -1082836.971381005\n",
      "Train Epoch: 97 [0/54000 (0%)] Loss: -1056031.375000\n",
      "Train Epoch: 97 [1408/54000 (3%)] Loss: -1133313.750000\n",
      "Train Epoch: 97 [2816/54000 (5%)] Loss: -1090333.000000\n",
      "Train Epoch: 97 [4224/54000 (8%)] Loss: -1095690.000000\n",
      "Train Epoch: 97 [5632/54000 (10%)] Loss: -1107229.000000\n",
      "Train Epoch: 97 [7040/54000 (13%)] Loss: -1067775.750000\n",
      "Train Epoch: 97 [8448/54000 (16%)] Loss: -1071346.750000\n",
      "Train Epoch: 97 [9856/54000 (18%)] Loss: -974093.750000\n",
      "Train Epoch: 97 [11264/54000 (21%)] Loss: -1060282.000000\n",
      "Train Epoch: 97 [12672/54000 (23%)] Loss: -1112032.000000\n",
      "Train Epoch: 97 [14080/54000 (26%)] Loss: -1017106.500000\n",
      "Train Epoch: 97 [15488/54000 (29%)] Loss: -1115859.000000\n",
      "Train Epoch: 97 [16896/54000 (31%)] Loss: -1057397.125000\n",
      "Train Epoch: 97 [18304/54000 (34%)] Loss: -1041758.625000\n",
      "Train Epoch: 97 [19712/54000 (37%)] Loss: -1076780.125000\n",
      "Train Epoch: 97 [21120/54000 (39%)] Loss: -1101929.875000\n",
      "Train Epoch: 97 [22528/54000 (42%)] Loss: -1076069.375000\n",
      "Train Epoch: 97 [23936/54000 (44%)] Loss: -1059530.625000\n",
      "Train Epoch: 97 [25344/54000 (47%)] Loss: -1127109.500000\n",
      "Train Epoch: 97 [26752/54000 (50%)] Loss: -1069467.500000\n",
      "Train Epoch: 97 [28160/54000 (52%)] Loss: -1093473.625000\n",
      "Train Epoch: 97 [29568/54000 (55%)] Loss: -1077328.250000\n",
      "Train Epoch: 97 [30976/54000 (57%)] Loss: -1115907.625000\n",
      "Train Epoch: 97 [32384/54000 (60%)] Loss: -1041972.937500\n",
      "Train Epoch: 97 [33792/54000 (63%)] Loss: -1094559.250000\n",
      "Train Epoch: 97 [35200/54000 (65%)] Loss: -1085651.250000\n",
      "Train Epoch: 97 [36608/54000 (68%)] Loss: -1081382.500000\n",
      "Train Epoch: 97 [38016/54000 (70%)] Loss: -1064391.500000\n",
      "Train Epoch: 97 [39424/54000 (73%)] Loss: -1085708.000000\n",
      "Train Epoch: 97 [40832/54000 (76%)] Loss: -1071483.500000\n",
      "Train Epoch: 97 [42240/54000 (78%)] Loss: -1040264.375000\n",
      "Train Epoch: 97 [43648/54000 (81%)] Loss: -1084070.125000\n",
      "Train Epoch: 97 [45056/54000 (83%)] Loss: -1112473.500000\n",
      "Train Epoch: 97 [46464/54000 (86%)] Loss: -1077200.875000\n",
      "Train Epoch: 97 [47872/54000 (89%)] Loss: -1072558.000000\n",
      "Train Epoch: 97 [49280/54000 (91%)] Loss: -1083860.875000\n",
      "Train Epoch: 97 [50688/54000 (94%)] Loss: -1130039.375000\n",
      "Train Epoch: 97 [52096/54000 (96%)] Loss: -1058629.625000\n",
      "Train Epoch: 97 [53504/54000 (99%)] Loss: -1065268.750000\n",
      "    epoch          : 97\n",
      "    loss           : -1080538.1261848342\n",
      "    val_loss       : -1083239.868805581\n",
      "Train Epoch: 98 [0/54000 (0%)] Loss: -1162471.625000\n",
      "Train Epoch: 98 [1408/54000 (3%)] Loss: -1103062.500000\n",
      "Train Epoch: 98 [2816/54000 (5%)] Loss: -1084534.500000\n",
      "Train Epoch: 98 [4224/54000 (8%)] Loss: -1019661.812500\n",
      "Train Epoch: 98 [5632/54000 (10%)] Loss: -1101123.000000\n",
      "Train Epoch: 98 [7040/54000 (13%)] Loss: -1077669.750000\n",
      "Train Epoch: 98 [8448/54000 (16%)] Loss: -1089900.750000\n",
      "Train Epoch: 98 [9856/54000 (18%)] Loss: -1114141.750000\n",
      "Train Epoch: 98 [11264/54000 (21%)] Loss: -1095436.250000\n",
      "Train Epoch: 98 [12672/54000 (23%)] Loss: -1163122.750000\n",
      "Train Epoch: 98 [14080/54000 (26%)] Loss: -1082332.625000\n",
      "Train Epoch: 98 [15488/54000 (29%)] Loss: -1076213.750000\n",
      "Train Epoch: 98 [16896/54000 (31%)] Loss: -1088704.750000\n",
      "Train Epoch: 98 [18304/54000 (34%)] Loss: -1120338.875000\n",
      "Train Epoch: 98 [19712/54000 (37%)] Loss: -1054335.875000\n",
      "Train Epoch: 98 [21120/54000 (39%)] Loss: -1092647.000000\n",
      "Train Epoch: 98 [22528/54000 (42%)] Loss: -1099062.125000\n",
      "Train Epoch: 98 [23936/54000 (44%)] Loss: -1104157.500000\n",
      "Train Epoch: 98 [25344/54000 (47%)] Loss: -1030197.812500\n",
      "Train Epoch: 98 [26752/54000 (50%)] Loss: -1076697.375000\n",
      "Train Epoch: 98 [28160/54000 (52%)] Loss: -1039650.937500\n",
      "Train Epoch: 98 [29568/54000 (55%)] Loss: -1070799.875000\n",
      "Train Epoch: 98 [30976/54000 (57%)] Loss: -1087250.750000\n",
      "Train Epoch: 98 [32384/54000 (60%)] Loss: -1057553.250000\n",
      "Train Epoch: 98 [33792/54000 (63%)] Loss: -1087072.875000\n",
      "Train Epoch: 98 [35200/54000 (65%)] Loss: -1056130.750000\n",
      "Train Epoch: 98 [36608/54000 (68%)] Loss: -1150896.375000\n",
      "Train Epoch: 98 [38016/54000 (70%)] Loss: -1150135.125000\n",
      "Train Epoch: 98 [39424/54000 (73%)] Loss: -1083392.750000\n",
      "Train Epoch: 98 [40832/54000 (76%)] Loss: -1067625.000000\n",
      "Train Epoch: 98 [42240/54000 (78%)] Loss: -1076968.125000\n",
      "Train Epoch: 98 [43648/54000 (81%)] Loss: -1068863.375000\n",
      "Train Epoch: 98 [45056/54000 (83%)] Loss: -1104961.250000\n",
      "Train Epoch: 98 [46464/54000 (86%)] Loss: -1056351.750000\n",
      "Train Epoch: 98 [47872/54000 (89%)] Loss: -1022650.562500\n",
      "Train Epoch: 98 [49280/54000 (91%)] Loss: -1126855.500000\n",
      "Train Epoch: 98 [50688/54000 (94%)] Loss: -1090571.125000\n",
      "Train Epoch: 98 [52096/54000 (96%)] Loss: -1079257.125000\n",
      "Train Epoch: 98 [53504/54000 (99%)] Loss: -1118894.500000\n",
      "    epoch          : 98\n",
      "    loss           : -1080673.7150473935\n",
      "    val_loss       : -1083367.65226843\n",
      "Train Epoch: 99 [0/54000 (0%)] Loss: -1068541.125000\n",
      "Train Epoch: 99 [1408/54000 (3%)] Loss: -1077096.625000\n",
      "Train Epoch: 99 [2816/54000 (5%)] Loss: -1113428.000000\n",
      "Train Epoch: 99 [4224/54000 (8%)] Loss: -1156643.250000\n",
      "Train Epoch: 99 [5632/54000 (10%)] Loss: -1136835.250000\n",
      "Train Epoch: 99 [7040/54000 (13%)] Loss: -1082487.875000\n",
      "Train Epoch: 99 [8448/54000 (16%)] Loss: -1082906.000000\n",
      "Train Epoch: 99 [9856/54000 (18%)] Loss: -1057500.500000\n",
      "Train Epoch: 99 [11264/54000 (21%)] Loss: -1074277.625000\n",
      "Train Epoch: 99 [12672/54000 (23%)] Loss: -1024480.312500\n",
      "Train Epoch: 99 [14080/54000 (26%)] Loss: -1122404.875000\n",
      "Train Epoch: 99 [15488/54000 (29%)] Loss: -1109006.000000\n",
      "Train Epoch: 99 [16896/54000 (31%)] Loss: -1096134.375000\n",
      "Train Epoch: 99 [18304/54000 (34%)] Loss: -1085779.250000\n",
      "Train Epoch: 99 [19712/54000 (37%)] Loss: -1109666.875000\n",
      "Train Epoch: 99 [21120/54000 (39%)] Loss: -1087457.000000\n",
      "Train Epoch: 99 [22528/54000 (42%)] Loss: -1012358.812500\n",
      "Train Epoch: 99 [23936/54000 (44%)] Loss: -1119619.875000\n",
      "Train Epoch: 99 [25344/54000 (47%)] Loss: -1073677.375000\n",
      "Train Epoch: 99 [26752/54000 (50%)] Loss: -1047067.187500\n",
      "Train Epoch: 99 [28160/54000 (52%)] Loss: -1130539.750000\n",
      "Train Epoch: 99 [29568/54000 (55%)] Loss: -1055123.500000\n",
      "Train Epoch: 99 [30976/54000 (57%)] Loss: -1065113.250000\n",
      "Train Epoch: 99 [32384/54000 (60%)] Loss: -1177169.875000\n",
      "Train Epoch: 99 [33792/54000 (63%)] Loss: -1068220.375000\n",
      "Train Epoch: 99 [35200/54000 (65%)] Loss: -1107661.500000\n",
      "Train Epoch: 99 [36608/54000 (68%)] Loss: -1098220.250000\n",
      "Train Epoch: 99 [38016/54000 (70%)] Loss: -1118760.375000\n",
      "Train Epoch: 99 [39424/54000 (73%)] Loss: -1070597.750000\n",
      "Train Epoch: 99 [40832/54000 (76%)] Loss: -1081863.875000\n",
      "Train Epoch: 99 [42240/54000 (78%)] Loss: -1129417.875000\n",
      "Train Epoch: 99 [43648/54000 (81%)] Loss: -1126216.875000\n",
      "Train Epoch: 99 [45056/54000 (83%)] Loss: -1084055.125000\n",
      "Train Epoch: 99 [46464/54000 (86%)] Loss: -1041554.812500\n",
      "Train Epoch: 99 [47872/54000 (89%)] Loss: -1068335.875000\n",
      "Train Epoch: 99 [49280/54000 (91%)] Loss: -1119379.125000\n",
      "Train Epoch: 99 [50688/54000 (94%)] Loss: -1065024.250000\n",
      "Train Epoch: 99 [52096/54000 (96%)] Loss: -1020397.875000\n",
      "Train Epoch: 99 [53504/54000 (99%)] Loss: -1079601.875000\n",
      "    epoch          : 99\n",
      "    loss           : -1081089.441943128\n",
      "    val_loss       : -1083607.3614242228\n",
      "Train Epoch: 100 [0/54000 (0%)] Loss: -1019830.000000\n",
      "Train Epoch: 100 [1408/54000 (3%)] Loss: -1079629.875000\n",
      "Train Epoch: 100 [2816/54000 (5%)] Loss: -1111348.875000\n",
      "Train Epoch: 100 [4224/54000 (8%)] Loss: -1092849.750000\n",
      "Train Epoch: 100 [5632/54000 (10%)] Loss: -1087470.000000\n",
      "Train Epoch: 100 [7040/54000 (13%)] Loss: -1078576.875000\n",
      "Train Epoch: 100 [8448/54000 (16%)] Loss: -1092071.875000\n",
      "Train Epoch: 100 [9856/54000 (18%)] Loss: -1129046.625000\n",
      "Train Epoch: 100 [11264/54000 (21%)] Loss: -1083764.250000\n",
      "Train Epoch: 100 [12672/54000 (23%)] Loss: -1086545.875000\n",
      "Train Epoch: 100 [14080/54000 (26%)] Loss: -1049940.375000\n",
      "Train Epoch: 100 [15488/54000 (29%)] Loss: -1082317.750000\n",
      "Train Epoch: 100 [16896/54000 (31%)] Loss: -1066477.875000\n",
      "Train Epoch: 100 [18304/54000 (34%)] Loss: -1144240.125000\n",
      "Train Epoch: 100 [19712/54000 (37%)] Loss: -1025989.500000\n",
      "Train Epoch: 100 [21120/54000 (39%)] Loss: -1050536.000000\n",
      "Train Epoch: 100 [22528/54000 (42%)] Loss: -1094215.875000\n",
      "Train Epoch: 100 [23936/54000 (44%)] Loss: -1079968.250000\n",
      "Train Epoch: 100 [25344/54000 (47%)] Loss: -1063027.750000\n",
      "Train Epoch: 100 [26752/54000 (50%)] Loss: -1129861.500000\n",
      "Train Epoch: 100 [28160/54000 (52%)] Loss: -1056156.875000\n",
      "Train Epoch: 100 [29568/54000 (55%)] Loss: -1093681.875000\n",
      "Train Epoch: 100 [30976/54000 (57%)] Loss: -1106043.500000\n",
      "Train Epoch: 100 [32384/54000 (60%)] Loss: -1023038.812500\n",
      "Train Epoch: 100 [33792/54000 (63%)] Loss: -1083249.750000\n",
      "Train Epoch: 100 [35200/54000 (65%)] Loss: -1070924.875000\n",
      "Train Epoch: 100 [36608/54000 (68%)] Loss: -1063861.750000\n",
      "Train Epoch: 100 [38016/54000 (70%)] Loss: -1133254.375000\n",
      "Train Epoch: 100 [39424/54000 (73%)] Loss: -1077692.250000\n",
      "Train Epoch: 100 [40832/54000 (76%)] Loss: -1087325.125000\n",
      "Train Epoch: 100 [42240/54000 (78%)] Loss: -1074893.625000\n",
      "Train Epoch: 100 [43648/54000 (81%)] Loss: -1078288.375000\n",
      "Train Epoch: 100 [45056/54000 (83%)] Loss: -1084812.625000\n",
      "Train Epoch: 100 [46464/54000 (86%)] Loss: -1095035.875000\n",
      "Train Epoch: 100 [47872/54000 (89%)] Loss: -1098412.750000\n",
      "Train Epoch: 100 [49280/54000 (91%)] Loss: -1056585.875000\n",
      "Train Epoch: 100 [50688/54000 (94%)] Loss: -1079533.000000\n",
      "Train Epoch: 100 [52096/54000 (96%)] Loss: -1119911.500000\n",
      "Train Epoch: 100 [53504/54000 (99%)] Loss: -1113102.125000\n",
      "    epoch          : 100\n",
      "    loss           : -1081286.6392180095\n",
      "    val_loss       : -1083755.576592628\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0427_111527/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [0/54000 (0%)] Loss: -1137662.750000\n",
      "Train Epoch: 101 [1408/54000 (3%)] Loss: -1062226.750000\n",
      "Train Epoch: 101 [2816/54000 (5%)] Loss: -1072881.125000\n",
      "Train Epoch: 101 [4224/54000 (8%)] Loss: -1096982.125000\n",
      "Train Epoch: 101 [5632/54000 (10%)] Loss: -1043237.125000\n",
      "Train Epoch: 101 [7040/54000 (13%)] Loss: -1045445.437500\n",
      "Train Epoch: 101 [8448/54000 (16%)] Loss: -1049985.625000\n",
      "Train Epoch: 101 [9856/54000 (18%)] Loss: -1127096.875000\n",
      "Train Epoch: 101 [11264/54000 (21%)] Loss: -1122987.750000\n",
      "Train Epoch: 101 [12672/54000 (23%)] Loss: -1088575.875000\n",
      "Train Epoch: 101 [14080/54000 (26%)] Loss: -1071785.750000\n",
      "Train Epoch: 101 [15488/54000 (29%)] Loss: -1099050.500000\n",
      "Train Epoch: 101 [16896/54000 (31%)] Loss: -1074884.250000\n",
      "Train Epoch: 101 [18304/54000 (34%)] Loss: -1093004.375000\n",
      "Train Epoch: 101 [19712/54000 (37%)] Loss: -1074017.875000\n",
      "Train Epoch: 101 [21120/54000 (39%)] Loss: -1084006.250000\n",
      "Train Epoch: 101 [22528/54000 (42%)] Loss: -1064663.750000\n",
      "Train Epoch: 101 [23936/54000 (44%)] Loss: -1045875.812500\n",
      "Train Epoch: 101 [25344/54000 (47%)] Loss: -1115169.125000\n",
      "Train Epoch: 101 [26752/54000 (50%)] Loss: -1055651.625000\n",
      "Train Epoch: 101 [28160/54000 (52%)] Loss: -1115559.625000\n",
      "Train Epoch: 101 [29568/54000 (55%)] Loss: -1112504.750000\n",
      "Train Epoch: 101 [30976/54000 (57%)] Loss: -1130136.500000\n",
      "Train Epoch: 101 [32384/54000 (60%)] Loss: -1055890.625000\n",
      "Train Epoch: 101 [33792/54000 (63%)] Loss: -1133740.375000\n",
      "Train Epoch: 101 [35200/54000 (65%)] Loss: -1108332.625000\n",
      "Train Epoch: 101 [36608/54000 (68%)] Loss: -1125197.875000\n",
      "Train Epoch: 101 [38016/54000 (70%)] Loss: -1030320.750000\n",
      "Train Epoch: 101 [39424/54000 (73%)] Loss: -1058729.000000\n",
      "Train Epoch: 101 [40832/54000 (76%)] Loss: -1107010.875000\n",
      "Train Epoch: 101 [42240/54000 (78%)] Loss: -1142139.875000\n",
      "Train Epoch: 101 [43648/54000 (81%)] Loss: -1065359.250000\n",
      "Train Epoch: 101 [45056/54000 (83%)] Loss: -1123624.875000\n",
      "Train Epoch: 101 [46464/54000 (86%)] Loss: -1051430.750000\n",
      "Train Epoch: 101 [47872/54000 (89%)] Loss: -1085555.750000\n",
      "Train Epoch: 101 [49280/54000 (91%)] Loss: -1055114.875000\n",
      "Train Epoch: 101 [50688/54000 (94%)] Loss: -1024354.687500\n",
      "Train Epoch: 101 [52096/54000 (96%)] Loss: -1102812.250000\n",
      "Train Epoch: 101 [53504/54000 (99%)] Loss: -1099160.375000\n",
      "    epoch          : 101\n",
      "    loss           : -1081548.6002665877\n",
      "    val_loss       : -1084027.0248581907\n",
      "Train Epoch: 102 [0/54000 (0%)] Loss: -1036083.312500\n",
      "Train Epoch: 102 [1408/54000 (3%)] Loss: -1181361.875000\n",
      "Train Epoch: 102 [2816/54000 (5%)] Loss: -1109641.250000\n",
      "Train Epoch: 102 [4224/54000 (8%)] Loss: -1061024.500000\n",
      "Train Epoch: 102 [5632/54000 (10%)] Loss: -1087297.375000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/AnacondaProjects/categorical_bpl/base/base_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mnot_improved_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# save logged informations into log dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AnacondaProjects/categorical_bpl/trainer/trainer.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RoutingCategories/lib/python3.8/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[0;32m~/anaconda3/envs/RoutingCategories/lib/python3.8/site-packages/pyro/infer/tracegraph_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mIf\u001b[0m \u001b[0mbaselines\u001b[0m \u001b[0mare\u001b[0m \u001b[0mpresent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0mbaseline\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0malso\u001b[0m \u001b[0mconstructed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdifferentiated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \"\"\"\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0melbo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurrogate_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss_and_surrogate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mtorch_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurrogate_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RoutingCategories/lib/python3.8/site-packages/pyro/infer/tracegraph_elbo.py\u001b[0m in \u001b[0;36m_loss_and_surrogate_loss\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0msurrogate_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_traces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss_and_surrogate_loss_particle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RoutingCategories/lib/python3.8/site-packages/pyro/infer/elbo.py\u001b[0m in \u001b[0;36m_get_traces\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/RoutingCategories/lib/python3.8/site-packages/pyro/infer/tracegraph_elbo.py\u001b[0m in \u001b[0;36m_get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0magainst\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m         model_trace, guide_trace = get_importance_trace(\n\u001b[0m\u001b[1;32m    225\u001b[0m             \"dense\", self.max_plate_nesting, model, guide, args, kwargs)\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RoutingCategories/lib/python3.8/site-packages/pyro/infer/enum.py\u001b[0m in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mrun\u001b[0m \u001b[0magainst\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \"\"\"\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mguide_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mguide_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RoutingCategories/lib/python3.8/site-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36mget_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mCalls\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mpoutine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mits\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsngr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RoutingCategories/lib/python3.8/site-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                                       args=args, kwargs=kwargs)\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AnacondaProjects/categorical_bpl/model/model.py\u001b[0m in \u001b[0;36mguide\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide_generator_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intuitive_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mlatent_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide_latent_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AnacondaProjects/categorical_bpl/model/model.py\u001b[0m in \u001b[0;36m_intuitive_distances\u001b[0;34m(self, step_distances)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mtransition_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mtransition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransition\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtransition_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_object_by_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AnacondaProjects/categorical_bpl/utils/util.py\u001b[0m in \u001b[0;36mexpm\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Pade 13 approximation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_pade13\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAscaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mU\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AnacondaProjects/categorical_bpl/utils/util.py\u001b[0m in \u001b[0;36mtorch_pade13\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mident\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mA4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mA6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mA6\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mA4\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mA6\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mA4\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mA2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mident\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RoutingCategories/lib/python3.8/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RoutingCategories/lib/python3.8/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RoutingCategories/lib/python3.8/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mfnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazycache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m             \u001b[0;31m# Must defer line lookups until we have called checkcache.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcapture_locals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RoutingCategories/lib/python3.8/linecache.py\u001b[0m in \u001b[0;36mlazycache\u001b[0;34m(filename, module_globals)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0malready\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \"\"\"\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
