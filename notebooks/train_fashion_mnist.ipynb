{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.5,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 51484.703125\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -6593.755859\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -24624.804688\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -29727.980469\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -43233.960938\n",
      "    epoch          : 1\n",
      "    loss           : -8533.596298828124\n",
      "    val_loss       : -40706.48725585938\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: 8643.025391\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -48221.562500\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -70023.609375\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -42378.242188\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -2834.643555\n",
      "    epoch          : 2\n",
      "    loss           : -43385.784038085934\n",
      "    val_loss       : -48822.147265625\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -46786.398438\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -63584.121094\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -36077.898438\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -52405.488281\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -59025.109375\n",
      "    epoch          : 3\n",
      "    loss           : -48051.35108398437\n",
      "    val_loss       : -52712.4830078125\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -41854.492188\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -31486.933594\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -60857.464844\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -68259.320312\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -6483.847656\n",
      "    epoch          : 4\n",
      "    loss           : -51389.10457519531\n",
      "    val_loss       : -54389.305078125\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -32706.828125\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -44400.777344\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -26366.337891\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -13938.471680\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -12791.344727\n",
      "    epoch          : 5\n",
      "    loss           : -54483.278671875\n",
      "    val_loss       : -57966.26396484375\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -59893.582031\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -40645.171875\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -52334.601562\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -57616.476562\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -75719.984375\n",
      "    epoch          : 6\n",
      "    loss           : -58550.18599609375\n",
      "    val_loss       : -63559.0853515625\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -65289.902344\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -63469.585938\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -90216.031250\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -76774.781250\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -69909.437500\n",
      "    epoch          : 7\n",
      "    loss           : -62390.55926757812\n",
      "    val_loss       : -69582.66484375\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -74018.000000\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -67227.867188\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -67100.125000\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -21506.695312\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -44014.484375\n",
      "    epoch          : 8\n",
      "    loss           : -65710.491875\n",
      "    val_loss       : -70697.37265625\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -97044.218750\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -67596.421875\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -45122.460938\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -80546.500000\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -84336.539062\n",
      "    epoch          : 9\n",
      "    loss           : -67689.61908203125\n",
      "    val_loss       : -73748.08642578125\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -65604.718750\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -97378.968750\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -87150.203125\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -77941.453125\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -71720.445312\n",
      "    epoch          : 10\n",
      "    loss           : -70361.2314453125\n",
      "    val_loss       : -77309.8552734375\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -97952.679688\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -77761.843750\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -85264.406250\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -77490.234375\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -79204.515625\n",
      "    epoch          : 11\n",
      "    loss           : -72099.50263671875\n",
      "    val_loss       : -77623.05146484375\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -52314.507812\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -32529.384766\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -32600.160156\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -78455.351562\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -81258.101562\n",
      "    epoch          : 12\n",
      "    loss           : -74015.9301953125\n",
      "    val_loss       : -79438.11240234374\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -83750.367188\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -84370.476562\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -53521.460938\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -97218.734375\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -34349.320312\n",
      "    epoch          : 13\n",
      "    loss           : -75567.2596484375\n",
      "    val_loss       : -81626.896875\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -102771.437500\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -85149.476562\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -83342.320312\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -86243.218750\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -78353.515625\n",
      "    epoch          : 14\n",
      "    loss           : -76865.7717578125\n",
      "    val_loss       : -80754.1984375\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -68488.562500\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -104252.656250\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -78938.156250\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -85837.875000\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -82225.250000\n",
      "    epoch          : 15\n",
      "    loss           : -78330.7926953125\n",
      "    val_loss       : -83814.82294921875\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -57464.460938\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -104362.703125\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -83645.562500\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -97449.507812\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -104432.898438\n",
      "    epoch          : 16\n",
      "    loss           : -79731.5993359375\n",
      "    val_loss       : -85326.1009765625\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -97231.757812\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -82015.609375\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -87305.695312\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -95858.031250\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -93639.218750\n",
      "    epoch          : 17\n",
      "    loss           : -81713.2300390625\n",
      "    val_loss       : -86023.25654296875\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -111499.421875\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -84919.968750\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -42359.988281\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -84380.593750\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -41912.503906\n",
      "    epoch          : 18\n",
      "    loss           : -82745.1252734375\n",
      "    val_loss       : -89827.07568359375\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -96751.046875\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -90948.062500\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -111427.578125\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -87461.375000\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -88720.218750\n",
      "    epoch          : 19\n",
      "    loss           : -84593.736171875\n",
      "    val_loss       : -89141.32412109376\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -86857.234375\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -93541.031250\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -51836.722656\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -107544.296875\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -91342.687500\n",
      "    epoch          : 20\n",
      "    loss           : -85298.8021484375\n",
      "    val_loss       : -89549.78447265625\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -65040.570312\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -87913.140625\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -95150.609375\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -86768.226562\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -98292.343750\n",
      "    epoch          : 21\n",
      "    loss           : -87153.555859375\n",
      "    val_loss       : -92602.399609375\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -96926.671875\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -102421.093750\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -113322.046875\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -92323.539062\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -67300.476562\n",
      "    epoch          : 22\n",
      "    loss           : -88184.3553125\n",
      "    val_loss       : -90799.72158203126\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -96364.000000\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -96105.007812\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -98011.031250\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -103580.585938\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -94278.265625\n",
      "    epoch          : 23\n",
      "    loss           : -89452.8636328125\n",
      "    val_loss       : -93662.38984375\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -85585.718750\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -90740.382812\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -107002.367188\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -91049.718750\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -49202.472656\n",
      "    epoch          : 24\n",
      "    loss           : -89912.289140625\n",
      "    val_loss       : -95372.48798828125\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -96957.890625\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -85703.343750\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -70546.359375\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -91410.257812\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -56902.984375\n",
      "    epoch          : 25\n",
      "    loss           : -91740.2463671875\n",
      "    val_loss       : -95194.51005859375\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -74348.257812\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -96391.664062\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -93932.078125\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -107707.406250\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -102804.148438\n",
      "    epoch          : 26\n",
      "    loss           : -93465.7396484375\n",
      "    val_loss       : -98764.91552734375\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -79540.523438\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -72421.945312\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -100237.359375\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -95880.609375\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -110827.953125\n",
      "    epoch          : 27\n",
      "    loss           : -94548.92203125\n",
      "    val_loss       : -97380.90966796875\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -107866.117188\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -97086.875000\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -119895.500000\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -117894.757812\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -111352.226562\n",
      "    epoch          : 28\n",
      "    loss           : -95007.003359375\n",
      "    val_loss       : -98279.78095703125\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -106883.601562\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -61483.789062\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -98288.171875\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -81154.335938\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -112977.242188\n",
      "    epoch          : 29\n",
      "    loss           : -96708.4544921875\n",
      "    val_loss       : -100554.59931640625\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -110474.773438\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -99010.031250\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -111312.906250\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -72847.937500\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -58601.414062\n",
      "    epoch          : 30\n",
      "    loss           : -97770.3179296875\n",
      "    val_loss       : -101084.54072265625\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -100463.265625\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -110971.265625\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -106299.679688\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -95494.187500\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -113368.000000\n",
      "    epoch          : 31\n",
      "    loss           : -98510.22859375\n",
      "    val_loss       : -102556.6962890625\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -99042.992188\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -101529.804688\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -100498.929688\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -117142.593750\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -64034.054688\n",
      "    epoch          : 32\n",
      "    loss           : -99422.56015625\n",
      "    val_loss       : -103961.2943359375\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -114374.578125\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -114671.921875\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -87695.507812\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -113025.992188\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -104286.906250\n",
      "    epoch          : 33\n",
      "    loss           : -100954.9927734375\n",
      "    val_loss       : -105193.3794921875\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -103903.664062\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -119591.843750\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -104789.476562\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -111768.257812\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -118129.734375\n",
      "    epoch          : 34\n",
      "    loss           : -101572.8472265625\n",
      "    val_loss       : -104025.25810546875\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -99575.328125\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -101382.570312\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -113248.367188\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -89528.734375\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -101578.164062\n",
      "    epoch          : 35\n",
      "    loss           : -102709.6258203125\n",
      "    val_loss       : -106591.544921875\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -114149.468750\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -100333.921875\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -105540.429688\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -101902.945312\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -113161.750000\n",
      "    epoch          : 36\n",
      "    loss           : -103100.79734375\n",
      "    val_loss       : -106082.1416015625\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -124327.671875\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -105067.843750\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -113539.125000\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -97153.125000\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -106716.882812\n",
      "    epoch          : 37\n",
      "    loss           : -104435.5641015625\n",
      "    val_loss       : -106395.06728515626\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -68799.531250\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -100022.718750\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -104013.656250\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -98414.820312\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -125946.570312\n",
      "    epoch          : 38\n",
      "    loss           : -105809.69296875\n",
      "    val_loss       : -108807.52431640626\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -102428.812500\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -98152.585938\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -85544.968750\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -104025.820312\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -121917.156250\n",
      "    epoch          : 39\n",
      "    loss           : -106557.8859375\n",
      "    val_loss       : -109507.62451171875\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -93952.984375\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -104367.609375\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -104123.882812\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -102729.984375\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -119469.101562\n",
      "    epoch          : 40\n",
      "    loss           : -107277.0350390625\n",
      "    val_loss       : -109260.6630859375\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -120929.960938\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -106263.726562\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -106950.609375\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -105495.414062\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -130257.312500\n",
      "    epoch          : 41\n",
      "    loss           : -108826.933515625\n",
      "    val_loss       : -109369.973046875\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -108318.656250\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -89889.117188\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -124540.375000\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -92274.257812\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -126966.609375\n",
      "    epoch          : 42\n",
      "    loss           : -109302.048515625\n",
      "    val_loss       : -115080.77294921875\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -122084.960938\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -121631.156250\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -102515.546875\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -123424.562500\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -73300.242188\n",
      "    epoch          : 43\n",
      "    loss           : -110751.2853125\n",
      "    val_loss       : -115218.159765625\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -129947.835938\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -132961.968750\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -106100.703125\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -111992.031250\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -96795.164062\n",
      "    epoch          : 44\n",
      "    loss           : -111567.803828125\n",
      "    val_loss       : -113903.3607421875\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -113203.484375\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -133089.000000\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -121708.398438\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -106938.125000\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -124162.164062\n",
      "    epoch          : 45\n",
      "    loss           : -112564.189765625\n",
      "    val_loss       : -114788.70595703126\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -100143.609375\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -104429.500000\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -100714.812500\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -134756.375000\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -127444.039062\n",
      "    epoch          : 46\n",
      "    loss           : -113165.20140625\n",
      "    val_loss       : -115878.19140625\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -111689.882812\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -109424.250000\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -136837.703125\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -98738.171875\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -81426.726562\n",
      "    epoch          : 47\n",
      "    loss           : -114095.373828125\n",
      "    val_loss       : -116504.64423828125\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -117676.078125\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -129434.343750\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -131047.437500\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -109851.835938\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -73637.640625\n",
      "    epoch          : 48\n",
      "    loss           : -114973.566015625\n",
      "    val_loss       : -118485.44931640624\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -129867.187500\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -113455.171875\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -128784.960938\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -108335.343750\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -133029.406250\n",
      "    epoch          : 49\n",
      "    loss           : -115339.59140625\n",
      "    val_loss       : -120460.55498046875\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -129051.687500\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -133872.328125\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -114184.109375\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -136681.406250\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -87542.187500\n",
      "    epoch          : 50\n",
      "    loss           : -115849.86390625\n",
      "    val_loss       : -119709.680859375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -114908.695312\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -118351.148438\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -117281.648438\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -108582.007812\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -112543.593750\n",
      "    epoch          : 51\n",
      "    loss           : -117834.0834375\n",
      "    val_loss       : -117708.85947265624\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -139424.828125\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -83049.218750\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -116522.054688\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -135779.937500\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -128358.335938\n",
      "    epoch          : 52\n",
      "    loss           : -118413.165625\n",
      "    val_loss       : -120001.19423828126\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -129264.812500\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -109676.046875\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -130092.281250\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -140922.765625\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -82556.742188\n",
      "    epoch          : 53\n",
      "    loss           : -119294.682265625\n",
      "    val_loss       : -122878.7884765625\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -134564.234375\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -131526.125000\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -129903.648438\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -116581.015625\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -132554.156250\n",
      "    epoch          : 54\n",
      "    loss           : -119934.252265625\n",
      "    val_loss       : -122718.1294921875\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -120122.203125\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -122471.242188\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -124779.546875\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -136967.453125\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -89134.406250\n",
      "    epoch          : 55\n",
      "    loss           : -121434.128671875\n",
      "    val_loss       : -126042.35751953124\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -125153.968750\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -133754.921875\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -116808.375000\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -115596.820312\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -79333.609375\n",
      "    epoch          : 56\n",
      "    loss           : -122938.7815625\n",
      "    val_loss       : -122300.55830078125\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -116365.203125\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -143466.343750\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -86287.312500\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -90199.585938\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -136432.234375\n",
      "    epoch          : 57\n",
      "    loss           : -123750.285078125\n",
      "    val_loss       : -126608.89287109375\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -116442.187500\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -128584.039062\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -140536.187500\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -145228.843750\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -141959.750000\n",
      "    epoch          : 58\n",
      "    loss           : -124612.3878125\n",
      "    val_loss       : -127972.99951171875\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -121451.976562\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -144035.750000\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -123201.921875\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -122471.945312\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -146298.078125\n",
      "    epoch          : 59\n",
      "    loss           : -126298.558203125\n",
      "    val_loss       : -128976.64765625\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -114504.921875\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -113518.234375\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -144982.156250\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -145249.437500\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -150582.250000\n",
      "    epoch          : 60\n",
      "    loss           : -127404.435\n",
      "    val_loss       : -129512.13583984374\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -119152.710938\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -124815.718750\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -144065.593750\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -145870.312500\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -123773.484375\n",
      "    epoch          : 61\n",
      "    loss           : -129103.975\n",
      "    val_loss       : -131578.9103515625\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -136346.437500\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -126707.976562\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -93439.781250\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -130539.507812\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -151062.906250\n",
      "    epoch          : 62\n",
      "    loss           : -129945.053125\n",
      "    val_loss       : -133835.59775390624\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -132621.156250\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -148520.718750\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -151546.453125\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -129170.726562\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -148054.156250\n",
      "    epoch          : 63\n",
      "    loss           : -131898.5290625\n",
      "    val_loss       : -135145.49541015626\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -132286.343750\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -148114.234375\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -123915.921875\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -134182.562500\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -149654.578125\n",
      "    epoch          : 64\n",
      "    loss           : -135333.855234375\n",
      "    val_loss       : -136668.22138671874\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -153605.187500\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -124392.921875\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -123573.671875\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -155012.093750\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -152376.500000\n",
      "    epoch          : 65\n",
      "    loss           : -136297.812578125\n",
      "    val_loss       : -139880.33154296875\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -127914.773438\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -127033.726562\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -130256.414062\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -136243.031250\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -162214.812500\n",
      "    epoch          : 66\n",
      "    loss           : -138961.970390625\n",
      "    val_loss       : -142551.6673828125\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -135168.437500\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -153996.078125\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -140374.109375\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -135920.531250\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -153298.375000\n",
      "    epoch          : 67\n",
      "    loss           : -142277.809140625\n",
      "    val_loss       : -145203.3974609375\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -162728.125000\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -145489.250000\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -134536.937500\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -157956.359375\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -142064.609375\n",
      "    epoch          : 68\n",
      "    loss           : -144123.299921875\n",
      "    val_loss       : -145726.67919921875\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -159767.500000\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -141618.937500\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -163636.703125\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -165769.781250\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -163001.875000\n",
      "    epoch          : 69\n",
      "    loss           : -146856.115078125\n",
      "    val_loss       : -147527.48486328125\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -145407.468750\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -166501.437500\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -168514.796875\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -144394.468750\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -138404.546875\n",
      "    epoch          : 70\n",
      "    loss           : -149004.42359375\n",
      "    val_loss       : -149632.07392578124\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -139315.687500\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -164831.453125\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -175169.843750\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -140802.140625\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -152607.906250\n",
      "    epoch          : 71\n",
      "    loss           : -151279.16046875\n",
      "    val_loss       : -151674.048046875\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -174552.718750\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -176310.171875\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -122048.429688\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -164431.000000\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -169419.125000\n",
      "    epoch          : 72\n",
      "    loss           : -153254.417109375\n",
      "    val_loss       : -151758.6283203125\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -137315.781250\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -141078.687500\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -148649.468750\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -170054.406250\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -146691.093750\n",
      "    epoch          : 73\n",
      "    loss           : -154655.35828125\n",
      "    val_loss       : -153783.72587890626\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -163314.218750\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -169235.562500\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -148149.562500\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -147179.203125\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -173324.312500\n",
      "    epoch          : 74\n",
      "    loss           : -156546.1459375\n",
      "    val_loss       : -154771.82431640624\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -145477.593750\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -158575.171875\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -152572.531250\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -182657.468750\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -126689.320312\n",
      "    epoch          : 75\n",
      "    loss           : -157582.479609375\n",
      "    val_loss       : -156829.99521484374\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -171594.500000\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -171307.687500\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -147761.125000\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -145547.859375\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -145634.828125\n",
      "    epoch          : 76\n",
      "    loss           : -158750.627421875\n",
      "    val_loss       : -160594.79130859376\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -151009.062500\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -146117.187500\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -150928.531250\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -172517.890625\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -154425.343750\n",
      "    epoch          : 77\n",
      "    loss           : -161106.5471875\n",
      "    val_loss       : -162022.3181640625\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -148524.484375\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -164068.875000\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -161235.765625\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -181900.437500\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -129545.867188\n",
      "    epoch          : 78\n",
      "    loss           : -163224.0509375\n",
      "    val_loss       : -163586.62529296876\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -154795.343750\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -133473.187500\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -124590.484375\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -149284.718750\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -156638.046875\n",
      "    epoch          : 79\n",
      "    loss           : -164107.77203125\n",
      "    val_loss       : -162408.56435546876\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -165826.343750\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -158064.968750\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -184070.328125\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -157568.375000\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -156868.250000\n",
      "    epoch          : 80\n",
      "    loss           : -165606.66078125\n",
      "    val_loss       : -163797.89423828124\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -154648.906250\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -169037.312500\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -148290.375000\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -175966.640625\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -136550.593750\n",
      "    epoch          : 81\n",
      "    loss           : -167355.60171875\n",
      "    val_loss       : -166891.22880859376\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -152678.281250\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -170656.281250\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -164557.031250\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -187911.593750\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -181736.875000\n",
      "    epoch          : 82\n",
      "    loss           : -169245.3159375\n",
      "    val_loss       : -167498.442578125\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -160723.968750\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -195383.390625\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -188338.687500\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -159709.093750\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -163118.734375\n",
      "    epoch          : 83\n",
      "    loss           : -170680.2853125\n",
      "    val_loss       : -169352.77109375\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -195587.046875\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -161704.468750\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -165670.656250\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -140834.078125\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -163909.843750\n",
      "    epoch          : 84\n",
      "    loss           : -171595.49828125\n",
      "    val_loss       : -170676.20556640625\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -191256.531250\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -159097.671875\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -162264.156250\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -168576.750000\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -184141.593750\n",
      "    epoch          : 85\n",
      "    loss           : -172845.899921875\n",
      "    val_loss       : -173170.73984375\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -177837.375000\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -164537.843750\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -141998.015625\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -185634.781250\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -163443.093750\n",
      "    epoch          : 86\n",
      "    loss           : -174692.3740625\n",
      "    val_loss       : -173634.99306640626\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -161977.984375\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -177393.578125\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -177897.875000\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -196080.156250\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -165266.250000\n",
      "    epoch          : 87\n",
      "    loss           : -176057.73015625\n",
      "    val_loss       : -175365.95419921874\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -170911.125000\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -204344.250000\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -167258.765625\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -204795.203125\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -147541.265625\n",
      "    epoch          : 88\n",
      "    loss           : -176693.98609375\n",
      "    val_loss       : -176391.86728515624\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -159782.093750\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -160341.968750\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -205304.484375\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -167615.718750\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -198938.328125\n",
      "    epoch          : 89\n",
      "    loss           : -178290.6575\n",
      "    val_loss       : -177208.5072265625\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -164654.687500\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -149691.859375\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -205319.812500\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -208136.406250\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -139472.484375\n",
      "    epoch          : 90\n",
      "    loss           : -179828.72546875\n",
      "    val_loss       : -179455.38212890626\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -162779.578125\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -172441.593750\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -182992.609375\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -170538.375000\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -198839.593750\n",
      "    epoch          : 91\n",
      "    loss           : -181341.90328125\n",
      "    val_loss       : -179291.70380859374\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -185918.109375\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -200392.796875\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -203453.296875\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -201752.812500\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -201290.609375\n",
      "    epoch          : 92\n",
      "    loss           : -182430.30875\n",
      "    val_loss       : -181988.48662109376\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -202503.562500\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -207891.625000\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -202429.343750\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -186219.343750\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -201099.656250\n",
      "    epoch          : 93\n",
      "    loss           : -183921.92640625\n",
      "    val_loss       : -182738.54453125\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -202858.609375\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -204888.906250\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -154938.968750\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -191079.250000\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -203008.437500\n",
      "    epoch          : 94\n",
      "    loss           : -184843.24671875\n",
      "    val_loss       : -182615.50966796876\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -181524.187500\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -165253.656250\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -157958.140625\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -214216.250000\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -206073.968750\n",
      "    epoch          : 95\n",
      "    loss           : -186044.81796875\n",
      "    val_loss       : -184154.4787109375\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -178804.484375\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -158632.453125\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -195809.687500\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -192683.921875\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -176759.171875\n",
      "    epoch          : 96\n",
      "    loss           : -187458.10515625\n",
      "    val_loss       : -185951.33310546874\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -158225.562500\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -181016.843750\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -208342.750000\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -176520.437500\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -206282.953125\n",
      "    epoch          : 97\n",
      "    loss           : -188720.98734375\n",
      "    val_loss       : -187515.12373046874\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -177175.375000\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -209011.531250\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -176653.218750\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -198253.093750\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -208692.531250\n",
      "    epoch          : 98\n",
      "    loss           : -189609.85359375\n",
      "    val_loss       : -187736.325390625\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -170767.281250\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -172506.031250\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -171806.968750\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -208290.640625\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -177709.515625\n",
      "    epoch          : 99\n",
      "    loss           : -191282.01796875\n",
      "    val_loss       : -189804.0236328125\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -222979.312500\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -172242.890625\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -183128.375000\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -213039.687500\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -210450.656250\n",
      "    epoch          : 100\n",
      "    loss           : -192313.6340625\n",
      "    val_loss       : -189964.72978515626\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -179153.187500\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -198332.609375\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -179533.156250\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -211655.781250\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -213401.156250\n",
      "    epoch          : 101\n",
      "    loss           : -193513.90734375\n",
      "    val_loss       : -192080.952734375\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -204418.875000\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -199789.718750\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -172977.312500\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -183358.031250\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -183010.609375\n",
      "    epoch          : 102\n",
      "    loss           : -194679.18\n",
      "    val_loss       : -193714.558984375\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -189464.843750\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -200914.906250\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -184591.546875\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -217390.875000\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -215501.953125\n",
      "    epoch          : 103\n",
      "    loss           : -195706.55796875\n",
      "    val_loss       : -193007.38828125\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -215597.640625\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -180809.500000\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -215598.234375\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -204651.937500\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -191954.406250\n",
      "    epoch          : 104\n",
      "    loss           : -197097.50265625\n",
      "    val_loss       : -195198.14365234374\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -185160.703125\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -170761.843750\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -217996.359375\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -190362.015625\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -227968.343750\n",
      "    epoch          : 105\n",
      "    loss           : -198140.01359375\n",
      "    val_loss       : -196713.54765625\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -219135.375000\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -191400.968750\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -178429.546875\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -182840.937500\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -218832.406250\n",
      "    epoch          : 106\n",
      "    loss           : -199388.130625\n",
      "    val_loss       : -197450.74892578126\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -205635.468750\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -179148.156250\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -187695.937500\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -212459.468750\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -186701.937500\n",
      "    epoch          : 107\n",
      "    loss           : -200264.5565625\n",
      "    val_loss       : -199082.15498046874\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -207814.187500\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -209662.484375\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -220710.093750\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -188766.156250\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -181526.671875\n",
      "    epoch          : 108\n",
      "    loss           : -201732.97765625\n",
      "    val_loss       : -199696.6388671875\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -173214.015625\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -190356.265625\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -188757.375000\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -208063.656250\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -209280.343750\n",
      "    epoch          : 109\n",
      "    loss           : -202807.1609375\n",
      "    val_loss       : -198378.61083984375\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -190451.125000\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -224606.140625\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -182634.234375\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -177666.390625\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -177757.531250\n",
      "    epoch          : 110\n",
      "    loss           : -203998.13609375\n",
      "    val_loss       : -202260.43056640626\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -195906.265625\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -200220.000000\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -226767.140625\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -186768.406250\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -200952.656250\n",
      "    epoch          : 111\n",
      "    loss           : -205027.5771875\n",
      "    val_loss       : -202843.57939453126\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -236621.187500\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -187268.375000\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -212741.937500\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -188063.312500\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -179833.078125\n",
      "    epoch          : 112\n",
      "    loss           : -206169.05578125\n",
      "    val_loss       : -203392.44716796876\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -193172.562500\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -226483.171875\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -215380.359375\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -227275.062500\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -228257.046875\n",
      "    epoch          : 113\n",
      "    loss           : -207276.20609375\n",
      "    val_loss       : -204603.7552734375\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -215975.515625\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -228275.718750\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -199807.921875\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -232669.312500\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -181823.218750\n",
      "    epoch          : 114\n",
      "    loss           : -208334.42375\n",
      "    val_loss       : -206275.3060546875\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -228244.515625\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -227786.296875\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -227956.734375\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -230740.312500\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -183581.625000\n",
      "    epoch          : 115\n",
      "    loss           : -209043.8675\n",
      "    val_loss       : -205616.8142578125\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -219116.203125\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -217214.531250\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -216740.046875\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -230672.937500\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -205403.093750\n",
      "    epoch          : 116\n",
      "    loss           : -210526.4525\n",
      "    val_loss       : -206153.65595703124\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -235101.937500\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -204018.328125\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -231093.250000\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -199259.312500\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -231840.640625\n",
      "    epoch          : 117\n",
      "    loss           : -211528.258125\n",
      "    val_loss       : -209734.09443359374\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -195945.203125\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -231050.437500\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -186792.812500\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -208655.296875\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -233679.328125\n",
      "    epoch          : 118\n",
      "    loss           : -212561.9840625\n",
      "    val_loss       : -206613.90732421874\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -232630.625000\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -233286.375000\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -248004.093750\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -194554.093750\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -237024.281250\n",
      "    epoch          : 119\n",
      "    loss           : -213448.25328125\n",
      "    val_loss       : -211981.19091796875\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -195585.359375\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -247080.625000\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -238186.093750\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -236052.406250\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -222968.578125\n",
      "    epoch          : 120\n",
      "    loss           : -214298.3740625\n",
      "    val_loss       : -210583.35341796876\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -248989.875000\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -200765.312500\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -238831.468750\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -233975.265625\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -239103.250000\n",
      "    epoch          : 121\n",
      "    loss           : -215564.6659375\n",
      "    val_loss       : -213752.2330078125\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -201346.875000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -209165.281250\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -249646.921875\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -193786.843750\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -190694.484375\n",
      "    epoch          : 122\n",
      "    loss           : -216796.9553125\n",
      "    val_loss       : -213382.2814453125\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -228477.093750\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -200876.796875\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -200824.781250\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -196251.000000\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -199783.093750\n",
      "    epoch          : 123\n",
      "    loss           : -217645.1546875\n",
      "    val_loss       : -212171.96201171874\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -241387.937500\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -209146.875000\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -224204.593750\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -252197.890625\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -203075.828125\n",
      "    epoch          : 124\n",
      "    loss           : -218801.8865625\n",
      "    val_loss       : -216560.7484375\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -204781.562500\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -238476.250000\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -195415.500000\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -239477.125000\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -203114.125000\n",
      "    epoch          : 125\n",
      "    loss           : -219112.66375\n",
      "    val_loss       : -217832.66279296874\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -212424.875000\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -199207.125000\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -196353.281250\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -241829.968750\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -241205.531250\n",
      "    epoch          : 126\n",
      "    loss           : -220439.23546875\n",
      "    val_loss       : -219016.70703125\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -244934.781250\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -241022.609375\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -199404.125000\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -204890.218750\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -196828.578125\n",
      "    epoch          : 127\n",
      "    loss           : -221653.15671875\n",
      "    val_loss       : -220168.27509765624\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -199254.234375\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -198222.218750\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -242500.968750\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -215234.984375\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -203675.562500\n",
      "    epoch          : 128\n",
      "    loss           : -222632.445625\n",
      "    val_loss       : -220253.6521484375\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -201296.406250\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -200103.968750\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -233884.156250\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -235981.375000\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -209633.281250\n",
      "    epoch          : 129\n",
      "    loss           : -223796.73390625\n",
      "    val_loss       : -219744.96640625\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -209260.546875\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -200384.812500\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -207282.265625\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -203507.578125\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -246853.468750\n",
      "    epoch          : 130\n",
      "    loss           : -224776.19078125\n",
      "    val_loss       : -222527.15234375\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -246760.875000\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -257838.796875\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -261232.078125\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -232833.234375\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -247577.718750\n",
      "    epoch          : 131\n",
      "    loss           : -225521.73734375\n",
      "    val_loss       : -223236.02333984376\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -250806.437500\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -206480.750000\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -205148.062500\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -200053.875000\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -231230.281250\n",
      "    epoch          : 132\n",
      "    loss           : -226694.65875\n",
      "    val_loss       : -224612.63193359374\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -245383.500000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -231123.515625\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -209370.718750\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -239544.218750\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -249653.859375\n",
      "    epoch          : 133\n",
      "    loss           : -227670.521875\n",
      "    val_loss       : -225022.4525390625\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -205464.984375\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -212194.203125\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -218544.500000\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -248530.046875\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -252466.250000\n",
      "    epoch          : 134\n",
      "    loss           : -228502.70953125\n",
      "    val_loss       : -221873.9998046875\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -204190.656250\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -205230.937500\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -263929.343750\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -207146.250000\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -251916.437500\n",
      "    epoch          : 135\n",
      "    loss           : -229627.016875\n",
      "    val_loss       : -225968.97822265624\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -241043.968750\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -206381.000000\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -213806.812500\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -265905.875000\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -206392.859375\n",
      "    epoch          : 136\n",
      "    loss           : -230012.28515625\n",
      "    val_loss       : -224584.895703125\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -207927.906250\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -214786.734375\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -213855.656250\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -235864.875000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -208970.062500\n",
      "    epoch          : 137\n",
      "    loss           : -231054.0746875\n",
      "    val_loss       : -229242.50244140625\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -257906.218750\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -214438.093750\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -238536.296875\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -243484.093750\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -210686.687500\n",
      "    epoch          : 138\n",
      "    loss           : -232572.4875\n",
      "    val_loss       : -230003.6208984375\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -216440.609375\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -216537.781250\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -271885.625000\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -258032.281250\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -255979.968750\n",
      "    epoch          : 139\n",
      "    loss           : -233483.6621875\n",
      "    val_loss       : -225446.03935546876\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -255269.203125\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -227841.312500\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -223841.468750\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -272308.156250\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -255019.328125\n",
      "    epoch          : 140\n",
      "    loss           : -234313.3321875\n",
      "    val_loss       : -229495.7556640625\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -213350.406250\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -215704.546875\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -255797.359375\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -226531.296875\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -254774.890625\n",
      "    epoch          : 141\n",
      "    loss           : -234955.09671875\n",
      "    val_loss       : -232176.84052734374\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -209189.328125\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -207470.218750\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -256705.875000\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -250362.234375\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -214189.406250\n",
      "    epoch          : 142\n",
      "    loss           : -236292.67046875\n",
      "    val_loss       : -228237.71923828125\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -228377.875000\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -248938.750000\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -241030.953125\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -213600.078125\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -216489.093750\n",
      "    epoch          : 143\n",
      "    loss           : -237121.905\n",
      "    val_loss       : -234731.25556640624\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -276249.656250\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -251565.875000\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -212853.796875\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -218349.187500\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -257156.156250\n",
      "    epoch          : 144\n",
      "    loss           : -238062.13890625\n",
      "    val_loss       : -232895.2666015625\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -217624.437500\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -260619.093750\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -215379.000000\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -229094.781250\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -213665.312500\n",
      "    epoch          : 145\n",
      "    loss           : -238991.0584375\n",
      "    val_loss       : -234360.83857421874\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -227127.500000\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -210301.406250\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -212174.375000\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -210687.093750\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -217525.390625\n",
      "    epoch          : 146\n",
      "    loss           : -239799.02515625\n",
      "    val_loss       : -234077.4697265625\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -220567.562500\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -277380.718750\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -213139.000000\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -242980.875000\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -222235.250000\n",
      "    epoch          : 147\n",
      "    loss           : -240722.9096875\n",
      "    val_loss       : -238161.45986328126\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -244337.250000\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -221088.187500\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -213134.640625\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -278698.781250\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -219005.843750\n",
      "    epoch          : 148\n",
      "    loss           : -241635.57140625\n",
      "    val_loss       : -236892.9373046875\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -222767.312500\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -209684.890625\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -266819.656250\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -264077.625000\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -263145.500000\n",
      "    epoch          : 149\n",
      "    loss           : -242550.46328125\n",
      "    val_loss       : -239477.24443359376\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -215826.875000\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -234101.953125\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -257717.906250\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -214466.953125\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -247166.765625\n",
      "    epoch          : 150\n",
      "    loss           : -243069.451875\n",
      "    val_loss       : -239543.72099609376\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch150.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -248766.625000\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -217056.312500\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -283337.750000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -283278.062500\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -264972.531250\n",
      "    epoch          : 151\n",
      "    loss           : -244363.68546875\n",
      "    val_loss       : -239939.4517578125\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -223201.312500\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -286454.937500\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -247276.109375\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -225149.750000\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -226024.843750\n",
      "    epoch          : 152\n",
      "    loss           : -245154.99109375\n",
      "    val_loss       : -242265.59365234376\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -284992.437500\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -282571.187500\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -285577.750000\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -270852.968750\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -225440.031250\n",
      "    epoch          : 153\n",
      "    loss           : -246081.04265625\n",
      "    val_loss       : -243166.52802734374\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -285736.156250\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -285504.250000\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -260725.468750\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -217933.562500\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -229100.203125\n",
      "    epoch          : 154\n",
      "    loss           : -246917.42953125\n",
      "    val_loss       : -237620.2107421875\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -288322.125000\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -219595.296875\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -223399.750000\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -217991.828125\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -269009.625000\n",
      "    epoch          : 155\n",
      "    loss           : -247770.76359375\n",
      "    val_loss       : -244499.50166015624\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -238296.406250\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -236955.687500\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -221051.250000\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -264838.437500\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -232153.859375\n",
      "    epoch          : 156\n",
      "    loss           : -248650.21375\n",
      "    val_loss       : -243225.97861328124\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -231165.531250\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -231709.921875\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -232230.140625\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -226970.968750\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -229156.281250\n",
      "    epoch          : 157\n",
      "    loss           : -249384.8375\n",
      "    val_loss       : -246225.1470703125\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -238806.843750\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -276131.875000\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -288819.875000\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -231767.015625\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -273528.968750\n",
      "    epoch          : 158\n",
      "    loss           : -250351.28875\n",
      "    val_loss       : -247190.5537109375\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -253111.703125\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -224878.781250\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -229957.312500\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -220376.562500\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -275966.250000\n",
      "    epoch          : 159\n",
      "    loss           : -251198.41734375\n",
      "    val_loss       : -247164.244921875\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -242867.765625\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -274232.187500\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -268049.625000\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -267432.062500\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -228216.078125\n",
      "    epoch          : 160\n",
      "    loss           : -252085.2540625\n",
      "    val_loss       : -249114.12529296876\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -279666.000000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -292448.687500\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -233657.218750\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -242980.218750\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -226317.171875\n",
      "    epoch          : 161\n",
      "    loss           : -252882.3990625\n",
      "    val_loss       : -247314.71455078124\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -268163.187500\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -229021.671875\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -225511.906250\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -230479.953125\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -280273.000000\n",
      "    epoch          : 162\n",
      "    loss           : -253572.35578125\n",
      "    val_loss       : -244998.6986328125\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -271642.281250\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -278905.500000\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -235362.937500\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -236704.937500\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -275122.968750\n",
      "    epoch          : 163\n",
      "    loss           : -254512.82390625\n",
      "    val_loss       : -251413.8693359375\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -242993.046875\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -229056.312500\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -272482.125000\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -230697.437500\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -281973.687500\n",
      "    epoch          : 164\n",
      "    loss           : -255314.68765625\n",
      "    val_loss       : -248277.1466796875\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -231915.546875\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -225968.859375\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -276381.375000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -231614.062500\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -227369.218750\n",
      "    epoch          : 165\n",
      "    loss           : -256198.00875\n",
      "    val_loss       : -253243.71552734374\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -228391.437500\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -282615.812500\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -277772.750000\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -228962.328125\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -277877.000000\n",
      "    epoch          : 166\n",
      "    loss           : -256963.29359375\n",
      "    val_loss       : -253396.0458984375\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -228692.187500\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -274303.281250\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -278814.500000\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -236035.406250\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -285000.187500\n",
      "    epoch          : 167\n",
      "    loss           : -257738.0565625\n",
      "    val_loss       : -254358.46513671876\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -241135.968750\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -238854.859375\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -228562.828125\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -243047.812500\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -239229.500000\n",
      "    epoch          : 168\n",
      "    loss           : -258613.6671875\n",
      "    val_loss       : -254980.8115234375\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -239839.921875\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -260068.390625\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -280137.375000\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -279956.500000\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -248293.265625\n",
      "    epoch          : 169\n",
      "    loss           : -259429.3590625\n",
      "    val_loss       : -256082.69013671874\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -278331.750000\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -234094.234375\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -282575.812500\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -244183.843750\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -281439.562500\n",
      "    epoch          : 170\n",
      "    loss           : -260208.05140625\n",
      "    val_loss       : -255145.6087890625\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -238770.187500\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -234949.015625\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -300751.968750\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -286592.468750\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -290431.843750\n",
      "    epoch          : 171\n",
      "    loss           : -260912.38359375\n",
      "    val_loss       : -256937.0353515625\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -287098.875000\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -283954.500000\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -254038.906250\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -231370.312500\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -262127.968750\n",
      "    epoch          : 172\n",
      "    loss           : -261892.96890625\n",
      "    val_loss       : -258414.26171875\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -281827.093750\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -299673.812500\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -233538.500000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -235322.328125\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -243859.937500\n",
      "    epoch          : 173\n",
      "    loss           : -262659.1125\n",
      "    val_loss       : -256813.72607421875\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -248972.640625\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -253481.656250\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -265216.250000\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -291148.937500\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -264482.593750\n",
      "    epoch          : 174\n",
      "    loss           : -263318.15109375\n",
      "    val_loss       : -259590.673828125\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -233175.375000\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -253576.843750\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -232092.625000\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -232874.843750\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -242806.625000\n",
      "    epoch          : 175\n",
      "    loss           : -264085.81875\n",
      "    val_loss       : -260413.433203125\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -285004.625000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -242649.359375\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -285081.250000\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -244818.718750\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -287386.187500\n",
      "    epoch          : 176\n",
      "    loss           : -264931.6715625\n",
      "    val_loss       : -257994.321875\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -294713.375000\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -288318.656250\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -254827.203125\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -285315.250000\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -308371.000000\n",
      "    epoch          : 177\n",
      "    loss           : -265712.96625\n",
      "    val_loss       : -262072.00078125\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -308206.187500\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -250150.000000\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -241960.062500\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -253989.218750\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -286986.750000\n",
      "    epoch          : 178\n",
      "    loss           : -266220.45625\n",
      "    val_loss       : -263285.11572265625\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -307791.718750\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -237226.765625\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -286471.687500\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -243435.781250\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -268437.093750\n",
      "    epoch          : 179\n",
      "    loss           : -267146.45\n",
      "    val_loss       : -263968.1026367188\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -267947.593750\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -233006.656250\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -295269.000000\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -311585.000000\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -253061.328125\n",
      "    epoch          : 180\n",
      "    loss           : -268034.68109375\n",
      "    val_loss       : -261174.9712890625\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -309750.750000\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -237860.812500\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -291444.187500\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -268600.750000\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -238867.031250\n",
      "    epoch          : 181\n",
      "    loss           : -268772.753125\n",
      "    val_loss       : -265523.63134765625\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -236773.875000\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -238829.343750\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -314562.718750\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -289620.250000\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -256038.906250\n",
      "    epoch          : 182\n",
      "    loss           : -269529.43390625\n",
      "    val_loss       : -265588.41240234376\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -313066.812500\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -256142.953125\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -289493.375000\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -313819.875000\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -256741.687500\n",
      "    epoch          : 183\n",
      "    loss           : -270314.490625\n",
      "    val_loss       : -266573.2518554687\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -257163.546875\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -244584.171875\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -311129.937500\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -315030.625000\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -290669.000000\n",
      "    epoch          : 184\n",
      "    loss           : -271008.90859375\n",
      "    val_loss       : -266447.76953125\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -242056.468750\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -257729.968750\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -260569.890625\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -315203.125000\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -294467.812500\n",
      "    epoch          : 185\n",
      "    loss           : -271811.3675\n",
      "    val_loss       : -263833.4510742187\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -314656.750000\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -240802.796875\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -242064.265625\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -302512.937500\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -293930.781250\n",
      "    epoch          : 186\n",
      "    loss           : -272559.163125\n",
      "    val_loss       : -268805.258984375\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -242781.625000\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -317413.500000\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -241234.453125\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -293329.906250\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -293863.968750\n",
      "    epoch          : 187\n",
      "    loss           : -273279.173125\n",
      "    val_loss       : -267968.610546875\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -274485.500000\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -301861.281250\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -273447.531250\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -244831.250000\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -241192.734375\n",
      "    epoch          : 188\n",
      "    loss           : -274027.35421875\n",
      "    val_loss       : -270325.11376953125\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -295344.437500\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -302854.250000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -241790.656250\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -273195.031250\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -242431.078125\n",
      "    epoch          : 189\n",
      "    loss           : -274773.763125\n",
      "    val_loss       : -270707.383203125\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -298102.875000\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -252065.671875\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -296449.343750\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -317986.906250\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -261717.406250\n",
      "    epoch          : 190\n",
      "    loss           : -275129.16328125\n",
      "    val_loss       : -268038.75810546876\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -262913.812500\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -297279.625000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -262970.625000\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -251382.765625\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -263068.718750\n",
      "    epoch          : 191\n",
      "    loss           : -276203.2309375\n",
      "    val_loss       : -272027.72509765625\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -248453.734375\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -304175.656250\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -304533.937500\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -322651.750000\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -247143.062500\n",
      "    epoch          : 192\n",
      "    loss           : -276889.71296875\n",
      "    val_loss       : -269382.6005859375\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -274790.562500\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -243408.000000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -277174.500000\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -304952.687500\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -268361.375000\n",
      "    epoch          : 193\n",
      "    loss           : -277673.22359375\n",
      "    val_loss       : -273067.055078125\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -264276.187500\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -245775.062500\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -265032.500000\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -301199.750000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -299322.218750\n",
      "    epoch          : 194\n",
      "    loss           : -278174.54734375\n",
      "    val_loss       : -270930.30732421874\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -306349.375000\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -300450.906250\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -266371.781250\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -245890.234375\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -256750.843750\n",
      "    epoch          : 195\n",
      "    loss           : -279010.49796875\n",
      "    val_loss       : -274993.35302734375\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -325333.062500\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -310673.593750\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -301204.312500\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -303162.250000\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -302419.812500\n",
      "    epoch          : 196\n",
      "    loss           : -279793.98734375\n",
      "    val_loss       : -274525.886328125\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -269111.406250\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -245822.875000\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -324563.875000\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -247421.828125\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -279565.312500\n",
      "    epoch          : 197\n",
      "    loss           : -280580.55125\n",
      "    val_loss       : -276402.0712890625\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -253765.328125\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -300147.625000\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -268505.156250\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -245491.906250\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -305947.750000\n",
      "    epoch          : 198\n",
      "    loss           : -281165.30765625\n",
      "    val_loss       : -277074.7935546875\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -304679.125000\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -271442.375000\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -326387.218750\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -249886.781250\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -253437.578125\n",
      "    epoch          : 199\n",
      "    loss           : -281896.475\n",
      "    val_loss       : -275076.07568359375\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -269034.375000\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -281184.312500\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -308967.031250\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -329944.062500\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -311375.781250\n",
      "    epoch          : 200\n",
      "    loss           : -282610.90046875\n",
      "    val_loss       : -277371.184765625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch200.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -305511.875000\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -272228.500000\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -305323.375000\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -253609.062500\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -311195.875000\n",
      "    epoch          : 201\n",
      "    loss           : -283331.84859375\n",
      "    val_loss       : -278037.4295898437\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -249291.250000\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -270441.687500\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -273742.937500\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -314679.625000\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -253913.171875\n",
      "    epoch          : 202\n",
      "    loss           : -283891.35171875\n",
      "    val_loss       : -278269.08916015626\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -251787.703125\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -310093.031250\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -315440.500000\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -284703.062500\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -254152.718750\n",
      "    epoch          : 203\n",
      "    loss           : -284684.29421875\n",
      "    val_loss       : -276374.56376953126\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -311974.750000\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -255506.484375\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -250175.062500\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -309719.343750\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -315035.406250\n",
      "    epoch          : 204\n",
      "    loss           : -285466.40125\n",
      "    val_loss       : -279708.68505859375\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -308456.218750\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -258602.000000\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -309584.187500\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -316516.000000\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -306672.218750\n",
      "    epoch          : 205\n",
      "    loss           : -285878.4584375\n",
      "    val_loss       : -277013.0421875\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -251899.250000\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -287898.875000\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -285304.531250\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -274277.625000\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -254963.031250\n",
      "    epoch          : 206\n",
      "    loss           : -286687.37640625\n",
      "    val_loss       : -280962.646484375\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -263886.562500\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -318210.562500\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -284083.593750\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -335224.156250\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -310623.781250\n",
      "    epoch          : 207\n",
      "    loss           : -287463.55609375\n",
      "    val_loss       : -283537.0225585938\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -276775.500000\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -272874.125000\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -272665.781250\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -317543.531250\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -277174.031250\n",
      "    epoch          : 208\n",
      "    loss           : -288116.03078125\n",
      "    val_loss       : -283870.6033203125\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -271819.218750\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -271056.500000\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -256620.015625\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -314094.062500\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -254213.046875\n",
      "    epoch          : 209\n",
      "    loss           : -288793.68046875\n",
      "    val_loss       : -283981.819921875\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -289821.468750\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -275348.875000\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -313980.156250\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -275567.468750\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -278366.062500\n",
      "    epoch          : 210\n",
      "    loss           : -289493.145\n",
      "    val_loss       : -284919.26640625\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -338692.687500\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -264750.187500\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -277496.812500\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -286403.812500\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -334475.718750\n",
      "    epoch          : 211\n",
      "    loss           : -290182.4384375\n",
      "    val_loss       : -279810.4247070312\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -254550.453125\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -254414.484375\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -282535.312500\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -286842.250000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -261165.218750\n",
      "    epoch          : 212\n",
      "    loss           : -290765.55390625\n",
      "    val_loss       : -286426.85380859376\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -276005.281250\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -260204.625000\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -337809.375000\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -315273.718750\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -281194.031250\n",
      "    epoch          : 213\n",
      "    loss           : -291337.07\n",
      "    val_loss       : -282798.79140625\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -319228.281250\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -316090.031250\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -316455.406250\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -279218.468750\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -314322.968750\n",
      "    epoch          : 214\n",
      "    loss           : -292038.03671875\n",
      "    val_loss       : -287063.2328125\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -278857.718750\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -277023.437500\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -283916.593750\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -279346.125000\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -289222.718750\n",
      "    epoch          : 215\n",
      "    loss           : -292797.58578125\n",
      "    val_loss       : -287779.13642578124\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -339789.937500\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -279956.937500\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -318814.750000\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -277938.375000\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -284838.937500\n",
      "    epoch          : 216\n",
      "    loss           : -293336.8846875\n",
      "    val_loss       : -288790.3345703125\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -282294.812500\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -266789.500000\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -267303.812500\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -325255.375000\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -322272.093750\n",
      "    epoch          : 217\n",
      "    loss           : -294102.32171875\n",
      "    val_loss       : -288701.7998046875\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -319645.843750\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -340841.093750\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -280142.375000\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -270434.375000\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -292144.812500\n",
      "    epoch          : 218\n",
      "    loss           : -294800.165\n",
      "    val_loss       : -286886.605859375\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -289927.562500\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -263077.781250\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -343517.000000\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -285224.656250\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -287887.750000\n",
      "    epoch          : 219\n",
      "    loss           : -295391.34703125\n",
      "    val_loss       : -290778.0469726563\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -292219.937500\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -327825.937500\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -286591.437500\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -260087.562500\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -258149.187500\n",
      "    epoch          : 220\n",
      "    loss           : -296046.76328125\n",
      "    val_loss       : -288510.6998046875\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -322235.156250\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -293251.500000\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -317411.281250\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -329323.781250\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -282349.531250\n",
      "    epoch          : 221\n",
      "    loss           : -296560.20328125\n",
      "    val_loss       : -292281.23046875\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -261747.437500\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -316320.375000\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -273627.406250\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -289598.437500\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -322175.312500\n",
      "    epoch          : 222\n",
      "    loss           : -297257.75015625\n",
      "    val_loss       : -287541.19580078125\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -256494.875000\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -267963.937500\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -270561.937500\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -269644.406250\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -321935.250000\n",
      "    epoch          : 223\n",
      "    loss           : -297981.71171875\n",
      "    val_loss       : -292280.555078125\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -282288.000000\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -320604.031250\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -348612.187500\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -342466.281250\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -332168.718750\n",
      "    epoch          : 224\n",
      "    loss           : -298498.06140625\n",
      "    val_loss       : -292718.211328125\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -326695.437500\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -291194.125000\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -263863.500000\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -347038.000000\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -264749.562500\n",
      "    epoch          : 225\n",
      "    loss           : -299185.6696875\n",
      "    val_loss       : -294131.116015625\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -267571.875000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -264671.187500\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -349664.500000\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -296107.312500\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -263121.750000\n",
      "    epoch          : 226\n",
      "    loss           : -299792.4396875\n",
      "    val_loss       : -294626.2540039063\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -332614.312500\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -273207.500000\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -292734.562500\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -271394.812500\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -292896.750000\n",
      "    epoch          : 227\n",
      "    loss           : -300476.3146875\n",
      "    val_loss       : -293752.2650390625\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -322519.750000\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -321888.093750\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -268040.031250\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -263122.968750\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -294720.625000\n",
      "    epoch          : 228\n",
      "    loss           : -301039.24515625\n",
      "    val_loss       : -296255.745703125\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -264591.437500\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -264970.062500\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -331723.375000\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -329408.531250\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -353512.781250\n",
      "    epoch          : 229\n",
      "    loss           : -301527.00640625\n",
      "    val_loss       : -296662.932421875\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -266529.781250\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -263017.156250\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -286113.250000\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -261739.843750\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -332631.875000\n",
      "    epoch          : 230\n",
      "    loss           : -302353.9784375\n",
      "    val_loss       : -293187.9984375\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -332213.593750\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -274859.437500\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -268613.875000\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -297817.187500\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -287110.437500\n",
      "    epoch          : 231\n",
      "    loss           : -303027.7878125\n",
      "    val_loss       : -296588.95146484376\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -330034.656250\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -299578.468750\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -295184.875000\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -269679.250000\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -324114.750000\n",
      "    epoch          : 232\n",
      "    loss           : -303569.61484375\n",
      "    val_loss       : -298571.0140625\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -334394.718750\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -332867.093750\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -354132.562500\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -266315.687500\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -296985.906250\n",
      "    epoch          : 233\n",
      "    loss           : -304183.11203125\n",
      "    val_loss       : -299254.5080078125\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -298457.437500\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -270567.312500\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -327608.562500\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -327612.187500\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -269163.875000\n",
      "    epoch          : 234\n",
      "    loss           : -304756.43375\n",
      "    val_loss       : -298945.689453125\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -299429.250000\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -331815.125000\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -298118.468750\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -268189.250000\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -272630.937500\n",
      "    epoch          : 235\n",
      "    loss           : -305285.586875\n",
      "    val_loss       : -300652.2080078125\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -266791.812500\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -272925.500000\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -357368.312500\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -273097.281250\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -333134.593750\n",
      "    epoch          : 236\n",
      "    loss           : -305924.2771875\n",
      "    val_loss       : -299913.1190429687\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -302360.625000\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -303443.437500\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -327728.187500\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -335061.093750\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -336105.156250\n",
      "    epoch          : 237\n",
      "    loss           : -306581.2865625\n",
      "    val_loss       : -299953.3622070312\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -357560.687500\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -339664.937500\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -337328.531250\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -302233.187500\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -302013.093750\n",
      "    epoch          : 238\n",
      "    loss           : -307209.8446875\n",
      "    val_loss       : -298678.9041015625\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -335552.437500\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -274008.531250\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -302029.125000\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -302667.031250\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -271882.343750\n",
      "    epoch          : 239\n",
      "    loss           : -307719.88375\n",
      "    val_loss       : -293350.8338867187\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -336876.843750\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -300148.531250\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -360301.656250\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -303581.562500\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -303778.937500\n",
      "    epoch          : 240\n",
      "    loss           : -308406.09375\n",
      "    val_loss       : -301710.73583984375\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -272459.968750\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -338172.531250\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -269993.625000\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -304337.281250\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -340125.562500\n",
      "    epoch          : 241\n",
      "    loss           : -309009.4634375\n",
      "    val_loss       : -303283.8552734375\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -338517.500000\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -289370.156250\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -303848.625000\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -281080.750000\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -266990.187500\n",
      "    epoch          : 242\n",
      "    loss           : -309536.3915625\n",
      "    val_loss       : -303383.1458984375\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -272794.437500\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -305889.125000\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -270404.500000\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -293205.656250\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -307685.156250\n",
      "    epoch          : 243\n",
      "    loss           : -310119.736875\n",
      "    val_loss       : -303982.97412109375\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -292338.125000\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -272854.625000\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -273238.718750\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -282989.687500\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -275477.531250\n",
      "    epoch          : 244\n",
      "    loss           : -310730.13875\n",
      "    val_loss       : -303095.48046875\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -283583.000000\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -340376.562500\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -340135.218750\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -341319.375000\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -308001.593750\n",
      "    epoch          : 245\n",
      "    loss           : -311225.90625\n",
      "    val_loss       : -305899.2469726562\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -344652.312500\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -331972.656250\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -294909.156250\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -293960.312500\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -362928.937500\n",
      "    epoch          : 246\n",
      "    loss           : -311969.09625\n",
      "    val_loss       : -306220.0450195313\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -308345.000000\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -295289.875000\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -305793.875000\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -304628.312500\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -343959.500000\n",
      "    epoch          : 247\n",
      "    loss           : -312455.2603125\n",
      "    val_loss       : -303242.20703125\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -308606.437500\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -280445.687500\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -333702.156250\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -332105.187500\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -345505.875000\n",
      "    epoch          : 248\n",
      "    loss           : -313087.3675\n",
      "    val_loss       : -307393.884375\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -363709.812500\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -281438.750000\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -334430.593750\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -348378.281250\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -346673.468750\n",
      "    epoch          : 249\n",
      "    loss           : -313513.2159375\n",
      "    val_loss       : -303930.6396484375\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -334441.000000\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -335314.625000\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -343873.906250\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -306237.062500\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -310233.031250\n",
      "    epoch          : 250\n",
      "    loss           : -314271.1046875\n",
      "    val_loss       : -303491.1189453125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -281352.875000\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -272616.125000\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -308181.125000\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -276260.968750\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -275824.437500\n",
      "    epoch          : 251\n",
      "    loss           : -314856.29375\n",
      "    val_loss       : -309032.28662109375\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -346528.187500\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -310717.250000\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -335322.375000\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -277785.562500\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -277922.812500\n",
      "    epoch          : 252\n",
      "    loss           : -315257.7075\n",
      "    val_loss       : -308136.7513671875\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -277709.531250\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -311627.187500\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -284377.906250\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -336877.187500\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -278087.000000\n",
      "    epoch          : 253\n",
      "    loss           : -315910.6453125\n",
      "    val_loss       : -308994.77001953125\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -307922.000000\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -347921.437500\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -338795.500000\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -301570.750000\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -278940.812500\n",
      "    epoch          : 254\n",
      "    loss           : -316427.5128125\n",
      "    val_loss       : -310719.562109375\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -339472.031250\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -367666.343750\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -277339.000000\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -346622.125000\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -297965.500000\n",
      "    epoch          : 255\n",
      "    loss           : -317019.7046875\n",
      "    val_loss       : -311557.70439453126\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -349222.625000\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -278331.812500\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -346881.968750\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -314872.531250\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -339398.187500\n",
      "    epoch          : 256\n",
      "    loss           : -317651.67875\n",
      "    val_loss       : -310413.43359375\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -280388.156250\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -275795.812500\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -305654.437500\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -340618.250000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -336651.125000\n",
      "    epoch          : 257\n",
      "    loss           : -318119.2596875\n",
      "    val_loss       : -310832.819921875\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -280718.250000\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -278431.500000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -286350.968750\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -285621.156250\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -313380.156250\n",
      "    epoch          : 258\n",
      "    loss           : -318220.023125\n",
      "    val_loss       : -313634.8379882813\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -316275.625000\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -279194.250000\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -316163.437500\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -314502.312500\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -372352.062500\n",
      "    epoch          : 259\n",
      "    loss           : -319257.4396875\n",
      "    val_loss       : -311932.0046875\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -287408.656250\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -349804.437500\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -350916.937500\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -280113.968750\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -317648.062500\n",
      "    epoch          : 260\n",
      "    loss           : -319761.5559375\n",
      "    val_loss       : -314437.8954101562\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -311781.812500\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -373813.218750\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -291681.750000\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -351252.687500\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -280574.968750\n",
      "    epoch          : 261\n",
      "    loss           : -320484.0640625\n",
      "    val_loss       : -314719.8701171875\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -354467.312500\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -371716.468750\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -305539.562500\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -344968.531250\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -352415.718750\n",
      "    epoch          : 262\n",
      "    loss           : -320931.9296875\n",
      "    val_loss       : -315301.3419921875\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -376202.937500\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -277504.500000\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -306021.437500\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -304105.468750\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -355552.687500\n",
      "    epoch          : 263\n",
      "    loss           : -321482.5490625\n",
      "    val_loss       : -313870.63046875\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -306112.625000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -352528.187500\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -286331.843750\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -344794.937500\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -280006.843750\n",
      "    epoch          : 264\n",
      "    loss           : -322030.640625\n",
      "    val_loss       : -316136.469140625\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -304607.375000\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -355160.500000\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -283682.875000\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -316527.562500\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -321301.437500\n",
      "    epoch          : 265\n",
      "    loss           : -322549.954375\n",
      "    val_loss       : -312436.2744140625\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -343929.156250\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -293887.312500\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -377721.812500\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -343040.187500\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -319631.875000\n",
      "    epoch          : 266\n",
      "    loss           : -323111.5996875\n",
      "    val_loss       : -315584.0578125\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -306795.437500\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -305261.218750\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -355651.875000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -304419.250000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -323332.062500\n",
      "    epoch          : 267\n",
      "    loss           : -323664.525\n",
      "    val_loss       : -316808.0625\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -376582.812500\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -379549.312500\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -307367.250000\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -358070.625000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -354961.000000\n",
      "    epoch          : 268\n",
      "    loss           : -324187.445\n",
      "    val_loss       : -316773.11953125\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -288238.500000\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -356249.937500\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -355117.875000\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -323949.406250\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -379080.500000\n",
      "    epoch          : 269\n",
      "    loss           : -324746.7928125\n",
      "    val_loss       : -318398.991796875\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -345584.250000\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -316830.531250\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -355042.593750\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -318790.343750\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -290821.656250\n",
      "    epoch          : 270\n",
      "    loss           : -325207.384375\n",
      "    val_loss       : -315649.72998046875\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -349589.625000\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -283198.843750\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -306099.093750\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -346500.812500\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -284675.500000\n",
      "    epoch          : 271\n",
      "    loss           : -325796.8284375\n",
      "    val_loss       : -319565.7029296875\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -283312.312500\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -379490.437500\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -326204.125000\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -316250.843750\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -325784.875000\n",
      "    epoch          : 272\n",
      "    loss           : -326400.4590625\n",
      "    val_loss       : -319976.348046875\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -319455.812500\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -298405.437500\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -287004.312500\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -282566.875000\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -325452.281250\n",
      "    epoch          : 273\n",
      "    loss           : -326863.085\n",
      "    val_loss       : -321921.6078125\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -359627.406250\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -324171.562500\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -292847.000000\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -327546.812500\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -327178.750000\n",
      "    epoch          : 274\n",
      "    loss           : -327337.8359375\n",
      "    val_loss       : -320819.6873046875\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -288460.000000\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -298169.812500\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -294637.593750\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -360512.187500\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -289939.375000\n",
      "    epoch          : 275\n",
      "    loss           : -328053.2803125\n",
      "    val_loss       : -321523.09619140625\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -310296.906250\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -286315.906250\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -311135.937500\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -286485.656250\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -328411.156250\n",
      "    epoch          : 276\n",
      "    loss           : -327618.978125\n",
      "    val_loss       : -313044.58427734376\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -320007.781250\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -360496.531250\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -306929.687500\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -362540.000000\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -360651.125000\n",
      "    epoch          : 277\n",
      "    loss           : -328849.1296875\n",
      "    val_loss       : -321164.10322265624\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -365083.125000\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -320528.625000\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -362321.250000\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -313131.062500\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -330256.937500\n",
      "    epoch          : 278\n",
      "    loss           : -329483.5728125\n",
      "    val_loss       : -322394.0365234375\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -330410.875000\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -323215.218750\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -362778.625000\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -312206.937500\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -330502.312500\n",
      "    epoch          : 279\n",
      "    loss           : -329985.05\n",
      "    val_loss       : -322340.33662109374\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -352621.250000\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -288594.062500\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -352111.406250\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -286306.531250\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -293802.406250\n",
      "    epoch          : 280\n",
      "    loss           : -330530.215625\n",
      "    val_loss       : -325094.23564453126\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -312306.656250\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -363461.343750\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -368284.968750\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -363857.687500\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -353588.281250\n",
      "    epoch          : 281\n",
      "    loss           : -331012.07\n",
      "    val_loss       : -324363.39423828124\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -329520.625000\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -325012.437500\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -365085.562500\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -289061.406250\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -289846.531250\n",
      "    epoch          : 282\n",
      "    loss           : -330925.5446875\n",
      "    val_loss       : -324606.5614257812\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -286094.937500\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -353865.281250\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -333809.812500\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -325703.718750\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -365059.937500\n",
      "    epoch          : 283\n",
      "    loss           : -332035.9759375\n",
      "    val_loss       : -325409.837890625\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -324071.375000\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -296680.406250\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -289963.593750\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -287453.875000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -385683.000000\n",
      "    epoch          : 284\n",
      "    loss           : -332559.3775\n",
      "    val_loss       : -325320.698828125\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -387366.406250\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -356254.625000\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -301842.562500\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -289769.687500\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -335632.343750\n",
      "    epoch          : 285\n",
      "    loss           : -333081.7128125\n",
      "    val_loss       : -326250.40859375\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -324123.031250\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -389140.312500\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -314571.218750\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -366114.968750\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -289864.625000\n",
      "    epoch          : 286\n",
      "    loss           : -333669.7634375\n",
      "    val_loss       : -321840.3448242188\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -289714.000000\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -293190.218750\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -299721.312500\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -334273.875000\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -334544.406250\n",
      "    epoch          : 287\n",
      "    loss           : -333576.6621875\n",
      "    val_loss       : -328360.75263671874\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -355971.187500\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -391381.593750\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -357365.750000\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -369291.218750\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -293227.687500\n",
      "    epoch          : 288\n",
      "    loss           : -334455.8321875\n",
      "    val_loss       : -327416.8545898438\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -314261.625000\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -301012.312500\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -355018.375000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -391025.750000\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -372293.875000\n",
      "    epoch          : 289\n",
      "    loss           : -335007.610625\n",
      "    val_loss       : -327808.95634765626\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -329012.562500\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -356523.562500\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -292083.562500\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -358019.812500\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -355912.562500\n",
      "    epoch          : 290\n",
      "    loss           : -335581.6953125\n",
      "    val_loss       : -327672.4766601563\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -338843.562500\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -294029.218750\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -370456.937500\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -352992.437500\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -295675.531250\n",
      "    epoch          : 291\n",
      "    loss           : -336062.910625\n",
      "    val_loss       : -330252.2333984375\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -368779.375000\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -327561.437500\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -328133.906250\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -295868.156250\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -292206.750000\n",
      "    epoch          : 292\n",
      "    loss           : -336512.2334375\n",
      "    val_loss       : -325697.8887695313\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -394079.187500\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -317435.312500\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -306081.625000\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -369314.625000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -360901.000000\n",
      "    epoch          : 293\n",
      "    loss           : -337121.964375\n",
      "    val_loss       : -329798.6654296875\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -330033.593750\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -373182.375000\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -373152.875000\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -319689.750000\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -297839.000000\n",
      "    epoch          : 294\n",
      "    loss           : -337635.069375\n",
      "    val_loss       : -330541.65478515625\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -328950.562500\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -297160.500000\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -327846.750000\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -297546.187500\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -325884.375000\n",
      "    epoch          : 295\n",
      "    loss           : -338022.870625\n",
      "    val_loss       : -331830.6763671875\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -294758.812500\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -302847.062500\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -293194.093750\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -373657.656250\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -372772.500000\n",
      "    epoch          : 296\n",
      "    loss           : -338420.2415625\n",
      "    val_loss       : -331289.3923828125\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -371698.718750\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -304873.468750\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -305878.625000\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -292192.750000\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -295514.062500\n",
      "    epoch          : 297\n",
      "    loss           : -339028.2034375\n",
      "    val_loss       : -332486.2530273438\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -332631.281250\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -339492.187500\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -375924.031250\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -372431.718750\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -298083.187500\n",
      "    epoch          : 298\n",
      "    loss           : -339615.87\n",
      "    val_loss       : -333189.06904296874\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -320150.687500\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -341440.125000\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -332162.906250\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -295171.937500\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -340368.781250\n",
      "    epoch          : 299\n",
      "    loss           : -340025.4196875\n",
      "    val_loss       : -332604.63701171876\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -320546.656250\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -319718.562500\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -318554.812500\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -331217.093750\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -374518.812500\n",
      "    epoch          : 300\n",
      "    loss           : -340375.18875\n",
      "    val_loss       : -330489.4307617188\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -296684.250000\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -362435.531250\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -297830.343750\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -332777.656250\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -300096.781250\n",
      "    epoch          : 301\n",
      "    loss           : -341050.660625\n",
      "    val_loss       : -334540.6931640625\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -395851.093750\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -378042.562500\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -376779.875000\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -333596.406250\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -364931.593750\n",
      "    epoch          : 302\n",
      "    loss           : -341431.105625\n",
      "    val_loss       : -335314.86640625\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -334781.750000\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -345331.468750\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -365891.750000\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -398150.875000\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -365080.125000\n",
      "    epoch          : 303\n",
      "    loss           : -341987.8725\n",
      "    val_loss       : -335590.5212890625\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -330333.468750\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -330987.000000\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -345124.093750\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -345036.875000\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -378653.250000\n",
      "    epoch          : 304\n",
      "    loss           : -342416.45375\n",
      "    val_loss       : -333823.99345703126\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -347170.781250\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -300834.968750\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -344347.500000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -376320.750000\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -334035.250000\n",
      "    epoch          : 305\n",
      "    loss           : -342682.595\n",
      "    val_loss       : -336783.2861328125\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -364374.500000\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -299928.843750\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -362567.875000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -367843.312500\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -378891.187500\n",
      "    epoch          : 306\n",
      "    loss           : -343324.1940625\n",
      "    val_loss       : -336644.65234375\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -334823.593750\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -397986.343750\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -400659.968750\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -333038.000000\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -347661.750000\n",
      "    epoch          : 307\n",
      "    loss           : -343838.4284375\n",
      "    val_loss       : -336784.5091796875\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -324530.187500\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -301043.750000\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -366322.187500\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -348919.125000\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -345844.906250\n",
      "    epoch          : 308\n",
      "    loss           : -344295.9859375\n",
      "    val_loss       : -330989.5837890625\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -337367.562500\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -348618.812500\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -336498.062500\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -325565.625000\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -384063.000000\n",
      "    epoch          : 309\n",
      "    loss           : -344789.0353125\n",
      "    val_loss       : -337497.62177734374\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -367087.718750\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -401064.093750\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -311616.000000\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -400987.781250\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -324616.031250\n",
      "    epoch          : 310\n",
      "    loss           : -345110.5590625\n",
      "    val_loss       : -337397.28671875\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -301916.750000\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -337012.718750\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -336796.812500\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -368890.968750\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -369328.031250\n",
      "    epoch          : 311\n",
      "    loss           : -345648.049375\n",
      "    val_loss       : -338277.0850585938\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -323940.187500\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -326045.781250\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -334041.250000\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -299873.750000\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -379154.437500\n",
      "    epoch          : 312\n",
      "    loss           : -346161.286875\n",
      "    val_loss       : -338367.4669921875\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -383081.625000\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -326859.156250\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -304649.968750\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -382831.187500\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -302865.375000\n",
      "    epoch          : 313\n",
      "    loss           : -346685.6321875\n",
      "    val_loss       : -338519.47314453125\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -404382.093750\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -402252.218750\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -368821.531250\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -384191.437500\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -303325.375000\n",
      "    epoch          : 314\n",
      "    loss           : -347151.6171875\n",
      "    val_loss       : -334346.6837890625\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -304325.062500\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -335411.937500\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -306440.031250\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -385025.000000\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -327757.875000\n",
      "    epoch          : 315\n",
      "    loss           : -347486.8096875\n",
      "    val_loss       : -339737.4198242187\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -382294.781250\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -339892.843750\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -404987.875000\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -353180.875000\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -372319.875000\n",
      "    epoch          : 316\n",
      "    loss           : -347998.67625\n",
      "    val_loss       : -340595.2369140625\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -385200.125000\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -370502.125000\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -368829.906250\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -306687.562500\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -336396.437500\n",
      "    epoch          : 317\n",
      "    loss           : -348510.454375\n",
      "    val_loss       : -337621.7142578125\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -403773.312500\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -369941.437500\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -312304.875000\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -370858.500000\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -306566.593750\n",
      "    epoch          : 318\n",
      "    loss           : -348854.613125\n",
      "    val_loss       : -340504.5633789062\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -386063.312500\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -384873.562500\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -355060.812500\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -309219.625000\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -408469.562500\n",
      "    epoch          : 319\n",
      "    loss           : -349331.5109375\n",
      "    val_loss       : -342344.9474609375\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -340332.500000\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -312920.781250\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -409210.625000\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -373407.093750\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -356505.750000\n",
      "    epoch          : 320\n",
      "    loss           : -349294.1946875\n",
      "    val_loss       : -342942.275\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -307054.593750\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -384938.531250\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -408985.968750\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -410735.500000\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -356359.562500\n",
      "    epoch          : 321\n",
      "    loss           : -350205.2775\n",
      "    val_loss       : -343192.68115234375\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -373351.000000\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -387598.125000\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -317186.562500\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -302363.093750\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -337248.031250\n",
      "    epoch          : 322\n",
      "    loss           : -350709.1128125\n",
      "    val_loss       : -344229.0525390625\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -386830.656250\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -386642.375000\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -316582.218750\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -386387.562500\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -383456.687500\n",
      "    epoch          : 323\n",
      "    loss           : -351143.2253125\n",
      "    val_loss       : -342698.2528320312\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -388079.906250\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -356965.062500\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -333899.937500\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -357080.812500\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -306323.593750\n",
      "    epoch          : 324\n",
      "    loss           : -351742.6815625\n",
      "    val_loss       : -343661.7525390625\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -318649.187500\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -304630.562500\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -314413.531250\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -307553.437500\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -357529.562500\n",
      "    epoch          : 325\n",
      "    loss           : -352027.695625\n",
      "    val_loss       : -345472.83154296875\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -390185.687500\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -300318.750000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -312056.500000\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -306148.187500\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -311645.812500\n",
      "    epoch          : 326\n",
      "    loss           : -352617.1990625\n",
      "    val_loss       : -345644.7225585937\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -390046.750000\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -388559.812500\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -411144.375000\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -409708.625000\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -359967.875000\n",
      "    epoch          : 327\n",
      "    loss           : -352884.536875\n",
      "    val_loss       : -346285.7890625\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -389227.718750\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -389235.625000\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -317125.312500\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -341076.812500\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -306198.937500\n",
      "    epoch          : 328\n",
      "    loss           : -353364.1725\n",
      "    val_loss       : -345394.83935546875\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -309088.406250\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -341336.968750\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -357157.562500\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -318049.937500\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -315071.937500\n",
      "    epoch          : 329\n",
      "    loss           : -353848.9159375\n",
      "    val_loss       : -346137.23955078126\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -394867.312500\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -312021.125000\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -377871.812500\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -344377.250000\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -360265.937500\n",
      "    epoch          : 330\n",
      "    loss           : -354296.345\n",
      "    val_loss       : -346130.9073242188\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -346238.218750\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -311509.437500\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -358953.781250\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -392673.125000\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -378537.562500\n",
      "    epoch          : 331\n",
      "    loss           : -354740.30125\n",
      "    val_loss       : -347418.40947265626\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -342200.281250\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -346229.187500\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -393862.125000\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -393427.312500\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -305906.437500\n",
      "    epoch          : 332\n",
      "    loss           : -355275.9159375\n",
      "    val_loss       : -346720.906640625\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -317413.812500\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -395157.812500\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -392976.562500\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -317329.031250\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -362287.093750\n",
      "    epoch          : 333\n",
      "    loss           : -355585.7990625\n",
      "    val_loss       : -347614.2432617188\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -344205.000000\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -316976.937500\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -391117.312500\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -362842.218750\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -378363.906250\n",
      "    epoch          : 334\n",
      "    loss           : -356028.1596875\n",
      "    val_loss       : -348953.68212890625\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -394912.906250\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -389872.531250\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -310975.656250\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -391955.625000\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -309860.718750\n",
      "    epoch          : 335\n",
      "    loss           : -356420.439375\n",
      "    val_loss       : -348613.28271484375\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -417673.312500\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -395246.937500\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -396304.125000\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -306255.750000\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -378830.406250\n",
      "    epoch          : 336\n",
      "    loss           : -356865.0896875\n",
      "    val_loss       : -350389.9837890625\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -340952.406250\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -364499.437500\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -348150.125000\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -311539.500000\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -377597.343750\n",
      "    epoch          : 337\n",
      "    loss           : -357418.014375\n",
      "    val_loss       : -347870.72109375\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -338279.156250\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -396524.437500\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -416682.843750\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -320390.375000\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -365896.343750\n",
      "    epoch          : 338\n",
      "    loss           : -357847.2215625\n",
      "    val_loss       : -344187.91748046875\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -396839.031250\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -396530.968750\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -338275.937500\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -322000.250000\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -311090.062500\n",
      "    epoch          : 339\n",
      "    loss           : -358266.43875\n",
      "    val_loss       : -349811.2658203125\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -397269.375000\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -316790.562500\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -347454.312500\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -312923.625000\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -381950.437500\n",
      "    epoch          : 340\n",
      "    loss           : -358580.854375\n",
      "    val_loss       : -351329.80439453124\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -366430.000000\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -297626.250000\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -312905.281250\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -337881.937500\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -364169.625000\n",
      "    epoch          : 341\n",
      "    loss           : -358559.4078125\n",
      "    val_loss       : -351493.8291015625\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -338789.218750\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -350652.187500\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -320236.937500\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -398338.218750\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -391857.375000\n",
      "    epoch          : 342\n",
      "    loss           : -359474.11\n",
      "    val_loss       : -351249.39423828124\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -421215.000000\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -310107.187500\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -366464.781250\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -419977.437500\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -369398.375000\n",
      "    epoch          : 343\n",
      "    loss           : -359970.314375\n",
      "    val_loss       : -351194.5509765625\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -419864.000000\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -418846.000000\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -400328.437500\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -397017.343750\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -381910.187500\n",
      "    epoch          : 344\n",
      "    loss           : -360330.7196875\n",
      "    val_loss       : -352346.63037109375\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -315693.000000\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -364063.406250\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -325046.187500\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -370705.875000\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -367413.937500\n",
      "    epoch          : 345\n",
      "    loss           : -360743.03\n",
      "    val_loss       : -353471.58173828124\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -395356.875000\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -323647.500000\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -395196.812500\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -310691.000000\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -396459.000000\n",
      "    epoch          : 346\n",
      "    loss           : -361258.1140625\n",
      "    val_loss       : -352556.2677734375\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -384262.437500\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -398231.500000\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -322103.093750\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -421559.437500\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -319525.718750\n",
      "    epoch          : 347\n",
      "    loss           : -361652.410625\n",
      "    val_loss       : -353378.49453125\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -316531.000000\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -398772.406250\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -313053.781250\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -313208.500000\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -386493.375000\n",
      "    epoch          : 348\n",
      "    loss           : -362059.549375\n",
      "    val_loss       : -347421.8145507813\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -338133.031250\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -422836.625000\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -312236.468750\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -398319.437500\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -315622.562500\n",
      "    epoch          : 349\n",
      "    loss           : -362522.8534375\n",
      "    val_loss       : -354692.973828125\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -398403.687500\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -316317.281250\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -324832.000000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -422134.375000\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -368075.250000\n",
      "    epoch          : 350\n",
      "    loss           : -362837.088125\n",
      "    val_loss       : -354825.3365234375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch350.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -424441.843750\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -322627.750000\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -401359.750000\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -316880.531250\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -354283.125000\n",
      "    epoch          : 351\n",
      "    loss           : -363307.269375\n",
      "    val_loss       : -349411.7224609375\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -325923.843750\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -340184.031250\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -314078.750000\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -404726.312500\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -315738.187500\n",
      "    epoch          : 352\n",
      "    loss           : -363658.3884375\n",
      "    val_loss       : -355936.039453125\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -427228.500000\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -385028.781250\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -326715.218750\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -397939.656250\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -339416.562500\n",
      "    epoch          : 353\n",
      "    loss           : -364141.479375\n",
      "    val_loss       : -355906.3623046875\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -340441.468750\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -404441.875000\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -406214.437500\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -425925.093750\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -404189.687500\n",
      "    epoch          : 354\n",
      "    loss           : -364548.6421875\n",
      "    val_loss       : -354494.5513671875\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -426395.500000\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -404879.781250\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -404126.625000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -388335.031250\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -384574.375000\n",
      "    epoch          : 355\n",
      "    loss           : -364979.26125\n",
      "    val_loss       : -351615.9400390625\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -374655.093750\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -389834.375000\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -318462.062500\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -372845.562500\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -374093.125000\n",
      "    epoch          : 356\n",
      "    loss           : -365337.00375\n",
      "    val_loss       : -358344.5044921875\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -315078.000000\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -430095.437500\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -344177.187500\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -356569.593750\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -399733.500000\n",
      "    epoch          : 357\n",
      "    loss           : -365695.701875\n",
      "    val_loss       : -355950.2862304688\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -424590.406250\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -428155.093750\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -404956.562500\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -408582.750000\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -376276.750000\n",
      "    epoch          : 358\n",
      "    loss           : -366165.5690625\n",
      "    val_loss       : -357887.85\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -324463.093750\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -327194.656250\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -375859.500000\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -404094.843750\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -321148.500000\n",
      "    epoch          : 359\n",
      "    loss           : -366545.4315625\n",
      "    val_loss       : -357771.5546875\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -326167.375000\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -407555.125000\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -429554.250000\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -344067.875000\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -388029.718750\n",
      "    epoch          : 360\n",
      "    loss           : -366929.3371875\n",
      "    val_loss       : -356885.70078125\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -376827.593750\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -376191.781250\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -389390.937500\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -329338.187500\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -355720.656250\n",
      "    epoch          : 361\n",
      "    loss           : -367423.2709375\n",
      "    val_loss       : -359394.78662109375\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -409429.000000\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -324594.375000\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -347744.312500\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -376326.031250\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -354853.843750\n",
      "    epoch          : 362\n",
      "    loss           : -367755.3965625\n",
      "    val_loss       : -358987.05703125\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -322965.312500\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -408058.750000\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -377487.312500\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -322705.187500\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -390290.343750\n",
      "    epoch          : 363\n",
      "    loss           : -368004.72375\n",
      "    val_loss       : -359058.79541015625\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -390436.687500\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -350211.625000\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -321583.625000\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -318618.562500\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -328942.375000\n",
      "    epoch          : 364\n",
      "    loss           : -368496.3921875\n",
      "    val_loss       : -359574.09052734374\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -432217.187500\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -328571.468750\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -409222.781250\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -319189.125000\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -376173.343750\n",
      "    epoch          : 365\n",
      "    loss           : -368989.110625\n",
      "    val_loss       : -361224.89013671875\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -348555.781250\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -391698.562500\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -391802.875000\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -430337.937500\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -323884.812500\n",
      "    epoch          : 366\n",
      "    loss           : -369190.916875\n",
      "    val_loss       : -361383.934375\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -339603.812500\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -345481.187500\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -320947.593750\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -320247.250000\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -321222.250000\n",
      "    epoch          : 367\n",
      "    loss           : -369728.5225\n",
      "    val_loss       : -360725.0254882813\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -380419.562500\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -319868.562500\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -381423.093750\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -432309.125000\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -318326.062500\n",
      "    epoch          : 368\n",
      "    loss           : -370202.1075\n",
      "    val_loss       : -360949.35\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -354816.343750\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -321951.375000\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -320296.750000\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -379857.843750\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -410417.062500\n",
      "    epoch          : 369\n",
      "    loss           : -370550.1534375\n",
      "    val_loss       : -361778.189453125\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -349214.437500\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -357807.750000\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -395502.312500\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -406182.812500\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -329355.875000\n",
      "    epoch          : 370\n",
      "    loss           : -370950.18125\n",
      "    val_loss       : -355868.87421875\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -331351.437500\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -323551.437500\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -322183.781250\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -322164.281250\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -402885.000000\n",
      "    epoch          : 371\n",
      "    loss           : -371384.20125\n",
      "    val_loss       : -362807.8985351563\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -436520.562500\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -322550.000000\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -352478.968750\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -430430.500000\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -323613.093750\n",
      "    epoch          : 372\n",
      "    loss           : -371835.2075\n",
      "    val_loss       : -362219.43330078124\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -322849.187500\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -433596.687500\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -322563.687500\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -325849.343750\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -326736.781250\n",
      "    epoch          : 373\n",
      "    loss           : -372111.3890625\n",
      "    val_loss       : -362567.13310546876\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -332794.812500\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -333517.375000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -347636.437500\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -322229.250000\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -382476.843750\n",
      "    epoch          : 374\n",
      "    loss           : -372601.2353125\n",
      "    val_loss       : -362204.92333984375\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -331807.125000\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -360936.687500\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -347168.093750\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -409328.500000\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -410454.812500\n",
      "    epoch          : 375\n",
      "    loss           : -372841.9075\n",
      "    val_loss       : -364302.35390625\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -327875.875000\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -352290.250000\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -333222.218750\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -416394.843750\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -382665.250000\n",
      "    epoch          : 376\n",
      "    loss           : -373334.1071875\n",
      "    val_loss       : -364683.4034179688\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -321320.406250\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -335139.031250\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -435196.187500\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -385110.906250\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -396163.500000\n",
      "    epoch          : 377\n",
      "    loss           : -373701.140625\n",
      "    val_loss       : -365169.0674804688\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -413372.562500\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -385548.937500\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -417577.718750\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -329137.812500\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -409760.875000\n",
      "    epoch          : 378\n",
      "    loss           : -373985.448125\n",
      "    val_loss       : -362553.9512695313\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -323594.250000\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -321344.125000\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -396849.906250\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -333524.468750\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -386016.218750\n",
      "    epoch          : 379\n",
      "    loss           : -374407.6659375\n",
      "    val_loss       : -364516.27763671876\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -335598.281250\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -349495.062500\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -383698.031250\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -326694.812500\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -395484.406250\n",
      "    epoch          : 380\n",
      "    loss           : -374910.3296875\n",
      "    val_loss       : -366160.562109375\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -419329.937500\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -384218.843750\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -355056.656250\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -360774.406250\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -386366.875000\n",
      "    epoch          : 381\n",
      "    loss           : -375184.913125\n",
      "    val_loss       : -366631.67841796874\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -418864.843750\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -362683.687500\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -438518.093750\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -438653.625000\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -414052.750000\n",
      "    epoch          : 382\n",
      "    loss           : -375601.5309375\n",
      "    val_loss       : -364939.9501953125\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -438677.687500\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -328376.625000\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -398478.781250\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -387399.218750\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -395623.656250\n",
      "    epoch          : 383\n",
      "    loss           : -375981.1559375\n",
      "    val_loss       : -366632.69765625\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -387769.000000\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -387746.531250\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -335259.312500\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -322986.375000\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -324598.000000\n",
      "    epoch          : 384\n",
      "    loss           : -376284.098125\n",
      "    val_loss       : -366854.8711914063\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -359954.312500\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -390346.468750\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -364151.781250\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -391038.562500\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -387296.562500\n",
      "    epoch          : 385\n",
      "    loss           : -376702.3371875\n",
      "    val_loss       : -368475.3845703125\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -390584.125000\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -399612.531250\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -390582.625000\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -440046.468750\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -324881.062500\n",
      "    epoch          : 386\n",
      "    loss           : -377044.7025\n",
      "    val_loss       : -360375.441796875\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -398965.375000\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -441382.625000\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -354327.750000\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -336744.937500\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -325539.625000\n",
      "    epoch          : 387\n",
      "    loss           : -377526.585625\n",
      "    val_loss       : -368974.265234375\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -421884.375000\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -332178.312500\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -356636.406250\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -327775.187500\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -331688.875000\n",
      "    epoch          : 388\n",
      "    loss           : -377901.4175\n",
      "    val_loss       : -363332.58828125\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -419436.687500\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -441523.437500\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -332082.562500\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -440891.375000\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -391822.250000\n",
      "    epoch          : 389\n",
      "    loss           : -378227.8475\n",
      "    val_loss       : -368446.019140625\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -440776.656250\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -442599.562500\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -339666.250000\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -334206.812500\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -357927.656250\n",
      "    epoch          : 390\n",
      "    loss           : -378694.871875\n",
      "    val_loss       : -369206.72822265624\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -338891.593750\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -325370.875000\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -335827.437500\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -394131.031250\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -400449.500000\n",
      "    epoch          : 391\n",
      "    loss           : -378960.6515625\n",
      "    val_loss       : -369518.5576171875\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -423321.187500\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -391301.875000\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -366453.312500\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -390736.812500\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -393115.875000\n",
      "    epoch          : 392\n",
      "    loss           : -379453.3575\n",
      "    val_loss       : -371012.18310546875\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -424300.531250\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -391701.406250\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -366741.406250\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -402452.718750\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -328096.031250\n",
      "    epoch          : 393\n",
      "    loss           : -379777.1015625\n",
      "    val_loss       : -369618.995703125\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -336227.625000\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -364065.687500\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -325630.625000\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -401062.343750\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -417930.687500\n",
      "    epoch          : 394\n",
      "    loss           : -380067.8146875\n",
      "    val_loss       : -370795.3708984375\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -352177.375000\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -404923.343750\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -447506.437500\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -327306.562500\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -392594.031250\n",
      "    epoch          : 395\n",
      "    loss           : -380480.5759375\n",
      "    val_loss       : -371870.3478515625\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -420910.218750\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -424988.250000\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -357443.093750\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -446054.312500\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -404670.062500\n",
      "    epoch          : 396\n",
      "    loss           : -380985.0690625\n",
      "    val_loss       : -372120.4734375\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -329951.812500\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -368315.437500\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -368349.625000\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -329298.343750\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -396540.593750\n",
      "    epoch          : 397\n",
      "    loss           : -381314.855\n",
      "    val_loss       : -371092.0109375\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -424891.250000\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -332499.250000\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -444165.531250\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -447070.750000\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -333951.593750\n",
      "    epoch          : 398\n",
      "    loss           : -381655.5409375\n",
      "    val_loss       : -371022.7619140625\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -332679.281250\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -447133.937500\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -365466.968750\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -370504.750000\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -426690.187500\n",
      "    epoch          : 399\n",
      "    loss           : -381905.5896875\n",
      "    val_loss       : -371875.8751953125\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -420954.781250\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -335153.218750\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -394922.218750\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -415866.906250\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -325148.125000\n",
      "    epoch          : 400\n",
      "    loss           : -382281.6875\n",
      "    val_loss       : -372255.6451171875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch400.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -344173.593750\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -329967.625000\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -355772.000000\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -405028.718750\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -334823.531250\n",
      "    epoch          : 401\n",
      "    loss           : -382651.4140625\n",
      "    val_loss       : -372359.57470703125\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -426650.218750\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -420770.312500\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -426420.875000\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -394862.062500\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -396471.062500\n",
      "    epoch          : 402\n",
      "    loss           : -383056.8821875\n",
      "    val_loss       : -373610.04716796876\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -427626.937500\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -447372.062500\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -330644.718750\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -329326.781250\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -397625.531250\n",
      "    epoch          : 403\n",
      "    loss           : -383393.3365625\n",
      "    val_loss       : -374876.43828125\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -425483.593750\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -331920.250000\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -404015.281250\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -420936.000000\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -404604.062500\n",
      "    epoch          : 404\n",
      "    loss           : -383709.8834375\n",
      "    val_loss       : -373554.10029296874\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -362948.062500\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -449928.250000\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -429700.125000\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -427413.718750\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -397385.375000\n",
      "    epoch          : 405\n",
      "    loss           : -384116.29375\n",
      "    val_loss       : -369119.009765625\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -362884.500000\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -332554.937500\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -359603.062500\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -364246.406250\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -332960.812500\n",
      "    epoch          : 406\n",
      "    loss           : -384443.720625\n",
      "    val_loss       : -375728.466796875\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -428909.250000\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -359076.875000\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -366772.250000\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -335157.718750\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -399527.875000\n",
      "    epoch          : 407\n",
      "    loss           : -384845.151875\n",
      "    val_loss       : -376235.1015625\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -368859.156250\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -338316.843750\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -429216.593750\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -450116.781250\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -412004.343750\n",
      "    epoch          : 408\n",
      "    loss           : -385331.0221875\n",
      "    val_loss       : -374545.70283203124\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -431091.843750\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -342579.281250\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -451504.156250\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -407852.437500\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -399451.687500\n",
      "    epoch          : 409\n",
      "    loss           : -385534.53625\n",
      "    val_loss       : -375910.52509765624\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -359486.468750\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -371666.281250\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -335595.968750\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -336931.218750\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -332584.000000\n",
      "    epoch          : 410\n",
      "    loss           : -386033.42375\n",
      "    val_loss       : -375350.05615234375\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -425296.500000\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -365622.718750\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -401284.750000\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -362925.187500\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -342959.843750\n",
      "    epoch          : 411\n",
      "    loss           : -386401.0734375\n",
      "    val_loss       : -376889.666796875\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -363666.281250\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -451203.500000\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -411737.937500\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -338971.625000\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -399591.687500\n",
      "    epoch          : 412\n",
      "    loss           : -386587.4378125\n",
      "    val_loss       : -375505.9854492188\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -450766.562500\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -431895.156250\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -338572.500000\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -374151.187500\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -400565.281250\n",
      "    epoch          : 413\n",
      "    loss           : -386859.649375\n",
      "    val_loss       : -376180.2048828125\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -432343.687500\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -409068.062500\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -375967.437500\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -334631.250000\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -363052.312500\n",
      "    epoch          : 414\n",
      "    loss           : -387370.4509375\n",
      "    val_loss       : -378319.15595703124\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -431062.343750\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -411130.062500\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -447182.562500\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -423339.906250\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -366013.812500\n",
      "    epoch          : 415\n",
      "    loss           : -387659.43\n",
      "    val_loss       : -377925.8869140625\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -454753.093750\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -331752.281250\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -336636.093750\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -451937.625000\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -453988.843750\n",
      "    epoch          : 416\n",
      "    loss           : -387757.125625\n",
      "    val_loss       : -377304.8702148438\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -344328.406250\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -374775.937500\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -433338.187500\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -457125.062500\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -402758.625000\n",
      "    epoch          : 417\n",
      "    loss           : -388430.8159375\n",
      "    val_loss       : -379984.53203125\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -374907.656250\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -434307.500000\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -365483.750000\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -364006.718750\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -403815.375000\n",
      "    epoch          : 418\n",
      "    loss           : -388671.1584375\n",
      "    val_loss       : -378962.02392578125\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -435446.000000\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -335897.343750\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -373376.750000\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -372812.468750\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -372373.843750\n",
      "    epoch          : 419\n",
      "    loss           : -389020.5878125\n",
      "    val_loss       : -380683.725\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -435139.875000\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -426914.531250\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -435712.875000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -426622.062500\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -404120.625000\n",
      "    epoch          : 420\n",
      "    loss           : -389396.9946875\n",
      "    val_loss       : -378914.81884765625\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -335574.281250\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -455758.343750\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -412221.250000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -414731.250000\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -367985.500000\n",
      "    epoch          : 421\n",
      "    loss           : -389808.8121875\n",
      "    val_loss       : -381378.41240234376\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -454952.625000\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -340611.062500\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -366279.812500\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -347559.468750\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -342434.812500\n",
      "    epoch          : 422\n",
      "    loss           : -390164.275\n",
      "    val_loss       : -381513.6229492187\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -342161.375000\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -433111.000000\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -349098.687500\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -367195.656250\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -429536.812500\n",
      "    epoch          : 423\n",
      "    loss           : -390494.1303125\n",
      "    val_loss       : -380779.4461914062\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -346110.937500\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -379714.000000\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -339988.625000\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -338162.781250\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -407003.062500\n",
      "    epoch          : 424\n",
      "    loss           : -390893.5775\n",
      "    val_loss       : -381004.444921875\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -340089.281250\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -340244.375000\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -414113.187500\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -436530.343750\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -405840.375000\n",
      "    epoch          : 425\n",
      "    loss           : -391112.385625\n",
      "    val_loss       : -382162.0291015625\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -438062.500000\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -371306.187500\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -337371.000000\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -364936.687500\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -343681.625000\n",
      "    epoch          : 426\n",
      "    loss           : -390844.1221875\n",
      "    val_loss       : -380317.54296875\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -347289.812500\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -412677.812500\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -366665.500000\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -438246.125000\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -373329.437500\n",
      "    epoch          : 427\n",
      "    loss           : -391815.1334375\n",
      "    val_loss       : -383390.2701171875\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -414961.312500\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -455275.281250\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -375361.875000\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -405579.062500\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -343315.875000\n",
      "    epoch          : 428\n",
      "    loss           : -392220.709375\n",
      "    val_loss       : -376304.955859375\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -408396.562500\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -459771.562500\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -371258.437500\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -414576.781250\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -344619.406250\n",
      "    epoch          : 429\n",
      "    loss           : -392631.5940625\n",
      "    val_loss       : -382702.3845703125\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -413766.687500\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -369471.218750\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -431168.406250\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -428885.593750\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -409622.437500\n",
      "    epoch          : 430\n",
      "    loss           : -392915.720625\n",
      "    val_loss       : -375413.09912109375\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -368654.281250\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -459254.312500\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -347970.812500\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -368259.156250\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -431488.843750\n",
      "    epoch          : 431\n",
      "    loss           : -393254.0409375\n",
      "    val_loss       : -382651.706640625\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -414531.250000\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -346433.187500\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -457803.562500\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -367724.687500\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -409449.125000\n",
      "    epoch          : 432\n",
      "    loss           : -393584.3284375\n",
      "    val_loss       : -375062.35771484376\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -459298.250000\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -339023.062500\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -369055.906250\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -348018.531250\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -434733.500000\n",
      "    epoch          : 433\n",
      "    loss           : -393968.85625\n",
      "    val_loss       : -383743.5890625\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -367947.500000\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -342403.375000\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -433480.906250\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -377445.312500\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -415663.687500\n",
      "    epoch          : 434\n",
      "    loss           : -394341.191875\n",
      "    val_loss       : -383069.3900390625\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -463031.437500\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -340212.531250\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -411409.781250\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -431725.718750\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -436815.437500\n",
      "    epoch          : 435\n",
      "    loss           : -394562.6471875\n",
      "    val_loss       : -383617.4346679688\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -431258.750000\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -442067.312500\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -353719.000000\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -347494.437500\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -419009.437500\n",
      "    epoch          : 436\n",
      "    loss           : -394852.04125\n",
      "    val_loss       : -382651.32060546876\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -435496.312500\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -417906.687500\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -417158.250000\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -417272.125000\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -353454.406250\n",
      "    epoch          : 437\n",
      "    loss           : -395260.7053125\n",
      "    val_loss       : -385092.31494140625\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -350072.125000\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -444241.593750\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -349244.125000\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -413692.312500\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -435630.062500\n",
      "    epoch          : 438\n",
      "    loss           : -395691.528125\n",
      "    val_loss       : -386094.08818359376\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -379469.625000\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -419180.656250\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -461728.562500\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -380814.000000\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -434790.812500\n",
      "    epoch          : 439\n",
      "    loss           : -395939.7709375\n",
      "    val_loss       : -386156.33916015626\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -460599.000000\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -351855.812500\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -412363.750000\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -379571.906250\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -340588.468750\n",
      "    epoch          : 440\n",
      "    loss           : -396285.63125\n",
      "    val_loss       : -384199.5474609375\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -353986.156250\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -343083.625000\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -381800.593750\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -350476.812500\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -352731.687500\n",
      "    epoch          : 441\n",
      "    loss           : -396188.3178125\n",
      "    val_loss       : -382147.31376953126\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -414801.000000\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -380244.312500\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -434883.375000\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -444294.843750\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -412866.562500\n",
      "    epoch          : 442\n",
      "    loss           : -396700.110625\n",
      "    val_loss       : -387353.9471679687\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -442906.250000\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -415292.656250\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -415705.500000\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -344622.375000\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -420572.750000\n",
      "    epoch          : 443\n",
      "    loss           : -397120.71\n",
      "    val_loss       : -388216.13603515626\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -466718.187500\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -414419.187500\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -347898.375000\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -346689.625000\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -415555.468750\n",
      "    epoch          : 444\n",
      "    loss           : -397729.059375\n",
      "    val_loss       : -387837.5275390625\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -383927.312500\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -465064.781250\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -346735.343750\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -418672.625000\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -377319.312500\n",
      "    epoch          : 445\n",
      "    loss           : -398000.4784375\n",
      "    val_loss       : -387683.8849609375\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -421910.187500\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -345717.250000\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -354265.312500\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -374183.875000\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -419729.531250\n",
      "    epoch          : 446\n",
      "    loss           : -398047.22625\n",
      "    val_loss       : -387756.945703125\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -380390.187500\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -354941.406250\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -465276.937500\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -383475.562500\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -419773.281250\n",
      "    epoch          : 447\n",
      "    loss           : -398429.4725\n",
      "    val_loss       : -388742.4336914063\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -356173.125000\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -358379.562500\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -438867.562500\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -420524.968750\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -417735.218750\n",
      "    epoch          : 448\n",
      "    loss           : -398929.888125\n",
      "    val_loss       : -383042.03486328124\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -435549.000000\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -418001.812500\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -346777.437500\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -347510.625000\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -422467.937500\n",
      "    epoch          : 449\n",
      "    loss           : -399325.04625\n",
      "    val_loss       : -387226.98193359375\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -384243.843750\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -357648.968750\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -344580.250000\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -467573.125000\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -439301.156250\n",
      "    epoch          : 450\n",
      "    loss           : -398660.601875\n",
      "    val_loss       : -388095.923046875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -469785.937500\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -376512.687500\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -422386.593750\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -448556.031250\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -384592.500000\n",
      "    epoch          : 451\n",
      "    loss           : -399658.8475\n",
      "    val_loss       : -381155.9828125\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -418517.062500\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -435235.750000\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -447356.781250\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -341324.812500\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -418610.625000\n",
      "    epoch          : 452\n",
      "    loss           : -400162.8653125\n",
      "    val_loss       : -387348.83642578125\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -346382.031250\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -370796.906250\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -424678.125000\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -347281.500000\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -345995.531250\n",
      "    epoch          : 453\n",
      "    loss           : -400611.2653125\n",
      "    val_loss       : -390092.1971679687\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -343539.125000\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -470313.500000\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -423190.687500\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -419691.843750\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -386989.625000\n",
      "    epoch          : 454\n",
      "    loss           : -400785.1809375\n",
      "    val_loss       : -390939.4141601563\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -345031.187500\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -357024.625000\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -347202.437500\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -356443.500000\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -422000.843750\n",
      "    epoch          : 455\n",
      "    loss           : -401140.8071875\n",
      "    val_loss       : -388802.8270507812\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -468057.750000\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -449663.843750\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -472627.031250\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -422337.750000\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -441387.343750\n",
      "    epoch          : 456\n",
      "    loss           : -401429.9509375\n",
      "    val_loss       : -389843.478515625\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -376213.625000\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -376286.593750\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -417280.000000\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -386325.343750\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -440296.125000\n",
      "    epoch          : 457\n",
      "    loss           : -401747.0109375\n",
      "    val_loss       : -388714.68603515625\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -449777.375000\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -376536.687500\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -344708.437500\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -347849.843750\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -350760.250000\n",
      "    epoch          : 458\n",
      "    loss           : -402174.3096875\n",
      "    val_loss       : -393200.8029296875\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -356073.031250\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -350113.687500\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -354937.875000\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -375742.750000\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -346822.968750\n",
      "    epoch          : 459\n",
      "    loss           : -402352.4096875\n",
      "    val_loss       : -392302.4984375\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -357574.531250\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -385550.375000\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -353834.250000\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -423897.093750\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -422948.375000\n",
      "    epoch          : 460\n",
      "    loss           : -402763.595\n",
      "    val_loss       : -393377.08271484374\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -351037.437500\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -374717.687500\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -419636.656250\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -441113.468750\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -426416.750000\n",
      "    epoch          : 461\n",
      "    loss           : -403101.5778125\n",
      "    val_loss       : -385801.2357421875\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -378278.750000\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -350120.406250\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -347493.875000\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -447544.656250\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -419481.562500\n",
      "    epoch          : 462\n",
      "    loss           : -403533.156875\n",
      "    val_loss       : -393489.117578125\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -389604.562500\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -358311.593750\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -441931.000000\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -421037.656250\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -446774.687500\n",
      "    epoch          : 463\n",
      "    loss           : -403696.6896875\n",
      "    val_loss       : -392835.6063476562\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -348023.031250\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -422951.750000\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -473315.093750\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -347588.000000\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -427018.750000\n",
      "    epoch          : 464\n",
      "    loss           : -403943.435\n",
      "    val_loss       : -392612.84912109375\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -377094.281250\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -452275.000000\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -356930.312500\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -452618.437500\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -350061.187500\n",
      "    epoch          : 465\n",
      "    loss           : -404287.3259375\n",
      "    val_loss       : -390515.594140625\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -445945.906250\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -377457.968750\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -472401.000000\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -389209.937500\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -349253.500000\n",
      "    epoch          : 466\n",
      "    loss           : -404550.2071875\n",
      "    val_loss       : -393784.10966796876\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -452294.687500\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -422212.031250\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -348798.750000\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -389338.312500\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -425220.156250\n",
      "    epoch          : 467\n",
      "    loss           : -404825.8425\n",
      "    val_loss       : -392108.512890625\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -378035.875000\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -349558.562500\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -359678.875000\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -350247.312500\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -446326.531250\n",
      "    epoch          : 468\n",
      "    loss           : -405216.726875\n",
      "    val_loss       : -394681.2833984375\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -377069.437500\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -351805.687500\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -387967.937500\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -387746.812500\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -446686.343750\n",
      "    epoch          : 469\n",
      "    loss           : -405580.5375\n",
      "    val_loss       : -394161.3165039063\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -448181.500000\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -477189.750000\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -357864.156250\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -423602.718750\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -354279.468750\n",
      "    epoch          : 470\n",
      "    loss           : -405779.30375\n",
      "    val_loss       : -395003.89609375\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -354596.593750\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -455592.125000\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -386716.656250\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -427645.531250\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -448938.437500\n",
      "    epoch          : 471\n",
      "    loss           : -406164.196875\n",
      "    val_loss       : -393832.089453125\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -357566.218750\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -454865.000000\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -359208.625000\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -363035.718750\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -422735.656250\n",
      "    epoch          : 472\n",
      "    loss           : -405931.685\n",
      "    val_loss       : -394048.016015625\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -351682.937500\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -379693.062500\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -387935.625000\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -430845.406250\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -425529.750000\n",
      "    epoch          : 473\n",
      "    loss           : -406858.021875\n",
      "    val_loss       : -396092.31328125\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -391052.875000\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -393383.625000\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -360598.125000\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -472183.687500\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -427684.000000\n",
      "    epoch          : 474\n",
      "    loss           : -407058.191875\n",
      "    val_loss       : -390030.448046875\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -455842.062500\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -364831.812500\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -364221.343750\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -449829.250000\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -430599.031250\n",
      "    epoch          : 475\n",
      "    loss           : -407416.3628125\n",
      "    val_loss       : -389565.8126953125\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -478974.562500\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -353358.593750\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -430177.468750\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -355896.500000\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -430060.531250\n",
      "    epoch          : 476\n",
      "    loss           : -407779.426875\n",
      "    val_loss       : -391113.1034179687\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -427006.375000\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -475916.062500\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -378765.312500\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -387616.250000\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -354271.531250\n",
      "    epoch          : 477\n",
      "    loss           : -408067.763125\n",
      "    val_loss       : -396579.391796875\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -363510.250000\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -363809.343750\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -432119.718750\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -427263.562500\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -448163.156250\n",
      "    epoch          : 478\n",
      "    loss           : -408388.1528125\n",
      "    val_loss       : -396706.7362304687\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -363770.593750\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -477018.375000\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -394843.250000\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -364056.906250\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -477236.187500\n",
      "    epoch          : 479\n",
      "    loss           : -408591.0590625\n",
      "    val_loss       : -396840.696484375\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -457053.875000\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -446663.937500\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -353332.937500\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -429428.812500\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -432005.187500\n",
      "    epoch          : 480\n",
      "    loss           : -408995.306875\n",
      "    val_loss       : -398088.5782226563\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -380477.937500\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -388795.562500\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -477705.781250\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -432448.125000\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -432060.531250\n",
      "    epoch          : 481\n",
      "    loss           : -409175.05875\n",
      "    val_loss       : -397524.43388671876\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -385735.031250\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -352651.187500\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -448607.750000\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -349130.750000\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -352114.187500\n",
      "    epoch          : 482\n",
      "    loss           : -409532.061875\n",
      "    val_loss       : -398924.969140625\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -460954.531250\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -431426.937500\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -390006.812500\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -479647.937500\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -430229.250000\n",
      "    epoch          : 483\n",
      "    loss           : -409784.333125\n",
      "    val_loss       : -395589.4599609375\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -477066.468750\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -431204.312500\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -394729.437500\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -481276.031250\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -386268.906250\n",
      "    epoch          : 484\n",
      "    loss           : -410304.565625\n",
      "    val_loss       : -399403.2666992188\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -478551.843750\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -478724.906250\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -383074.687500\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -353516.875000\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -432098.812500\n",
      "    epoch          : 485\n",
      "    loss           : -410428.238125\n",
      "    val_loss       : -397281.4806640625\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -432397.343750\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -429985.000000\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -427885.281250\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -435290.875000\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -395912.375000\n",
      "    epoch          : 486\n",
      "    loss           : -410687.0278125\n",
      "    val_loss       : -400375.653125\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -364454.000000\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -450335.062500\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -447203.031250\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -431767.562500\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -432756.031250\n",
      "    epoch          : 487\n",
      "    loss           : -411075.4684375\n",
      "    val_loss       : -398897.70869140624\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -434073.906250\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -460372.718750\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -356094.656250\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -450903.562500\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -482136.625000\n",
      "    epoch          : 488\n",
      "    loss           : -411354.2078125\n",
      "    val_loss       : -398657.3140625\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -432487.187500\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -360130.937500\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -391989.000000\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -352575.875000\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -433419.843750\n",
      "    epoch          : 489\n",
      "    loss           : -411468.0815625\n",
      "    val_loss       : -400254.2357421875\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -460509.031250\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -449827.968750\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -453857.250000\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -354706.375000\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -436953.375000\n",
      "    epoch          : 490\n",
      "    loss           : -411793.9696875\n",
      "    val_loss       : -400021.42421875\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -359533.875000\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -434125.937500\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -454887.281250\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -393941.718750\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -364846.812500\n",
      "    epoch          : 491\n",
      "    loss           : -412340.186875\n",
      "    val_loss       : -398938.1053710937\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -480285.437500\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -395187.250000\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -453854.437500\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -385004.781250\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -454548.531250\n",
      "    epoch          : 492\n",
      "    loss           : -412422.8203125\n",
      "    val_loss       : -401960.6454101562\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -393415.906250\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -357856.125000\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -451715.937500\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -464539.437500\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -433819.906250\n",
      "    epoch          : 493\n",
      "    loss           : -412879.493125\n",
      "    val_loss       : -401472.24755859375\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -462263.593750\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -397359.312500\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -462738.625000\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -455218.343750\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -359036.500000\n",
      "    epoch          : 494\n",
      "    loss           : -413088.588125\n",
      "    val_loss       : -403373.034765625\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -482851.812500\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -364763.062500\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -452788.187500\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -454353.718750\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -358254.218750\n",
      "    epoch          : 495\n",
      "    loss           : -413496.1153125\n",
      "    val_loss       : -402197.53671875\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -464026.250000\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -397691.843750\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -456140.187500\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -369657.343750\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -358247.625000\n",
      "    epoch          : 496\n",
      "    loss           : -413801.1809375\n",
      "    val_loss       : -393991.18603515625\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -363600.375000\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -401171.562500\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -396143.312500\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -483551.968750\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -457356.562500\n",
      "    epoch          : 497\n",
      "    loss           : -413936.5334375\n",
      "    val_loss       : -394415.051953125\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -464359.375000\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -364636.031250\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -481260.312500\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -435973.437500\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -457936.187500\n",
      "    epoch          : 498\n",
      "    loss           : -414227.03625\n",
      "    val_loss       : -400062.1119140625\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -389396.187500\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -390702.625000\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -401809.250000\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -463844.125000\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -489951.218750\n",
      "    epoch          : 499\n",
      "    loss           : -414617.1934375\n",
      "    val_loss       : -404024.3583984375\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -486087.375000\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -399437.593750\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -364902.218750\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -455022.406250\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -362790.562500\n",
      "    epoch          : 500\n",
      "    loss           : -414746.27875\n",
      "    val_loss       : -404780.6171875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch500.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 501 [512/54000 (1%)] Loss: -396741.250000\n",
      "Train Epoch: 501 [11776/54000 (22%)] Loss: -353642.375000\n",
      "Train Epoch: 501 [23040/54000 (43%)] Loss: -364802.781250\n",
      "Train Epoch: 501 [34304/54000 (64%)] Loss: -438513.750000\n",
      "Train Epoch: 501 [45568/54000 (84%)] Loss: -437794.281250\n",
      "    epoch          : 501\n",
      "    loss           : -415184.6553125\n",
      "    val_loss       : -399088.8680664062\n",
      "Train Epoch: 502 [512/54000 (1%)] Loss: -436848.843750\n",
      "Train Epoch: 502 [11776/54000 (22%)] Loss: -369853.656250\n",
      "Train Epoch: 502 [23040/54000 (43%)] Loss: -361083.093750\n",
      "Train Epoch: 502 [34304/54000 (64%)] Loss: -485207.187500\n",
      "Train Epoch: 502 [45568/54000 (84%)] Loss: -357056.906250\n",
      "    epoch          : 502\n",
      "    loss           : -415580.9371875\n",
      "    val_loss       : -404242.33828125\n",
      "Train Epoch: 503 [512/54000 (1%)] Loss: -396978.437500\n",
      "Train Epoch: 503 [11776/54000 (22%)] Loss: -487335.687500\n",
      "Train Epoch: 503 [23040/54000 (43%)] Loss: -467360.656250\n",
      "Train Epoch: 503 [34304/54000 (64%)] Loss: -367017.187500\n",
      "Train Epoch: 503 [45568/54000 (84%)] Loss: -361204.437500\n",
      "    epoch          : 503\n",
      "    loss           : -415693.969375\n",
      "    val_loss       : -399518.1533203125\n",
      "Train Epoch: 504 [512/54000 (1%)] Loss: -368347.937500\n",
      "Train Epoch: 504 [11776/54000 (22%)] Loss: -487549.000000\n",
      "Train Epoch: 504 [23040/54000 (43%)] Loss: -456475.125000\n",
      "Train Epoch: 504 [34304/54000 (64%)] Loss: -485247.562500\n",
      "Train Epoch: 504 [45568/54000 (84%)] Loss: -399083.906250\n",
      "    epoch          : 504\n",
      "    loss           : -416079.68125\n",
      "    val_loss       : -402479.8708984375\n",
      "Train Epoch: 505 [512/54000 (1%)] Loss: -400571.250000\n",
      "Train Epoch: 505 [11776/54000 (22%)] Loss: -387059.312500\n",
      "Train Epoch: 505 [23040/54000 (43%)] Loss: -356313.937500\n",
      "Train Epoch: 505 [34304/54000 (64%)] Loss: -363108.656250\n",
      "Train Epoch: 505 [45568/54000 (84%)] Loss: -359622.562500\n",
      "    epoch          : 505\n",
      "    loss           : -416294.755625\n",
      "    val_loss       : -403557.71416015626\n",
      "Train Epoch: 506 [512/54000 (1%)] Loss: -357837.000000\n",
      "Train Epoch: 506 [11776/54000 (22%)] Loss: -458478.968750\n",
      "Train Epoch: 506 [23040/54000 (43%)] Loss: -397109.531250\n",
      "Train Epoch: 506 [34304/54000 (64%)] Loss: -489993.031250\n",
      "Train Epoch: 506 [45568/54000 (84%)] Loss: -489912.500000\n",
      "    epoch          : 506\n",
      "    loss           : -416630.3940625\n",
      "    val_loss       : -404695.289453125\n",
      "Train Epoch: 507 [512/54000 (1%)] Loss: -469475.437500\n",
      "Train Epoch: 507 [11776/54000 (22%)] Loss: -467905.062500\n",
      "Train Epoch: 507 [23040/54000 (43%)] Loss: -399096.281250\n",
      "Train Epoch: 507 [34304/54000 (64%)] Loss: -455480.000000\n",
      "Train Epoch: 507 [45568/54000 (84%)] Loss: -397374.375000\n",
      "    epoch          : 507\n",
      "    loss           : -416928.7925\n",
      "    val_loss       : -404420.6916015625\n",
      "Train Epoch: 508 [512/54000 (1%)] Loss: -487146.937500\n",
      "Train Epoch: 508 [11776/54000 (22%)] Loss: -359894.312500\n",
      "Train Epoch: 508 [23040/54000 (43%)] Loss: -353415.468750\n",
      "Train Epoch: 508 [34304/54000 (64%)] Loss: -463692.812500\n",
      "Train Epoch: 508 [45568/54000 (84%)] Loss: -457404.687500\n",
      "    epoch          : 508\n",
      "    loss           : -417166.214375\n",
      "    val_loss       : -406138.7330078125\n",
      "Train Epoch: 509 [512/54000 (1%)] Loss: -362756.375000\n",
      "Train Epoch: 509 [11776/54000 (22%)] Loss: -402246.281250\n",
      "Train Epoch: 509 [23040/54000 (43%)] Loss: -358472.375000\n",
      "Train Epoch: 509 [34304/54000 (64%)] Loss: -460634.843750\n",
      "Train Epoch: 509 [45568/54000 (84%)] Loss: -491334.968750\n",
      "    epoch          : 509\n",
      "    loss           : -417454.0321875\n",
      "    val_loss       : -403026.6553710938\n",
      "Train Epoch: 510 [512/54000 (1%)] Loss: -370965.937500\n",
      "Train Epoch: 510 [11776/54000 (22%)] Loss: -440127.500000\n",
      "Train Epoch: 510 [23040/54000 (43%)] Loss: -468989.281250\n",
      "Train Epoch: 510 [34304/54000 (64%)] Loss: -469071.093750\n",
      "Train Epoch: 510 [45568/54000 (84%)] Loss: -441737.937500\n",
      "    epoch          : 510\n",
      "    loss           : -417769.99125\n",
      "    val_loss       : -405533.4971679688\n",
      "Train Epoch: 511 [512/54000 (1%)] Loss: -399672.125000\n",
      "Train Epoch: 511 [11776/54000 (22%)] Loss: -366789.062500\n",
      "Train Epoch: 511 [23040/54000 (43%)] Loss: -469477.656250\n",
      "Train Epoch: 511 [34304/54000 (64%)] Loss: -470112.031250\n",
      "Train Epoch: 511 [45568/54000 (84%)] Loss: -486824.031250\n",
      "    epoch          : 511\n",
      "    loss           : -418027.9275\n",
      "    val_loss       : -406384.67646484374\n",
      "Train Epoch: 512 [512/54000 (1%)] Loss: -357062.656250\n",
      "Train Epoch: 512 [11776/54000 (22%)] Loss: -468189.937500\n",
      "Train Epoch: 512 [23040/54000 (43%)] Loss: -489638.187500\n",
      "Train Epoch: 512 [34304/54000 (64%)] Loss: -360763.718750\n",
      "Train Epoch: 512 [45568/54000 (84%)] Loss: -490563.312500\n",
      "    epoch          : 512\n",
      "    loss           : -418495.775\n",
      "    val_loss       : -399590.9327148438\n",
      "Train Epoch: 513 [512/54000 (1%)] Loss: -491577.687500\n",
      "Train Epoch: 513 [11776/54000 (22%)] Loss: -398985.375000\n",
      "Train Epoch: 513 [23040/54000 (43%)] Loss: -360485.375000\n",
      "Train Epoch: 513 [34304/54000 (64%)] Loss: -362665.187500\n",
      "Train Epoch: 513 [45568/54000 (84%)] Loss: -441233.437500\n",
      "    epoch          : 513\n",
      "    loss           : -418598.561875\n",
      "    val_loss       : -405618.1416015625\n",
      "Train Epoch: 514 [512/54000 (1%)] Loss: -460468.437500\n",
      "Train Epoch: 514 [11776/54000 (22%)] Loss: -459192.593750\n",
      "Train Epoch: 514 [23040/54000 (43%)] Loss: -440228.812500\n",
      "Train Epoch: 514 [34304/54000 (64%)] Loss: -457944.843750\n",
      "Train Epoch: 514 [45568/54000 (84%)] Loss: -362994.437500\n",
      "    epoch          : 514\n",
      "    loss           : -418668.9515625\n",
      "    val_loss       : -407718.8044921875\n",
      "Train Epoch: 515 [512/54000 (1%)] Loss: -361509.687500\n",
      "Train Epoch: 515 [11776/54000 (22%)] Loss: -366574.000000\n",
      "Train Epoch: 515 [23040/54000 (43%)] Loss: -398013.062500\n",
      "Train Epoch: 515 [34304/54000 (64%)] Loss: -393166.406250\n",
      "Train Epoch: 515 [45568/54000 (84%)] Loss: -491092.875000\n",
      "    epoch          : 515\n",
      "    loss           : -419238.379375\n",
      "    val_loss       : -401051.01025390625\n",
      "Train Epoch: 516 [512/54000 (1%)] Loss: -361063.875000\n",
      "Train Epoch: 516 [11776/54000 (22%)] Loss: -438546.312500\n",
      "Train Epoch: 516 [23040/54000 (43%)] Loss: -370664.062500\n",
      "Train Epoch: 516 [34304/54000 (64%)] Loss: -362840.031250\n",
      "Train Epoch: 516 [45568/54000 (84%)] Loss: -360355.187500\n",
      "    epoch          : 516\n",
      "    loss           : -419443.7640625\n",
      "    val_loss       : -406721.42431640625\n",
      "Train Epoch: 517 [512/54000 (1%)] Loss: -471189.656250\n",
      "Train Epoch: 517 [11776/54000 (22%)] Loss: -369512.968750\n",
      "Train Epoch: 517 [23040/54000 (43%)] Loss: -442196.468750\n",
      "Train Epoch: 517 [34304/54000 (64%)] Loss: -362932.000000\n",
      "Train Epoch: 517 [45568/54000 (84%)] Loss: -444414.250000\n",
      "    epoch          : 517\n",
      "    loss           : -419681.35125\n",
      "    val_loss       : -406646.67333984375\n",
      "Train Epoch: 518 [512/54000 (1%)] Loss: -367514.656250\n",
      "Train Epoch: 518 [11776/54000 (22%)] Loss: -367386.562500\n",
      "Train Epoch: 518 [23040/54000 (43%)] Loss: -395769.812500\n",
      "Train Epoch: 518 [34304/54000 (64%)] Loss: -364579.375000\n",
      "Train Epoch: 518 [45568/54000 (84%)] Loss: -365721.687500\n",
      "    epoch          : 518\n",
      "    loss           : -420069.908125\n",
      "    val_loss       : -406941.53876953124\n",
      "Train Epoch: 519 [512/54000 (1%)] Loss: -491969.312500\n",
      "Train Epoch: 519 [11776/54000 (22%)] Loss: -489979.843750\n",
      "Train Epoch: 519 [23040/54000 (43%)] Loss: -461104.687500\n",
      "Train Epoch: 519 [34304/54000 (64%)] Loss: -367311.031250\n",
      "Train Epoch: 519 [45568/54000 (84%)] Loss: -491598.718750\n",
      "    epoch          : 519\n",
      "    loss           : -420314.5503125\n",
      "    val_loss       : -408020.8435546875\n",
      "Train Epoch: 520 [512/54000 (1%)] Loss: -462157.406250\n",
      "Train Epoch: 520 [11776/54000 (22%)] Loss: -394125.562500\n",
      "Train Epoch: 520 [23040/54000 (43%)] Loss: -401768.625000\n",
      "Train Epoch: 520 [34304/54000 (64%)] Loss: -442723.250000\n",
      "Train Epoch: 520 [45568/54000 (84%)] Loss: -359947.187500\n",
      "    epoch          : 520\n",
      "    loss           : -420398.8409375\n",
      "    val_loss       : -400811.73017578124\n",
      "Train Epoch: 521 [512/54000 (1%)] Loss: -391120.281250\n",
      "Train Epoch: 521 [11776/54000 (22%)] Loss: -493712.750000\n",
      "Train Epoch: 521 [23040/54000 (43%)] Loss: -493276.250000\n",
      "Train Epoch: 521 [34304/54000 (64%)] Loss: -462767.343750\n",
      "Train Epoch: 521 [45568/54000 (84%)] Loss: -443835.812500\n",
      "    epoch          : 521\n",
      "    loss           : -420789.910625\n",
      "    val_loss       : -410502.1046875\n",
      "Train Epoch: 522 [512/54000 (1%)] Loss: -402809.906250\n",
      "Train Epoch: 522 [11776/54000 (22%)] Loss: -460169.687500\n",
      "Train Epoch: 522 [23040/54000 (43%)] Loss: -446279.562500\n",
      "Train Epoch: 522 [34304/54000 (64%)] Loss: -471616.250000\n",
      "Train Epoch: 522 [45568/54000 (84%)] Loss: -363710.093750\n",
      "    epoch          : 522\n",
      "    loss           : -420935.6346875\n",
      "    val_loss       : -409399.4745117187\n",
      "Train Epoch: 523 [512/54000 (1%)] Loss: -474284.968750\n",
      "Train Epoch: 523 [11776/54000 (22%)] Loss: -493740.343750\n",
      "Train Epoch: 523 [23040/54000 (43%)] Loss: -371485.093750\n",
      "Train Epoch: 523 [34304/54000 (64%)] Loss: -360880.250000\n",
      "Train Epoch: 523 [45568/54000 (84%)] Loss: -441432.250000\n",
      "    epoch          : 523\n",
      "    loss           : -421377.08\n",
      "    val_loss       : -410902.12578125\n",
      "Train Epoch: 524 [512/54000 (1%)] Loss: -465856.406250\n",
      "Train Epoch: 524 [11776/54000 (22%)] Loss: -492606.125000\n",
      "Train Epoch: 524 [23040/54000 (43%)] Loss: -444055.906250\n",
      "Train Epoch: 524 [34304/54000 (64%)] Loss: -364762.875000\n",
      "Train Epoch: 524 [45568/54000 (84%)] Loss: -464913.625000\n",
      "    epoch          : 524\n",
      "    loss           : -421640.83375\n",
      "    val_loss       : -410704.5467773437\n",
      "Train Epoch: 525 [512/54000 (1%)] Loss: -491924.562500\n",
      "Train Epoch: 525 [11776/54000 (22%)] Loss: -358562.093750\n",
      "Train Epoch: 525 [23040/54000 (43%)] Loss: -491602.625000\n",
      "Train Epoch: 525 [34304/54000 (64%)] Loss: -446976.500000\n",
      "Train Epoch: 525 [45568/54000 (84%)] Loss: -446321.687500\n",
      "    epoch          : 525\n",
      "    loss           : -421919.7646875\n",
      "    val_loss       : -409286.215625\n",
      "Train Epoch: 526 [512/54000 (1%)] Loss: -398421.781250\n",
      "Train Epoch: 526 [11776/54000 (22%)] Loss: -364629.562500\n",
      "Train Epoch: 526 [23040/54000 (43%)] Loss: -374728.281250\n",
      "Train Epoch: 526 [34304/54000 (64%)] Loss: -461374.250000\n",
      "Train Epoch: 526 [45568/54000 (84%)] Loss: -374274.281250\n",
      "    epoch          : 526\n",
      "    loss           : -422138.28125\n",
      "    val_loss       : -410304.12236328126\n",
      "Train Epoch: 527 [512/54000 (1%)] Loss: -472387.812500\n",
      "Train Epoch: 527 [11776/54000 (22%)] Loss: -473971.843750\n",
      "Train Epoch: 527 [23040/54000 (43%)] Loss: -361321.125000\n",
      "Train Epoch: 527 [34304/54000 (64%)] Loss: -373996.656250\n",
      "Train Epoch: 527 [45568/54000 (84%)] Loss: -363001.281250\n",
      "    epoch          : 527\n",
      "    loss           : -422542.5525\n",
      "    val_loss       : -411958.233984375\n",
      "Train Epoch: 528 [512/54000 (1%)] Loss: -393765.281250\n",
      "Train Epoch: 528 [11776/54000 (22%)] Loss: -393360.062500\n",
      "Train Epoch: 528 [23040/54000 (43%)] Loss: -463978.687500\n",
      "Train Epoch: 528 [34304/54000 (64%)] Loss: -396416.437500\n",
      "Train Epoch: 528 [45568/54000 (84%)] Loss: -466143.750000\n",
      "    epoch          : 528\n",
      "    loss           : -423028.3625\n",
      "    val_loss       : -410291.4805664063\n",
      "Train Epoch: 529 [512/54000 (1%)] Loss: -366634.062500\n",
      "Train Epoch: 529 [11776/54000 (22%)] Loss: -446763.125000\n",
      "Train Epoch: 529 [23040/54000 (43%)] Loss: -446883.875000\n",
      "Train Epoch: 529 [34304/54000 (64%)] Loss: -445657.187500\n",
      "Train Epoch: 529 [45568/54000 (84%)] Loss: -364729.156250\n",
      "    epoch          : 529\n",
      "    loss           : -423086.5753125\n",
      "    val_loss       : -410985.232421875\n",
      "Train Epoch: 530 [512/54000 (1%)] Loss: -375372.000000\n",
      "Train Epoch: 530 [11776/54000 (22%)] Loss: -401765.468750\n",
      "Train Epoch: 530 [23040/54000 (43%)] Loss: -388502.843750\n",
      "Train Epoch: 530 [34304/54000 (64%)] Loss: -474113.937500\n",
      "Train Epoch: 530 [45568/54000 (84%)] Loss: -448494.156250\n",
      "    epoch          : 530\n",
      "    loss           : -423296.35875\n",
      "    val_loss       : -409264.8578125\n",
      "Train Epoch: 531 [512/54000 (1%)] Loss: -395581.500000\n",
      "Train Epoch: 531 [11776/54000 (22%)] Loss: -375694.437500\n",
      "Train Epoch: 531 [23040/54000 (43%)] Loss: -447357.312500\n",
      "Train Epoch: 531 [34304/54000 (64%)] Loss: -446993.437500\n",
      "Train Epoch: 531 [45568/54000 (84%)] Loss: -368196.218750\n",
      "    epoch          : 531\n",
      "    loss           : -422800.7490625\n",
      "    val_loss       : -411270.19111328124\n",
      "Train Epoch: 532 [512/54000 (1%)] Loss: -374613.000000\n",
      "Train Epoch: 532 [11776/54000 (22%)] Loss: -462752.468750\n",
      "Train Epoch: 532 [23040/54000 (43%)] Loss: -404311.531250\n",
      "Train Epoch: 532 [34304/54000 (64%)] Loss: -468095.656250\n",
      "Train Epoch: 532 [45568/54000 (84%)] Loss: -367681.000000\n",
      "    epoch          : 532\n",
      "    loss           : -424031.446875\n",
      "    val_loss       : -410291.47421875\n",
      "Train Epoch: 533 [512/54000 (1%)] Loss: -412406.625000\n",
      "Train Epoch: 533 [11776/54000 (22%)] Loss: -449915.250000\n",
      "Train Epoch: 533 [23040/54000 (43%)] Loss: -442763.125000\n",
      "Train Epoch: 533 [34304/54000 (64%)] Loss: -369413.187500\n",
      "Train Epoch: 533 [45568/54000 (84%)] Loss: -362697.937500\n",
      "    epoch          : 533\n",
      "    loss           : -424143.310625\n",
      "    val_loss       : -409416.68017578125\n",
      "Train Epoch: 534 [512/54000 (1%)] Loss: -364991.875000\n",
      "Train Epoch: 534 [11776/54000 (22%)] Loss: -376907.406250\n",
      "Train Epoch: 534 [23040/54000 (43%)] Loss: -465936.437500\n",
      "Train Epoch: 534 [34304/54000 (64%)] Loss: -372876.625000\n",
      "Train Epoch: 534 [45568/54000 (84%)] Loss: -447660.875000\n",
      "    epoch          : 534\n",
      "    loss           : -424402.063125\n",
      "    val_loss       : -411869.89189453126\n",
      "Train Epoch: 535 [512/54000 (1%)] Loss: -377506.625000\n",
      "Train Epoch: 535 [11776/54000 (22%)] Loss: -399922.000000\n",
      "Train Epoch: 535 [23040/54000 (43%)] Loss: -403389.312500\n",
      "Train Epoch: 535 [34304/54000 (64%)] Loss: -357789.562500\n",
      "Train Epoch: 535 [45568/54000 (84%)] Loss: -370885.093750\n",
      "    epoch          : 535\n",
      "    loss           : -424431.9396875\n",
      "    val_loss       : -406811.304296875\n",
      "Train Epoch: 536 [512/54000 (1%)] Loss: -447183.875000\n",
      "Train Epoch: 536 [11776/54000 (22%)] Loss: -464671.093750\n",
      "Train Epoch: 536 [23040/54000 (43%)] Loss: -493214.843750\n",
      "Train Epoch: 536 [34304/54000 (64%)] Loss: -363924.656250\n",
      "Train Epoch: 536 [45568/54000 (84%)] Loss: -446295.531250\n",
      "    epoch          : 536\n",
      "    loss           : -424895.8728125\n",
      "    val_loss       : -414167.17548828124\n",
      "Train Epoch: 537 [512/54000 (1%)] Loss: -465520.406250\n",
      "Train Epoch: 537 [11776/54000 (22%)] Loss: -365932.562500\n",
      "Train Epoch: 537 [23040/54000 (43%)] Loss: -498441.562500\n",
      "Train Epoch: 537 [34304/54000 (64%)] Loss: -496003.218750\n",
      "Train Epoch: 537 [45568/54000 (84%)] Loss: -365135.062500\n",
      "    epoch          : 537\n",
      "    loss           : -425215.8859375\n",
      "    val_loss       : -410082.11796875\n",
      "Train Epoch: 538 [512/54000 (1%)] Loss: -368435.562500\n",
      "Train Epoch: 538 [11776/54000 (22%)] Loss: -448716.656250\n",
      "Train Epoch: 538 [23040/54000 (43%)] Loss: -449005.250000\n",
      "Train Epoch: 538 [34304/54000 (64%)] Loss: -368333.562500\n",
      "Train Epoch: 538 [45568/54000 (84%)] Loss: -447800.906250\n",
      "    epoch          : 538\n",
      "    loss           : -425626.890625\n",
      "    val_loss       : -410830.92890625\n",
      "Train Epoch: 539 [512/54000 (1%)] Loss: -400383.937500\n",
      "Train Epoch: 539 [11776/54000 (22%)] Loss: -396945.031250\n",
      "Train Epoch: 539 [23040/54000 (43%)] Loss: -391461.875000\n",
      "Train Epoch: 539 [34304/54000 (64%)] Loss: -451411.593750\n",
      "Train Epoch: 539 [45568/54000 (84%)] Loss: -449423.625000\n",
      "    epoch          : 539\n",
      "    loss           : -425542.4834375\n",
      "    val_loss       : -410713.18408203125\n",
      "Train Epoch: 540 [512/54000 (1%)] Loss: -366656.312500\n",
      "Train Epoch: 540 [11776/54000 (22%)] Loss: -405256.125000\n",
      "Train Epoch: 540 [23040/54000 (43%)] Loss: -478119.468750\n",
      "Train Epoch: 540 [34304/54000 (64%)] Loss: -366828.531250\n",
      "Train Epoch: 540 [45568/54000 (84%)] Loss: -468888.062500\n",
      "    epoch          : 540\n",
      "    loss           : -425999.5796875\n",
      "    val_loss       : -412541.34345703124\n",
      "Train Epoch: 541 [512/54000 (1%)] Loss: -497310.687500\n",
      "Train Epoch: 541 [11776/54000 (22%)] Loss: -450977.593750\n",
      "Train Epoch: 541 [23040/54000 (43%)] Loss: -397739.687500\n",
      "Train Epoch: 541 [34304/54000 (64%)] Loss: -465123.156250\n",
      "Train Epoch: 541 [45568/54000 (84%)] Loss: -452864.125000\n",
      "    epoch          : 541\n",
      "    loss           : -426294.6403125\n",
      "    val_loss       : -412906.65458984376\n",
      "Train Epoch: 542 [512/54000 (1%)] Loss: -379400.125000\n",
      "Train Epoch: 542 [11776/54000 (22%)] Loss: -478281.750000\n",
      "Train Epoch: 542 [23040/54000 (43%)] Loss: -396762.531250\n",
      "Train Epoch: 542 [34304/54000 (64%)] Loss: -372018.750000\n",
      "Train Epoch: 542 [45568/54000 (84%)] Loss: -448306.281250\n",
      "    epoch          : 542\n",
      "    loss           : -426530.7959375\n",
      "    val_loss       : -412616.9251953125\n",
      "Train Epoch: 543 [512/54000 (1%)] Loss: -376339.000000\n",
      "Train Epoch: 543 [11776/54000 (22%)] Loss: -366275.937500\n",
      "Train Epoch: 543 [23040/54000 (43%)] Loss: -500559.437500\n",
      "Train Epoch: 543 [34304/54000 (64%)] Loss: -479378.187500\n",
      "Train Epoch: 543 [45568/54000 (84%)] Loss: -405623.781250\n",
      "    epoch          : 543\n",
      "    loss           : -426732.904375\n",
      "    val_loss       : -414310.9637695312\n",
      "Train Epoch: 544 [512/54000 (1%)] Loss: -470598.250000\n",
      "Train Epoch: 544 [11776/54000 (22%)] Loss: -407014.187500\n",
      "Train Epoch: 544 [23040/54000 (43%)] Loss: -497812.562500\n",
      "Train Epoch: 544 [34304/54000 (64%)] Loss: -408327.031250\n",
      "Train Epoch: 544 [45568/54000 (84%)] Loss: -451819.937500\n",
      "    epoch          : 544\n",
      "    loss           : -427063.5603125\n",
      "    val_loss       : -413501.97001953126\n",
      "Train Epoch: 545 [512/54000 (1%)] Loss: -365838.718750\n",
      "Train Epoch: 545 [11776/54000 (22%)] Loss: -479599.593750\n",
      "Train Epoch: 545 [23040/54000 (43%)] Loss: -479896.125000\n",
      "Train Epoch: 545 [34304/54000 (64%)] Loss: -471196.531250\n",
      "Train Epoch: 545 [45568/54000 (84%)] Loss: -453304.968750\n",
      "    epoch          : 545\n",
      "    loss           : -427214.5765625\n",
      "    val_loss       : -415190.51826171874\n",
      "Train Epoch: 546 [512/54000 (1%)] Loss: -450939.750000\n",
      "Train Epoch: 546 [11776/54000 (22%)] Loss: -450639.875000\n",
      "Train Epoch: 546 [23040/54000 (43%)] Loss: -378495.250000\n",
      "Train Epoch: 546 [34304/54000 (64%)] Loss: -479376.625000\n",
      "Train Epoch: 546 [45568/54000 (84%)] Loss: -454257.437500\n",
      "    epoch          : 546\n",
      "    loss           : -427725.7503125\n",
      "    val_loss       : -416389.67353515624\n",
      "Train Epoch: 547 [512/54000 (1%)] Loss: -370339.593750\n",
      "Train Epoch: 547 [11776/54000 (22%)] Loss: -481254.125000\n",
      "Train Epoch: 547 [23040/54000 (43%)] Loss: -397548.718750\n",
      "Train Epoch: 547 [34304/54000 (64%)] Loss: -399162.125000\n",
      "Train Epoch: 547 [45568/54000 (84%)] Loss: -469943.718750\n",
      "    epoch          : 547\n",
      "    loss           : -427912.6840625\n",
      "    val_loss       : -414007.6326171875\n",
      "Train Epoch: 548 [512/54000 (1%)] Loss: -482642.000000\n",
      "Train Epoch: 548 [11776/54000 (22%)] Loss: -378970.875000\n",
      "Train Epoch: 548 [23040/54000 (43%)] Loss: -479515.687500\n",
      "Train Epoch: 548 [34304/54000 (64%)] Loss: -368687.125000\n",
      "Train Epoch: 548 [45568/54000 (84%)] Loss: -450631.000000\n",
      "    epoch          : 548\n",
      "    loss           : -428165.746875\n",
      "    val_loss       : -415870.0580078125\n",
      "Train Epoch: 549 [512/54000 (1%)] Loss: -500641.812500\n",
      "Train Epoch: 549 [11776/54000 (22%)] Loss: -468167.656250\n",
      "Train Epoch: 549 [23040/54000 (43%)] Loss: -413284.468750\n",
      "Train Epoch: 549 [34304/54000 (64%)] Loss: -370260.125000\n",
      "Train Epoch: 549 [45568/54000 (84%)] Loss: -375991.187500\n",
      "    epoch          : 549\n",
      "    loss           : -428303.049375\n",
      "    val_loss       : -416941.4689453125\n",
      "Train Epoch: 550 [512/54000 (1%)] Loss: -480459.875000\n",
      "Train Epoch: 550 [11776/54000 (22%)] Loss: -448858.500000\n",
      "Train Epoch: 550 [23040/54000 (43%)] Loss: -501992.250000\n",
      "Train Epoch: 550 [34304/54000 (64%)] Loss: -398667.437500\n",
      "Train Epoch: 550 [45568/54000 (84%)] Loss: -454201.125000\n",
      "    epoch          : 550\n",
      "    loss           : -428752.4334375\n",
      "    val_loss       : -416294.72265625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch550.pth ...\n",
      "Train Epoch: 551 [512/54000 (1%)] Loss: -370798.062500\n",
      "Train Epoch: 551 [11776/54000 (22%)] Loss: -452279.687500\n",
      "Train Epoch: 551 [23040/54000 (43%)] Loss: -504525.343750\n",
      "Train Epoch: 551 [34304/54000 (64%)] Loss: -481136.718750\n",
      "Train Epoch: 551 [45568/54000 (84%)] Loss: -411176.843750\n",
      "    epoch          : 551\n",
      "    loss           : -428996.635625\n",
      "    val_loss       : -417100.2206054687\n",
      "Train Epoch: 552 [512/54000 (1%)] Loss: -481037.593750\n",
      "Train Epoch: 552 [11776/54000 (22%)] Loss: -376444.625000\n",
      "Train Epoch: 552 [23040/54000 (43%)] Loss: -399425.750000\n",
      "Train Epoch: 552 [34304/54000 (64%)] Loss: -370451.437500\n",
      "Train Epoch: 552 [45568/54000 (84%)] Loss: -452296.437500\n",
      "    epoch          : 552\n",
      "    loss           : -428996.19\n",
      "    val_loss       : -412333.10390625\n",
      "Train Epoch: 553 [512/54000 (1%)] Loss: -452430.937500\n",
      "Train Epoch: 553 [11776/54000 (22%)] Loss: -375693.375000\n",
      "Train Epoch: 553 [23040/54000 (43%)] Loss: -406616.562500\n",
      "Train Epoch: 553 [34304/54000 (64%)] Loss: -449475.156250\n",
      "Train Epoch: 553 [45568/54000 (84%)] Loss: -471315.562500\n",
      "    epoch          : 553\n",
      "    loss           : -429617.6128125\n",
      "    val_loss       : -418424.34599609376\n",
      "Train Epoch: 554 [512/54000 (1%)] Loss: -396985.437500\n",
      "Train Epoch: 554 [11776/54000 (22%)] Loss: -374002.625000\n",
      "Train Epoch: 554 [23040/54000 (43%)] Loss: -504113.312500\n",
      "Train Epoch: 554 [34304/54000 (64%)] Loss: -406143.000000\n",
      "Train Epoch: 554 [45568/54000 (84%)] Loss: -410394.593750\n",
      "    epoch          : 554\n",
      "    loss           : -429693.8096875\n",
      "    val_loss       : -415896.2411132812\n",
      "Train Epoch: 555 [512/54000 (1%)] Loss: -500170.906250\n",
      "Train Epoch: 555 [11776/54000 (22%)] Loss: -399506.437500\n",
      "Train Epoch: 555 [23040/54000 (43%)] Loss: -452987.562500\n",
      "Train Epoch: 555 [34304/54000 (64%)] Loss: -368097.812500\n",
      "Train Epoch: 555 [45568/54000 (84%)] Loss: -364195.250000\n",
      "    epoch          : 555\n",
      "    loss           : -429994.500625\n",
      "    val_loss       : -415297.5291992187\n",
      "Train Epoch: 556 [512/54000 (1%)] Loss: -484382.812500\n",
      "Train Epoch: 556 [11776/54000 (22%)] Loss: -483371.500000\n",
      "Train Epoch: 556 [23040/54000 (43%)] Loss: -480841.750000\n",
      "Train Epoch: 556 [34304/54000 (64%)] Loss: -502157.687500\n",
      "Train Epoch: 556 [45568/54000 (84%)] Loss: -372291.093750\n",
      "    epoch          : 556\n",
      "    loss           : -430240.375\n",
      "    val_loss       : -417352.6954101563\n",
      "Train Epoch: 557 [512/54000 (1%)] Loss: -381513.750000\n",
      "Train Epoch: 557 [11776/54000 (22%)] Loss: -457457.781250\n",
      "Train Epoch: 557 [23040/54000 (43%)] Loss: -472310.218750\n",
      "Train Epoch: 557 [34304/54000 (64%)] Loss: -505207.750000\n",
      "Train Epoch: 557 [45568/54000 (84%)] Loss: -373697.187500\n",
      "    epoch          : 557\n",
      "    loss           : -430494.949375\n",
      "    val_loss       : -419305.51513671875\n",
      "Train Epoch: 558 [512/54000 (1%)] Loss: -380853.468750\n",
      "Train Epoch: 558 [11776/54000 (22%)] Loss: -473709.093750\n",
      "Train Epoch: 558 [23040/54000 (43%)] Loss: -457330.468750\n",
      "Train Epoch: 558 [34304/54000 (64%)] Loss: -374768.406250\n",
      "Train Epoch: 558 [45568/54000 (84%)] Loss: -368416.468750\n",
      "    epoch          : 558\n",
      "    loss           : -430674.4925\n",
      "    val_loss       : -416633.856640625\n",
      "Train Epoch: 559 [512/54000 (1%)] Loss: -483538.031250\n",
      "Train Epoch: 559 [11776/54000 (22%)] Loss: -455474.750000\n",
      "Train Epoch: 559 [23040/54000 (43%)] Loss: -403951.843750\n",
      "Train Epoch: 559 [34304/54000 (64%)] Loss: -409533.437500\n",
      "Train Epoch: 559 [45568/54000 (84%)] Loss: -454661.937500\n",
      "    epoch          : 559\n",
      "    loss           : -430909.2728125\n",
      "    val_loss       : -419889.0873046875\n",
      "Train Epoch: 560 [512/54000 (1%)] Loss: -483112.031250\n",
      "Train Epoch: 560 [11776/54000 (22%)] Loss: -371581.625000\n",
      "Train Epoch: 560 [23040/54000 (43%)] Loss: -504971.906250\n",
      "Train Epoch: 560 [34304/54000 (64%)] Loss: -476125.000000\n",
      "Train Epoch: 560 [45568/54000 (84%)] Loss: -452711.312500\n",
      "    epoch          : 560\n",
      "    loss           : -431242.8284375\n",
      "    val_loss       : -418206.8770507813\n",
      "Train Epoch: 561 [512/54000 (1%)] Loss: -379706.687500\n",
      "Train Epoch: 561 [11776/54000 (22%)] Loss: -503575.562500\n",
      "Train Epoch: 561 [23040/54000 (43%)] Loss: -374040.375000\n",
      "Train Epoch: 561 [34304/54000 (64%)] Loss: -474495.500000\n",
      "Train Epoch: 561 [45568/54000 (84%)] Loss: -412477.531250\n",
      "    epoch          : 561\n",
      "    loss           : -431504.1546875\n",
      "    val_loss       : -417158.7489257812\n",
      "Train Epoch: 562 [512/54000 (1%)] Loss: -403532.875000\n",
      "Train Epoch: 562 [11776/54000 (22%)] Loss: -505325.125000\n",
      "Train Epoch: 562 [23040/54000 (43%)] Loss: -485296.750000\n",
      "Train Epoch: 562 [34304/54000 (64%)] Loss: -384328.812500\n",
      "Train Epoch: 562 [45568/54000 (84%)] Loss: -455070.468750\n",
      "    epoch          : 562\n",
      "    loss           : -431842.9234375\n",
      "    val_loss       : -418215.1338867188\n",
      "Train Epoch: 563 [512/54000 (1%)] Loss: -372856.625000\n",
      "Train Epoch: 563 [11776/54000 (22%)] Loss: -507489.937500\n",
      "Train Epoch: 563 [23040/54000 (43%)] Loss: -403143.687500\n",
      "Train Epoch: 563 [34304/54000 (64%)] Loss: -454935.937500\n",
      "Train Epoch: 563 [45568/54000 (84%)] Loss: -458865.312500\n",
      "    epoch          : 563\n",
      "    loss           : -432127.625625\n",
      "    val_loss       : -417855.75849609374\n",
      "Train Epoch: 564 [512/54000 (1%)] Loss: -386284.687500\n",
      "Train Epoch: 564 [11776/54000 (22%)] Loss: -481870.687500\n",
      "Train Epoch: 564 [23040/54000 (43%)] Loss: -410608.218750\n",
      "Train Epoch: 564 [34304/54000 (64%)] Loss: -459550.312500\n",
      "Train Epoch: 564 [45568/54000 (84%)] Loss: -371510.406250\n",
      "    epoch          : 564\n",
      "    loss           : -432417.608125\n",
      "    val_loss       : -420633.08427734376\n",
      "Train Epoch: 565 [512/54000 (1%)] Loss: -374197.125000\n",
      "Train Epoch: 565 [11776/54000 (22%)] Loss: -413771.593750\n",
      "Train Epoch: 565 [23040/54000 (43%)] Loss: -506295.312500\n",
      "Train Epoch: 565 [34304/54000 (64%)] Loss: -486348.875000\n",
      "Train Epoch: 565 [45568/54000 (84%)] Loss: -469897.281250\n",
      "    epoch          : 565\n",
      "    loss           : -432566.411875\n",
      "    val_loss       : -420549.12236328126\n",
      "Train Epoch: 566 [512/54000 (1%)] Loss: -414471.375000\n",
      "Train Epoch: 566 [11776/54000 (22%)] Loss: -459336.875000\n",
      "Train Epoch: 566 [23040/54000 (43%)] Loss: -458955.843750\n",
      "Train Epoch: 566 [34304/54000 (64%)] Loss: -412276.343750\n",
      "Train Epoch: 566 [45568/54000 (84%)] Loss: -451466.281250\n",
      "    epoch          : 566\n",
      "    loss           : -432831.62625\n",
      "    val_loss       : -421478.7703125\n",
      "Train Epoch: 567 [512/54000 (1%)] Loss: -478430.125000\n",
      "Train Epoch: 567 [11776/54000 (22%)] Loss: -419419.125000\n",
      "Train Epoch: 567 [23040/54000 (43%)] Loss: -371538.531250\n",
      "Train Epoch: 567 [34304/54000 (64%)] Loss: -507217.875000\n",
      "Train Epoch: 567 [45568/54000 (84%)] Loss: -459808.250000\n",
      "    epoch          : 567\n",
      "    loss           : -433019.8446875\n",
      "    val_loss       : -412028.91669921874\n",
      "Train Epoch: 568 [512/54000 (1%)] Loss: -371656.281250\n",
      "Train Epoch: 568 [11776/54000 (22%)] Loss: -486552.812500\n",
      "Train Epoch: 568 [23040/54000 (43%)] Loss: -478166.625000\n",
      "Train Epoch: 568 [34304/54000 (64%)] Loss: -486587.531250\n",
      "Train Epoch: 568 [45568/54000 (84%)] Loss: -461397.187500\n",
      "    epoch          : 568\n",
      "    loss           : -433302.5496875\n",
      "    val_loss       : -419710.8923828125\n",
      "Train Epoch: 569 [512/54000 (1%)] Loss: -384717.000000\n",
      "Train Epoch: 569 [11776/54000 (22%)] Loss: -486972.531250\n",
      "Train Epoch: 569 [23040/54000 (43%)] Loss: -472541.406250\n",
      "Train Epoch: 569 [34304/54000 (64%)] Loss: -458165.187500\n",
      "Train Epoch: 569 [45568/54000 (84%)] Loss: -457541.250000\n",
      "    epoch          : 569\n",
      "    loss           : -433501.6334375\n",
      "    val_loss       : -418316.812109375\n",
      "Train Epoch: 570 [512/54000 (1%)] Loss: -478121.375000\n",
      "Train Epoch: 570 [11776/54000 (22%)] Loss: -374019.312500\n",
      "Train Epoch: 570 [23040/54000 (43%)] Loss: -488406.843750\n",
      "Train Epoch: 570 [34304/54000 (64%)] Loss: -454854.343750\n",
      "Train Epoch: 570 [45568/54000 (84%)] Loss: -382897.718750\n",
      "    epoch          : 570\n",
      "    loss           : -433836.338125\n",
      "    val_loss       : -418677.219921875\n",
      "Train Epoch: 571 [512/54000 (1%)] Loss: -380705.156250\n",
      "Train Epoch: 571 [11776/54000 (22%)] Loss: -487650.031250\n",
      "Train Epoch: 571 [23040/54000 (43%)] Loss: -457231.562500\n",
      "Train Epoch: 571 [34304/54000 (64%)] Loss: -459616.156250\n",
      "Train Epoch: 571 [45568/54000 (84%)] Loss: -373714.875000\n",
      "    epoch          : 571\n",
      "    loss           : -434228.0346875\n",
      "    val_loss       : -418991.72705078125\n",
      "Train Epoch: 572 [512/54000 (1%)] Loss: -486105.593750\n",
      "Train Epoch: 572 [11776/54000 (22%)] Loss: -382409.000000\n",
      "Train Epoch: 572 [23040/54000 (43%)] Loss: -504947.906250\n",
      "Train Epoch: 572 [34304/54000 (64%)] Loss: -404816.968750\n",
      "Train Epoch: 572 [45568/54000 (84%)] Loss: -454157.093750\n",
      "    epoch          : 572\n",
      "    loss           : -434283.3571875\n",
      "    val_loss       : -421574.255078125\n",
      "Train Epoch: 573 [512/54000 (1%)] Loss: -404570.250000\n",
      "Train Epoch: 573 [11776/54000 (22%)] Loss: -375449.125000\n",
      "Train Epoch: 573 [23040/54000 (43%)] Loss: -507961.343750\n",
      "Train Epoch: 573 [34304/54000 (64%)] Loss: -454938.937500\n",
      "Train Epoch: 573 [45568/54000 (84%)] Loss: -481047.718750\n",
      "    epoch          : 573\n",
      "    loss           : -434723.3575\n",
      "    val_loss       : -420527.016015625\n",
      "Train Epoch: 574 [512/54000 (1%)] Loss: -456610.781250\n",
      "Train Epoch: 574 [11776/54000 (22%)] Loss: -478512.250000\n",
      "Train Epoch: 574 [23040/54000 (43%)] Loss: -372492.937500\n",
      "Train Epoch: 574 [34304/54000 (64%)] Loss: -510070.093750\n",
      "Train Epoch: 574 [45568/54000 (84%)] Loss: -460051.906250\n",
      "    epoch          : 574\n",
      "    loss           : -434770.145\n",
      "    val_loss       : -421023.105078125\n",
      "Train Epoch: 575 [512/54000 (1%)] Loss: -401625.718750\n",
      "Train Epoch: 575 [11776/54000 (22%)] Loss: -407373.000000\n",
      "Train Epoch: 575 [23040/54000 (43%)] Loss: -514530.968750\n",
      "Train Epoch: 575 [34304/54000 (64%)] Loss: -377678.562500\n",
      "Train Epoch: 575 [45568/54000 (84%)] Loss: -376507.750000\n",
      "    epoch          : 575\n",
      "    loss           : -435205.930625\n",
      "    val_loss       : -421999.6466796875\n",
      "Train Epoch: 576 [512/54000 (1%)] Loss: -410903.437500\n",
      "Train Epoch: 576 [11776/54000 (22%)] Loss: -374890.187500\n",
      "Train Epoch: 576 [23040/54000 (43%)] Loss: -405687.343750\n",
      "Train Epoch: 576 [34304/54000 (64%)] Loss: -406685.562500\n",
      "Train Epoch: 576 [45568/54000 (84%)] Loss: -476959.656250\n",
      "    epoch          : 576\n",
      "    loss           : -435240.321875\n",
      "    val_loss       : -423018.7673828125\n",
      "Train Epoch: 577 [512/54000 (1%)] Loss: -456060.500000\n",
      "Train Epoch: 577 [11776/54000 (22%)] Loss: -403937.500000\n",
      "Train Epoch: 577 [23040/54000 (43%)] Loss: -408862.687500\n",
      "Train Epoch: 577 [34304/54000 (64%)] Loss: -377514.406250\n",
      "Train Epoch: 577 [45568/54000 (84%)] Loss: -479190.062500\n",
      "    epoch          : 577\n",
      "    loss           : -435469.2071875\n",
      "    val_loss       : -422115.587890625\n",
      "Train Epoch: 578 [512/54000 (1%)] Loss: -475942.968750\n",
      "Train Epoch: 578 [11776/54000 (22%)] Loss: -380622.093750\n",
      "Train Epoch: 578 [23040/54000 (43%)] Loss: -406825.593750\n",
      "Train Epoch: 578 [34304/54000 (64%)] Loss: -456976.937500\n",
      "Train Epoch: 578 [45568/54000 (84%)] Loss: -412734.375000\n",
      "    epoch          : 578\n",
      "    loss           : -435757.39125\n",
      "    val_loss       : -417366.19560546876\n",
      "Train Epoch: 579 [512/54000 (1%)] Loss: -457737.437500\n",
      "Train Epoch: 579 [11776/54000 (22%)] Loss: -388450.125000\n",
      "Train Epoch: 579 [23040/54000 (43%)] Loss: -406286.406250\n",
      "Train Epoch: 579 [34304/54000 (64%)] Loss: -457936.625000\n",
      "Train Epoch: 579 [45568/54000 (84%)] Loss: -509806.062500\n",
      "    epoch          : 579\n",
      "    loss           : -436080.1634375\n",
      "    val_loss       : -421935.2998046875\n",
      "Train Epoch: 580 [512/54000 (1%)] Loss: -490425.281250\n",
      "Train Epoch: 580 [11776/54000 (22%)] Loss: -414533.843750\n",
      "Train Epoch: 580 [23040/54000 (43%)] Loss: -463882.625000\n",
      "Train Epoch: 580 [34304/54000 (64%)] Loss: -489742.375000\n",
      "Train Epoch: 580 [45568/54000 (84%)] Loss: -477509.625000\n",
      "    epoch          : 580\n",
      "    loss           : -436345.1696875\n",
      "    val_loss       : -423237.790625\n",
      "Train Epoch: 581 [512/54000 (1%)] Loss: -407355.375000\n",
      "Train Epoch: 581 [11776/54000 (22%)] Loss: -404666.250000\n",
      "Train Epoch: 581 [23040/54000 (43%)] Loss: -384349.687500\n",
      "Train Epoch: 581 [34304/54000 (64%)] Loss: -465950.500000\n",
      "Train Epoch: 581 [45568/54000 (84%)] Loss: -481269.406250\n",
      "    epoch          : 581\n",
      "    loss           : -436686.9415625\n",
      "    val_loss       : -421559.47724609374\n",
      "Train Epoch: 582 [512/54000 (1%)] Loss: -374951.468750\n",
      "Train Epoch: 582 [11776/54000 (22%)] Loss: -374752.312500\n",
      "Train Epoch: 582 [23040/54000 (43%)] Loss: -407946.687500\n",
      "Train Epoch: 582 [34304/54000 (64%)] Loss: -379403.500000\n",
      "Train Epoch: 582 [45568/54000 (84%)] Loss: -513546.750000\n",
      "    epoch          : 582\n",
      "    loss           : -436889.1096875\n",
      "    val_loss       : -423695.25810546876\n",
      "Train Epoch: 583 [512/54000 (1%)] Loss: -480483.125000\n",
      "Train Epoch: 583 [11776/54000 (22%)] Loss: -493739.937500\n",
      "Train Epoch: 583 [23040/54000 (43%)] Loss: -388115.437500\n",
      "Train Epoch: 583 [34304/54000 (64%)] Loss: -510806.468750\n",
      "Train Epoch: 583 [45568/54000 (84%)] Loss: -381626.000000\n",
      "    epoch          : 583\n",
      "    loss           : -436993.59125\n",
      "    val_loss       : -424159.8626953125\n",
      "Train Epoch: 584 [512/54000 (1%)] Loss: -377766.687500\n",
      "Train Epoch: 584 [11776/54000 (22%)] Loss: -464187.281250\n",
      "Train Epoch: 584 [23040/54000 (43%)] Loss: -489497.312500\n",
      "Train Epoch: 584 [34304/54000 (64%)] Loss: -418234.093750\n",
      "Train Epoch: 584 [45568/54000 (84%)] Loss: -480411.875000\n",
      "    epoch          : 584\n",
      "    loss           : -437370.8425\n",
      "    val_loss       : -423096.5254882813\n",
      "Train Epoch: 585 [512/54000 (1%)] Loss: -481581.968750\n",
      "Train Epoch: 585 [11776/54000 (22%)] Loss: -457944.343750\n",
      "Train Epoch: 585 [23040/54000 (43%)] Loss: -411435.625000\n",
      "Train Epoch: 585 [34304/54000 (64%)] Loss: -410645.250000\n",
      "Train Epoch: 585 [45568/54000 (84%)] Loss: -419122.500000\n",
      "    epoch          : 585\n",
      "    loss           : -437582.81625\n",
      "    val_loss       : -425641.37626953126\n",
      "Train Epoch: 586 [512/54000 (1%)] Loss: -492604.468750\n",
      "Train Epoch: 586 [11776/54000 (22%)] Loss: -381509.687500\n",
      "Train Epoch: 586 [23040/54000 (43%)] Loss: -377755.406250\n",
      "Train Epoch: 586 [34304/54000 (64%)] Loss: -418210.500000\n",
      "Train Epoch: 586 [45568/54000 (84%)] Loss: -480054.187500\n",
      "    epoch          : 586\n",
      "    loss           : -437756.20625\n",
      "    val_loss       : -421314.0301757812\n",
      "Train Epoch: 587 [512/54000 (1%)] Loss: -376154.937500\n",
      "Train Epoch: 587 [11776/54000 (22%)] Loss: -462212.593750\n",
      "Train Epoch: 587 [23040/54000 (43%)] Loss: -419397.875000\n",
      "Train Epoch: 587 [34304/54000 (64%)] Loss: -376236.218750\n",
      "Train Epoch: 587 [45568/54000 (84%)] Loss: -483416.906250\n",
      "    epoch          : 587\n",
      "    loss           : -438135.10625\n",
      "    val_loss       : -424285.6514648438\n",
      "Train Epoch: 588 [512/54000 (1%)] Loss: -479096.843750\n",
      "Train Epoch: 588 [11776/54000 (22%)] Loss: -417730.375000\n",
      "Train Epoch: 588 [23040/54000 (43%)] Loss: -482986.437500\n",
      "Train Epoch: 588 [34304/54000 (64%)] Loss: -494533.062500\n",
      "Train Epoch: 588 [45568/54000 (84%)] Loss: -374649.750000\n",
      "    epoch          : 588\n",
      "    loss           : -438391.7453125\n",
      "    val_loss       : -426589.27978515625\n",
      "Train Epoch: 589 [512/54000 (1%)] Loss: -410269.093750\n",
      "Train Epoch: 589 [11776/54000 (22%)] Loss: -413668.375000\n",
      "Train Epoch: 589 [23040/54000 (43%)] Loss: -407594.250000\n",
      "Train Epoch: 589 [34304/54000 (64%)] Loss: -467531.406250\n",
      "Train Epoch: 589 [45568/54000 (84%)] Loss: -375659.125000\n",
      "    epoch          : 589\n",
      "    loss           : -438626.435625\n",
      "    val_loss       : -424881.3982421875\n",
      "Train Epoch: 590 [512/54000 (1%)] Loss: -391699.718750\n",
      "Train Epoch: 590 [11776/54000 (22%)] Loss: -409860.562500\n",
      "Train Epoch: 590 [23040/54000 (43%)] Loss: -462289.500000\n",
      "Train Epoch: 590 [34304/54000 (64%)] Loss: -416590.000000\n",
      "Train Epoch: 590 [45568/54000 (84%)] Loss: -462351.375000\n",
      "    epoch          : 590\n",
      "    loss           : -438888.21875\n",
      "    val_loss       : -426819.5734375\n",
      "Train Epoch: 591 [512/54000 (1%)] Loss: -483074.000000\n",
      "Train Epoch: 591 [11776/54000 (22%)] Loss: -416454.656250\n",
      "Train Epoch: 591 [23040/54000 (43%)] Loss: -466725.843750\n",
      "Train Epoch: 591 [34304/54000 (64%)] Loss: -468379.906250\n",
      "Train Epoch: 591 [45568/54000 (84%)] Loss: -380910.000000\n",
      "    epoch          : 591\n",
      "    loss           : -439051.8078125\n",
      "    val_loss       : -426896.4249023438\n",
      "Train Epoch: 592 [512/54000 (1%)] Loss: -408389.937500\n",
      "Train Epoch: 592 [11776/54000 (22%)] Loss: -404499.875000\n",
      "Train Epoch: 592 [23040/54000 (43%)] Loss: -484264.156250\n",
      "Train Epoch: 592 [34304/54000 (64%)] Loss: -480918.531250\n",
      "Train Epoch: 592 [45568/54000 (84%)] Loss: -460671.062500\n",
      "    epoch          : 592\n",
      "    loss           : -439337.6465625\n",
      "    val_loss       : -421821.4728515625\n",
      "Train Epoch: 593 [512/54000 (1%)] Loss: -512611.718750\n",
      "Train Epoch: 593 [11776/54000 (22%)] Loss: -493932.500000\n",
      "Train Epoch: 593 [23040/54000 (43%)] Loss: -392810.281250\n",
      "Train Epoch: 593 [34304/54000 (64%)] Loss: -383396.437500\n",
      "Train Epoch: 593 [45568/54000 (84%)] Loss: -461216.437500\n",
      "    epoch          : 593\n",
      "    loss           : -439569.4534375\n",
      "    val_loss       : -427665.36591796874\n",
      "Train Epoch: 594 [512/54000 (1%)] Loss: -426015.687500\n",
      "Train Epoch: 594 [11776/54000 (22%)] Loss: -386321.562500\n",
      "Train Epoch: 594 [23040/54000 (43%)] Loss: -388717.875000\n",
      "Train Epoch: 594 [34304/54000 (64%)] Loss: -389097.375000\n",
      "Train Epoch: 594 [45568/54000 (84%)] Loss: -461777.687500\n",
      "    epoch          : 594\n",
      "    loss           : -439731.3521875\n",
      "    val_loss       : -426914.1392578125\n",
      "Train Epoch: 595 [512/54000 (1%)] Loss: -390600.000000\n",
      "Train Epoch: 595 [11776/54000 (22%)] Loss: -482552.781250\n",
      "Train Epoch: 595 [23040/54000 (43%)] Loss: -482170.468750\n",
      "Train Epoch: 595 [34304/54000 (64%)] Loss: -388868.187500\n",
      "Train Epoch: 595 [45568/54000 (84%)] Loss: -479851.375000\n",
      "    epoch          : 595\n",
      "    loss           : -440077.379375\n",
      "    val_loss       : -423529.955078125\n",
      "Train Epoch: 596 [512/54000 (1%)] Loss: -408801.000000\n",
      "Train Epoch: 596 [11776/54000 (22%)] Loss: -483572.093750\n",
      "Train Epoch: 596 [23040/54000 (43%)] Loss: -387738.625000\n",
      "Train Epoch: 596 [34304/54000 (64%)] Loss: -388874.906250\n",
      "Train Epoch: 596 [45568/54000 (84%)] Loss: -460585.281250\n",
      "    epoch          : 596\n",
      "    loss           : -440255.2240625\n",
      "    val_loss       : -425281.23955078126\n",
      "Train Epoch: 597 [512/54000 (1%)] Loss: -391158.187500\n",
      "Train Epoch: 597 [11776/54000 (22%)] Loss: -389467.000000\n",
      "Train Epoch: 597 [23040/54000 (43%)] Loss: -463714.687500\n",
      "Train Epoch: 597 [34304/54000 (64%)] Loss: -519309.718750\n",
      "Train Epoch: 597 [45568/54000 (84%)] Loss: -374491.781250\n",
      "    epoch          : 597\n",
      "    loss           : -440325.3403125\n",
      "    val_loss       : -425048.4501953125\n",
      "Train Epoch: 598 [512/54000 (1%)] Loss: -461814.125000\n",
      "Train Epoch: 598 [11776/54000 (22%)] Loss: -467557.687500\n",
      "Train Epoch: 598 [23040/54000 (43%)] Loss: -390263.718750\n",
      "Train Epoch: 598 [34304/54000 (64%)] Loss: -483525.062500\n",
      "Train Epoch: 598 [45568/54000 (84%)] Loss: -482547.312500\n",
      "    epoch          : 598\n",
      "    loss           : -440722.9415625\n",
      "    val_loss       : -425969.1548828125\n",
      "Train Epoch: 599 [512/54000 (1%)] Loss: -407417.750000\n",
      "Train Epoch: 599 [11776/54000 (22%)] Loss: -495480.906250\n",
      "Train Epoch: 599 [23040/54000 (43%)] Loss: -420715.312500\n",
      "Train Epoch: 599 [34304/54000 (64%)] Loss: -413711.718750\n",
      "Train Epoch: 599 [45568/54000 (84%)] Loss: -470431.468750\n",
      "    epoch          : 599\n",
      "    loss           : -441191.9825\n",
      "    val_loss       : -426495.5225585938\n",
      "Train Epoch: 600 [512/54000 (1%)] Loss: -406563.937500\n",
      "Train Epoch: 600 [11776/54000 (22%)] Loss: -486027.718750\n",
      "Train Epoch: 600 [23040/54000 (43%)] Loss: -515810.187500\n",
      "Train Epoch: 600 [34304/54000 (64%)] Loss: -485336.000000\n",
      "Train Epoch: 600 [45568/54000 (84%)] Loss: -519142.218750\n",
      "    epoch          : 600\n",
      "    loss           : -441111.12625\n",
      "    val_loss       : -420587.1150390625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch600.pth ...\n",
      "Train Epoch: 601 [512/54000 (1%)] Loss: -424486.375000\n",
      "Train Epoch: 601 [11776/54000 (22%)] Loss: -517197.437500\n",
      "Train Epoch: 601 [23040/54000 (43%)] Loss: -411211.500000\n",
      "Train Epoch: 601 [34304/54000 (64%)] Loss: -384803.125000\n",
      "Train Epoch: 601 [45568/54000 (84%)] Loss: -462140.125000\n",
      "    epoch          : 601\n",
      "    loss           : -441414.0975\n",
      "    val_loss       : -430118.6154296875\n",
      "Train Epoch: 602 [512/54000 (1%)] Loss: -472392.062500\n",
      "Train Epoch: 602 [11776/54000 (22%)] Loss: -391602.437500\n",
      "Train Epoch: 602 [23040/54000 (43%)] Loss: -387592.968750\n",
      "Train Epoch: 602 [34304/54000 (64%)] Loss: -378820.125000\n",
      "Train Epoch: 602 [45568/54000 (84%)] Loss: -486287.312500\n",
      "    epoch          : 602\n",
      "    loss           : -441768.6659375\n",
      "    val_loss       : -427031.5763671875\n",
      "Train Epoch: 603 [512/54000 (1%)] Loss: -389071.812500\n",
      "Train Epoch: 603 [11776/54000 (22%)] Loss: -518642.281250\n",
      "Train Epoch: 603 [23040/54000 (43%)] Loss: -515409.562500\n",
      "Train Epoch: 603 [34304/54000 (64%)] Loss: -384295.062500\n",
      "Train Epoch: 603 [45568/54000 (84%)] Loss: -464913.375000\n",
      "    epoch          : 603\n",
      "    loss           : -441908.51625\n",
      "    val_loss       : -426582.00517578126\n",
      "Train Epoch: 604 [512/54000 (1%)] Loss: -409001.468750\n",
      "Train Epoch: 604 [11776/54000 (22%)] Loss: -388315.187500\n",
      "Train Epoch: 604 [23040/54000 (43%)] Loss: -420930.250000\n",
      "Train Epoch: 604 [34304/54000 (64%)] Loss: -485251.843750\n",
      "Train Epoch: 604 [45568/54000 (84%)] Loss: -486190.375000\n",
      "    epoch          : 604\n",
      "    loss           : -442087.28375\n",
      "    val_loss       : -427210.7403320313\n",
      "Train Epoch: 605 [512/54000 (1%)] Loss: -496064.500000\n",
      "Train Epoch: 605 [11776/54000 (22%)] Loss: -407929.062500\n",
      "Train Epoch: 605 [23040/54000 (43%)] Loss: -496482.125000\n",
      "Train Epoch: 605 [34304/54000 (64%)] Loss: -420369.781250\n",
      "Train Epoch: 605 [45568/54000 (84%)] Loss: -462188.312500\n",
      "    epoch          : 605\n",
      "    loss           : -442542.5453125\n",
      "    val_loss       : -427785.2607421875\n",
      "Train Epoch: 606 [512/54000 (1%)] Loss: -413317.812500\n",
      "Train Epoch: 606 [11776/54000 (22%)] Loss: -521460.312500\n",
      "Train Epoch: 606 [23040/54000 (43%)] Loss: -517315.875000\n",
      "Train Epoch: 606 [34304/54000 (64%)] Loss: -463620.250000\n",
      "Train Epoch: 606 [45568/54000 (84%)] Loss: -473738.718750\n",
      "    epoch          : 606\n",
      "    loss           : -442715.47\n",
      "    val_loss       : -430561.9091796875\n",
      "Train Epoch: 607 [512/54000 (1%)] Loss: -422677.750000\n",
      "Train Epoch: 607 [11776/54000 (22%)] Loss: -471276.500000\n",
      "Train Epoch: 607 [23040/54000 (43%)] Loss: -412403.062500\n",
      "Train Epoch: 607 [34304/54000 (64%)] Loss: -472239.187500\n",
      "Train Epoch: 607 [45568/54000 (84%)] Loss: -472184.187500\n",
      "    epoch          : 607\n",
      "    loss           : -442925.1725\n",
      "    val_loss       : -428942.6873046875\n",
      "Train Epoch: 608 [512/54000 (1%)] Loss: -496634.875000\n",
      "Train Epoch: 608 [11776/54000 (22%)] Loss: -407474.312500\n",
      "Train Epoch: 608 [23040/54000 (43%)] Loss: -518386.906250\n",
      "Train Epoch: 608 [34304/54000 (64%)] Loss: -466876.625000\n",
      "Train Epoch: 608 [45568/54000 (84%)] Loss: -425644.468750\n",
      "    epoch          : 608\n",
      "    loss           : -443105.98875\n",
      "    val_loss       : -429608.634375\n",
      "Train Epoch: 609 [512/54000 (1%)] Loss: -422488.218750\n",
      "Train Epoch: 609 [11776/54000 (22%)] Loss: -485894.625000\n",
      "Train Epoch: 609 [23040/54000 (43%)] Loss: -424173.187500\n",
      "Train Epoch: 609 [34304/54000 (64%)] Loss: -487205.375000\n",
      "Train Epoch: 609 [45568/54000 (84%)] Loss: -473009.906250\n",
      "    epoch          : 609\n",
      "    loss           : -443471.8409375\n",
      "    val_loss       : -427646.7275390625\n",
      "Train Epoch: 610 [512/54000 (1%)] Loss: -499491.531250\n",
      "Train Epoch: 610 [11776/54000 (22%)] Loss: -406549.437500\n",
      "Train Epoch: 610 [23040/54000 (43%)] Loss: -474262.531250\n",
      "Train Epoch: 610 [34304/54000 (64%)] Loss: -521578.687500\n",
      "Train Epoch: 610 [45568/54000 (84%)] Loss: -467605.250000\n",
      "    epoch          : 610\n",
      "    loss           : -442794.1078125\n",
      "    val_loss       : -428669.48359375\n",
      "Train Epoch: 611 [512/54000 (1%)] Loss: -390474.812500\n",
      "Train Epoch: 611 [11776/54000 (22%)] Loss: -379059.500000\n",
      "Train Epoch: 611 [23040/54000 (43%)] Loss: -475887.062500\n",
      "Train Epoch: 611 [34304/54000 (64%)] Loss: -413616.437500\n",
      "Train Epoch: 611 [45568/54000 (84%)] Loss: -485827.593750\n",
      "    epoch          : 611\n",
      "    loss           : -443816.1078125\n",
      "    val_loss       : -427454.78828125\n",
      "Train Epoch: 612 [512/54000 (1%)] Loss: -397582.156250\n",
      "Train Epoch: 612 [11776/54000 (22%)] Loss: -379632.437500\n",
      "Train Epoch: 612 [23040/54000 (43%)] Loss: -470871.375000\n",
      "Train Epoch: 612 [34304/54000 (64%)] Loss: -423168.875000\n",
      "Train Epoch: 612 [45568/54000 (84%)] Loss: -521540.062500\n",
      "    epoch          : 612\n",
      "    loss           : -444103.1009375\n",
      "    val_loss       : -427102.0326171875\n",
      "Train Epoch: 613 [512/54000 (1%)] Loss: -523462.000000\n",
      "Train Epoch: 613 [11776/54000 (22%)] Loss: -517794.843750\n",
      "Train Epoch: 613 [23040/54000 (43%)] Loss: -422996.500000\n",
      "Train Epoch: 613 [34304/54000 (64%)] Loss: -409341.218750\n",
      "Train Epoch: 613 [45568/54000 (84%)] Loss: -422144.593750\n",
      "    epoch          : 613\n",
      "    loss           : -444098.763125\n",
      "    val_loss       : -428498.06669921876\n",
      "Train Epoch: 614 [512/54000 (1%)] Loss: -522478.531250\n",
      "Train Epoch: 614 [11776/54000 (22%)] Loss: -393737.437500\n",
      "Train Epoch: 614 [23040/54000 (43%)] Loss: -499896.218750\n",
      "Train Epoch: 614 [34304/54000 (64%)] Loss: -473755.312500\n",
      "Train Epoch: 614 [45568/54000 (84%)] Loss: -423903.656250\n",
      "    epoch          : 614\n",
      "    loss           : -444606.390625\n",
      "    val_loss       : -429925.34150390624\n",
      "Train Epoch: 615 [512/54000 (1%)] Loss: -475502.250000\n",
      "Train Epoch: 615 [11776/54000 (22%)] Loss: -419231.781250\n",
      "Train Epoch: 615 [23040/54000 (43%)] Loss: -377404.875000\n",
      "Train Epoch: 615 [34304/54000 (64%)] Loss: -391530.500000\n",
      "Train Epoch: 615 [45568/54000 (84%)] Loss: -501398.562500\n",
      "    epoch          : 615\n",
      "    loss           : -444766.3028125\n",
      "    val_loss       : -431338.35322265624\n",
      "Train Epoch: 616 [512/54000 (1%)] Loss: -380298.437500\n",
      "Train Epoch: 616 [11776/54000 (22%)] Loss: -410600.625000\n",
      "Train Epoch: 616 [23040/54000 (43%)] Loss: -414956.437500\n",
      "Train Epoch: 616 [34304/54000 (64%)] Loss: -477446.343750\n",
      "Train Epoch: 616 [45568/54000 (84%)] Loss: -494283.218750\n",
      "    epoch          : 616\n",
      "    loss           : -444966.30625\n",
      "    val_loss       : -431722.67255859374\n",
      "Train Epoch: 617 [512/54000 (1%)] Loss: -490149.968750\n",
      "Train Epoch: 617 [11776/54000 (22%)] Loss: -499680.937500\n",
      "Train Epoch: 617 [23040/54000 (43%)] Loss: -520183.937500\n",
      "Train Epoch: 617 [34304/54000 (64%)] Loss: -486445.937500\n",
      "Train Epoch: 617 [45568/54000 (84%)] Loss: -491760.843750\n",
      "    epoch          : 617\n",
      "    loss           : -445286.6775\n",
      "    val_loss       : -429276.69970703125\n",
      "Train Epoch: 618 [512/54000 (1%)] Loss: -476042.187500\n",
      "Train Epoch: 618 [11776/54000 (22%)] Loss: -502033.250000\n",
      "Train Epoch: 618 [23040/54000 (43%)] Loss: -501441.937500\n",
      "Train Epoch: 618 [34304/54000 (64%)] Loss: -384780.125000\n",
      "Train Epoch: 618 [45568/54000 (84%)] Loss: -379958.218750\n",
      "    epoch          : 618\n",
      "    loss           : -445454.5534375\n",
      "    val_loss       : -433622.04814453126\n",
      "Train Epoch: 619 [512/54000 (1%)] Loss: -466267.093750\n",
      "Train Epoch: 619 [11776/54000 (22%)] Loss: -472101.500000\n",
      "Train Epoch: 619 [23040/54000 (43%)] Loss: -502169.343750\n",
      "Train Epoch: 619 [34304/54000 (64%)] Loss: -425070.843750\n",
      "Train Epoch: 619 [45568/54000 (84%)] Loss: -380498.093750\n",
      "    epoch          : 619\n",
      "    loss           : -445718.153125\n",
      "    val_loss       : -431769.26025390625\n",
      "Train Epoch: 620 [512/54000 (1%)] Loss: -501889.375000\n",
      "Train Epoch: 620 [11776/54000 (22%)] Loss: -385639.125000\n",
      "Train Epoch: 620 [23040/54000 (43%)] Loss: -384799.875000\n",
      "Train Epoch: 620 [34304/54000 (64%)] Loss: -391218.875000\n",
      "Train Epoch: 620 [45568/54000 (84%)] Loss: -475121.281250\n",
      "    epoch          : 620\n",
      "    loss           : -445962.38875\n",
      "    val_loss       : -429908.86884765624\n",
      "Train Epoch: 621 [512/54000 (1%)] Loss: -427648.625000\n",
      "Train Epoch: 621 [11776/54000 (22%)] Loss: -415505.031250\n",
      "Train Epoch: 621 [23040/54000 (43%)] Loss: -469944.281250\n",
      "Train Epoch: 621 [34304/54000 (64%)] Loss: -382332.906250\n",
      "Train Epoch: 621 [45568/54000 (84%)] Loss: -477537.250000\n",
      "    epoch          : 621\n",
      "    loss           : -446059.32875\n",
      "    val_loss       : -429562.5057617187\n",
      "Train Epoch: 622 [512/54000 (1%)] Loss: -384928.000000\n",
      "Train Epoch: 622 [11776/54000 (22%)] Loss: -387382.531250\n",
      "Train Epoch: 622 [23040/54000 (43%)] Loss: -384194.625000\n",
      "Train Epoch: 622 [34304/54000 (64%)] Loss: -383416.937500\n",
      "Train Epoch: 622 [45568/54000 (84%)] Loss: -467143.625000\n",
      "    epoch          : 622\n",
      "    loss           : -446495.5690625\n",
      "    val_loss       : -431264.64501953125\n",
      "Train Epoch: 623 [512/54000 (1%)] Loss: -471655.125000\n",
      "Train Epoch: 623 [11776/54000 (22%)] Loss: -526138.312500\n",
      "Train Epoch: 623 [23040/54000 (43%)] Loss: -522655.062500\n",
      "Train Epoch: 623 [34304/54000 (64%)] Loss: -491118.781250\n",
      "Train Epoch: 623 [45568/54000 (84%)] Loss: -436373.062500\n",
      "    epoch          : 623\n",
      "    loss           : -446571.409375\n",
      "    val_loss       : -430211.43486328126\n",
      "Train Epoch: 624 [512/54000 (1%)] Loss: -476823.343750\n",
      "Train Epoch: 624 [11776/54000 (22%)] Loss: -415760.437500\n",
      "Train Epoch: 624 [23040/54000 (43%)] Loss: -491531.593750\n",
      "Train Epoch: 624 [34304/54000 (64%)] Loss: -378513.593750\n",
      "Train Epoch: 624 [45568/54000 (84%)] Loss: -386793.437500\n",
      "    epoch          : 624\n",
      "    loss           : -447030.3415625\n",
      "    val_loss       : -431130.4999023437\n",
      "Train Epoch: 625 [512/54000 (1%)] Loss: -502123.687500\n",
      "Train Epoch: 625 [11776/54000 (22%)] Loss: -521865.562500\n",
      "Train Epoch: 625 [23040/54000 (43%)] Loss: -525735.812500\n",
      "Train Epoch: 625 [34304/54000 (64%)] Loss: -469255.406250\n",
      "Train Epoch: 625 [45568/54000 (84%)] Loss: -476748.000000\n",
      "    epoch          : 625\n",
      "    loss           : -447050.04625\n",
      "    val_loss       : -432525.4131835938\n",
      "Train Epoch: 626 [512/54000 (1%)] Loss: -426021.812500\n",
      "Train Epoch: 626 [11776/54000 (22%)] Loss: -382676.093750\n",
      "Train Epoch: 626 [23040/54000 (43%)] Loss: -491621.000000\n",
      "Train Epoch: 626 [34304/54000 (64%)] Loss: -502736.531250\n",
      "Train Epoch: 626 [45568/54000 (84%)] Loss: -385340.125000\n",
      "    epoch          : 626\n",
      "    loss           : -447208.425\n",
      "    val_loss       : -425458.4786132813\n",
      "Train Epoch: 627 [512/54000 (1%)] Loss: -489618.750000\n",
      "Train Epoch: 627 [11776/54000 (22%)] Loss: -493614.218750\n",
      "Train Epoch: 627 [23040/54000 (43%)] Loss: -391306.687500\n",
      "Train Epoch: 627 [34304/54000 (64%)] Loss: -502416.125000\n",
      "Train Epoch: 627 [45568/54000 (84%)] Loss: -383715.187500\n",
      "    epoch          : 627\n",
      "    loss           : -447515.384375\n",
      "    val_loss       : -432132.63603515626\n",
      "Train Epoch: 628 [512/54000 (1%)] Loss: -386522.812500\n",
      "Train Epoch: 628 [11776/54000 (22%)] Loss: -523871.937500\n",
      "Train Epoch: 628 [23040/54000 (43%)] Loss: -412469.656250\n",
      "Train Epoch: 628 [34304/54000 (64%)] Loss: -419147.656250\n",
      "Train Epoch: 628 [45568/54000 (84%)] Loss: -478358.218750\n",
      "    epoch          : 628\n",
      "    loss           : -447726.579375\n",
      "    val_loss       : -431704.49453125\n",
      "Train Epoch: 629 [512/54000 (1%)] Loss: -489723.781250\n",
      "Train Epoch: 629 [11776/54000 (22%)] Loss: -420342.468750\n",
      "Train Epoch: 629 [23040/54000 (43%)] Loss: -390492.312500\n",
      "Train Epoch: 629 [34304/54000 (64%)] Loss: -428232.437500\n",
      "Train Epoch: 629 [45568/54000 (84%)] Loss: -388864.000000\n",
      "    epoch          : 629\n",
      "    loss           : -448059.3678125\n",
      "    val_loss       : -430828.5333007813\n",
      "Train Epoch: 630 [512/54000 (1%)] Loss: -490024.687500\n",
      "Train Epoch: 630 [11776/54000 (22%)] Loss: -397021.593750\n",
      "Train Epoch: 630 [23040/54000 (43%)] Loss: -522929.937500\n",
      "Train Epoch: 630 [34304/54000 (64%)] Loss: -503963.812500\n",
      "Train Epoch: 630 [45568/54000 (84%)] Loss: -384095.750000\n",
      "    epoch          : 630\n",
      "    loss           : -448202.339375\n",
      "    val_loss       : -433155.320703125\n",
      "Train Epoch: 631 [512/54000 (1%)] Loss: -387567.375000\n",
      "Train Epoch: 631 [11776/54000 (22%)] Loss: -423979.375000\n",
      "Train Epoch: 631 [23040/54000 (43%)] Loss: -426178.062500\n",
      "Train Epoch: 631 [34304/54000 (64%)] Loss: -501892.343750\n",
      "Train Epoch: 631 [45568/54000 (84%)] Loss: -392973.875000\n",
      "    epoch          : 631\n",
      "    loss           : -448375.47\n",
      "    val_loss       : -435325.0501953125\n",
      "Train Epoch: 632 [512/54000 (1%)] Loss: -426528.187500\n",
      "Train Epoch: 632 [11776/54000 (22%)] Loss: -504757.843750\n",
      "Train Epoch: 632 [23040/54000 (43%)] Loss: -397492.406250\n",
      "Train Epoch: 632 [34304/54000 (64%)] Loss: -505421.250000\n",
      "Train Epoch: 632 [45568/54000 (84%)] Loss: -480957.937500\n",
      "    epoch          : 632\n",
      "    loss           : -448725.075\n",
      "    val_loss       : -430964.0390625\n",
      "Train Epoch: 633 [512/54000 (1%)] Loss: -491225.000000\n",
      "Train Epoch: 633 [11776/54000 (22%)] Loss: -490840.000000\n",
      "Train Epoch: 633 [23040/54000 (43%)] Loss: -504499.500000\n",
      "Train Epoch: 633 [34304/54000 (64%)] Loss: -426837.343750\n",
      "Train Epoch: 633 [45568/54000 (84%)] Loss: -473659.343750\n",
      "    epoch          : 633\n",
      "    loss           : -448845.2\n",
      "    val_loss       : -433833.820703125\n",
      "Train Epoch: 634 [512/54000 (1%)] Loss: -386818.937500\n",
      "Train Epoch: 634 [11776/54000 (22%)] Loss: -471112.281250\n",
      "Train Epoch: 634 [23040/54000 (43%)] Loss: -525433.375000\n",
      "Train Epoch: 634 [34304/54000 (64%)] Loss: -494526.218750\n",
      "Train Epoch: 634 [45568/54000 (84%)] Loss: -470909.500000\n",
      "    epoch          : 634\n",
      "    loss           : -449200.6553125\n",
      "    val_loss       : -436793.2998046875\n",
      "Train Epoch: 635 [512/54000 (1%)] Loss: -527180.125000\n",
      "Train Epoch: 635 [11776/54000 (22%)] Loss: -470295.531250\n",
      "Train Epoch: 635 [23040/54000 (43%)] Loss: -383665.781250\n",
      "Train Epoch: 635 [34304/54000 (64%)] Loss: -429298.531250\n",
      "Train Epoch: 635 [45568/54000 (84%)] Loss: -489760.500000\n",
      "    epoch          : 635\n",
      "    loss           : -449294.9459375\n",
      "    val_loss       : -427708.628125\n",
      "Train Epoch: 636 [512/54000 (1%)] Loss: -506615.656250\n",
      "Train Epoch: 636 [11776/54000 (22%)] Loss: -399572.875000\n",
      "Train Epoch: 636 [23040/54000 (43%)] Loss: -494814.125000\n",
      "Train Epoch: 636 [34304/54000 (64%)] Loss: -505031.562500\n",
      "Train Epoch: 636 [45568/54000 (84%)] Loss: -388875.718750\n",
      "    epoch          : 636\n",
      "    loss           : -449253.0909375\n",
      "    val_loss       : -432957.989453125\n",
      "Train Epoch: 637 [512/54000 (1%)] Loss: -417874.375000\n",
      "Train Epoch: 637 [11776/54000 (22%)] Loss: -416590.375000\n",
      "Train Epoch: 637 [23040/54000 (43%)] Loss: -468806.812500\n",
      "Train Epoch: 637 [34304/54000 (64%)] Loss: -389451.625000\n",
      "Train Epoch: 637 [45568/54000 (84%)] Loss: -475685.406250\n",
      "    epoch          : 637\n",
      "    loss           : -449717.2784375\n",
      "    val_loss       : -434095.595703125\n",
      "Train Epoch: 638 [512/54000 (1%)] Loss: -507270.906250\n",
      "Train Epoch: 638 [11776/54000 (22%)] Loss: -528346.312500\n",
      "Train Epoch: 638 [23040/54000 (43%)] Loss: -430948.031250\n",
      "Train Epoch: 638 [34304/54000 (64%)] Loss: -525789.187500\n",
      "Train Epoch: 638 [45568/54000 (84%)] Loss: -481926.531250\n",
      "    epoch          : 638\n",
      "    loss           : -450038.4884375\n",
      "    val_loss       : -438239.04755859374\n",
      "Train Epoch: 639 [512/54000 (1%)] Loss: -480925.687500\n",
      "Train Epoch: 639 [11776/54000 (22%)] Loss: -386461.125000\n",
      "Train Epoch: 639 [23040/54000 (43%)] Loss: -388880.281250\n",
      "Train Epoch: 639 [34304/54000 (64%)] Loss: -480975.062500\n",
      "Train Epoch: 639 [45568/54000 (84%)] Loss: -388014.437500\n",
      "    epoch          : 639\n",
      "    loss           : -450180.784375\n",
      "    val_loss       : -433964.79267578124\n",
      "Train Epoch: 640 [512/54000 (1%)] Loss: -493143.593750\n",
      "Train Epoch: 640 [11776/54000 (22%)] Loss: -523734.562500\n",
      "Train Epoch: 640 [23040/54000 (43%)] Loss: -397304.031250\n",
      "Train Epoch: 640 [34304/54000 (64%)] Loss: -398068.500000\n",
      "Train Epoch: 640 [45568/54000 (84%)] Loss: -390353.375000\n",
      "    epoch          : 640\n",
      "    loss           : -450406.1853125\n",
      "    val_loss       : -436335.93525390624\n",
      "Train Epoch: 641 [512/54000 (1%)] Loss: -507177.000000\n",
      "Train Epoch: 641 [11776/54000 (22%)] Loss: -507123.218750\n",
      "Train Epoch: 641 [23040/54000 (43%)] Loss: -493728.000000\n",
      "Train Epoch: 641 [34304/54000 (64%)] Loss: -481830.531250\n",
      "Train Epoch: 641 [45568/54000 (84%)] Loss: -388105.218750\n",
      "    epoch          : 641\n",
      "    loss           : -450561.4559375\n",
      "    val_loss       : -434485.8099609375\n",
      "Train Epoch: 642 [512/54000 (1%)] Loss: -391707.375000\n",
      "Train Epoch: 642 [11776/54000 (22%)] Loss: -423615.500000\n",
      "Train Epoch: 642 [23040/54000 (43%)] Loss: -495131.562500\n",
      "Train Epoch: 642 [34304/54000 (64%)] Loss: -419087.656250\n",
      "Train Epoch: 642 [45568/54000 (84%)] Loss: -496160.937500\n",
      "    epoch          : 642\n",
      "    loss           : -450765.3540625\n",
      "    val_loss       : -434583.46923828125\n",
      "Train Epoch: 643 [512/54000 (1%)] Loss: -506558.343750\n",
      "Train Epoch: 643 [11776/54000 (22%)] Loss: -386428.593750\n",
      "Train Epoch: 643 [23040/54000 (43%)] Loss: -527646.750000\n",
      "Train Epoch: 643 [34304/54000 (64%)] Loss: -399036.656250\n",
      "Train Epoch: 643 [45568/54000 (84%)] Loss: -494978.062500\n",
      "    epoch          : 643\n",
      "    loss           : -451112.3584375\n",
      "    val_loss       : -433864.56376953126\n",
      "Train Epoch: 644 [512/54000 (1%)] Loss: -397336.625000\n",
      "Train Epoch: 644 [11776/54000 (22%)] Loss: -509190.593750\n",
      "Train Epoch: 644 [23040/54000 (43%)] Loss: -494563.281250\n",
      "Train Epoch: 644 [34304/54000 (64%)] Loss: -416584.812500\n",
      "Train Epoch: 644 [45568/54000 (84%)] Loss: -496270.218750\n",
      "    epoch          : 644\n",
      "    loss           : -451369.7390625\n",
      "    val_loss       : -434314.9755859375\n",
      "Train Epoch: 645 [512/54000 (1%)] Loss: -399530.843750\n",
      "Train Epoch: 645 [11776/54000 (22%)] Loss: -428205.843750\n",
      "Train Epoch: 645 [23040/54000 (43%)] Loss: -421767.437500\n",
      "Train Epoch: 645 [34304/54000 (64%)] Loss: -494289.312500\n",
      "Train Epoch: 645 [45568/54000 (84%)] Loss: -401682.562500\n",
      "    epoch          : 645\n",
      "    loss           : -451531.1534375\n",
      "    val_loss       : -438563.6963867188\n",
      "Train Epoch: 646 [512/54000 (1%)] Loss: -420383.906250\n",
      "Train Epoch: 646 [11776/54000 (22%)] Loss: -508077.500000\n",
      "Train Epoch: 646 [23040/54000 (43%)] Loss: -484791.093750\n",
      "Train Epoch: 646 [34304/54000 (64%)] Loss: -479518.625000\n",
      "Train Epoch: 646 [45568/54000 (84%)] Loss: -530900.375000\n",
      "    epoch          : 646\n",
      "    loss           : -451764.43625\n",
      "    val_loss       : -435109.202734375\n",
      "Train Epoch: 647 [512/54000 (1%)] Loss: -432025.812500\n",
      "Train Epoch: 647 [11776/54000 (22%)] Loss: -507312.031250\n",
      "Train Epoch: 647 [23040/54000 (43%)] Loss: -390459.000000\n",
      "Train Epoch: 647 [34304/54000 (64%)] Loss: -485939.656250\n",
      "Train Epoch: 647 [45568/54000 (84%)] Loss: -398650.000000\n",
      "    epoch          : 647\n",
      "    loss           : -451920.9065625\n",
      "    val_loss       : -438826.0509765625\n",
      "Train Epoch: 648 [512/54000 (1%)] Loss: -394793.312500\n",
      "Train Epoch: 648 [11776/54000 (22%)] Loss: -421552.843750\n",
      "Train Epoch: 648 [23040/54000 (43%)] Loss: -431869.875000\n",
      "Train Epoch: 648 [34304/54000 (64%)] Loss: -529828.750000\n",
      "Train Epoch: 648 [45568/54000 (84%)] Loss: -482155.312500\n",
      "    epoch          : 648\n",
      "    loss           : -452209.6646875\n",
      "    val_loss       : -432538.5391601563\n",
      "Train Epoch: 649 [512/54000 (1%)] Loss: -397936.500000\n",
      "Train Epoch: 649 [11776/54000 (22%)] Loss: -399569.562500\n",
      "Train Epoch: 649 [23040/54000 (43%)] Loss: -529952.250000\n",
      "Train Epoch: 649 [34304/54000 (64%)] Loss: -388144.937500\n",
      "Train Epoch: 649 [45568/54000 (84%)] Loss: -476893.187500\n",
      "    epoch          : 649\n",
      "    loss           : -452536.075625\n",
      "    val_loss       : -436763.3427734375\n",
      "Train Epoch: 650 [512/54000 (1%)] Loss: -430043.031250\n",
      "Train Epoch: 650 [11776/54000 (22%)] Loss: -423746.312500\n",
      "Train Epoch: 650 [23040/54000 (43%)] Loss: -529622.875000\n",
      "Train Epoch: 650 [34304/54000 (64%)] Loss: -481636.406250\n",
      "Train Epoch: 650 [45568/54000 (84%)] Loss: -391410.125000\n",
      "    epoch          : 650\n",
      "    loss           : -452713.774375\n",
      "    val_loss       : -435665.5314453125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch650.pth ...\n",
      "Train Epoch: 651 [512/54000 (1%)] Loss: -417108.531250\n",
      "Train Epoch: 651 [11776/54000 (22%)] Loss: -474650.062500\n",
      "Train Epoch: 651 [23040/54000 (43%)] Loss: -400701.312500\n",
      "Train Epoch: 651 [34304/54000 (64%)] Loss: -383424.968750\n",
      "Train Epoch: 651 [45568/54000 (84%)] Loss: -482946.062500\n",
      "    epoch          : 651\n",
      "    loss           : -452704.4940625\n",
      "    val_loss       : -436911.64404296875\n",
      "Train Epoch: 652 [512/54000 (1%)] Loss: -494950.875000\n",
      "Train Epoch: 652 [11776/54000 (22%)] Loss: -435872.281250\n",
      "Train Epoch: 652 [23040/54000 (43%)] Loss: -495893.750000\n",
      "Train Epoch: 652 [34304/54000 (64%)] Loss: -535834.000000\n",
      "Train Epoch: 652 [45568/54000 (84%)] Loss: -391389.437500\n",
      "    epoch          : 652\n",
      "    loss           : -452755.941875\n",
      "    val_loss       : -440038.0252929687\n",
      "Train Epoch: 653 [512/54000 (1%)] Loss: -474933.437500\n",
      "Train Epoch: 653 [11776/54000 (22%)] Loss: -424231.750000\n",
      "Train Epoch: 653 [23040/54000 (43%)] Loss: -423480.843750\n",
      "Train Epoch: 653 [34304/54000 (64%)] Loss: -386977.375000\n",
      "Train Epoch: 653 [45568/54000 (84%)] Loss: -389506.687500\n",
      "    epoch          : 653\n",
      "    loss           : -452841.9384375\n",
      "    val_loss       : -435762.76640625\n",
      "Train Epoch: 654 [512/54000 (1%)] Loss: -474651.906250\n",
      "Train Epoch: 654 [11776/54000 (22%)] Loss: -502235.375000\n",
      "Train Epoch: 654 [23040/54000 (43%)] Loss: -511157.625000\n",
      "Train Epoch: 654 [34304/54000 (64%)] Loss: -425387.937500\n",
      "Train Epoch: 654 [45568/54000 (84%)] Loss: -399428.125000\n",
      "    epoch          : 654\n",
      "    loss           : -453525.104375\n",
      "    val_loss       : -441370.96533203125\n",
      "Train Epoch: 655 [512/54000 (1%)] Loss: -499356.593750\n",
      "Train Epoch: 655 [11776/54000 (22%)] Loss: -390078.250000\n",
      "Train Epoch: 655 [23040/54000 (43%)] Loss: -512054.937500\n",
      "Train Epoch: 655 [34304/54000 (64%)] Loss: -429077.125000\n",
      "Train Epoch: 655 [45568/54000 (84%)] Loss: -477431.593750\n",
      "    epoch          : 655\n",
      "    loss           : -453744.148125\n",
      "    val_loss       : -439340.1888671875\n",
      "Train Epoch: 656 [512/54000 (1%)] Loss: -390415.187500\n",
      "Train Epoch: 656 [11776/54000 (22%)] Loss: -510470.218750\n",
      "Train Epoch: 656 [23040/54000 (43%)] Loss: -388817.437500\n",
      "Train Epoch: 656 [34304/54000 (64%)] Loss: -429265.062500\n",
      "Train Epoch: 656 [45568/54000 (84%)] Loss: -509898.562500\n",
      "    epoch          : 656\n",
      "    loss           : -454038.71\n",
      "    val_loss       : -436986.8623046875\n",
      "Train Epoch: 657 [512/54000 (1%)] Loss: -426231.906250\n",
      "Train Epoch: 657 [11776/54000 (22%)] Loss: -389960.968750\n",
      "Train Epoch: 657 [23040/54000 (43%)] Loss: -397616.125000\n",
      "Train Epoch: 657 [34304/54000 (64%)] Loss: -483998.875000\n",
      "Train Epoch: 657 [45568/54000 (84%)] Loss: -535674.500000\n",
      "    epoch          : 657\n",
      "    loss           : -454127.1015625\n",
      "    val_loss       : -436577.9424804688\n",
      "Train Epoch: 658 [512/54000 (1%)] Loss: -382969.375000\n",
      "Train Epoch: 658 [11776/54000 (22%)] Loss: -404816.125000\n",
      "Train Epoch: 658 [23040/54000 (43%)] Loss: -409083.125000\n",
      "Train Epoch: 658 [34304/54000 (64%)] Loss: -392595.375000\n",
      "Train Epoch: 658 [45568/54000 (84%)] Loss: -534545.250000\n",
      "    epoch          : 658\n",
      "    loss           : -454454.9653125\n",
      "    val_loss       : -438395.6833984375\n",
      "Train Epoch: 659 [512/54000 (1%)] Loss: -420612.562500\n",
      "Train Epoch: 659 [11776/54000 (22%)] Loss: -499688.250000\n",
      "Train Epoch: 659 [23040/54000 (43%)] Loss: -538397.375000\n",
      "Train Epoch: 659 [34304/54000 (64%)] Loss: -485636.468750\n",
      "Train Epoch: 659 [45568/54000 (84%)] Loss: -485968.187500\n",
      "    epoch          : 659\n",
      "    loss           : -454441.57375\n",
      "    val_loss       : -435246.78251953126\n",
      "Train Epoch: 660 [512/54000 (1%)] Loss: -400334.593750\n",
      "Train Epoch: 660 [11776/54000 (22%)] Loss: -486004.562500\n",
      "Train Epoch: 660 [23040/54000 (43%)] Loss: -497232.437500\n",
      "Train Epoch: 660 [34304/54000 (64%)] Loss: -502733.718750\n",
      "Train Epoch: 660 [45568/54000 (84%)] Loss: -387965.937500\n",
      "    epoch          : 660\n",
      "    loss           : -454861.6146875\n",
      "    val_loss       : -438677.2052734375\n",
      "Train Epoch: 661 [512/54000 (1%)] Loss: -395487.437500\n",
      "Train Epoch: 661 [11776/54000 (22%)] Loss: -401897.000000\n",
      "Train Epoch: 661 [23040/54000 (43%)] Loss: -533830.312500\n",
      "Train Epoch: 661 [34304/54000 (64%)] Loss: -484924.718750\n",
      "Train Epoch: 661 [45568/54000 (84%)] Loss: -432056.125000\n",
      "    epoch          : 661\n",
      "    loss           : -455033.585\n",
      "    val_loss       : -440575.8095703125\n",
      "Train Epoch: 662 [512/54000 (1%)] Loss: -392488.406250\n",
      "Train Epoch: 662 [11776/54000 (22%)] Loss: -487896.593750\n",
      "Train Epoch: 662 [23040/54000 (43%)] Loss: -391253.218750\n",
      "Train Epoch: 662 [34304/54000 (64%)] Loss: -476769.156250\n",
      "Train Epoch: 662 [45568/54000 (84%)] Loss: -500974.218750\n",
      "    epoch          : 662\n",
      "    loss           : -455303.4178125\n",
      "    val_loss       : -439978.8134765625\n",
      "Train Epoch: 663 [512/54000 (1%)] Loss: -391527.906250\n",
      "Train Epoch: 663 [11776/54000 (22%)] Loss: -425048.750000\n",
      "Train Epoch: 663 [23040/54000 (43%)] Loss: -487294.312500\n",
      "Train Epoch: 663 [34304/54000 (64%)] Loss: -502544.000000\n",
      "Train Epoch: 663 [45568/54000 (84%)] Loss: -500298.187500\n",
      "    epoch          : 663\n",
      "    loss           : -455474.1859375\n",
      "    val_loss       : -441715.92509765626\n",
      "Train Epoch: 664 [512/54000 (1%)] Loss: -485484.437500\n",
      "Train Epoch: 664 [11776/54000 (22%)] Loss: -435956.812500\n",
      "Train Epoch: 664 [23040/54000 (43%)] Loss: -394579.281250\n",
      "Train Epoch: 664 [34304/54000 (64%)] Loss: -402154.437500\n",
      "Train Epoch: 664 [45568/54000 (84%)] Loss: -476753.500000\n",
      "    epoch          : 664\n",
      "    loss           : -455689.6734375\n",
      "    val_loss       : -441281.2793945313\n",
      "Train Epoch: 665 [512/54000 (1%)] Loss: -477934.250000\n",
      "Train Epoch: 665 [11776/54000 (22%)] Loss: -537087.750000\n",
      "Train Epoch: 665 [23040/54000 (43%)] Loss: -392943.125000\n",
      "Train Epoch: 665 [34304/54000 (64%)] Loss: -490144.187500\n",
      "Train Epoch: 665 [45568/54000 (84%)] Loss: -486866.187500\n",
      "    epoch          : 665\n",
      "    loss           : -455918.3296875\n",
      "    val_loss       : -437485.5302734375\n",
      "Train Epoch: 666 [512/54000 (1%)] Loss: -395976.375000\n",
      "Train Epoch: 666 [11776/54000 (22%)] Loss: -536546.187500\n",
      "Train Epoch: 666 [23040/54000 (43%)] Loss: -477310.000000\n",
      "Train Epoch: 666 [34304/54000 (64%)] Loss: -503091.031250\n",
      "Train Epoch: 666 [45568/54000 (84%)] Loss: -487938.843750\n",
      "    epoch          : 666\n",
      "    loss           : -456084.7696875\n",
      "    val_loss       : -442863.1073242187\n",
      "Train Epoch: 667 [512/54000 (1%)] Loss: -490214.375000\n",
      "Train Epoch: 667 [11776/54000 (22%)] Loss: -478253.250000\n",
      "Train Epoch: 667 [23040/54000 (43%)] Loss: -477771.250000\n",
      "Train Epoch: 667 [34304/54000 (64%)] Loss: -490302.312500\n",
      "Train Epoch: 667 [45568/54000 (84%)] Loss: -488042.281250\n",
      "    epoch          : 667\n",
      "    loss           : -456348.5821875\n",
      "    val_loss       : -440560.0693359375\n",
      "Train Epoch: 668 [512/54000 (1%)] Loss: -486377.125000\n",
      "Train Epoch: 668 [11776/54000 (22%)] Loss: -405728.093750\n",
      "Train Epoch: 668 [23040/54000 (43%)] Loss: -429894.187500\n",
      "Train Epoch: 668 [34304/54000 (64%)] Loss: -395560.687500\n",
      "Train Epoch: 668 [45568/54000 (84%)] Loss: -393529.281250\n",
      "    epoch          : 668\n",
      "    loss           : -456465.419375\n",
      "    val_loss       : -439666.24287109374\n",
      "Train Epoch: 669 [512/54000 (1%)] Loss: -488243.687500\n",
      "Train Epoch: 669 [11776/54000 (22%)] Loss: -396746.218750\n",
      "Train Epoch: 669 [23040/54000 (43%)] Loss: -393840.812500\n",
      "Train Epoch: 669 [34304/54000 (64%)] Loss: -409078.156250\n",
      "Train Epoch: 669 [45568/54000 (84%)] Loss: -434916.687500\n",
      "    epoch          : 669\n",
      "    loss           : -456642.681875\n",
      "    val_loss       : -432069.7872070313\n",
      "Train Epoch: 670 [512/54000 (1%)] Loss: -533856.500000\n",
      "Train Epoch: 670 [11776/54000 (22%)] Loss: -491083.375000\n",
      "Train Epoch: 670 [23040/54000 (43%)] Loss: -476636.843750\n",
      "Train Epoch: 670 [34304/54000 (64%)] Loss: -390484.000000\n",
      "Train Epoch: 670 [45568/54000 (84%)] Loss: -489157.781250\n",
      "    epoch          : 670\n",
      "    loss           : -456804.779375\n",
      "    val_loss       : -442285.549609375\n",
      "Train Epoch: 671 [512/54000 (1%)] Loss: -487403.718750\n",
      "Train Epoch: 671 [11776/54000 (22%)] Loss: -424547.156250\n",
      "Train Epoch: 671 [23040/54000 (43%)] Loss: -514872.125000\n",
      "Train Epoch: 671 [34304/54000 (64%)] Loss: -396423.312500\n",
      "Train Epoch: 671 [45568/54000 (84%)] Loss: -396114.875000\n",
      "    epoch          : 671\n",
      "    loss           : -457024.373125\n",
      "    val_loss       : -440516.66181640624\n",
      "Train Epoch: 672 [512/54000 (1%)] Loss: -518416.125000\n",
      "Train Epoch: 672 [11776/54000 (22%)] Loss: -514843.125000\n",
      "Train Epoch: 672 [23040/54000 (43%)] Loss: -504987.187500\n",
      "Train Epoch: 672 [34304/54000 (64%)] Loss: -499730.312500\n",
      "Train Epoch: 672 [45568/54000 (84%)] Loss: -538129.875000\n",
      "    epoch          : 672\n",
      "    loss           : -457084.195\n",
      "    val_loss       : -440032.30224609375\n",
      "Train Epoch: 673 [512/54000 (1%)] Loss: -492629.656250\n",
      "Train Epoch: 673 [11776/54000 (22%)] Loss: -493892.500000\n",
      "Train Epoch: 673 [23040/54000 (43%)] Loss: -535014.625000\n",
      "Train Epoch: 673 [34304/54000 (64%)] Loss: -423937.843750\n",
      "Train Epoch: 673 [45568/54000 (84%)] Loss: -396233.718750\n",
      "    epoch          : 673\n",
      "    loss           : -457429.84375\n",
      "    val_loss       : -444600.2989257813\n",
      "Train Epoch: 674 [512/54000 (1%)] Loss: -535172.375000\n",
      "Train Epoch: 674 [11776/54000 (22%)] Loss: -425173.437500\n",
      "Train Epoch: 674 [23040/54000 (43%)] Loss: -502495.125000\n",
      "Train Epoch: 674 [34304/54000 (64%)] Loss: -494647.375000\n",
      "Train Epoch: 674 [45568/54000 (84%)] Loss: -504279.000000\n",
      "    epoch          : 674\n",
      "    loss           : -457886.1384375\n",
      "    val_loss       : -443881.6861328125\n",
      "Train Epoch: 675 [512/54000 (1%)] Loss: -514261.562500\n",
      "Train Epoch: 675 [11776/54000 (22%)] Loss: -536954.125000\n",
      "Train Epoch: 675 [23040/54000 (43%)] Loss: -421392.781250\n",
      "Train Epoch: 675 [34304/54000 (64%)] Loss: -402814.125000\n",
      "Train Epoch: 675 [45568/54000 (84%)] Loss: -477901.500000\n",
      "    epoch          : 675\n",
      "    loss           : -458117.303125\n",
      "    val_loss       : -443929.721484375\n",
      "Train Epoch: 676 [512/54000 (1%)] Loss: -491895.218750\n",
      "Train Epoch: 676 [11776/54000 (22%)] Loss: -402579.062500\n",
      "Train Epoch: 676 [23040/54000 (43%)] Loss: -423405.281250\n",
      "Train Epoch: 676 [34304/54000 (64%)] Loss: -436323.500000\n",
      "Train Epoch: 676 [45568/54000 (84%)] Loss: -390221.250000\n",
      "    epoch          : 676\n",
      "    loss           : -458131.4940625\n",
      "    val_loss       : -441465.14306640625\n",
      "Train Epoch: 677 [512/54000 (1%)] Loss: -438397.250000\n",
      "Train Epoch: 677 [11776/54000 (22%)] Loss: -515089.343750\n",
      "Train Epoch: 677 [23040/54000 (43%)] Loss: -536184.750000\n",
      "Train Epoch: 677 [34304/54000 (64%)] Loss: -503226.375000\n",
      "Train Epoch: 677 [45568/54000 (84%)] Loss: -490967.625000\n",
      "    epoch          : 677\n",
      "    loss           : -458237.8325\n",
      "    val_loss       : -443969.72041015624\n",
      "Train Epoch: 678 [512/54000 (1%)] Loss: -537509.750000\n",
      "Train Epoch: 678 [11776/54000 (22%)] Loss: -495654.750000\n",
      "Train Epoch: 678 [23040/54000 (43%)] Loss: -538346.937500\n",
      "Train Epoch: 678 [34304/54000 (64%)] Loss: -431976.218750\n",
      "Train Epoch: 678 [45568/54000 (84%)] Loss: -489735.375000\n",
      "    epoch          : 678\n",
      "    loss           : -458478.28125\n",
      "    val_loss       : -440010.4048828125\n",
      "Train Epoch: 679 [512/54000 (1%)] Loss: -516873.437500\n",
      "Train Epoch: 679 [11776/54000 (22%)] Loss: -428415.906250\n",
      "Train Epoch: 679 [23040/54000 (43%)] Loss: -405849.812500\n",
      "Train Epoch: 679 [34304/54000 (64%)] Loss: -395471.375000\n",
      "Train Epoch: 679 [45568/54000 (84%)] Loss: -436273.187500\n",
      "    epoch          : 679\n",
      "    loss           : -458928.216875\n",
      "    val_loss       : -444625.13544921874\n",
      "Train Epoch: 680 [512/54000 (1%)] Loss: -392559.343750\n",
      "Train Epoch: 680 [11776/54000 (22%)] Loss: -537695.500000\n",
      "Train Epoch: 680 [23040/54000 (43%)] Loss: -404328.500000\n",
      "Train Epoch: 680 [34304/54000 (64%)] Loss: -480475.250000\n",
      "Train Epoch: 680 [45568/54000 (84%)] Loss: -492210.156250\n",
      "    epoch          : 680\n",
      "    loss           : -458872.0153125\n",
      "    val_loss       : -444779.5770507812\n",
      "Train Epoch: 681 [512/54000 (1%)] Loss: -507624.375000\n",
      "Train Epoch: 681 [11776/54000 (22%)] Loss: -428186.968750\n",
      "Train Epoch: 681 [23040/54000 (43%)] Loss: -438035.031250\n",
      "Train Epoch: 681 [34304/54000 (64%)] Loss: -504241.062500\n",
      "Train Epoch: 681 [45568/54000 (84%)] Loss: -492106.687500\n",
      "    epoch          : 681\n",
      "    loss           : -459207.9815625\n",
      "    val_loss       : -440611.6583984375\n",
      "Train Epoch: 682 [512/54000 (1%)] Loss: -422472.125000\n",
      "Train Epoch: 682 [11776/54000 (22%)] Loss: -518095.875000\n",
      "Train Epoch: 682 [23040/54000 (43%)] Loss: -516723.031250\n",
      "Train Epoch: 682 [34304/54000 (64%)] Loss: -395095.531250\n",
      "Train Epoch: 682 [45568/54000 (84%)] Loss: -485332.437500\n",
      "    epoch          : 682\n",
      "    loss           : -459389.846875\n",
      "    val_loss       : -441323.68720703124\n",
      "Train Epoch: 683 [512/54000 (1%)] Loss: -403521.187500\n",
      "Train Epoch: 683 [11776/54000 (22%)] Loss: -504858.250000\n",
      "Train Epoch: 683 [23040/54000 (43%)] Loss: -517063.187500\n",
      "Train Epoch: 683 [34304/54000 (64%)] Loss: -518884.093750\n",
      "Train Epoch: 683 [45568/54000 (84%)] Loss: -492179.718750\n",
      "    epoch          : 683\n",
      "    loss           : -459613.295625\n",
      "    val_loss       : -444981.6073242187\n",
      "Train Epoch: 684 [512/54000 (1%)] Loss: -505514.375000\n",
      "Train Epoch: 684 [11776/54000 (22%)] Loss: -405646.656250\n",
      "Train Epoch: 684 [23040/54000 (43%)] Loss: -394945.375000\n",
      "Train Epoch: 684 [34304/54000 (64%)] Loss: -402729.031250\n",
      "Train Epoch: 684 [45568/54000 (84%)] Loss: -509398.312500\n",
      "    epoch          : 684\n",
      "    loss           : -459826.529375\n",
      "    val_loss       : -443560.37060546875\n",
      "Train Epoch: 685 [512/54000 (1%)] Loss: -540741.875000\n",
      "Train Epoch: 685 [11776/54000 (22%)] Loss: -541640.500000\n",
      "Train Epoch: 685 [23040/54000 (43%)] Loss: -433680.687500\n",
      "Train Epoch: 685 [34304/54000 (64%)] Loss: -436957.468750\n",
      "Train Epoch: 685 [45568/54000 (84%)] Loss: -492640.750000\n",
      "    epoch          : 685\n",
      "    loss           : -459938.6971875\n",
      "    val_loss       : -445900.3884765625\n",
      "Train Epoch: 686 [512/54000 (1%)] Loss: -519811.125000\n",
      "Train Epoch: 686 [11776/54000 (22%)] Loss: -426204.062500\n",
      "Train Epoch: 686 [23040/54000 (43%)] Loss: -503682.437500\n",
      "Train Epoch: 686 [34304/54000 (64%)] Loss: -535691.000000\n",
      "Train Epoch: 686 [45568/54000 (84%)] Loss: -544563.875000\n",
      "    epoch          : 686\n",
      "    loss           : -460225.5396875\n",
      "    val_loss       : -444473.915625\n",
      "Train Epoch: 687 [512/54000 (1%)] Loss: -540820.125000\n",
      "Train Epoch: 687 [11776/54000 (22%)] Loss: -492400.312500\n",
      "Train Epoch: 687 [23040/54000 (43%)] Loss: -432270.125000\n",
      "Train Epoch: 687 [34304/54000 (64%)] Loss: -493055.531250\n",
      "Train Epoch: 687 [45568/54000 (84%)] Loss: -478880.781250\n",
      "    epoch          : 687\n",
      "    loss           : -460506.701875\n",
      "    val_loss       : -442771.41845703125\n",
      "Train Epoch: 688 [512/54000 (1%)] Loss: -394363.218750\n",
      "Train Epoch: 688 [11776/54000 (22%)] Loss: -483075.500000\n",
      "Train Epoch: 688 [23040/54000 (43%)] Loss: -399753.218750\n",
      "Train Epoch: 688 [34304/54000 (64%)] Loss: -543265.937500\n",
      "Train Epoch: 688 [45568/54000 (84%)] Loss: -396800.500000\n",
      "    epoch          : 688\n",
      "    loss           : -460697.7584375\n",
      "    val_loss       : -446137.607421875\n",
      "Train Epoch: 689 [512/54000 (1%)] Loss: -508168.375000\n",
      "Train Epoch: 689 [11776/54000 (22%)] Loss: -504183.500000\n",
      "Train Epoch: 689 [23040/54000 (43%)] Loss: -541274.187500\n",
      "Train Epoch: 689 [34304/54000 (64%)] Loss: -403862.250000\n",
      "Train Epoch: 689 [45568/54000 (84%)] Loss: -507240.156250\n",
      "    epoch          : 689\n",
      "    loss           : -460721.6240625\n",
      "    val_loss       : -444391.4092773438\n",
      "Train Epoch: 690 [512/54000 (1%)] Loss: -484168.906250\n",
      "Train Epoch: 690 [11776/54000 (22%)] Loss: -538633.500000\n",
      "Train Epoch: 690 [23040/54000 (43%)] Loss: -547627.000000\n",
      "Train Epoch: 690 [34304/54000 (64%)] Loss: -497337.062500\n",
      "Train Epoch: 690 [45568/54000 (84%)] Loss: -506206.187500\n",
      "    epoch          : 690\n",
      "    loss           : -460966.165\n",
      "    val_loss       : -444912.91728515626\n",
      "Train Epoch: 691 [512/54000 (1%)] Loss: -541951.562500\n",
      "Train Epoch: 691 [11776/54000 (22%)] Loss: -519845.625000\n",
      "Train Epoch: 691 [23040/54000 (43%)] Loss: -539363.250000\n",
      "Train Epoch: 691 [34304/54000 (64%)] Loss: -495935.750000\n",
      "Train Epoch: 691 [45568/54000 (84%)] Loss: -483977.750000\n",
      "    epoch          : 691\n",
      "    loss           : -461120.9659375\n",
      "    val_loss       : -447773.56572265626\n",
      "Train Epoch: 692 [512/54000 (1%)] Loss: -543373.937500\n",
      "Train Epoch: 692 [11776/54000 (22%)] Loss: -481274.531250\n",
      "Train Epoch: 692 [23040/54000 (43%)] Loss: -483102.250000\n",
      "Train Epoch: 692 [34304/54000 (64%)] Loss: -522561.937500\n",
      "Train Epoch: 692 [45568/54000 (84%)] Loss: -507759.937500\n",
      "    epoch          : 692\n",
      "    loss           : -461080.8940625\n",
      "    val_loss       : -447169.86279296875\n",
      "Train Epoch: 693 [512/54000 (1%)] Loss: -543345.375000\n",
      "Train Epoch: 693 [11776/54000 (22%)] Loss: -430520.562500\n",
      "Train Epoch: 693 [23040/54000 (43%)] Loss: -497475.437500\n",
      "Train Epoch: 693 [34304/54000 (64%)] Loss: -438602.250000\n",
      "Train Epoch: 693 [45568/54000 (84%)] Loss: -498801.812500\n",
      "    epoch          : 693\n",
      "    loss           : -461614.759375\n",
      "    val_loss       : -443817.2009765625\n",
      "Train Epoch: 694 [512/54000 (1%)] Loss: -520034.437500\n",
      "Train Epoch: 694 [11776/54000 (22%)] Loss: -409798.906250\n",
      "Train Epoch: 694 [23040/54000 (43%)] Loss: -524782.312500\n",
      "Train Epoch: 694 [34304/54000 (64%)] Loss: -427383.750000\n",
      "Train Epoch: 694 [45568/54000 (84%)] Loss: -428569.812500\n",
      "    epoch          : 694\n",
      "    loss           : -461958.585625\n",
      "    val_loss       : -445431.98896484374\n",
      "Train Epoch: 695 [512/54000 (1%)] Loss: -538996.937500\n",
      "Train Epoch: 695 [11776/54000 (22%)] Loss: -398680.812500\n",
      "Train Epoch: 695 [23040/54000 (43%)] Loss: -482636.812500\n",
      "Train Epoch: 695 [34304/54000 (64%)] Loss: -497367.625000\n",
      "Train Epoch: 695 [45568/54000 (84%)] Loss: -398445.187500\n",
      "    epoch          : 695\n",
      "    loss           : -462016.28125\n",
      "    val_loss       : -445317.10703125\n",
      "Train Epoch: 696 [512/54000 (1%)] Loss: -439696.562500\n",
      "Train Epoch: 696 [11776/54000 (22%)] Loss: -497826.218750\n",
      "Train Epoch: 696 [23040/54000 (43%)] Loss: -396230.562500\n",
      "Train Epoch: 696 [34304/54000 (64%)] Loss: -396330.718750\n",
      "Train Epoch: 696 [45568/54000 (84%)] Loss: -400674.718750\n",
      "    epoch          : 696\n",
      "    loss           : -462189.430625\n",
      "    val_loss       : -442718.0515625\n",
      "Train Epoch: 697 [512/54000 (1%)] Loss: -402778.187500\n",
      "Train Epoch: 697 [11776/54000 (22%)] Loss: -522527.812500\n",
      "Train Epoch: 697 [23040/54000 (43%)] Loss: -400513.062500\n",
      "Train Epoch: 697 [34304/54000 (64%)] Loss: -496910.125000\n",
      "Train Epoch: 697 [45568/54000 (84%)] Loss: -497741.062500\n",
      "    epoch          : 697\n",
      "    loss           : -462234.675625\n",
      "    val_loss       : -444101.4197265625\n",
      "Train Epoch: 698 [512/54000 (1%)] Loss: -393710.093750\n",
      "Train Epoch: 698 [11776/54000 (22%)] Loss: -520214.625000\n",
      "Train Epoch: 698 [23040/54000 (43%)] Loss: -440076.375000\n",
      "Train Epoch: 698 [34304/54000 (64%)] Loss: -505924.437500\n",
      "Train Epoch: 698 [45568/54000 (84%)] Loss: -499286.625000\n",
      "    epoch          : 698\n",
      "    loss           : -462565.3253125\n",
      "    val_loss       : -448840.30986328126\n",
      "Train Epoch: 699 [512/54000 (1%)] Loss: -406162.093750\n",
      "Train Epoch: 699 [11776/54000 (22%)] Loss: -409877.750000\n",
      "Train Epoch: 699 [23040/54000 (43%)] Loss: -403302.812500\n",
      "Train Epoch: 699 [34304/54000 (64%)] Loss: -435780.812500\n",
      "Train Epoch: 699 [45568/54000 (84%)] Loss: -498441.437500\n",
      "    epoch          : 699\n",
      "    loss           : -462965.7565625\n",
      "    val_loss       : -445894.96337890625\n",
      "Train Epoch: 700 [512/54000 (1%)] Loss: -522621.125000\n",
      "Train Epoch: 700 [11776/54000 (22%)] Loss: -408338.812500\n",
      "Train Epoch: 700 [23040/54000 (43%)] Loss: -403514.156250\n",
      "Train Epoch: 700 [34304/54000 (64%)] Loss: -398336.375000\n",
      "Train Epoch: 700 [45568/54000 (84%)] Loss: -403567.812500\n",
      "    epoch          : 700\n",
      "    loss           : -463116.49375\n",
      "    val_loss       : -448960.0272460937\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch700.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 701 [512/54000 (1%)] Loss: -401078.000000\n",
      "Train Epoch: 701 [11776/54000 (22%)] Loss: -405042.125000\n",
      "Train Epoch: 701 [23040/54000 (43%)] Loss: -432770.562500\n",
      "Train Epoch: 701 [34304/54000 (64%)] Loss: -401405.000000\n",
      "Train Epoch: 701 [45568/54000 (84%)] Loss: -488300.843750\n",
      "    epoch          : 701\n",
      "    loss           : -463218.62\n",
      "    val_loss       : -437127.63798828126\n",
      "Train Epoch: 702 [512/54000 (1%)] Loss: -406772.812500\n",
      "Train Epoch: 702 [11776/54000 (22%)] Loss: -433094.312500\n",
      "Train Epoch: 702 [23040/54000 (43%)] Loss: -486890.875000\n",
      "Train Epoch: 702 [34304/54000 (64%)] Loss: -546253.875000\n",
      "Train Epoch: 702 [45568/54000 (84%)] Loss: -545940.875000\n",
      "    epoch          : 702\n",
      "    loss           : -463490.631875\n",
      "    val_loss       : -444107.24365234375\n",
      "Train Epoch: 703 [512/54000 (1%)] Loss: -437188.437500\n",
      "Train Epoch: 703 [11776/54000 (22%)] Loss: -497523.937500\n",
      "Train Epoch: 703 [23040/54000 (43%)] Loss: -498425.500000\n",
      "Train Epoch: 703 [34304/54000 (64%)] Loss: -525389.375000\n",
      "Train Epoch: 703 [45568/54000 (84%)] Loss: -505221.812500\n",
      "    epoch          : 703\n",
      "    loss           : -463644.241875\n",
      "    val_loss       : -445963.03671875\n",
      "Train Epoch: 704 [512/54000 (1%)] Loss: -522936.781250\n",
      "Train Epoch: 704 [11776/54000 (22%)] Loss: -401628.906250\n",
      "Train Epoch: 704 [23040/54000 (43%)] Loss: -397747.312500\n",
      "Train Epoch: 704 [34304/54000 (64%)] Loss: -443166.375000\n",
      "Train Epoch: 704 [45568/54000 (84%)] Loss: -510842.625000\n",
      "    epoch          : 704\n",
      "    loss           : -463807.7884375\n",
      "    val_loss       : -445992.364453125\n",
      "Train Epoch: 705 [512/54000 (1%)] Loss: -446209.937500\n",
      "Train Epoch: 705 [11776/54000 (22%)] Loss: -400492.750000\n",
      "Train Epoch: 705 [23040/54000 (43%)] Loss: -405814.406250\n",
      "Train Epoch: 705 [34304/54000 (64%)] Loss: -401158.687500\n",
      "Train Epoch: 705 [45568/54000 (84%)] Loss: -403766.500000\n",
      "    epoch          : 705\n",
      "    loss           : -463853.1315625\n",
      "    val_loss       : -448519.816796875\n",
      "Train Epoch: 706 [512/54000 (1%)] Loss: -396493.781250\n",
      "Train Epoch: 706 [11776/54000 (22%)] Loss: -402261.406250\n",
      "Train Epoch: 706 [23040/54000 (43%)] Loss: -437542.375000\n",
      "Train Epoch: 706 [34304/54000 (64%)] Loss: -408798.468750\n",
      "Train Epoch: 706 [45568/54000 (84%)] Loss: -509260.156250\n",
      "    epoch          : 706\n",
      "    loss           : -464197.306875\n",
      "    val_loss       : -447548.4392578125\n",
      "Train Epoch: 707 [512/54000 (1%)] Loss: -545334.062500\n",
      "Train Epoch: 707 [11776/54000 (22%)] Loss: -552027.312500\n",
      "Train Epoch: 707 [23040/54000 (43%)] Loss: -435949.187500\n",
      "Train Epoch: 707 [34304/54000 (64%)] Loss: -484524.625000\n",
      "Train Epoch: 707 [45568/54000 (84%)] Loss: -491052.125000\n",
      "    epoch          : 707\n",
      "    loss           : -464475.6496875\n",
      "    val_loss       : -449289.5494140625\n",
      "Train Epoch: 708 [512/54000 (1%)] Loss: -547614.187500\n",
      "Train Epoch: 708 [11776/54000 (22%)] Loss: -400728.062500\n",
      "Train Epoch: 708 [23040/54000 (43%)] Loss: -524281.968750\n",
      "Train Epoch: 708 [34304/54000 (64%)] Loss: -402850.375000\n",
      "Train Epoch: 708 [45568/54000 (84%)] Loss: -484955.125000\n",
      "    epoch          : 708\n",
      "    loss           : -464725.6021875\n",
      "    val_loss       : -446034.73115234374\n",
      "Train Epoch: 709 [512/54000 (1%)] Loss: -398879.250000\n",
      "Train Epoch: 709 [11776/54000 (22%)] Loss: -509521.156250\n",
      "Train Epoch: 709 [23040/54000 (43%)] Loss: -401713.343750\n",
      "Train Epoch: 709 [34304/54000 (64%)] Loss: -513864.187500\n",
      "Train Epoch: 709 [45568/54000 (84%)] Loss: -403736.812500\n",
      "    epoch          : 709\n",
      "    loss           : -464867.263125\n",
      "    val_loss       : -449984.139453125\n",
      "Train Epoch: 710 [512/54000 (1%)] Loss: -404643.375000\n",
      "Train Epoch: 710 [11776/54000 (22%)] Loss: -433697.656250\n",
      "Train Epoch: 710 [23040/54000 (43%)] Loss: -526624.312500\n",
      "Train Epoch: 710 [34304/54000 (64%)] Loss: -395281.218750\n",
      "Train Epoch: 710 [45568/54000 (84%)] Loss: -441523.875000\n",
      "    epoch          : 710\n",
      "    loss           : -465093.6984375\n",
      "    val_loss       : -450306.81005859375\n",
      "Train Epoch: 711 [512/54000 (1%)] Loss: -432648.281250\n",
      "Train Epoch: 711 [11776/54000 (22%)] Loss: -523730.968750\n",
      "Train Epoch: 711 [23040/54000 (43%)] Loss: -398374.093750\n",
      "Train Epoch: 711 [34304/54000 (64%)] Loss: -395133.437500\n",
      "Train Epoch: 711 [45568/54000 (84%)] Loss: -486259.375000\n",
      "    epoch          : 711\n",
      "    loss           : -465333.6453125\n",
      "    val_loss       : -448597.232421875\n",
      "Train Epoch: 712 [512/54000 (1%)] Loss: -437235.000000\n",
      "Train Epoch: 712 [11776/54000 (22%)] Loss: -402751.687500\n",
      "Train Epoch: 712 [23040/54000 (43%)] Loss: -500618.625000\n",
      "Train Epoch: 712 [34304/54000 (64%)] Loss: -546955.500000\n",
      "Train Epoch: 712 [45568/54000 (84%)] Loss: -550478.437500\n",
      "    epoch          : 712\n",
      "    loss           : -465442.015625\n",
      "    val_loss       : -446582.29375\n",
      "Train Epoch: 713 [512/54000 (1%)] Loss: -400688.000000\n",
      "Train Epoch: 713 [11776/54000 (22%)] Loss: -522962.687500\n",
      "Train Epoch: 713 [23040/54000 (43%)] Loss: -410644.218750\n",
      "Train Epoch: 713 [34304/54000 (64%)] Loss: -408765.375000\n",
      "Train Epoch: 713 [45568/54000 (84%)] Loss: -413949.875000\n",
      "    epoch          : 713\n",
      "    loss           : -465535.31875\n",
      "    val_loss       : -449551.87353515625\n",
      "Train Epoch: 714 [512/54000 (1%)] Loss: -509315.937500\n",
      "Train Epoch: 714 [11776/54000 (22%)] Loss: -547763.750000\n",
      "Train Epoch: 714 [23040/54000 (43%)] Loss: -547394.812500\n",
      "Train Epoch: 714 [34304/54000 (64%)] Loss: -433021.656250\n",
      "Train Epoch: 714 [45568/54000 (84%)] Loss: -435678.218750\n",
      "    epoch          : 714\n",
      "    loss           : -465790.1071875\n",
      "    val_loss       : -450043.431640625\n",
      "Train Epoch: 715 [512/54000 (1%)] Loss: -524548.000000\n",
      "Train Epoch: 715 [11776/54000 (22%)] Loss: -399026.625000\n",
      "Train Epoch: 715 [23040/54000 (43%)] Loss: -511642.750000\n",
      "Train Epoch: 715 [34304/54000 (64%)] Loss: -511026.156250\n",
      "Train Epoch: 715 [45568/54000 (84%)] Loss: -507899.500000\n",
      "    epoch          : 715\n",
      "    loss           : -465916.9475\n",
      "    val_loss       : -446333.025390625\n",
      "Train Epoch: 716 [512/54000 (1%)] Loss: -484832.031250\n",
      "Train Epoch: 716 [11776/54000 (22%)] Loss: -443263.687500\n",
      "Train Epoch: 716 [23040/54000 (43%)] Loss: -399763.843750\n",
      "Train Epoch: 716 [34304/54000 (64%)] Loss: -514748.500000\n",
      "Train Epoch: 716 [45568/54000 (84%)] Loss: -432915.437500\n",
      "    epoch          : 716\n",
      "    loss           : -466126.03375\n",
      "    val_loss       : -448195.2451171875\n",
      "Train Epoch: 717 [512/54000 (1%)] Loss: -489249.343750\n",
      "Train Epoch: 717 [11776/54000 (22%)] Loss: -404114.593750\n",
      "Train Epoch: 717 [23040/54000 (43%)] Loss: -399362.875000\n",
      "Train Epoch: 717 [34304/54000 (64%)] Loss: -504042.375000\n",
      "Train Epoch: 717 [45568/54000 (84%)] Loss: -486912.093750\n",
      "    epoch          : 717\n",
      "    loss           : -466457.3246875\n",
      "    val_loss       : -446016.95048828126\n",
      "Train Epoch: 718 [512/54000 (1%)] Loss: -511008.406250\n",
      "Train Epoch: 718 [11776/54000 (22%)] Loss: -400757.781250\n",
      "Train Epoch: 718 [23040/54000 (43%)] Loss: -503957.906250\n",
      "Train Epoch: 718 [34304/54000 (64%)] Loss: -510426.437500\n",
      "Train Epoch: 718 [45568/54000 (84%)] Loss: -501670.562500\n",
      "    epoch          : 718\n",
      "    loss           : -466584.3746875\n",
      "    val_loss       : -451184.88232421875\n",
      "Train Epoch: 719 [512/54000 (1%)] Loss: -402218.312500\n",
      "Train Epoch: 719 [11776/54000 (22%)] Loss: -550396.000000\n",
      "Train Epoch: 719 [23040/54000 (43%)] Loss: -526722.000000\n",
      "Train Epoch: 719 [34304/54000 (64%)] Loss: -439932.187500\n",
      "Train Epoch: 719 [45568/54000 (84%)] Loss: -489182.062500\n",
      "    epoch          : 719\n",
      "    loss           : -466822.5721875\n",
      "    val_loss       : -452047.7916015625\n",
      "Train Epoch: 720 [512/54000 (1%)] Loss: -411446.687500\n",
      "Train Epoch: 720 [11776/54000 (22%)] Loss: -437881.843750\n",
      "Train Epoch: 720 [23040/54000 (43%)] Loss: -435759.375000\n",
      "Train Epoch: 720 [34304/54000 (64%)] Loss: -501601.375000\n",
      "Train Epoch: 720 [45568/54000 (84%)] Loss: -515525.468750\n",
      "    epoch          : 720\n",
      "    loss           : -466992.6440625\n",
      "    val_loss       : -449749.75419921876\n",
      "Train Epoch: 721 [512/54000 (1%)] Loss: -440697.593750\n",
      "Train Epoch: 721 [11776/54000 (22%)] Loss: -548620.125000\n",
      "Train Epoch: 721 [23040/54000 (43%)] Loss: -501787.062500\n",
      "Train Epoch: 721 [34304/54000 (64%)] Loss: -489038.437500\n",
      "Train Epoch: 721 [45568/54000 (84%)] Loss: -426117.281250\n",
      "    epoch          : 721\n",
      "    loss           : -467324.805625\n",
      "    val_loss       : -450874.7541015625\n",
      "Train Epoch: 722 [512/54000 (1%)] Loss: -405065.000000\n",
      "Train Epoch: 722 [11776/54000 (22%)] Loss: -401383.750000\n",
      "Train Epoch: 722 [23040/54000 (43%)] Loss: -444609.812500\n",
      "Train Epoch: 722 [34304/54000 (64%)] Loss: -398139.375000\n",
      "Train Epoch: 722 [45568/54000 (84%)] Loss: -486218.187500\n",
      "    epoch          : 722\n",
      "    loss           : -467302.6090625\n",
      "    val_loss       : -448464.9366210938\n",
      "Train Epoch: 723 [512/54000 (1%)] Loss: -413305.968750\n",
      "Train Epoch: 723 [11776/54000 (22%)] Loss: -438338.656250\n",
      "Train Epoch: 723 [23040/54000 (43%)] Loss: -434805.187500\n",
      "Train Epoch: 723 [34304/54000 (64%)] Loss: -444805.687500\n",
      "Train Epoch: 723 [45568/54000 (84%)] Loss: -490997.531250\n",
      "    epoch          : 723\n",
      "    loss           : -467602.4278125\n",
      "    val_loss       : -451151.81767578126\n",
      "Train Epoch: 724 [512/54000 (1%)] Loss: -517056.562500\n",
      "Train Epoch: 724 [11776/54000 (22%)] Loss: -430476.625000\n",
      "Train Epoch: 724 [23040/54000 (43%)] Loss: -439086.937500\n",
      "Train Epoch: 724 [34304/54000 (64%)] Loss: -551321.625000\n",
      "Train Epoch: 724 [45568/54000 (84%)] Loss: -397540.875000\n",
      "    epoch          : 724\n",
      "    loss           : -467625.0125\n",
      "    val_loss       : -448993.97529296874\n",
      "Train Epoch: 725 [512/54000 (1%)] Loss: -444896.343750\n",
      "Train Epoch: 725 [11776/54000 (22%)] Loss: -403275.500000\n",
      "Train Epoch: 725 [23040/54000 (43%)] Loss: -492634.937500\n",
      "Train Epoch: 725 [34304/54000 (64%)] Loss: -488679.281250\n",
      "Train Epoch: 725 [45568/54000 (84%)] Loss: -504389.312500\n",
      "    epoch          : 725\n",
      "    loss           : -467866.91625\n",
      "    val_loss       : -449444.1602539063\n",
      "Train Epoch: 726 [512/54000 (1%)] Loss: -553066.875000\n",
      "Train Epoch: 726 [11776/54000 (22%)] Loss: -552541.000000\n",
      "Train Epoch: 726 [23040/54000 (43%)] Loss: -552345.437500\n",
      "Train Epoch: 726 [34304/54000 (64%)] Loss: -516202.875000\n",
      "Train Epoch: 726 [45568/54000 (84%)] Loss: -488537.812500\n",
      "    epoch          : 726\n",
      "    loss           : -468036.1203125\n",
      "    val_loss       : -453704.114453125\n",
      "Train Epoch: 727 [512/54000 (1%)] Loss: -549773.937500\n",
      "Train Epoch: 727 [11776/54000 (22%)] Loss: -551390.312500\n",
      "Train Epoch: 727 [23040/54000 (43%)] Loss: -546146.750000\n",
      "Train Epoch: 727 [34304/54000 (64%)] Loss: -504888.218750\n",
      "Train Epoch: 727 [45568/54000 (84%)] Loss: -396006.437500\n",
      "    epoch          : 727\n",
      "    loss           : -468345.780625\n",
      "    val_loss       : -451619.2978515625\n",
      "Train Epoch: 728 [512/54000 (1%)] Loss: -552255.812500\n",
      "Train Epoch: 728 [11776/54000 (22%)] Loss: -410861.906250\n",
      "Train Epoch: 728 [23040/54000 (43%)] Loss: -488828.500000\n",
      "Train Epoch: 728 [34304/54000 (64%)] Loss: -550663.750000\n",
      "Train Epoch: 728 [45568/54000 (84%)] Loss: -492012.281250\n",
      "    epoch          : 728\n",
      "    loss           : -468372.55125\n",
      "    val_loss       : -452353.6416015625\n",
      "Train Epoch: 729 [512/54000 (1%)] Loss: -399497.500000\n",
      "Train Epoch: 729 [11776/54000 (22%)] Loss: -398762.843750\n",
      "Train Epoch: 729 [23040/54000 (43%)] Loss: -402194.687500\n",
      "Train Epoch: 729 [34304/54000 (64%)] Loss: -554080.000000\n",
      "Train Epoch: 729 [45568/54000 (84%)] Loss: -506655.656250\n",
      "    epoch          : 729\n",
      "    loss           : -468608.515\n",
      "    val_loss       : -453219.57568359375\n",
      "Train Epoch: 730 [512/54000 (1%)] Loss: -550904.937500\n",
      "Train Epoch: 730 [11776/54000 (22%)] Loss: -505289.250000\n",
      "Train Epoch: 730 [23040/54000 (43%)] Loss: -404060.437500\n",
      "Train Epoch: 730 [34304/54000 (64%)] Loss: -503782.843750\n",
      "Train Epoch: 730 [45568/54000 (84%)] Loss: -442931.812500\n",
      "    epoch          : 730\n",
      "    loss           : -468864.4846875\n",
      "    val_loss       : -449046.71748046874\n",
      "Train Epoch: 731 [512/54000 (1%)] Loss: -404595.531250\n",
      "Train Epoch: 731 [11776/54000 (22%)] Loss: -528066.375000\n",
      "Train Epoch: 731 [23040/54000 (43%)] Loss: -553960.000000\n",
      "Train Epoch: 731 [34304/54000 (64%)] Loss: -435882.000000\n",
      "Train Epoch: 731 [45568/54000 (84%)] Loss: -504758.812500\n",
      "    epoch          : 731\n",
      "    loss           : -469135.6571875\n",
      "    val_loss       : -453748.4353515625\n",
      "Train Epoch: 732 [512/54000 (1%)] Loss: -413647.093750\n",
      "Train Epoch: 732 [11776/54000 (22%)] Loss: -513982.375000\n",
      "Train Epoch: 732 [23040/54000 (43%)] Loss: -527045.437500\n",
      "Train Epoch: 732 [34304/54000 (64%)] Loss: -438347.312500\n",
      "Train Epoch: 732 [45568/54000 (84%)] Loss: -502536.875000\n",
      "    epoch          : 732\n",
      "    loss           : -469350.8190625\n",
      "    val_loss       : -452059.9923828125\n",
      "Train Epoch: 733 [512/54000 (1%)] Loss: -492741.875000\n",
      "Train Epoch: 733 [11776/54000 (22%)] Loss: -404945.187500\n",
      "Train Epoch: 733 [23040/54000 (43%)] Loss: -447624.375000\n",
      "Train Epoch: 733 [34304/54000 (64%)] Loss: -402986.937500\n",
      "Train Epoch: 733 [45568/54000 (84%)] Loss: -404046.812500\n",
      "    epoch          : 733\n",
      "    loss           : -469571.970625\n",
      "    val_loss       : -454725.450390625\n",
      "Train Epoch: 734 [512/54000 (1%)] Loss: -398767.000000\n",
      "Train Epoch: 734 [11776/54000 (22%)] Loss: -436960.906250\n",
      "Train Epoch: 734 [23040/54000 (43%)] Loss: -397182.500000\n",
      "Train Epoch: 734 [34304/54000 (64%)] Loss: -512024.125000\n",
      "Train Epoch: 734 [45568/54000 (84%)] Loss: -515850.343750\n",
      "    epoch          : 734\n",
      "    loss           : -469680.759375\n",
      "    val_loss       : -454349.13544921874\n",
      "Train Epoch: 735 [512/54000 (1%)] Loss: -505798.375000\n",
      "Train Epoch: 735 [11776/54000 (22%)] Loss: -413334.437500\n",
      "Train Epoch: 735 [23040/54000 (43%)] Loss: -504444.656250\n",
      "Train Epoch: 735 [34304/54000 (64%)] Loss: -492588.375000\n",
      "Train Epoch: 735 [45568/54000 (84%)] Loss: -516757.500000\n",
      "    epoch          : 735\n",
      "    loss           : -469767.4353125\n",
      "    val_loss       : -452811.409375\n",
      "Train Epoch: 736 [512/54000 (1%)] Loss: -495257.562500\n",
      "Train Epoch: 736 [11776/54000 (22%)] Loss: -397484.593750\n",
      "Train Epoch: 736 [23040/54000 (43%)] Loss: -491466.562500\n",
      "Train Epoch: 736 [34304/54000 (64%)] Loss: -517243.062500\n",
      "Train Epoch: 736 [45568/54000 (84%)] Loss: -505753.875000\n",
      "    epoch          : 736\n",
      "    loss           : -470194.9546875\n",
      "    val_loss       : -455073.3185546875\n",
      "Train Epoch: 737 [512/54000 (1%)] Loss: -491203.156250\n",
      "Train Epoch: 737 [11776/54000 (22%)] Loss: -493774.218750\n",
      "Train Epoch: 737 [23040/54000 (43%)] Loss: -530091.812500\n",
      "Train Epoch: 737 [34304/54000 (64%)] Loss: -491801.968750\n",
      "Train Epoch: 737 [45568/54000 (84%)] Loss: -401151.843750\n",
      "    epoch          : 737\n",
      "    loss           : -470024.875625\n",
      "    val_loss       : -452533.3282226563\n",
      "Train Epoch: 738 [512/54000 (1%)] Loss: -448825.656250\n",
      "Train Epoch: 738 [11776/54000 (22%)] Loss: -435269.625000\n",
      "Train Epoch: 738 [23040/54000 (43%)] Loss: -405271.500000\n",
      "Train Epoch: 738 [34304/54000 (64%)] Loss: -520324.875000\n",
      "Train Epoch: 738 [45568/54000 (84%)] Loss: -554728.562500\n",
      "    epoch          : 738\n",
      "    loss           : -470232.31\n",
      "    val_loss       : -452982.356640625\n",
      "Train Epoch: 739 [512/54000 (1%)] Loss: -405451.312500\n",
      "Train Epoch: 739 [11776/54000 (22%)] Loss: -490785.531250\n",
      "Train Epoch: 739 [23040/54000 (43%)] Loss: -512480.281250\n",
      "Train Epoch: 739 [34304/54000 (64%)] Loss: -406047.187500\n",
      "Train Epoch: 739 [45568/54000 (84%)] Loss: -403120.343750\n",
      "    epoch          : 739\n",
      "    loss           : -470643.9715625\n",
      "    val_loss       : -454345.6984375\n",
      "Train Epoch: 740 [512/54000 (1%)] Loss: -509815.656250\n",
      "Train Epoch: 740 [11776/54000 (22%)] Loss: -495487.843750\n",
      "Train Epoch: 740 [23040/54000 (43%)] Loss: -404657.875000\n",
      "Train Epoch: 740 [34304/54000 (64%)] Loss: -406949.375000\n",
      "Train Epoch: 740 [45568/54000 (84%)] Loss: -506043.500000\n",
      "    epoch          : 740\n",
      "    loss           : -470760.04375\n",
      "    val_loss       : -451597.13408203126\n",
      "Train Epoch: 741 [512/54000 (1%)] Loss: -493335.500000\n",
      "Train Epoch: 741 [11776/54000 (22%)] Loss: -531396.062500\n",
      "Train Epoch: 741 [23040/54000 (43%)] Loss: -436756.437500\n",
      "Train Epoch: 741 [34304/54000 (64%)] Loss: -447865.625000\n",
      "Train Epoch: 741 [45568/54000 (84%)] Loss: -397901.312500\n",
      "    epoch          : 741\n",
      "    loss           : -470847.2340625\n",
      "    val_loss       : -454037.46826171875\n",
      "Train Epoch: 742 [512/54000 (1%)] Loss: -400009.437500\n",
      "Train Epoch: 742 [11776/54000 (22%)] Loss: -531794.750000\n",
      "Train Epoch: 742 [23040/54000 (43%)] Loss: -409595.437500\n",
      "Train Epoch: 742 [34304/54000 (64%)] Loss: -416400.875000\n",
      "Train Epoch: 742 [45568/54000 (84%)] Loss: -491710.062500\n",
      "    epoch          : 742\n",
      "    loss           : -471141.2675\n",
      "    val_loss       : -449027.51708984375\n",
      "Train Epoch: 743 [512/54000 (1%)] Loss: -404530.875000\n",
      "Train Epoch: 743 [11776/54000 (22%)] Loss: -557755.937500\n",
      "Train Epoch: 743 [23040/54000 (43%)] Loss: -553389.375000\n",
      "Train Epoch: 743 [34304/54000 (64%)] Loss: -401196.437500\n",
      "Train Epoch: 743 [45568/54000 (84%)] Loss: -493229.812500\n",
      "    epoch          : 743\n",
      "    loss           : -471371.30125\n",
      "    val_loss       : -454605.6283203125\n",
      "Train Epoch: 744 [512/54000 (1%)] Loss: -497447.781250\n",
      "Train Epoch: 744 [11776/54000 (22%)] Loss: -532338.812500\n",
      "Train Epoch: 744 [23040/54000 (43%)] Loss: -438254.125000\n",
      "Train Epoch: 744 [34304/54000 (64%)] Loss: -515537.656250\n",
      "Train Epoch: 744 [45568/54000 (84%)] Loss: -495986.000000\n",
      "    epoch          : 744\n",
      "    loss           : -471281.5165625\n",
      "    val_loss       : -451540.08515625\n",
      "Train Epoch: 745 [512/54000 (1%)] Loss: -518482.968750\n",
      "Train Epoch: 745 [11776/54000 (22%)] Loss: -533954.937500\n",
      "Train Epoch: 745 [23040/54000 (43%)] Loss: -414002.437500\n",
      "Train Epoch: 745 [34304/54000 (64%)] Loss: -517880.875000\n",
      "Train Epoch: 745 [45568/54000 (84%)] Loss: -438641.750000\n",
      "    epoch          : 745\n",
      "    loss           : -471808.8334375\n",
      "    val_loss       : -451712.60419921874\n",
      "Train Epoch: 746 [512/54000 (1%)] Loss: -495504.656250\n",
      "Train Epoch: 746 [11776/54000 (22%)] Loss: -520686.625000\n",
      "Train Epoch: 746 [23040/54000 (43%)] Loss: -517575.406250\n",
      "Train Epoch: 746 [34304/54000 (64%)] Loss: -534622.312500\n",
      "Train Epoch: 746 [45568/54000 (84%)] Loss: -397705.093750\n",
      "    epoch          : 746\n",
      "    loss           : -471864.9565625\n",
      "    val_loss       : -456203.8797851562\n",
      "Train Epoch: 747 [512/54000 (1%)] Loss: -532999.125000\n",
      "Train Epoch: 747 [11776/54000 (22%)] Loss: -448544.062500\n",
      "Train Epoch: 747 [23040/54000 (43%)] Loss: -497630.062500\n",
      "Train Epoch: 747 [34304/54000 (64%)] Loss: -509001.562500\n",
      "Train Epoch: 747 [45568/54000 (84%)] Loss: -495654.125000\n",
      "    epoch          : 747\n",
      "    loss           : -472136.1878125\n",
      "    val_loss       : -455065.01611328125\n",
      "Train Epoch: 748 [512/54000 (1%)] Loss: -410452.781250\n",
      "Train Epoch: 748 [11776/54000 (22%)] Loss: -400736.437500\n",
      "Train Epoch: 748 [23040/54000 (43%)] Loss: -443628.687500\n",
      "Train Epoch: 748 [34304/54000 (64%)] Loss: -437712.656250\n",
      "Train Epoch: 748 [45568/54000 (84%)] Loss: -493971.937500\n",
      "    epoch          : 748\n",
      "    loss           : -472192.5496875\n",
      "    val_loss       : -457709.3287109375\n",
      "Train Epoch: 749 [512/54000 (1%)] Loss: -533189.937500\n",
      "Train Epoch: 749 [11776/54000 (22%)] Loss: -557732.187500\n",
      "Train Epoch: 749 [23040/54000 (43%)] Loss: -518567.437500\n",
      "Train Epoch: 749 [34304/54000 (64%)] Loss: -408162.843750\n",
      "Train Epoch: 749 [45568/54000 (84%)] Loss: -438402.031250\n",
      "    epoch          : 749\n",
      "    loss           : -472257.280625\n",
      "    val_loss       : -456182.62060546875\n",
      "Train Epoch: 750 [512/54000 (1%)] Loss: -533208.625000\n",
      "Train Epoch: 750 [11776/54000 (22%)] Loss: -412409.562500\n",
      "Train Epoch: 750 [23040/54000 (43%)] Loss: -406764.437500\n",
      "Train Epoch: 750 [34304/54000 (64%)] Loss: -406195.312500\n",
      "Train Epoch: 750 [45568/54000 (84%)] Loss: -509422.437500\n",
      "    epoch          : 750\n",
      "    loss           : -472452.7784375\n",
      "    val_loss       : -453168.03564453125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch750.pth ...\n",
      "Train Epoch: 751 [512/54000 (1%)] Loss: -532497.562500\n",
      "Train Epoch: 751 [11776/54000 (22%)] Loss: -413526.750000\n",
      "Train Epoch: 751 [23040/54000 (43%)] Loss: -442499.156250\n",
      "Train Epoch: 751 [34304/54000 (64%)] Loss: -495545.062500\n",
      "Train Epoch: 751 [45568/54000 (84%)] Loss: -521655.875000\n",
      "    epoch          : 751\n",
      "    loss           : -472770.389375\n",
      "    val_loss       : -457910.3633789063\n",
      "Train Epoch: 752 [512/54000 (1%)] Loss: -532625.875000\n",
      "Train Epoch: 752 [11776/54000 (22%)] Loss: -409891.000000\n",
      "Train Epoch: 752 [23040/54000 (43%)] Loss: -519897.281250\n",
      "Train Epoch: 752 [34304/54000 (64%)] Loss: -512264.812500\n",
      "Train Epoch: 752 [45568/54000 (84%)] Loss: -558800.250000\n",
      "    epoch          : 752\n",
      "    loss           : -472974.89625\n",
      "    val_loss       : -456890.358203125\n",
      "Train Epoch: 753 [512/54000 (1%)] Loss: -450139.187500\n",
      "Train Epoch: 753 [11776/54000 (22%)] Loss: -408955.625000\n",
      "Train Epoch: 753 [23040/54000 (43%)] Loss: -557864.500000\n",
      "Train Epoch: 753 [34304/54000 (64%)] Loss: -418852.000000\n",
      "Train Epoch: 753 [45568/54000 (84%)] Loss: -495879.562500\n",
      "    epoch          : 753\n",
      "    loss           : -473176.1446875\n",
      "    val_loss       : -454132.0146484375\n",
      "Train Epoch: 754 [512/54000 (1%)] Loss: -533688.812500\n",
      "Train Epoch: 754 [11776/54000 (22%)] Loss: -533507.937500\n",
      "Train Epoch: 754 [23040/54000 (43%)] Loss: -420645.906250\n",
      "Train Epoch: 754 [34304/54000 (64%)] Loss: -410279.156250\n",
      "Train Epoch: 754 [45568/54000 (84%)] Loss: -402409.531250\n",
      "    epoch          : 754\n",
      "    loss           : -473373.8696875\n",
      "    val_loss       : -454269.85234375\n",
      "Train Epoch: 755 [512/54000 (1%)] Loss: -557961.812500\n",
      "Train Epoch: 755 [11776/54000 (22%)] Loss: -511309.718750\n",
      "Train Epoch: 755 [23040/54000 (43%)] Loss: -439056.187500\n",
      "Train Epoch: 755 [34304/54000 (64%)] Loss: -556337.250000\n",
      "Train Epoch: 755 [45568/54000 (84%)] Loss: -511914.437500\n",
      "    epoch          : 755\n",
      "    loss           : -473489.7140625\n",
      "    val_loss       : -459044.91875\n",
      "Train Epoch: 756 [512/54000 (1%)] Loss: -439805.312500\n",
      "Train Epoch: 756 [11776/54000 (22%)] Loss: -439718.031250\n",
      "Train Epoch: 756 [23040/54000 (43%)] Loss: -439225.218750\n",
      "Train Epoch: 756 [34304/54000 (64%)] Loss: -406406.437500\n",
      "Train Epoch: 756 [45568/54000 (84%)] Loss: -402006.500000\n",
      "    epoch          : 756\n",
      "    loss           : -473774.835\n",
      "    val_loss       : -458379.8279296875\n",
      "Train Epoch: 757 [512/54000 (1%)] Loss: -556432.562500\n",
      "Train Epoch: 757 [11776/54000 (22%)] Loss: -536523.375000\n",
      "Train Epoch: 757 [23040/54000 (43%)] Loss: -512140.187500\n",
      "Train Epoch: 757 [34304/54000 (64%)] Loss: -437716.437500\n",
      "Train Epoch: 757 [45568/54000 (84%)] Loss: -407283.125000\n",
      "    epoch          : 757\n",
      "    loss           : -473886.45625\n",
      "    val_loss       : -456022.7462890625\n",
      "Train Epoch: 758 [512/54000 (1%)] Loss: -512827.812500\n",
      "Train Epoch: 758 [11776/54000 (22%)] Loss: -412129.187500\n",
      "Train Epoch: 758 [23040/54000 (43%)] Loss: -559786.687500\n",
      "Train Epoch: 758 [34304/54000 (64%)] Loss: -409239.187500\n",
      "Train Epoch: 758 [45568/54000 (84%)] Loss: -409484.343750\n",
      "    epoch          : 758\n",
      "    loss           : -473882.3353125\n",
      "    val_loss       : -453908.57158203126\n",
      "Train Epoch: 759 [512/54000 (1%)] Loss: -493401.781250\n",
      "Train Epoch: 759 [11776/54000 (22%)] Loss: -448116.750000\n",
      "Train Epoch: 759 [23040/54000 (43%)] Loss: -537679.625000\n",
      "Train Epoch: 759 [34304/54000 (64%)] Loss: -509636.187500\n",
      "Train Epoch: 759 [45568/54000 (84%)] Loss: -523842.125000\n",
      "    epoch          : 759\n",
      "    loss           : -474263.2925\n",
      "    val_loss       : -447189.726171875\n",
      "Train Epoch: 760 [512/54000 (1%)] Loss: -450583.750000\n",
      "Train Epoch: 760 [11776/54000 (22%)] Loss: -400464.156250\n",
      "Train Epoch: 760 [23040/54000 (43%)] Loss: -533649.062500\n",
      "Train Epoch: 760 [34304/54000 (64%)] Loss: -510870.656250\n",
      "Train Epoch: 760 [45568/54000 (84%)] Loss: -439156.062500\n",
      "    epoch          : 760\n",
      "    loss           : -474449.5109375\n",
      "    val_loss       : -454423.0431640625\n",
      "Train Epoch: 761 [512/54000 (1%)] Loss: -525491.250000\n",
      "Train Epoch: 761 [11776/54000 (22%)] Loss: -408377.562500\n",
      "Train Epoch: 761 [23040/54000 (43%)] Loss: -524156.875000\n",
      "Train Epoch: 761 [34304/54000 (64%)] Loss: -511108.625000\n",
      "Train Epoch: 761 [45568/54000 (84%)] Loss: -511975.250000\n",
      "    epoch          : 761\n",
      "    loss           : -474559.7628125\n",
      "    val_loss       : -456313.6456054688\n",
      "Train Epoch: 762 [512/54000 (1%)] Loss: -533140.312500\n",
      "Train Epoch: 762 [11776/54000 (22%)] Loss: -499867.281250\n",
      "Train Epoch: 762 [23040/54000 (43%)] Loss: -451031.687500\n",
      "Train Epoch: 762 [34304/54000 (64%)] Loss: -556875.062500\n",
      "Train Epoch: 762 [45568/54000 (84%)] Loss: -510825.718750\n",
      "    epoch          : 762\n",
      "    loss           : -474854.58625\n",
      "    val_loss       : -458908.7255859375\n",
      "Train Epoch: 763 [512/54000 (1%)] Loss: -518727.562500\n",
      "Train Epoch: 763 [11776/54000 (22%)] Loss: -556896.000000\n",
      "Train Epoch: 763 [23040/54000 (43%)] Loss: -512432.781250\n",
      "Train Epoch: 763 [34304/54000 (64%)] Loss: -499595.000000\n",
      "Train Epoch: 763 [45568/54000 (84%)] Loss: -512876.312500\n",
      "    epoch          : 763\n",
      "    loss           : -474852.9678125\n",
      "    val_loss       : -458238.7703125\n",
      "Train Epoch: 764 [512/54000 (1%)] Loss: -450205.812500\n",
      "Train Epoch: 764 [11776/54000 (22%)] Loss: -414670.843750\n",
      "Train Epoch: 764 [23040/54000 (43%)] Loss: -409648.375000\n",
      "Train Epoch: 764 [34304/54000 (64%)] Loss: -536635.000000\n",
      "Train Epoch: 764 [45568/54000 (84%)] Loss: -511111.312500\n",
      "    epoch          : 764\n",
      "    loss           : -475118.3578125\n",
      "    val_loss       : -460132.471484375\n",
      "Train Epoch: 765 [512/54000 (1%)] Loss: -442384.093750\n",
      "Train Epoch: 765 [11776/54000 (22%)] Loss: -453365.593750\n",
      "Train Epoch: 765 [23040/54000 (43%)] Loss: -511142.937500\n",
      "Train Epoch: 765 [34304/54000 (64%)] Loss: -522820.812500\n",
      "Train Epoch: 765 [45568/54000 (84%)] Loss: -511736.968750\n",
      "    epoch          : 765\n",
      "    loss           : -475468.975625\n",
      "    val_loss       : -457528.586328125\n",
      "Train Epoch: 766 [512/54000 (1%)] Loss: -511039.562500\n",
      "Train Epoch: 766 [11776/54000 (22%)] Loss: -405901.937500\n",
      "Train Epoch: 766 [23040/54000 (43%)] Loss: -403968.375000\n",
      "Train Epoch: 766 [34304/54000 (64%)] Loss: -444964.812500\n",
      "Train Epoch: 766 [45568/54000 (84%)] Loss: -509472.406250\n",
      "    epoch          : 766\n",
      "    loss           : -475612.0703125\n",
      "    val_loss       : -460516.02958984376\n",
      "Train Epoch: 767 [512/54000 (1%)] Loss: -412217.250000\n",
      "Train Epoch: 767 [11776/54000 (22%)] Loss: -418860.812500\n",
      "Train Epoch: 767 [23040/54000 (43%)] Loss: -420373.718750\n",
      "Train Epoch: 767 [34304/54000 (64%)] Loss: -497763.406250\n",
      "Train Epoch: 767 [45568/54000 (84%)] Loss: -498730.062500\n",
      "    epoch          : 767\n",
      "    loss           : -475692.6953125\n",
      "    val_loss       : -460240.187890625\n",
      "Train Epoch: 768 [512/54000 (1%)] Loss: -404229.625000\n",
      "Train Epoch: 768 [11776/54000 (22%)] Loss: -501869.781250\n",
      "Train Epoch: 768 [23040/54000 (43%)] Loss: -512905.468750\n",
      "Train Epoch: 768 [34304/54000 (64%)] Loss: -522387.593750\n",
      "Train Epoch: 768 [45568/54000 (84%)] Loss: -438565.531250\n",
      "    epoch          : 768\n",
      "    loss           : -475980.4734375\n",
      "    val_loss       : -459115.18720703124\n",
      "Train Epoch: 769 [512/54000 (1%)] Loss: -416163.781250\n",
      "Train Epoch: 769 [11776/54000 (22%)] Loss: -453647.375000\n",
      "Train Epoch: 769 [23040/54000 (43%)] Loss: -517041.406250\n",
      "Train Epoch: 769 [34304/54000 (64%)] Loss: -405038.187500\n",
      "Train Epoch: 769 [45568/54000 (84%)] Loss: -496543.937500\n",
      "    epoch          : 769\n",
      "    loss           : -475945.1703125\n",
      "    val_loss       : -460416.2276367188\n",
      "Train Epoch: 770 [512/54000 (1%)] Loss: -441245.437500\n",
      "Train Epoch: 770 [11776/54000 (22%)] Loss: -538722.312500\n",
      "Train Epoch: 770 [23040/54000 (43%)] Loss: -553071.000000\n",
      "Train Epoch: 770 [34304/54000 (64%)] Loss: -501483.000000\n",
      "Train Epoch: 770 [45568/54000 (84%)] Loss: -524179.968750\n",
      "    epoch          : 770\n",
      "    loss           : -476382.540625\n",
      "    val_loss       : -454783.7662109375\n",
      "Train Epoch: 771 [512/54000 (1%)] Loss: -419707.250000\n",
      "Train Epoch: 771 [11776/54000 (22%)] Loss: -501508.312500\n",
      "Train Epoch: 771 [23040/54000 (43%)] Loss: -417188.312500\n",
      "Train Epoch: 771 [34304/54000 (64%)] Loss: -405677.562500\n",
      "Train Epoch: 771 [45568/54000 (84%)] Loss: -405678.687500\n",
      "    epoch          : 771\n",
      "    loss           : -476417.024375\n",
      "    val_loss       : -458919.2999023438\n",
      "Train Epoch: 772 [512/54000 (1%)] Loss: -519566.875000\n",
      "Train Epoch: 772 [11776/54000 (22%)] Loss: -440596.750000\n",
      "Train Epoch: 772 [23040/54000 (43%)] Loss: -422205.656250\n",
      "Train Epoch: 772 [34304/54000 (64%)] Loss: -441283.875000\n",
      "Train Epoch: 772 [45568/54000 (84%)] Loss: -443422.812500\n",
      "    epoch          : 772\n",
      "    loss           : -476603.324375\n",
      "    val_loss       : -460779.7244140625\n",
      "Train Epoch: 773 [512/54000 (1%)] Loss: -412185.312500\n",
      "Train Epoch: 773 [11776/54000 (22%)] Loss: -535623.250000\n",
      "Train Epoch: 773 [23040/54000 (43%)] Loss: -563134.750000\n",
      "Train Epoch: 773 [34304/54000 (64%)] Loss: -502061.000000\n",
      "Train Epoch: 773 [45568/54000 (84%)] Loss: -498461.062500\n",
      "    epoch          : 773\n",
      "    loss           : -476864.755625\n",
      "    val_loss       : -458751.26015625\n",
      "Train Epoch: 774 [512/54000 (1%)] Loss: -413019.687500\n",
      "Train Epoch: 774 [11776/54000 (22%)] Loss: -410040.281250\n",
      "Train Epoch: 774 [23040/54000 (43%)] Loss: -405323.437500\n",
      "Train Epoch: 774 [34304/54000 (64%)] Loss: -454105.406250\n",
      "Train Epoch: 774 [45568/54000 (84%)] Loss: -415015.156250\n",
      "    epoch          : 774\n",
      "    loss           : -477015.45375\n",
      "    val_loss       : -458245.4369140625\n",
      "Train Epoch: 775 [512/54000 (1%)] Loss: -408343.500000\n",
      "Train Epoch: 775 [11776/54000 (22%)] Loss: -560241.250000\n",
      "Train Epoch: 775 [23040/54000 (43%)] Loss: -562576.500000\n",
      "Train Epoch: 775 [34304/54000 (64%)] Loss: -441545.718750\n",
      "Train Epoch: 775 [45568/54000 (84%)] Loss: -518161.750000\n",
      "    epoch          : 775\n",
      "    loss           : -476927.2746875\n",
      "    val_loss       : -461380.45625\n",
      "Train Epoch: 776 [512/54000 (1%)] Loss: -562533.437500\n",
      "Train Epoch: 776 [11776/54000 (22%)] Loss: -421165.000000\n",
      "Train Epoch: 776 [23040/54000 (43%)] Loss: -442148.218750\n",
      "Train Epoch: 776 [34304/54000 (64%)] Loss: -441701.968750\n",
      "Train Epoch: 776 [45568/54000 (84%)] Loss: -502099.312500\n",
      "    epoch          : 776\n",
      "    loss           : -477399.4796875\n",
      "    val_loss       : -455813.25341796875\n",
      "Train Epoch: 777 [512/54000 (1%)] Loss: -540088.937500\n",
      "Train Epoch: 777 [11776/54000 (22%)] Loss: -524139.312500\n",
      "Train Epoch: 777 [23040/54000 (43%)] Loss: -563164.000000\n",
      "Train Epoch: 777 [34304/54000 (64%)] Loss: -560109.125000\n",
      "Train Epoch: 777 [45568/54000 (84%)] Loss: -524389.250000\n",
      "    epoch          : 777\n",
      "    loss           : -477353.86625\n",
      "    val_loss       : -456494.828515625\n",
      "Train Epoch: 778 [512/54000 (1%)] Loss: -538906.312500\n",
      "Train Epoch: 778 [11776/54000 (22%)] Loss: -521905.875000\n",
      "Train Epoch: 778 [23040/54000 (43%)] Loss: -402419.875000\n",
      "Train Epoch: 778 [34304/54000 (64%)] Loss: -409545.781250\n",
      "Train Epoch: 778 [45568/54000 (84%)] Loss: -562095.687500\n",
      "    epoch          : 778\n",
      "    loss           : -477569.20625\n",
      "    val_loss       : -459714.7482421875\n",
      "Train Epoch: 779 [512/54000 (1%)] Loss: -409611.562500\n",
      "Train Epoch: 779 [11776/54000 (22%)] Loss: -540726.562500\n",
      "Train Epoch: 779 [23040/54000 (43%)] Loss: -411992.437500\n",
      "Train Epoch: 779 [34304/54000 (64%)] Loss: -498781.093750\n",
      "Train Epoch: 779 [45568/54000 (84%)] Loss: -452133.687500\n",
      "    epoch          : 779\n",
      "    loss           : -478160.99375\n",
      "    val_loss       : -458418.6197265625\n",
      "Train Epoch: 780 [512/54000 (1%)] Loss: -455302.937500\n",
      "Train Epoch: 780 [11776/54000 (22%)] Loss: -411933.375000\n",
      "Train Epoch: 780 [23040/54000 (43%)] Loss: -528289.750000\n",
      "Train Epoch: 780 [34304/54000 (64%)] Loss: -501836.187500\n",
      "Train Epoch: 780 [45568/54000 (84%)] Loss: -498228.937500\n",
      "    epoch          : 780\n",
      "    loss           : -478231.7609375\n",
      "    val_loss       : -462658.7176757812\n",
      "Train Epoch: 781 [512/54000 (1%)] Loss: -517380.375000\n",
      "Train Epoch: 781 [11776/54000 (22%)] Loss: -412837.937500\n",
      "Train Epoch: 781 [23040/54000 (43%)] Loss: -565685.437500\n",
      "Train Epoch: 781 [34304/54000 (64%)] Loss: -443728.875000\n",
      "Train Epoch: 781 [45568/54000 (84%)] Loss: -452512.187500\n",
      "    epoch          : 781\n",
      "    loss           : -478302.4928125\n",
      "    val_loss       : -459853.0849609375\n",
      "Train Epoch: 782 [512/54000 (1%)] Loss: -408983.718750\n",
      "Train Epoch: 782 [11776/54000 (22%)] Loss: -443737.187500\n",
      "Train Epoch: 782 [23040/54000 (43%)] Loss: -541888.375000\n",
      "Train Epoch: 782 [34304/54000 (64%)] Loss: -562338.000000\n",
      "Train Epoch: 782 [45568/54000 (84%)] Loss: -412725.843750\n",
      "    epoch          : 782\n",
      "    loss           : -478463.9040625\n",
      "    val_loss       : -459027.4383789062\n",
      "Train Epoch: 783 [512/54000 (1%)] Loss: -514184.031250\n",
      "Train Epoch: 783 [11776/54000 (22%)] Loss: -418439.625000\n",
      "Train Epoch: 783 [23040/54000 (43%)] Loss: -564125.000000\n",
      "Train Epoch: 783 [34304/54000 (64%)] Loss: -503721.250000\n",
      "Train Epoch: 783 [45568/54000 (84%)] Loss: -516630.281250\n",
      "    epoch          : 783\n",
      "    loss           : -478557.0453125\n",
      "    val_loss       : -458895.4771484375\n",
      "Train Epoch: 784 [512/54000 (1%)] Loss: -408089.562500\n",
      "Train Epoch: 784 [11776/54000 (22%)] Loss: -447983.687500\n",
      "Train Epoch: 784 [23040/54000 (43%)] Loss: -417342.312500\n",
      "Train Epoch: 784 [34304/54000 (64%)] Loss: -525457.062500\n",
      "Train Epoch: 784 [45568/54000 (84%)] Loss: -412049.437500\n",
      "    epoch          : 784\n",
      "    loss           : -478581.9296875\n",
      "    val_loss       : -459282.408984375\n",
      "Train Epoch: 785 [512/54000 (1%)] Loss: -527892.250000\n",
      "Train Epoch: 785 [11776/54000 (22%)] Loss: -525998.250000\n",
      "Train Epoch: 785 [23040/54000 (43%)] Loss: -444898.437500\n",
      "Train Epoch: 785 [34304/54000 (64%)] Loss: -502806.718750\n",
      "Train Epoch: 785 [45568/54000 (84%)] Loss: -409754.406250\n",
      "    epoch          : 785\n",
      "    loss           : -478917.784375\n",
      "    val_loss       : -457907.114453125\n",
      "Train Epoch: 786 [512/54000 (1%)] Loss: -565170.625000\n",
      "Train Epoch: 786 [11776/54000 (22%)] Loss: -422287.562500\n",
      "Train Epoch: 786 [23040/54000 (43%)] Loss: -410720.125000\n",
      "Train Epoch: 786 [34304/54000 (64%)] Loss: -455432.593750\n",
      "Train Epoch: 786 [45568/54000 (84%)] Loss: -406167.625000\n",
      "    epoch          : 786\n",
      "    loss           : -479086.809375\n",
      "    val_loss       : -463485.3171875\n",
      "Train Epoch: 787 [512/54000 (1%)] Loss: -422113.687500\n",
      "Train Epoch: 787 [11776/54000 (22%)] Loss: -456488.562500\n",
      "Train Epoch: 787 [23040/54000 (43%)] Loss: -455942.562500\n",
      "Train Epoch: 787 [34304/54000 (64%)] Loss: -440570.218750\n",
      "Train Epoch: 787 [45568/54000 (84%)] Loss: -411084.593750\n",
      "    epoch          : 787\n",
      "    loss           : -479298.80875\n",
      "    val_loss       : -458082.62890625\n",
      "Train Epoch: 788 [512/54000 (1%)] Loss: -542490.250000\n",
      "Train Epoch: 788 [11776/54000 (22%)] Loss: -442437.187500\n",
      "Train Epoch: 788 [23040/54000 (43%)] Loss: -411742.125000\n",
      "Train Epoch: 788 [34304/54000 (64%)] Loss: -448814.843750\n",
      "Train Epoch: 788 [45568/54000 (84%)] Loss: -410515.531250\n",
      "    epoch          : 788\n",
      "    loss           : -479379.2103125\n",
      "    val_loss       : -459720.6163085938\n",
      "Train Epoch: 789 [512/54000 (1%)] Loss: -407977.875000\n",
      "Train Epoch: 789 [11776/54000 (22%)] Loss: -542503.500000\n",
      "Train Epoch: 789 [23040/54000 (43%)] Loss: -457958.312500\n",
      "Train Epoch: 789 [34304/54000 (64%)] Loss: -516952.906250\n",
      "Train Epoch: 789 [45568/54000 (84%)] Loss: -520051.781250\n",
      "    epoch          : 789\n",
      "    loss           : -479647.2675\n",
      "    val_loss       : -463055.57373046875\n",
      "Train Epoch: 790 [512/54000 (1%)] Loss: -455777.531250\n",
      "Train Epoch: 790 [11776/54000 (22%)] Loss: -454417.875000\n",
      "Train Epoch: 790 [23040/54000 (43%)] Loss: -569329.500000\n",
      "Train Epoch: 790 [34304/54000 (64%)] Loss: -413040.156250\n",
      "Train Epoch: 790 [45568/54000 (84%)] Loss: -407230.375000\n",
      "    epoch          : 790\n",
      "    loss           : -479686.7834375\n",
      "    val_loss       : -457096.17265625\n",
      "Train Epoch: 791 [512/54000 (1%)] Loss: -564716.250000\n",
      "Train Epoch: 791 [11776/54000 (22%)] Loss: -415046.437500\n",
      "Train Epoch: 791 [23040/54000 (43%)] Loss: -406898.125000\n",
      "Train Epoch: 791 [34304/54000 (64%)] Loss: -455441.625000\n",
      "Train Epoch: 791 [45568/54000 (84%)] Loss: -520542.656250\n",
      "    epoch          : 791\n",
      "    loss           : -479972.391875\n",
      "    val_loss       : -463348.2994140625\n",
      "Train Epoch: 792 [512/54000 (1%)] Loss: -420633.875000\n",
      "Train Epoch: 792 [11776/54000 (22%)] Loss: -540952.000000\n",
      "Train Epoch: 792 [23040/54000 (43%)] Loss: -410495.968750\n",
      "Train Epoch: 792 [34304/54000 (64%)] Loss: -561943.187500\n",
      "Train Epoch: 792 [45568/54000 (84%)] Loss: -562981.562500\n",
      "    epoch          : 792\n",
      "    loss           : -480059.4425\n",
      "    val_loss       : -464850.6345703125\n",
      "Train Epoch: 793 [512/54000 (1%)] Loss: -526288.312500\n",
      "Train Epoch: 793 [11776/54000 (22%)] Loss: -565087.250000\n",
      "Train Epoch: 793 [23040/54000 (43%)] Loss: -429464.312500\n",
      "Train Epoch: 793 [34304/54000 (64%)] Loss: -518996.562500\n",
      "Train Epoch: 793 [45568/54000 (84%)] Loss: -524580.437500\n",
      "    epoch          : 793\n",
      "    loss           : -480155.6596875\n",
      "    val_loss       : -464918.2966796875\n",
      "Train Epoch: 794 [512/54000 (1%)] Loss: -443915.500000\n",
      "Train Epoch: 794 [11776/54000 (22%)] Loss: -523960.718750\n",
      "Train Epoch: 794 [23040/54000 (43%)] Loss: -410356.937500\n",
      "Train Epoch: 794 [34304/54000 (64%)] Loss: -501413.250000\n",
      "Train Epoch: 794 [45568/54000 (84%)] Loss: -503486.125000\n",
      "    epoch          : 794\n",
      "    loss           : -480380.30875\n",
      "    val_loss       : -462563.4500976562\n",
      "Train Epoch: 795 [512/54000 (1%)] Loss: -416157.375000\n",
      "Train Epoch: 795 [11776/54000 (22%)] Loss: -569430.875000\n",
      "Train Epoch: 795 [23040/54000 (43%)] Loss: -524507.375000\n",
      "Train Epoch: 795 [34304/54000 (64%)] Loss: -504202.343750\n",
      "Train Epoch: 795 [45568/54000 (84%)] Loss: -530634.312500\n",
      "    epoch          : 795\n",
      "    loss           : -480650.798125\n",
      "    val_loss       : -459615.2520507813\n",
      "Train Epoch: 796 [512/54000 (1%)] Loss: -446537.718750\n",
      "Train Epoch: 796 [11776/54000 (22%)] Loss: -565416.437500\n",
      "Train Epoch: 796 [23040/54000 (43%)] Loss: -563795.437500\n",
      "Train Epoch: 796 [34304/54000 (64%)] Loss: -531922.000000\n",
      "Train Epoch: 796 [45568/54000 (84%)] Loss: -501291.000000\n",
      "    epoch          : 796\n",
      "    loss           : -480719.8640625\n",
      "    val_loss       : -460267.66142578126\n",
      "Train Epoch: 797 [512/54000 (1%)] Loss: -450783.593750\n",
      "Train Epoch: 797 [11776/54000 (22%)] Loss: -454822.437500\n",
      "Train Epoch: 797 [23040/54000 (43%)] Loss: -412329.656250\n",
      "Train Epoch: 797 [34304/54000 (64%)] Loss: -454938.281250\n",
      "Train Epoch: 797 [45568/54000 (84%)] Loss: -406936.781250\n",
      "    epoch          : 797\n",
      "    loss           : -480792.6009375\n",
      "    val_loss       : -461293.57001953124\n",
      "Train Epoch: 798 [512/54000 (1%)] Loss: -455283.937500\n",
      "Train Epoch: 798 [11776/54000 (22%)] Loss: -502466.687500\n",
      "Train Epoch: 798 [23040/54000 (43%)] Loss: -531918.875000\n",
      "Train Epoch: 798 [34304/54000 (64%)] Loss: -518389.500000\n",
      "Train Epoch: 798 [45568/54000 (84%)] Loss: -519856.656250\n",
      "    epoch          : 798\n",
      "    loss           : -481189.8671875\n",
      "    val_loss       : -462749.5119140625\n",
      "Train Epoch: 799 [512/54000 (1%)] Loss: -447412.437500\n",
      "Train Epoch: 799 [11776/54000 (22%)] Loss: -504536.062500\n",
      "Train Epoch: 799 [23040/54000 (43%)] Loss: -413610.625000\n",
      "Train Epoch: 799 [34304/54000 (64%)] Loss: -566995.375000\n",
      "Train Epoch: 799 [45568/54000 (84%)] Loss: -504752.812500\n",
      "    epoch          : 799\n",
      "    loss           : -481254.4896875\n",
      "    val_loss       : -461885.11875\n",
      "Train Epoch: 800 [512/54000 (1%)] Loss: -453911.093750\n",
      "Train Epoch: 800 [11776/54000 (22%)] Loss: -447205.218750\n",
      "Train Epoch: 800 [23040/54000 (43%)] Loss: -444127.468750\n",
      "Train Epoch: 800 [34304/54000 (64%)] Loss: -566181.000000\n",
      "Train Epoch: 800 [45568/54000 (84%)] Loss: -521560.125000\n",
      "    epoch          : 800\n",
      "    loss           : -481452.500625\n",
      "    val_loss       : -455397.21513671876\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch800.pth ...\n",
      "Train Epoch: 801 [512/54000 (1%)] Loss: -410920.281250\n",
      "Train Epoch: 801 [11776/54000 (22%)] Loss: -446561.000000\n",
      "Train Epoch: 801 [23040/54000 (43%)] Loss: -546657.125000\n",
      "Train Epoch: 801 [34304/54000 (64%)] Loss: -558051.125000\n",
      "Train Epoch: 801 [45568/54000 (84%)] Loss: -526185.312500\n",
      "    epoch          : 801\n",
      "    loss           : -481706.9071875\n",
      "    val_loss       : -460404.33857421874\n",
      "Train Epoch: 802 [512/54000 (1%)] Loss: -568120.250000\n",
      "Train Epoch: 802 [11776/54000 (22%)] Loss: -446424.500000\n",
      "Train Epoch: 802 [23040/54000 (43%)] Loss: -421960.062500\n",
      "Train Epoch: 802 [34304/54000 (64%)] Loss: -449853.468750\n",
      "Train Epoch: 802 [45568/54000 (84%)] Loss: -458026.125000\n",
      "    epoch          : 802\n",
      "    loss           : -481789.5296875\n",
      "    val_loss       : -465823.72734375\n",
      "Train Epoch: 803 [512/54000 (1%)] Loss: -455902.843750\n",
      "Train Epoch: 803 [11776/54000 (22%)] Loss: -568883.250000\n",
      "Train Epoch: 803 [23040/54000 (43%)] Loss: -567539.500000\n",
      "Train Epoch: 803 [34304/54000 (64%)] Loss: -499311.625000\n",
      "Train Epoch: 803 [45568/54000 (84%)] Loss: -521041.937500\n",
      "    epoch          : 803\n",
      "    loss           : -481937.316875\n",
      "    val_loss       : -462595.050390625\n",
      "Train Epoch: 804 [512/54000 (1%)] Loss: -459318.812500\n",
      "Train Epoch: 804 [11776/54000 (22%)] Loss: -456966.687500\n",
      "Train Epoch: 804 [23040/54000 (43%)] Loss: -525885.250000\n",
      "Train Epoch: 804 [34304/54000 (64%)] Loss: -544896.375000\n",
      "Train Epoch: 804 [45568/54000 (84%)] Loss: -425119.718750\n",
      "    epoch          : 804\n",
      "    loss           : -482163.00625\n",
      "    val_loss       : -466587.6290039063\n",
      "Train Epoch: 805 [512/54000 (1%)] Loss: -413777.156250\n",
      "Train Epoch: 805 [11776/54000 (22%)] Loss: -415942.062500\n",
      "Train Epoch: 805 [23040/54000 (43%)] Loss: -423061.156250\n",
      "Train Epoch: 805 [34304/54000 (64%)] Loss: -449775.406250\n",
      "Train Epoch: 805 [45568/54000 (84%)] Loss: -505696.156250\n",
      "    epoch          : 805\n",
      "    loss           : -482289.1134375\n",
      "    val_loss       : -461890.82587890624\n",
      "Train Epoch: 806 [512/54000 (1%)] Loss: -412641.093750\n",
      "Train Epoch: 806 [11776/54000 (22%)] Loss: -446627.843750\n",
      "Train Epoch: 806 [23040/54000 (43%)] Loss: -508581.500000\n",
      "Train Epoch: 806 [34304/54000 (64%)] Loss: -531466.562500\n",
      "Train Epoch: 806 [45568/54000 (84%)] Loss: -565677.250000\n",
      "    epoch          : 806\n",
      "    loss           : -482132.5021875\n",
      "    val_loss       : -465721.22099609376\n",
      "Train Epoch: 807 [512/54000 (1%)] Loss: -565774.812500\n",
      "Train Epoch: 807 [11776/54000 (22%)] Loss: -446547.781250\n",
      "Train Epoch: 807 [23040/54000 (43%)] Loss: -523575.281250\n",
      "Train Epoch: 807 [34304/54000 (64%)] Loss: -545752.875000\n",
      "Train Epoch: 807 [45568/54000 (84%)] Loss: -531099.750000\n",
      "    epoch          : 807\n",
      "    loss           : -482705.1178125\n",
      "    val_loss       : -462000.3087890625\n",
      "Train Epoch: 808 [512/54000 (1%)] Loss: -425677.250000\n",
      "Train Epoch: 808 [11776/54000 (22%)] Loss: -456316.593750\n",
      "Train Epoch: 808 [23040/54000 (43%)] Loss: -452090.687500\n",
      "Train Epoch: 808 [34304/54000 (64%)] Loss: -523088.781250\n",
      "Train Epoch: 808 [45568/54000 (84%)] Loss: -509741.625000\n",
      "    epoch          : 808\n",
      "    loss           : -482871.5715625\n",
      "    val_loss       : -464312.9431640625\n",
      "Train Epoch: 809 [512/54000 (1%)] Loss: -529761.750000\n",
      "Train Epoch: 809 [11776/54000 (22%)] Loss: -451674.218750\n",
      "Train Epoch: 809 [23040/54000 (43%)] Loss: -567458.750000\n",
      "Train Epoch: 809 [34304/54000 (64%)] Loss: -545562.625000\n",
      "Train Epoch: 809 [45568/54000 (84%)] Loss: -569827.750000\n",
      "    epoch          : 809\n",
      "    loss           : -483012.6965625\n",
      "    val_loss       : -464557.8916015625\n",
      "Train Epoch: 810 [512/54000 (1%)] Loss: -570607.687500\n",
      "Train Epoch: 810 [11776/54000 (22%)] Loss: -546290.625000\n",
      "Train Epoch: 810 [23040/54000 (43%)] Loss: -417229.500000\n",
      "Train Epoch: 810 [34304/54000 (64%)] Loss: -568150.312500\n",
      "Train Epoch: 810 [45568/54000 (84%)] Loss: -414246.750000\n",
      "    epoch          : 810\n",
      "    loss           : -483120.266875\n",
      "    val_loss       : -462087.4706054687\n",
      "Train Epoch: 811 [512/54000 (1%)] Loss: -407898.187500\n",
      "Train Epoch: 811 [11776/54000 (22%)] Loss: -548126.312500\n",
      "Train Epoch: 811 [23040/54000 (43%)] Loss: -427163.468750\n",
      "Train Epoch: 811 [34304/54000 (64%)] Loss: -446637.656250\n",
      "Train Epoch: 811 [45568/54000 (84%)] Loss: -427863.531250\n",
      "    epoch          : 811\n",
      "    loss           : -483457.3171875\n",
      "    val_loss       : -467106.98466796876\n",
      "Train Epoch: 812 [512/54000 (1%)] Loss: -530453.750000\n",
      "Train Epoch: 812 [11776/54000 (22%)] Loss: -529318.437500\n",
      "Train Epoch: 812 [23040/54000 (43%)] Loss: -449918.250000\n",
      "Train Epoch: 812 [34304/54000 (64%)] Loss: -522206.250000\n",
      "Train Epoch: 812 [45568/54000 (84%)] Loss: -412074.187500\n",
      "    epoch          : 812\n",
      "    loss           : -483473.31\n",
      "    val_loss       : -465288.96455078124\n",
      "Train Epoch: 813 [512/54000 (1%)] Loss: -422664.437500\n",
      "Train Epoch: 813 [11776/54000 (22%)] Loss: -424013.687500\n",
      "Train Epoch: 813 [23040/54000 (43%)] Loss: -416591.000000\n",
      "Train Epoch: 813 [34304/54000 (64%)] Loss: -410290.500000\n",
      "Train Epoch: 813 [45568/54000 (84%)] Loss: -501666.312500\n",
      "    epoch          : 813\n",
      "    loss           : -483593.2003125\n",
      "    val_loss       : -466044.8612304687\n",
      "Train Epoch: 814 [512/54000 (1%)] Loss: -571158.750000\n",
      "Train Epoch: 814 [11776/54000 (22%)] Loss: -546507.000000\n",
      "Train Epoch: 814 [23040/54000 (43%)] Loss: -429684.218750\n",
      "Train Epoch: 814 [34304/54000 (64%)] Loss: -522720.812500\n",
      "Train Epoch: 814 [45568/54000 (84%)] Loss: -531644.062500\n",
      "    epoch          : 814\n",
      "    loss           : -483829.3528125\n",
      "    val_loss       : -462432.47119140625\n",
      "Train Epoch: 815 [512/54000 (1%)] Loss: -442218.468750\n",
      "Train Epoch: 815 [11776/54000 (22%)] Loss: -412914.187500\n",
      "Train Epoch: 815 [23040/54000 (43%)] Loss: -531216.875000\n",
      "Train Epoch: 815 [34304/54000 (64%)] Loss: -419221.156250\n",
      "Train Epoch: 815 [45568/54000 (84%)] Loss: -535651.562500\n",
      "    epoch          : 815\n",
      "    loss           : -484141.74125\n",
      "    val_loss       : -466954.520703125\n",
      "Train Epoch: 816 [512/54000 (1%)] Loss: -534133.125000\n",
      "Train Epoch: 816 [11776/54000 (22%)] Loss: -418870.062500\n",
      "Train Epoch: 816 [23040/54000 (43%)] Loss: -573825.812500\n",
      "Train Epoch: 816 [34304/54000 (64%)] Loss: -547764.250000\n",
      "Train Epoch: 816 [45568/54000 (84%)] Loss: -458472.562500\n",
      "    epoch          : 816\n",
      "    loss           : -484213.2821875\n",
      "    val_loss       : -465694.0731445312\n",
      "Train Epoch: 817 [512/54000 (1%)] Loss: -545902.000000\n",
      "Train Epoch: 817 [11776/54000 (22%)] Loss: -548585.562500\n",
      "Train Epoch: 817 [23040/54000 (43%)] Loss: -457061.437500\n",
      "Train Epoch: 817 [34304/54000 (64%)] Loss: -523971.875000\n",
      "Train Epoch: 817 [45568/54000 (84%)] Loss: -528002.562500\n",
      "    epoch          : 817\n",
      "    loss           : -484431.9153125\n",
      "    val_loss       : -464922.0814453125\n",
      "Train Epoch: 818 [512/54000 (1%)] Loss: -572632.312500\n",
      "Train Epoch: 818 [11776/54000 (22%)] Loss: -417723.843750\n",
      "Train Epoch: 818 [23040/54000 (43%)] Loss: -529603.000000\n",
      "Train Epoch: 818 [34304/54000 (64%)] Loss: -419080.062500\n",
      "Train Epoch: 818 [45568/54000 (84%)] Loss: -508521.062500\n",
      "    epoch          : 818\n",
      "    loss           : -484429.79625\n",
      "    val_loss       : -464729.2490234375\n",
      "Train Epoch: 819 [512/54000 (1%)] Loss: -571440.750000\n",
      "Train Epoch: 819 [11776/54000 (22%)] Loss: -548563.625000\n",
      "Train Epoch: 819 [23040/54000 (43%)] Loss: -446731.718750\n",
      "Train Epoch: 819 [34304/54000 (64%)] Loss: -525736.937500\n",
      "Train Epoch: 819 [45568/54000 (84%)] Loss: -524622.812500\n",
      "    epoch          : 819\n",
      "    loss           : -484726.221875\n",
      "    val_loss       : -465983.37734375\n",
      "Train Epoch: 820 [512/54000 (1%)] Loss: -424807.125000\n",
      "Train Epoch: 820 [11776/54000 (22%)] Loss: -544975.125000\n",
      "Train Epoch: 820 [23040/54000 (43%)] Loss: -570260.937500\n",
      "Train Epoch: 820 [34304/54000 (64%)] Loss: -510810.656250\n",
      "Train Epoch: 820 [45568/54000 (84%)] Loss: -533992.750000\n",
      "    epoch          : 820\n",
      "    loss           : -484836.0409375\n",
      "    val_loss       : -464941.5060546875\n",
      "Train Epoch: 821 [512/54000 (1%)] Loss: -450026.562500\n",
      "Train Epoch: 821 [11776/54000 (22%)] Loss: -549435.062500\n",
      "Train Epoch: 821 [23040/54000 (43%)] Loss: -574908.437500\n",
      "Train Epoch: 821 [34304/54000 (64%)] Loss: -528696.187500\n",
      "Train Epoch: 821 [45568/54000 (84%)] Loss: -503218.625000\n",
      "    epoch          : 821\n",
      "    loss           : -485028.9075\n",
      "    val_loss       : -465518.43681640626\n",
      "Train Epoch: 822 [512/54000 (1%)] Loss: -423191.750000\n",
      "Train Epoch: 822 [11776/54000 (22%)] Loss: -523919.406250\n",
      "Train Epoch: 822 [23040/54000 (43%)] Loss: -416307.625000\n",
      "Train Epoch: 822 [34304/54000 (64%)] Loss: -464040.500000\n",
      "Train Epoch: 822 [45568/54000 (84%)] Loss: -463091.562500\n",
      "    epoch          : 822\n",
      "    loss           : -485232.4946875\n",
      "    val_loss       : -464655.8338867187\n",
      "Train Epoch: 823 [512/54000 (1%)] Loss: -524244.437500\n",
      "Train Epoch: 823 [11776/54000 (22%)] Loss: -534731.687500\n",
      "Train Epoch: 823 [23040/54000 (43%)] Loss: -533868.312500\n",
      "Train Epoch: 823 [34304/54000 (64%)] Loss: -530924.625000\n",
      "Train Epoch: 823 [45568/54000 (84%)] Loss: -528368.687500\n",
      "    epoch          : 823\n",
      "    loss           : -485227.5796875\n",
      "    val_loss       : -463044.49580078124\n",
      "Train Epoch: 824 [512/54000 (1%)] Loss: -420345.031250\n",
      "Train Epoch: 824 [11776/54000 (22%)] Loss: -428253.750000\n",
      "Train Epoch: 824 [23040/54000 (43%)] Loss: -572977.125000\n",
      "Train Epoch: 824 [34304/54000 (64%)] Loss: -510314.062500\n",
      "Train Epoch: 824 [45568/54000 (84%)] Loss: -414487.937500\n",
      "    epoch          : 824\n",
      "    loss           : -485507.8890625\n",
      "    val_loss       : -464995.3524414062\n",
      "Train Epoch: 825 [512/54000 (1%)] Loss: -428042.250000\n",
      "Train Epoch: 825 [11776/54000 (22%)] Loss: -532578.375000\n",
      "Train Epoch: 825 [23040/54000 (43%)] Loss: -523897.062500\n",
      "Train Epoch: 825 [34304/54000 (64%)] Loss: -531908.687500\n",
      "Train Epoch: 825 [45568/54000 (84%)] Loss: -529874.875000\n",
      "    epoch          : 825\n",
      "    loss           : -485843.1309375\n",
      "    val_loss       : -467169.81298828125\n",
      "Train Epoch: 826 [512/54000 (1%)] Loss: -459961.843750\n",
      "Train Epoch: 826 [11776/54000 (22%)] Loss: -434057.093750\n",
      "Train Epoch: 826 [23040/54000 (43%)] Loss: -566993.875000\n",
      "Train Epoch: 826 [34304/54000 (64%)] Loss: -525228.875000\n",
      "Train Epoch: 826 [45568/54000 (84%)] Loss: -414862.187500\n",
      "    epoch          : 826\n",
      "    loss           : -485922.9634375\n",
      "    val_loss       : -469263.05029296875\n",
      "Train Epoch: 827 [512/54000 (1%)] Loss: -537001.875000\n",
      "Train Epoch: 827 [11776/54000 (22%)] Loss: -569970.500000\n",
      "Train Epoch: 827 [23040/54000 (43%)] Loss: -422518.968750\n",
      "Train Epoch: 827 [34304/54000 (64%)] Loss: -461852.250000\n",
      "Train Epoch: 827 [45568/54000 (84%)] Loss: -531030.187500\n",
      "    epoch          : 827\n",
      "    loss           : -486035.5903125\n",
      "    val_loss       : -463409.3387695312\n",
      "Train Epoch: 828 [512/54000 (1%)] Loss: -569853.875000\n",
      "Train Epoch: 828 [11776/54000 (22%)] Loss: -569607.500000\n",
      "Train Epoch: 828 [23040/54000 (43%)] Loss: -568855.375000\n",
      "Train Epoch: 828 [34304/54000 (64%)] Loss: -418410.687500\n",
      "Train Epoch: 828 [45568/54000 (84%)] Loss: -506999.500000\n",
      "    epoch          : 828\n",
      "    loss           : -486091.6459375\n",
      "    val_loss       : -466802.64716796874\n",
      "Train Epoch: 829 [512/54000 (1%)] Loss: -459746.500000\n",
      "Train Epoch: 829 [11776/54000 (22%)] Loss: -548529.062500\n",
      "Train Epoch: 829 [23040/54000 (43%)] Loss: -572950.625000\n",
      "Train Epoch: 829 [34304/54000 (64%)] Loss: -425606.437500\n",
      "Train Epoch: 829 [45568/54000 (84%)] Loss: -458732.343750\n",
      "    epoch          : 829\n",
      "    loss           : -486119.2428125\n",
      "    val_loss       : -464815.66455078125\n",
      "Train Epoch: 830 [512/54000 (1%)] Loss: -453204.406250\n",
      "Train Epoch: 830 [11776/54000 (22%)] Loss: -527249.125000\n",
      "Train Epoch: 830 [23040/54000 (43%)] Loss: -534033.625000\n",
      "Train Epoch: 830 [34304/54000 (64%)] Loss: -508900.343750\n",
      "Train Epoch: 830 [45568/54000 (84%)] Loss: -509682.968750\n",
      "    epoch          : 830\n",
      "    loss           : -486522.954375\n",
      "    val_loss       : -466252.270703125\n",
      "Train Epoch: 831 [512/54000 (1%)] Loss: -449771.281250\n",
      "Train Epoch: 831 [11776/54000 (22%)] Loss: -453354.937500\n",
      "Train Epoch: 831 [23040/54000 (43%)] Loss: -549466.000000\n",
      "Train Epoch: 831 [34304/54000 (64%)] Loss: -528463.687500\n",
      "Train Epoch: 831 [45568/54000 (84%)] Loss: -531251.125000\n",
      "    epoch          : 831\n",
      "    loss           : -486595.4375\n",
      "    val_loss       : -469662.3641601562\n",
      "Train Epoch: 832 [512/54000 (1%)] Loss: -459027.531250\n",
      "Train Epoch: 832 [11776/54000 (22%)] Loss: -466334.250000\n",
      "Train Epoch: 832 [23040/54000 (43%)] Loss: -510681.156250\n",
      "Train Epoch: 832 [34304/54000 (64%)] Loss: -534814.062500\n",
      "Train Epoch: 832 [45568/54000 (84%)] Loss: -540520.812500\n",
      "    epoch          : 832\n",
      "    loss           : -486945.8421875\n",
      "    val_loss       : -466198.4141601563\n",
      "Train Epoch: 833 [512/54000 (1%)] Loss: -552038.750000\n",
      "Train Epoch: 833 [11776/54000 (22%)] Loss: -460573.406250\n",
      "Train Epoch: 833 [23040/54000 (43%)] Loss: -457387.125000\n",
      "Train Epoch: 833 [34304/54000 (64%)] Loss: -536105.125000\n",
      "Train Epoch: 833 [45568/54000 (84%)] Loss: -528192.750000\n",
      "    epoch          : 833\n",
      "    loss           : -486693.7046875\n",
      "    val_loss       : -466657.6786132812\n",
      "Train Epoch: 834 [512/54000 (1%)] Loss: -572759.875000\n",
      "Train Epoch: 834 [11776/54000 (22%)] Loss: -460184.812500\n",
      "Train Epoch: 834 [23040/54000 (43%)] Loss: -417554.000000\n",
      "Train Epoch: 834 [34304/54000 (64%)] Loss: -415967.687500\n",
      "Train Epoch: 834 [45568/54000 (84%)] Loss: -527563.000000\n",
      "    epoch          : 834\n",
      "    loss           : -486985.7209375\n",
      "    val_loss       : -455517.33369140624\n",
      "Train Epoch: 835 [512/54000 (1%)] Loss: -509959.468750\n",
      "Train Epoch: 835 [11776/54000 (22%)] Loss: -460835.406250\n",
      "Train Epoch: 835 [23040/54000 (43%)] Loss: -534000.750000\n",
      "Train Epoch: 835 [34304/54000 (64%)] Loss: -571216.562500\n",
      "Train Epoch: 835 [45568/54000 (84%)] Loss: -414265.000000\n",
      "    epoch          : 835\n",
      "    loss           : -486920.3821875\n",
      "    val_loss       : -464695.89306640625\n",
      "Train Epoch: 836 [512/54000 (1%)] Loss: -537664.812500\n",
      "Train Epoch: 836 [11776/54000 (22%)] Loss: -533905.062500\n",
      "Train Epoch: 836 [23040/54000 (43%)] Loss: -568704.187500\n",
      "Train Epoch: 836 [34304/54000 (64%)] Loss: -530191.500000\n",
      "Train Epoch: 836 [45568/54000 (84%)] Loss: -511632.781250\n",
      "    epoch          : 836\n",
      "    loss           : -487562.9853125\n",
      "    val_loss       : -467911.5591796875\n",
      "Train Epoch: 837 [512/54000 (1%)] Loss: -512044.843750\n",
      "Train Epoch: 837 [11776/54000 (22%)] Loss: -426840.843750\n",
      "Train Epoch: 837 [23040/54000 (43%)] Loss: -419890.812500\n",
      "Train Epoch: 837 [34304/54000 (64%)] Loss: -510150.437500\n",
      "Train Epoch: 837 [45568/54000 (84%)] Loss: -512593.656250\n",
      "    epoch          : 837\n",
      "    loss           : -487442.2215625\n",
      "    val_loss       : -465567.25166015624\n",
      "Train Epoch: 838 [512/54000 (1%)] Loss: -414375.093750\n",
      "Train Epoch: 838 [11776/54000 (22%)] Loss: -464156.000000\n",
      "Train Epoch: 838 [23040/54000 (43%)] Loss: -509623.437500\n",
      "Train Epoch: 838 [34304/54000 (64%)] Loss: -427867.468750\n",
      "Train Epoch: 838 [45568/54000 (84%)] Loss: -415754.781250\n",
      "    epoch          : 838\n",
      "    loss           : -487819.0690625\n",
      "    val_loss       : -466611.91376953124\n",
      "Train Epoch: 839 [512/54000 (1%)] Loss: -417293.750000\n",
      "Train Epoch: 839 [11776/54000 (22%)] Loss: -448723.562500\n",
      "Train Epoch: 839 [23040/54000 (43%)] Loss: -528828.500000\n",
      "Train Epoch: 839 [34304/54000 (64%)] Loss: -527281.437500\n",
      "Train Epoch: 839 [45568/54000 (84%)] Loss: -540648.625000\n",
      "    epoch          : 839\n",
      "    loss           : -487916.7740625\n",
      "    val_loss       : -456808.86103515624\n",
      "Train Epoch: 840 [512/54000 (1%)] Loss: -549816.375000\n",
      "Train Epoch: 840 [11776/54000 (22%)] Loss: -428938.625000\n",
      "Train Epoch: 840 [23040/54000 (43%)] Loss: -417239.781250\n",
      "Train Epoch: 840 [34304/54000 (64%)] Loss: -575408.437500\n",
      "Train Epoch: 840 [45568/54000 (84%)] Loss: -535497.187500\n",
      "    epoch          : 840\n",
      "    loss           : -488051.4140625\n",
      "    val_loss       : -468270.26318359375\n",
      "Train Epoch: 841 [512/54000 (1%)] Loss: -459954.687500\n",
      "Train Epoch: 841 [11776/54000 (22%)] Loss: -552272.625000\n",
      "Train Epoch: 841 [23040/54000 (43%)] Loss: -552959.437500\n",
      "Train Epoch: 841 [34304/54000 (64%)] Loss: -509806.250000\n",
      "Train Epoch: 841 [45568/54000 (84%)] Loss: -416269.250000\n",
      "    epoch          : 841\n",
      "    loss           : -488320.6675\n",
      "    val_loss       : -467095.5557617188\n",
      "Train Epoch: 842 [512/54000 (1%)] Loss: -423839.875000\n",
      "Train Epoch: 842 [11776/54000 (22%)] Loss: -455307.187500\n",
      "Train Epoch: 842 [23040/54000 (43%)] Loss: -552506.125000\n",
      "Train Epoch: 842 [34304/54000 (64%)] Loss: -509331.125000\n",
      "Train Epoch: 842 [45568/54000 (84%)] Loss: -510645.406250\n",
      "    epoch          : 842\n",
      "    loss           : -488285.3859375\n",
      "    val_loss       : -471409.3697265625\n",
      "Train Epoch: 843 [512/54000 (1%)] Loss: -453773.562500\n",
      "Train Epoch: 843 [11776/54000 (22%)] Loss: -509836.125000\n",
      "Train Epoch: 843 [23040/54000 (43%)] Loss: -427300.375000\n",
      "Train Epoch: 843 [34304/54000 (64%)] Loss: -572053.125000\n",
      "Train Epoch: 843 [45568/54000 (84%)] Loss: -420419.062500\n",
      "    epoch          : 843\n",
      "    loss           : -488342.2328125\n",
      "    val_loss       : -470709.51708984375\n",
      "Train Epoch: 844 [512/54000 (1%)] Loss: -461880.375000\n",
      "Train Epoch: 844 [11776/54000 (22%)] Loss: -514246.093750\n",
      "Train Epoch: 844 [23040/54000 (43%)] Loss: -553598.750000\n",
      "Train Epoch: 844 [34304/54000 (64%)] Loss: -417269.281250\n",
      "Train Epoch: 844 [45568/54000 (84%)] Loss: -536881.250000\n",
      "    epoch          : 844\n",
      "    loss           : -488831.2625\n",
      "    val_loss       : -466795.28671875\n",
      "Train Epoch: 845 [512/54000 (1%)] Loss: -538265.062500\n",
      "Train Epoch: 845 [11776/54000 (22%)] Loss: -434930.375000\n",
      "Train Epoch: 845 [23040/54000 (43%)] Loss: -508992.812500\n",
      "Train Epoch: 845 [34304/54000 (64%)] Loss: -527244.750000\n",
      "Train Epoch: 845 [45568/54000 (84%)] Loss: -460314.937500\n",
      "    epoch          : 845\n",
      "    loss           : -489035.0640625\n",
      "    val_loss       : -468767.5625976563\n",
      "Train Epoch: 846 [512/54000 (1%)] Loss: -554674.312500\n",
      "Train Epoch: 846 [11776/54000 (22%)] Loss: -416468.031250\n",
      "Train Epoch: 846 [23040/54000 (43%)] Loss: -423307.062500\n",
      "Train Epoch: 846 [34304/54000 (64%)] Loss: -512938.281250\n",
      "Train Epoch: 846 [45568/54000 (84%)] Loss: -414295.031250\n",
      "    epoch          : 846\n",
      "    loss           : -489101.9365625\n",
      "    val_loss       : -467743.0860351563\n",
      "Train Epoch: 847 [512/54000 (1%)] Loss: -433316.937500\n",
      "Train Epoch: 847 [11776/54000 (22%)] Loss: -553365.500000\n",
      "Train Epoch: 847 [23040/54000 (43%)] Loss: -421800.375000\n",
      "Train Epoch: 847 [34304/54000 (64%)] Loss: -422576.468750\n",
      "Train Epoch: 847 [45568/54000 (84%)] Loss: -412486.718750\n",
      "    epoch          : 847\n",
      "    loss           : -489108.2684375\n",
      "    val_loss       : -469610.8080078125\n",
      "Train Epoch: 848 [512/54000 (1%)] Loss: -416003.250000\n",
      "Train Epoch: 848 [11776/54000 (22%)] Loss: -432888.125000\n",
      "Train Epoch: 848 [23040/54000 (43%)] Loss: -539101.062500\n",
      "Train Epoch: 848 [34304/54000 (64%)] Loss: -536200.875000\n",
      "Train Epoch: 848 [45568/54000 (84%)] Loss: -537354.187500\n",
      "    epoch          : 848\n",
      "    loss           : -489464.0759375\n",
      "    val_loss       : -470791.22880859376\n",
      "Train Epoch: 849 [512/54000 (1%)] Loss: -441211.437500\n",
      "Train Epoch: 849 [11776/54000 (22%)] Loss: -461116.593750\n",
      "Train Epoch: 849 [23040/54000 (43%)] Loss: -451939.250000\n",
      "Train Epoch: 849 [34304/54000 (64%)] Loss: -420141.875000\n",
      "Train Epoch: 849 [45568/54000 (84%)] Loss: -582659.500000\n",
      "    epoch          : 849\n",
      "    loss           : -489531.2584375\n",
      "    val_loss       : -468992.8462890625\n",
      "Train Epoch: 850 [512/54000 (1%)] Loss: -417124.843750\n",
      "Train Epoch: 850 [11776/54000 (22%)] Loss: -541370.500000\n",
      "Train Epoch: 850 [23040/54000 (43%)] Loss: -451906.687500\n",
      "Train Epoch: 850 [34304/54000 (64%)] Loss: -454707.218750\n",
      "Train Epoch: 850 [45568/54000 (84%)] Loss: -539180.187500\n",
      "    epoch          : 850\n",
      "    loss           : -489553.6446875\n",
      "    val_loss       : -463465.1076171875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch850.pth ...\n",
      "Train Epoch: 851 [512/54000 (1%)] Loss: -531669.375000\n",
      "Train Epoch: 851 [11776/54000 (22%)] Loss: -542235.375000\n",
      "Train Epoch: 851 [23040/54000 (43%)] Loss: -532230.500000\n",
      "Train Epoch: 851 [34304/54000 (64%)] Loss: -419032.750000\n",
      "Train Epoch: 851 [45568/54000 (84%)] Loss: -420752.375000\n",
      "    epoch          : 851\n",
      "    loss           : -489892.1609375\n",
      "    val_loss       : -470145.54130859376\n",
      "Train Epoch: 852 [512/54000 (1%)] Loss: -433274.125000\n",
      "Train Epoch: 852 [11776/54000 (22%)] Loss: -464879.062500\n",
      "Train Epoch: 852 [23040/54000 (43%)] Loss: -431182.906250\n",
      "Train Epoch: 852 [34304/54000 (64%)] Loss: -512234.000000\n",
      "Train Epoch: 852 [45568/54000 (84%)] Loss: -415527.687500\n",
      "    epoch          : 852\n",
      "    loss           : -489992.5028125\n",
      "    val_loss       : -469838.39189453126\n",
      "Train Epoch: 853 [512/54000 (1%)] Loss: -511867.343750\n",
      "Train Epoch: 853 [11776/54000 (22%)] Loss: -453069.281250\n",
      "Train Epoch: 853 [23040/54000 (43%)] Loss: -583361.437500\n",
      "Train Epoch: 853 [34304/54000 (64%)] Loss: -418680.562500\n",
      "Train Epoch: 853 [45568/54000 (84%)] Loss: -413268.187500\n",
      "    epoch          : 853\n",
      "    loss           : -490172.649375\n",
      "    val_loss       : -472776.03251953126\n",
      "Train Epoch: 854 [512/54000 (1%)] Loss: -463980.093750\n",
      "Train Epoch: 854 [11776/54000 (22%)] Loss: -554251.687500\n",
      "Train Epoch: 854 [23040/54000 (43%)] Loss: -452251.718750\n",
      "Train Epoch: 854 [34304/54000 (64%)] Loss: -575533.250000\n",
      "Train Epoch: 854 [45568/54000 (84%)] Loss: -530822.750000\n",
      "    epoch          : 854\n",
      "    loss           : -490118.9684375\n",
      "    val_loss       : -470779.0586914063\n",
      "Train Epoch: 855 [512/54000 (1%)] Loss: -517482.406250\n",
      "Train Epoch: 855 [11776/54000 (22%)] Loss: -510312.875000\n",
      "Train Epoch: 855 [23040/54000 (43%)] Loss: -518032.625000\n",
      "Train Epoch: 855 [34304/54000 (64%)] Loss: -431528.937500\n",
      "Train Epoch: 855 [45568/54000 (84%)] Loss: -464235.343750\n",
      "    epoch          : 855\n",
      "    loss           : -490445.41875\n",
      "    val_loss       : -467118.8009765625\n",
      "Train Epoch: 856 [512/54000 (1%)] Loss: -430439.031250\n",
      "Train Epoch: 856 [11776/54000 (22%)] Loss: -420692.593750\n",
      "Train Epoch: 856 [23040/54000 (43%)] Loss: -541694.750000\n",
      "Train Epoch: 856 [34304/54000 (64%)] Loss: -419969.375000\n",
      "Train Epoch: 856 [45568/54000 (84%)] Loss: -531073.437500\n",
      "    epoch          : 856\n",
      "    loss           : -490533.995625\n",
      "    val_loss       : -469347.2216796875\n",
      "Train Epoch: 857 [512/54000 (1%)] Loss: -556256.000000\n",
      "Train Epoch: 857 [11776/54000 (22%)] Loss: -539471.437500\n",
      "Train Epoch: 857 [23040/54000 (43%)] Loss: -422461.875000\n",
      "Train Epoch: 857 [34304/54000 (64%)] Loss: -578853.687500\n",
      "Train Epoch: 857 [45568/54000 (84%)] Loss: -423443.125000\n",
      "    epoch          : 857\n",
      "    loss           : -490799.12125\n",
      "    val_loss       : -467505.1455078125\n",
      "Train Epoch: 858 [512/54000 (1%)] Loss: -420246.687500\n",
      "Train Epoch: 858 [11776/54000 (22%)] Loss: -580667.687500\n",
      "Train Epoch: 858 [23040/54000 (43%)] Loss: -417142.937500\n",
      "Train Epoch: 858 [34304/54000 (64%)] Loss: -418890.375000\n",
      "Train Epoch: 858 [45568/54000 (84%)] Loss: -535258.250000\n",
      "    epoch          : 858\n",
      "    loss           : -490843.534375\n",
      "    val_loss       : -474134.82109375\n",
      "Train Epoch: 859 [512/54000 (1%)] Loss: -574584.750000\n",
      "Train Epoch: 859 [11776/54000 (22%)] Loss: -556854.750000\n",
      "Train Epoch: 859 [23040/54000 (43%)] Loss: -576734.250000\n",
      "Train Epoch: 859 [34304/54000 (64%)] Loss: -418337.218750\n",
      "Train Epoch: 859 [45568/54000 (84%)] Loss: -455750.500000\n",
      "    epoch          : 859\n",
      "    loss           : -491015.8815625\n",
      "    val_loss       : -474555.5341796875\n",
      "Train Epoch: 860 [512/54000 (1%)] Loss: -575939.437500\n",
      "Train Epoch: 860 [11776/54000 (22%)] Loss: -578412.125000\n",
      "Train Epoch: 860 [23040/54000 (43%)] Loss: -431224.812500\n",
      "Train Epoch: 860 [34304/54000 (64%)] Loss: -534184.562500\n",
      "Train Epoch: 860 [45568/54000 (84%)] Loss: -457699.500000\n",
      "    epoch          : 860\n",
      "    loss           : -491399.735625\n",
      "    val_loss       : -468564.20185546874\n",
      "Train Epoch: 861 [512/54000 (1%)] Loss: -464531.781250\n",
      "Train Epoch: 861 [11776/54000 (22%)] Loss: -425733.875000\n",
      "Train Epoch: 861 [23040/54000 (43%)] Loss: -580197.625000\n",
      "Train Epoch: 861 [34304/54000 (64%)] Loss: -419083.250000\n",
      "Train Epoch: 861 [45568/54000 (84%)] Loss: -412166.218750\n",
      "    epoch          : 861\n",
      "    loss           : -491227.85875\n",
      "    val_loss       : -468897.0669921875\n",
      "Train Epoch: 862 [512/54000 (1%)] Loss: -532775.625000\n",
      "Train Epoch: 862 [11776/54000 (22%)] Loss: -516473.562500\n",
      "Train Epoch: 862 [23040/54000 (43%)] Loss: -535377.000000\n",
      "Train Epoch: 862 [34304/54000 (64%)] Loss: -539978.562500\n",
      "Train Epoch: 862 [45568/54000 (84%)] Loss: -537952.375000\n",
      "    epoch          : 862\n",
      "    loss           : -491146.1825\n",
      "    val_loss       : -474840.7630859375\n",
      "Train Epoch: 863 [512/54000 (1%)] Loss: -426225.312500\n",
      "Train Epoch: 863 [11776/54000 (22%)] Loss: -578702.750000\n",
      "Train Epoch: 863 [23040/54000 (43%)] Loss: -431066.218750\n",
      "Train Epoch: 863 [34304/54000 (64%)] Loss: -540235.625000\n",
      "Train Epoch: 863 [45568/54000 (84%)] Loss: -533125.250000\n",
      "    epoch          : 863\n",
      "    loss           : -491363.135\n",
      "    val_loss       : -470468.9826171875\n",
      "Train Epoch: 864 [512/54000 (1%)] Loss: -555823.125000\n",
      "Train Epoch: 864 [11776/54000 (22%)] Loss: -578657.687500\n",
      "Train Epoch: 864 [23040/54000 (43%)] Loss: -582034.875000\n",
      "Train Epoch: 864 [34304/54000 (64%)] Loss: -418910.687500\n",
      "Train Epoch: 864 [45568/54000 (84%)] Loss: -458016.250000\n",
      "    epoch          : 864\n",
      "    loss           : -491951.103125\n",
      "    val_loss       : -471520.4715820312\n",
      "Train Epoch: 865 [512/54000 (1%)] Loss: -557522.125000\n",
      "Train Epoch: 865 [11776/54000 (22%)] Loss: -430293.375000\n",
      "Train Epoch: 865 [23040/54000 (43%)] Loss: -432765.218750\n",
      "Train Epoch: 865 [34304/54000 (64%)] Loss: -578329.500000\n",
      "Train Epoch: 865 [45568/54000 (84%)] Loss: -511117.750000\n",
      "    epoch          : 865\n",
      "    loss           : -491896.1109375\n",
      "    val_loss       : -469667.88154296874\n",
      "Train Epoch: 866 [512/54000 (1%)] Loss: -431086.750000\n",
      "Train Epoch: 866 [11776/54000 (22%)] Loss: -516517.531250\n",
      "Train Epoch: 866 [23040/54000 (43%)] Loss: -579595.312500\n",
      "Train Epoch: 866 [34304/54000 (64%)] Loss: -423143.125000\n",
      "Train Epoch: 866 [45568/54000 (84%)] Loss: -419382.000000\n",
      "    epoch          : 866\n",
      "    loss           : -492055.0528125\n",
      "    val_loss       : -467182.6241210938\n",
      "Train Epoch: 867 [512/54000 (1%)] Loss: -558543.250000\n",
      "Train Epoch: 867 [11776/54000 (22%)] Loss: -530211.437500\n",
      "Train Epoch: 867 [23040/54000 (43%)] Loss: -542451.375000\n",
      "Train Epoch: 867 [34304/54000 (64%)] Loss: -437178.062500\n",
      "Train Epoch: 867 [45568/54000 (84%)] Loss: -421819.593750\n",
      "    epoch          : 867\n",
      "    loss           : -492244.6928125\n",
      "    val_loss       : -464979.3956054688\n",
      "Train Epoch: 868 [512/54000 (1%)] Loss: -416083.187500\n",
      "Train Epoch: 868 [11776/54000 (22%)] Loss: -439253.562500\n",
      "Train Epoch: 868 [23040/54000 (43%)] Loss: -424276.031250\n",
      "Train Epoch: 868 [34304/54000 (64%)] Loss: -429809.875000\n",
      "Train Epoch: 868 [45568/54000 (84%)] Loss: -511535.968750\n",
      "    epoch          : 868\n",
      "    loss           : -492550.1865625\n",
      "    val_loss       : -470190.66650390625\n",
      "Train Epoch: 869 [512/54000 (1%)] Loss: -584887.500000\n",
      "Train Epoch: 869 [11776/54000 (22%)] Loss: -580694.250000\n",
      "Train Epoch: 869 [23040/54000 (43%)] Loss: -537279.000000\n",
      "Train Epoch: 869 [34304/54000 (64%)] Loss: -429371.125000\n",
      "Train Epoch: 869 [45568/54000 (84%)] Loss: -419155.968750\n",
      "    epoch          : 869\n",
      "    loss           : -492646.701875\n",
      "    val_loss       : -472343.4279296875\n",
      "Train Epoch: 870 [512/54000 (1%)] Loss: -532730.312500\n",
      "Train Epoch: 870 [11776/54000 (22%)] Loss: -425084.281250\n",
      "Train Epoch: 870 [23040/54000 (43%)] Loss: -537183.375000\n",
      "Train Epoch: 870 [34304/54000 (64%)] Loss: -417501.500000\n",
      "Train Epoch: 870 [45568/54000 (84%)] Loss: -516500.812500\n",
      "    epoch          : 870\n",
      "    loss           : -492712.4121875\n",
      "    val_loss       : -475270.3778320312\n",
      "Train Epoch: 871 [512/54000 (1%)] Loss: -556684.062500\n",
      "Train Epoch: 871 [11776/54000 (22%)] Loss: -556256.500000\n",
      "Train Epoch: 871 [23040/54000 (43%)] Loss: -420084.156250\n",
      "Train Epoch: 871 [34304/54000 (64%)] Loss: -556866.187500\n",
      "Train Epoch: 871 [45568/54000 (84%)] Loss: -535638.125000\n",
      "    epoch          : 871\n",
      "    loss           : -493003.2375\n",
      "    val_loss       : -475256.61494140624\n",
      "Train Epoch: 872 [512/54000 (1%)] Loss: -468059.406250\n",
      "Train Epoch: 872 [11776/54000 (22%)] Loss: -430668.937500\n",
      "Train Epoch: 872 [23040/54000 (43%)] Loss: -579848.375000\n",
      "Train Epoch: 872 [34304/54000 (64%)] Loss: -457019.750000\n",
      "Train Epoch: 872 [45568/54000 (84%)] Loss: -533760.625000\n",
      "    epoch          : 872\n",
      "    loss           : -493047.5890625\n",
      "    val_loss       : -470638.0770507812\n",
      "Train Epoch: 873 [512/54000 (1%)] Loss: -535205.125000\n",
      "Train Epoch: 873 [11776/54000 (22%)] Loss: -418367.343750\n",
      "Train Epoch: 873 [23040/54000 (43%)] Loss: -463583.781250\n",
      "Train Epoch: 873 [34304/54000 (64%)] Loss: -555897.250000\n",
      "Train Epoch: 873 [45568/54000 (84%)] Loss: -517496.562500\n",
      "    epoch          : 873\n",
      "    loss           : -493135.8690625\n",
      "    val_loss       : -474296.0614257812\n",
      "Train Epoch: 874 [512/54000 (1%)] Loss: -585615.375000\n",
      "Train Epoch: 874 [11776/54000 (22%)] Loss: -420872.687500\n",
      "Train Epoch: 874 [23040/54000 (43%)] Loss: -468149.000000\n",
      "Train Epoch: 874 [34304/54000 (64%)] Loss: -424549.156250\n",
      "Train Epoch: 874 [45568/54000 (84%)] Loss: -543674.875000\n",
      "    epoch          : 874\n",
      "    loss           : -493350.2871875\n",
      "    val_loss       : -471130.65400390624\n",
      "Train Epoch: 875 [512/54000 (1%)] Loss: -557921.312500\n",
      "Train Epoch: 875 [11776/54000 (22%)] Loss: -438522.656250\n",
      "Train Epoch: 875 [23040/54000 (43%)] Loss: -456985.687500\n",
      "Train Epoch: 875 [34304/54000 (64%)] Loss: -422532.062500\n",
      "Train Epoch: 875 [45568/54000 (84%)] Loss: -520643.625000\n",
      "    epoch          : 875\n",
      "    loss           : -493404.7225\n",
      "    val_loss       : -473673.8716796875\n",
      "Train Epoch: 876 [512/54000 (1%)] Loss: -579831.812500\n",
      "Train Epoch: 876 [11776/54000 (22%)] Loss: -419775.562500\n",
      "Train Epoch: 876 [23040/54000 (43%)] Loss: -534626.250000\n",
      "Train Epoch: 876 [34304/54000 (64%)] Loss: -546130.875000\n",
      "Train Epoch: 876 [45568/54000 (84%)] Loss: -538870.687500\n",
      "    epoch          : 876\n",
      "    loss           : -493457.136875\n",
      "    val_loss       : -469082.49287109374\n",
      "Train Epoch: 877 [512/54000 (1%)] Loss: -576584.937500\n",
      "Train Epoch: 877 [11776/54000 (22%)] Loss: -518130.625000\n",
      "Train Epoch: 877 [23040/54000 (43%)] Loss: -423982.562500\n",
      "Train Epoch: 877 [34304/54000 (64%)] Loss: -513197.343750\n",
      "Train Epoch: 877 [45568/54000 (84%)] Loss: -537388.437500\n",
      "    epoch          : 877\n",
      "    loss           : -493721.515625\n",
      "    val_loss       : -475711.878125\n",
      "Train Epoch: 878 [512/54000 (1%)] Loss: -517371.812500\n",
      "Train Epoch: 878 [11776/54000 (22%)] Loss: -559586.250000\n",
      "Train Epoch: 878 [23040/54000 (43%)] Loss: -519397.593750\n",
      "Train Epoch: 878 [34304/54000 (64%)] Loss: -538645.250000\n",
      "Train Epoch: 878 [45568/54000 (84%)] Loss: -420912.500000\n",
      "    epoch          : 878\n",
      "    loss           : -493903.8621875\n",
      "    val_loss       : -475171.52578125\n",
      "Train Epoch: 879 [512/54000 (1%)] Loss: -516566.750000\n",
      "Train Epoch: 879 [11776/54000 (22%)] Loss: -559116.875000\n",
      "Train Epoch: 879 [23040/54000 (43%)] Loss: -462161.781250\n",
      "Train Epoch: 879 [34304/54000 (64%)] Loss: -469853.093750\n",
      "Train Epoch: 879 [45568/54000 (84%)] Loss: -582621.875000\n",
      "    epoch          : 879\n",
      "    loss           : -494222.0553125\n",
      "    val_loss       : -473823.7955078125\n",
      "Train Epoch: 880 [512/54000 (1%)] Loss: -538751.437500\n",
      "Train Epoch: 880 [11776/54000 (22%)] Loss: -460541.218750\n",
      "Train Epoch: 880 [23040/54000 (43%)] Loss: -561375.750000\n",
      "Train Epoch: 880 [34304/54000 (64%)] Loss: -541791.750000\n",
      "Train Epoch: 880 [45568/54000 (84%)] Loss: -546210.625000\n",
      "    epoch          : 880\n",
      "    loss           : -494188.9084375\n",
      "    val_loss       : -467255.34912109375\n",
      "Train Epoch: 881 [512/54000 (1%)] Loss: -432431.125000\n",
      "Train Epoch: 881 [11776/54000 (22%)] Loss: -585635.250000\n",
      "Train Epoch: 881 [23040/54000 (43%)] Loss: -562084.750000\n",
      "Train Epoch: 881 [34304/54000 (64%)] Loss: -535998.062500\n",
      "Train Epoch: 881 [45568/54000 (84%)] Loss: -517109.343750\n",
      "    epoch          : 881\n",
      "    loss           : -494405.40125\n",
      "    val_loss       : -473868.50615234376\n",
      "Train Epoch: 882 [512/54000 (1%)] Loss: -461067.937500\n",
      "Train Epoch: 882 [11776/54000 (22%)] Loss: -584285.562500\n",
      "Train Epoch: 882 [23040/54000 (43%)] Loss: -426647.062500\n",
      "Train Epoch: 882 [34304/54000 (64%)] Loss: -580348.875000\n",
      "Train Epoch: 882 [45568/54000 (84%)] Loss: -463527.125000\n",
      "    epoch          : 882\n",
      "    loss           : -494644.5090625\n",
      "    val_loss       : -476564.7009765625\n",
      "Train Epoch: 883 [512/54000 (1%)] Loss: -545981.562500\n",
      "Train Epoch: 883 [11776/54000 (22%)] Loss: -560757.625000\n",
      "Train Epoch: 883 [23040/54000 (43%)] Loss: -434053.625000\n",
      "Train Epoch: 883 [34304/54000 (64%)] Loss: -465065.968750\n",
      "Train Epoch: 883 [45568/54000 (84%)] Loss: -539804.125000\n",
      "    epoch          : 883\n",
      "    loss           : -494847.28375\n",
      "    val_loss       : -472734.14677734376\n",
      "Train Epoch: 884 [512/54000 (1%)] Loss: -560191.125000\n",
      "Train Epoch: 884 [11776/54000 (22%)] Loss: -463759.500000\n",
      "Train Epoch: 884 [23040/54000 (43%)] Loss: -423051.031250\n",
      "Train Epoch: 884 [34304/54000 (64%)] Loss: -433111.000000\n",
      "Train Epoch: 884 [45568/54000 (84%)] Loss: -533296.000000\n",
      "    epoch          : 884\n",
      "    loss           : -494950.0953125\n",
      "    val_loss       : -475403.9139648437\n",
      "Train Epoch: 885 [512/54000 (1%)] Loss: -425694.968750\n",
      "Train Epoch: 885 [11776/54000 (22%)] Loss: -544161.187500\n",
      "Train Epoch: 885 [23040/54000 (43%)] Loss: -428949.937500\n",
      "Train Epoch: 885 [34304/54000 (64%)] Loss: -583581.250000\n",
      "Train Epoch: 885 [45568/54000 (84%)] Loss: -515092.843750\n",
      "    epoch          : 885\n",
      "    loss           : -495030.796875\n",
      "    val_loss       : -472423.8421875\n",
      "Train Epoch: 886 [512/54000 (1%)] Loss: -559948.875000\n",
      "Train Epoch: 886 [11776/54000 (22%)] Loss: -425348.437500\n",
      "Train Epoch: 886 [23040/54000 (43%)] Loss: -563693.812500\n",
      "Train Epoch: 886 [34304/54000 (64%)] Loss: -518379.093750\n",
      "Train Epoch: 886 [45568/54000 (84%)] Loss: -542423.750000\n",
      "    epoch          : 886\n",
      "    loss           : -495092.656875\n",
      "    val_loss       : -474912.760546875\n",
      "Train Epoch: 887 [512/54000 (1%)] Loss: -464332.812500\n",
      "Train Epoch: 887 [11776/54000 (22%)] Loss: -536926.000000\n",
      "Train Epoch: 887 [23040/54000 (43%)] Loss: -560070.875000\n",
      "Train Epoch: 887 [34304/54000 (64%)] Loss: -468730.937500\n",
      "Train Epoch: 887 [45568/54000 (84%)] Loss: -519222.375000\n",
      "    epoch          : 887\n",
      "    loss           : -495335.414375\n",
      "    val_loss       : -469160.9970703125\n",
      "Train Epoch: 888 [512/54000 (1%)] Loss: -430931.718750\n",
      "Train Epoch: 888 [11776/54000 (22%)] Loss: -457279.062500\n",
      "Train Epoch: 888 [23040/54000 (43%)] Loss: -471827.562500\n",
      "Train Epoch: 888 [34304/54000 (64%)] Loss: -545777.375000\n",
      "Train Epoch: 888 [45568/54000 (84%)] Loss: -465732.375000\n",
      "    epoch          : 888\n",
      "    loss           : -495399.391875\n",
      "    val_loss       : -472985.0806640625\n",
      "Train Epoch: 889 [512/54000 (1%)] Loss: -539088.750000\n",
      "Train Epoch: 889 [11776/54000 (22%)] Loss: -427056.500000\n",
      "Train Epoch: 889 [23040/54000 (43%)] Loss: -416503.000000\n",
      "Train Epoch: 889 [34304/54000 (64%)] Loss: -517771.187500\n",
      "Train Epoch: 889 [45568/54000 (84%)] Loss: -422683.437500\n",
      "    epoch          : 889\n",
      "    loss           : -495528.1009375\n",
      "    val_loss       : -474769.49541015626\n",
      "Train Epoch: 890 [512/54000 (1%)] Loss: -421140.843750\n",
      "Train Epoch: 890 [11776/54000 (22%)] Loss: -437825.625000\n",
      "Train Epoch: 890 [23040/54000 (43%)] Loss: -464105.937500\n",
      "Train Epoch: 890 [34304/54000 (64%)] Loss: -584516.562500\n",
      "Train Epoch: 890 [45568/54000 (84%)] Loss: -540567.000000\n",
      "    epoch          : 890\n",
      "    loss           : -495743.9390625\n",
      "    val_loss       : -468768.20185546874\n",
      "Train Epoch: 891 [512/54000 (1%)] Loss: -545773.312500\n",
      "Train Epoch: 891 [11776/54000 (22%)] Loss: -560925.875000\n",
      "Train Epoch: 891 [23040/54000 (43%)] Loss: -560921.812500\n",
      "Train Epoch: 891 [34304/54000 (64%)] Loss: -418357.187500\n",
      "Train Epoch: 891 [45568/54000 (84%)] Loss: -421407.625000\n",
      "    epoch          : 891\n",
      "    loss           : -495955.9565625\n",
      "    val_loss       : -474826.690625\n",
      "Train Epoch: 892 [512/54000 (1%)] Loss: -545854.687500\n",
      "Train Epoch: 892 [11776/54000 (22%)] Loss: -436145.062500\n",
      "Train Epoch: 892 [23040/54000 (43%)] Loss: -587981.000000\n",
      "Train Epoch: 892 [34304/54000 (64%)] Loss: -583095.625000\n",
      "Train Epoch: 892 [45568/54000 (84%)] Loss: -538121.875000\n",
      "    epoch          : 892\n",
      "    loss           : -496051.1915625\n",
      "    val_loss       : -478266.10615234374\n",
      "Train Epoch: 893 [512/54000 (1%)] Loss: -582834.375000\n",
      "Train Epoch: 893 [11776/54000 (22%)] Loss: -434115.781250\n",
      "Train Epoch: 893 [23040/54000 (43%)] Loss: -451835.625000\n",
      "Train Epoch: 893 [34304/54000 (64%)] Loss: -419961.281250\n",
      "Train Epoch: 893 [45568/54000 (84%)] Loss: -430539.968750\n",
      "    epoch          : 893\n",
      "    loss           : -496157.2971875\n",
      "    val_loss       : -477468.261328125\n",
      "Train Epoch: 894 [512/54000 (1%)] Loss: -440800.000000\n",
      "Train Epoch: 894 [11776/54000 (22%)] Loss: -561675.250000\n",
      "Train Epoch: 894 [23040/54000 (43%)] Loss: -425079.562500\n",
      "Train Epoch: 894 [34304/54000 (64%)] Loss: -580490.125000\n",
      "Train Epoch: 894 [45568/54000 (84%)] Loss: -548656.500000\n",
      "    epoch          : 894\n",
      "    loss           : -496341.9925\n",
      "    val_loss       : -475152.18896484375\n",
      "Train Epoch: 895 [512/54000 (1%)] Loss: -462326.375000\n",
      "Train Epoch: 895 [11776/54000 (22%)] Loss: -427212.187500\n",
      "Train Epoch: 895 [23040/54000 (43%)] Loss: -581961.750000\n",
      "Train Epoch: 895 [34304/54000 (64%)] Loss: -523207.875000\n",
      "Train Epoch: 895 [45568/54000 (84%)] Loss: -426188.718750\n",
      "    epoch          : 895\n",
      "    loss           : -496595.4440625\n",
      "    val_loss       : -476141.045703125\n",
      "Train Epoch: 896 [512/54000 (1%)] Loss: -585649.937500\n",
      "Train Epoch: 896 [11776/54000 (22%)] Loss: -545185.250000\n",
      "Train Epoch: 896 [23040/54000 (43%)] Loss: -425315.531250\n",
      "Train Epoch: 896 [34304/54000 (64%)] Loss: -469206.812500\n",
      "Train Epoch: 896 [45568/54000 (84%)] Loss: -520478.781250\n",
      "    epoch          : 896\n",
      "    loss           : -496723.25375\n",
      "    val_loss       : -476923.5142578125\n",
      "Train Epoch: 897 [512/54000 (1%)] Loss: -435717.062500\n",
      "Train Epoch: 897 [11776/54000 (22%)] Loss: -437809.250000\n",
      "Train Epoch: 897 [23040/54000 (43%)] Loss: -544206.937500\n",
      "Train Epoch: 897 [34304/54000 (64%)] Loss: -468394.437500\n",
      "Train Epoch: 897 [45568/54000 (84%)] Loss: -457486.125000\n",
      "    epoch          : 897\n",
      "    loss           : -496783.876875\n",
      "    val_loss       : -478060.4650390625\n",
      "Train Epoch: 898 [512/54000 (1%)] Loss: -457912.875000\n",
      "Train Epoch: 898 [11776/54000 (22%)] Loss: -542267.687500\n",
      "Train Epoch: 898 [23040/54000 (43%)] Loss: -583676.000000\n",
      "Train Epoch: 898 [34304/54000 (64%)] Loss: -456951.375000\n",
      "Train Epoch: 898 [45568/54000 (84%)] Loss: -424502.093750\n",
      "    epoch          : 898\n",
      "    loss           : -496942.855625\n",
      "    val_loss       : -480062.680859375\n",
      "Train Epoch: 899 [512/54000 (1%)] Loss: -468733.000000\n",
      "Train Epoch: 899 [11776/54000 (22%)] Loss: -549427.062500\n",
      "Train Epoch: 899 [23040/54000 (43%)] Loss: -520158.250000\n",
      "Train Epoch: 899 [34304/54000 (64%)] Loss: -436332.000000\n",
      "Train Epoch: 899 [45568/54000 (84%)] Loss: -428743.531250\n",
      "    epoch          : 899\n",
      "    loss           : -497222.340625\n",
      "    val_loss       : -474355.787890625\n",
      "Train Epoch: 900 [512/54000 (1%)] Loss: -590618.625000\n",
      "Train Epoch: 900 [11776/54000 (22%)] Loss: -455407.812500\n",
      "Train Epoch: 900 [23040/54000 (43%)] Loss: -422849.406250\n",
      "Train Epoch: 900 [34304/54000 (64%)] Loss: -460385.906250\n",
      "Train Epoch: 900 [45568/54000 (84%)] Loss: -548108.125000\n",
      "    epoch          : 900\n",
      "    loss           : -497352.4215625\n",
      "    val_loss       : -477404.88232421875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch900.pth ...\n",
      "Train Epoch: 901 [512/54000 (1%)] Loss: -471400.062500\n",
      "Train Epoch: 901 [11776/54000 (22%)] Loss: -542638.375000\n",
      "Train Epoch: 901 [23040/54000 (43%)] Loss: -564203.875000\n",
      "Train Epoch: 901 [34304/54000 (64%)] Loss: -423514.250000\n",
      "Train Epoch: 901 [45568/54000 (84%)] Loss: -427607.531250\n",
      "    epoch          : 901\n",
      "    loss           : -497341.4903125\n",
      "    val_loss       : -478283.25341796875\n",
      "Train Epoch: 902 [512/54000 (1%)] Loss: -424726.625000\n",
      "Train Epoch: 902 [11776/54000 (22%)] Loss: -517269.937500\n",
      "Train Epoch: 902 [23040/54000 (43%)] Loss: -470139.500000\n",
      "Train Epoch: 902 [34304/54000 (64%)] Loss: -539201.875000\n",
      "Train Epoch: 902 [45568/54000 (84%)] Loss: -522139.906250\n",
      "    epoch          : 902\n",
      "    loss           : -497474.5409375\n",
      "    val_loss       : -471348.69892578124\n",
      "Train Epoch: 903 [512/54000 (1%)] Loss: -424652.812500\n",
      "Train Epoch: 903 [11776/54000 (22%)] Loss: -588206.812500\n",
      "Train Epoch: 903 [23040/54000 (43%)] Loss: -465406.718750\n",
      "Train Epoch: 903 [34304/54000 (64%)] Loss: -468681.812500\n",
      "Train Epoch: 903 [45568/54000 (84%)] Loss: -469871.312500\n",
      "    epoch          : 903\n",
      "    loss           : -497740.7978125\n",
      "    val_loss       : -474348.3423828125\n",
      "Train Epoch: 904 [512/54000 (1%)] Loss: -566500.625000\n",
      "Train Epoch: 904 [11776/54000 (22%)] Loss: -523276.937500\n",
      "Train Epoch: 904 [23040/54000 (43%)] Loss: -545263.750000\n",
      "Train Epoch: 904 [34304/54000 (64%)] Loss: -424674.968750\n",
      "Train Epoch: 904 [45568/54000 (84%)] Loss: -544172.250000\n",
      "    epoch          : 904\n",
      "    loss           : -497642.46875\n",
      "    val_loss       : -476380.9435546875\n",
      "Train Epoch: 905 [512/54000 (1%)] Loss: -459913.812500\n",
      "Train Epoch: 905 [11776/54000 (22%)] Loss: -425270.125000\n",
      "Train Epoch: 905 [23040/54000 (43%)] Loss: -542877.750000\n",
      "Train Epoch: 905 [34304/54000 (64%)] Loss: -540906.875000\n",
      "Train Epoch: 905 [45568/54000 (84%)] Loss: -423591.343750\n",
      "    epoch          : 905\n",
      "    loss           : -497952.105\n",
      "    val_loss       : -478824.946484375\n",
      "Train Epoch: 906 [512/54000 (1%)] Loss: -590338.625000\n",
      "Train Epoch: 906 [11776/54000 (22%)] Loss: -522385.343750\n",
      "Train Epoch: 906 [23040/54000 (43%)] Loss: -563484.000000\n",
      "Train Epoch: 906 [34304/54000 (64%)] Loss: -426162.250000\n",
      "Train Epoch: 906 [45568/54000 (84%)] Loss: -542945.562500\n",
      "    epoch          : 906\n",
      "    loss           : -498118.68875\n",
      "    val_loss       : -481008.39287109376\n",
      "Train Epoch: 907 [512/54000 (1%)] Loss: -588813.000000\n",
      "Train Epoch: 907 [11776/54000 (22%)] Loss: -434554.968750\n",
      "Train Epoch: 907 [23040/54000 (43%)] Loss: -430239.875000\n",
      "Train Epoch: 907 [34304/54000 (64%)] Loss: -421951.281250\n",
      "Train Epoch: 907 [45568/54000 (84%)] Loss: -457835.062500\n",
      "    epoch          : 907\n",
      "    loss           : -498430.36625\n",
      "    val_loss       : -479055.4779296875\n",
      "Train Epoch: 908 [512/54000 (1%)] Loss: -564252.437500\n",
      "Train Epoch: 908 [11776/54000 (22%)] Loss: -591944.875000\n",
      "Train Epoch: 908 [23040/54000 (43%)] Loss: -566500.500000\n",
      "Train Epoch: 908 [34304/54000 (64%)] Loss: -545484.187500\n",
      "Train Epoch: 908 [45568/54000 (84%)] Loss: -519822.250000\n",
      "    epoch          : 908\n",
      "    loss           : -498486.484375\n",
      "    val_loss       : -479271.9295898437\n",
      "Train Epoch: 909 [512/54000 (1%)] Loss: -550242.875000\n",
      "Train Epoch: 909 [11776/54000 (22%)] Loss: -550148.062500\n",
      "Train Epoch: 909 [23040/54000 (43%)] Loss: -521884.875000\n",
      "Train Epoch: 909 [34304/54000 (64%)] Loss: -541311.625000\n",
      "Train Epoch: 909 [45568/54000 (84%)] Loss: -540301.875000\n",
      "    epoch          : 909\n",
      "    loss           : -498581.369375\n",
      "    val_loss       : -476902.101171875\n",
      "Train Epoch: 910 [512/54000 (1%)] Loss: -550597.500000\n",
      "Train Epoch: 910 [11776/54000 (22%)] Loss: -520938.187500\n",
      "Train Epoch: 910 [23040/54000 (43%)] Loss: -586908.750000\n",
      "Train Epoch: 910 [34304/54000 (64%)] Loss: -589497.375000\n",
      "Train Epoch: 910 [45568/54000 (84%)] Loss: -547910.062500\n",
      "    epoch          : 910\n",
      "    loss           : -498647.115625\n",
      "    val_loss       : -478022.50732421875\n",
      "Train Epoch: 911 [512/54000 (1%)] Loss: -546300.500000\n",
      "Train Epoch: 911 [11776/54000 (22%)] Loss: -564291.000000\n",
      "Train Epoch: 911 [23040/54000 (43%)] Loss: -426232.000000\n",
      "Train Epoch: 911 [34304/54000 (64%)] Loss: -521875.187500\n",
      "Train Epoch: 911 [45568/54000 (84%)] Loss: -585761.062500\n",
      "    epoch          : 911\n",
      "    loss           : -499027.176875\n",
      "    val_loss       : -479312.37265625\n",
      "Train Epoch: 912 [512/54000 (1%)] Loss: -588661.937500\n",
      "Train Epoch: 912 [11776/54000 (22%)] Loss: -565820.000000\n",
      "Train Epoch: 912 [23040/54000 (43%)] Loss: -550258.875000\n",
      "Train Epoch: 912 [34304/54000 (64%)] Loss: -585732.125000\n",
      "Train Epoch: 912 [45568/54000 (84%)] Loss: -434125.500000\n",
      "    epoch          : 912\n",
      "    loss           : -499023.1521875\n",
      "    val_loss       : -478106.5216796875\n",
      "Train Epoch: 913 [512/54000 (1%)] Loss: -523279.843750\n",
      "Train Epoch: 913 [11776/54000 (22%)] Loss: -425266.250000\n",
      "Train Epoch: 913 [23040/54000 (43%)] Loss: -423530.312500\n",
      "Train Epoch: 913 [34304/54000 (64%)] Loss: -541548.875000\n",
      "Train Epoch: 913 [45568/54000 (84%)] Loss: -550045.000000\n",
      "    epoch          : 913\n",
      "    loss           : -499029.85375\n",
      "    val_loss       : -475808.7420898437\n",
      "Train Epoch: 914 [512/54000 (1%)] Loss: -587589.875000\n",
      "Train Epoch: 914 [11776/54000 (22%)] Loss: -460536.687500\n",
      "Train Epoch: 914 [23040/54000 (43%)] Loss: -538104.687500\n",
      "Train Epoch: 914 [34304/54000 (64%)] Loss: -427461.468750\n",
      "Train Epoch: 914 [45568/54000 (84%)] Loss: -469374.000000\n",
      "    epoch          : 914\n",
      "    loss           : -499215.51625\n",
      "    val_loss       : -478584.3673828125\n",
      "Train Epoch: 915 [512/54000 (1%)] Loss: -522721.343750\n",
      "Train Epoch: 915 [11776/54000 (22%)] Loss: -523955.750000\n",
      "Train Epoch: 915 [23040/54000 (43%)] Loss: -521757.562500\n",
      "Train Epoch: 915 [34304/54000 (64%)] Loss: -456420.343750\n",
      "Train Epoch: 915 [45568/54000 (84%)] Loss: -463302.812500\n",
      "    epoch          : 915\n",
      "    loss           : -499039.355625\n",
      "    val_loss       : -479941.55322265625\n",
      "Train Epoch: 916 [512/54000 (1%)] Loss: -420458.937500\n",
      "Train Epoch: 916 [11776/54000 (22%)] Loss: -465813.000000\n",
      "Train Epoch: 916 [23040/54000 (43%)] Loss: -568143.125000\n",
      "Train Epoch: 916 [34304/54000 (64%)] Loss: -587302.562500\n",
      "Train Epoch: 916 [45568/54000 (84%)] Loss: -547416.875000\n",
      "    epoch          : 916\n",
      "    loss           : -499406.119375\n",
      "    val_loss       : -462929.6104492188\n",
      "Train Epoch: 917 [512/54000 (1%)] Loss: -459012.000000\n",
      "Train Epoch: 917 [11776/54000 (22%)] Loss: -566055.125000\n",
      "Train Epoch: 917 [23040/54000 (43%)] Loss: -462169.781250\n",
      "Train Epoch: 917 [34304/54000 (64%)] Loss: -555902.437500\n",
      "Train Epoch: 917 [45568/54000 (84%)] Loss: -525504.812500\n",
      "    epoch          : 917\n",
      "    loss           : -499697.7284375\n",
      "    val_loss       : -479323.1250976563\n",
      "Train Epoch: 918 [512/54000 (1%)] Loss: -429057.312500\n",
      "Train Epoch: 918 [11776/54000 (22%)] Loss: -522532.531250\n",
      "Train Epoch: 918 [23040/54000 (43%)] Loss: -568097.250000\n",
      "Train Epoch: 918 [34304/54000 (64%)] Loss: -524909.062500\n",
      "Train Epoch: 918 [45568/54000 (84%)] Loss: -542470.937500\n",
      "    epoch          : 918\n",
      "    loss           : -499903.278125\n",
      "    val_loss       : -478995.64326171874\n",
      "Train Epoch: 919 [512/54000 (1%)] Loss: -473346.812500\n",
      "Train Epoch: 919 [11776/54000 (22%)] Loss: -437882.375000\n",
      "Train Epoch: 919 [23040/54000 (43%)] Loss: -566115.750000\n",
      "Train Epoch: 919 [34304/54000 (64%)] Loss: -546840.125000\n",
      "Train Epoch: 919 [45568/54000 (84%)] Loss: -549919.187500\n",
      "    epoch          : 919\n",
      "    loss           : -499944.343125\n",
      "    val_loss       : -477510.4922851563\n",
      "Train Epoch: 920 [512/54000 (1%)] Loss: -552226.437500\n",
      "Train Epoch: 920 [11776/54000 (22%)] Loss: -566499.187500\n",
      "Train Epoch: 920 [23040/54000 (43%)] Loss: -594800.875000\n",
      "Train Epoch: 920 [34304/54000 (64%)] Loss: -569459.562500\n",
      "Train Epoch: 920 [45568/54000 (84%)] Loss: -546809.812500\n",
      "    epoch          : 920\n",
      "    loss           : -500218.26125\n",
      "    val_loss       : -472024.55732421874\n",
      "Train Epoch: 921 [512/54000 (1%)] Loss: -525755.187500\n",
      "Train Epoch: 921 [11776/54000 (22%)] Loss: -423087.812500\n",
      "Train Epoch: 921 [23040/54000 (43%)] Loss: -545936.000000\n",
      "Train Epoch: 921 [34304/54000 (64%)] Loss: -568230.000000\n",
      "Train Epoch: 921 [45568/54000 (84%)] Loss: -435430.125000\n",
      "    epoch          : 921\n",
      "    loss           : -500330.4740625\n",
      "    val_loss       : -479783.1638671875\n",
      "Train Epoch: 922 [512/54000 (1%)] Loss: -435552.843750\n",
      "Train Epoch: 922 [11776/54000 (22%)] Loss: -458019.218750\n",
      "Train Epoch: 922 [23040/54000 (43%)] Loss: -523515.812500\n",
      "Train Epoch: 922 [34304/54000 (64%)] Loss: -428289.125000\n",
      "Train Epoch: 922 [45568/54000 (84%)] Loss: -427019.125000\n",
      "    epoch          : 922\n",
      "    loss           : -500536.3215625\n",
      "    val_loss       : -479751.20732421876\n",
      "Train Epoch: 923 [512/54000 (1%)] Loss: -469065.468750\n",
      "Train Epoch: 923 [11776/54000 (22%)] Loss: -567823.000000\n",
      "Train Epoch: 923 [23040/54000 (43%)] Loss: -525962.687500\n",
      "Train Epoch: 923 [34304/54000 (64%)] Loss: -418180.187500\n",
      "Train Epoch: 923 [45568/54000 (84%)] Loss: -547618.375000\n",
      "    epoch          : 923\n",
      "    loss           : -500649.0546875\n",
      "    val_loss       : -478072.1943359375\n",
      "Train Epoch: 924 [512/54000 (1%)] Loss: -523012.625000\n",
      "Train Epoch: 924 [11776/54000 (22%)] Loss: -522757.281250\n",
      "Train Epoch: 924 [23040/54000 (43%)] Loss: -465391.531250\n",
      "Train Epoch: 924 [34304/54000 (64%)] Loss: -565583.500000\n",
      "Train Epoch: 924 [45568/54000 (84%)] Loss: -524521.375000\n",
      "    epoch          : 924\n",
      "    loss           : -500584.48875\n",
      "    val_loss       : -482669.4364257812\n",
      "Train Epoch: 925 [512/54000 (1%)] Loss: -546415.500000\n",
      "Train Epoch: 925 [11776/54000 (22%)] Loss: -461010.687500\n",
      "Train Epoch: 925 [23040/54000 (43%)] Loss: -462135.812500\n",
      "Train Epoch: 925 [34304/54000 (64%)] Loss: -465926.250000\n",
      "Train Epoch: 925 [45568/54000 (84%)] Loss: -428640.687500\n",
      "    epoch          : 925\n",
      "    loss           : -500521.1640625\n",
      "    val_loss       : -480411.2755859375\n",
      "Train Epoch: 926 [512/54000 (1%)] Loss: -437397.250000\n",
      "Train Epoch: 926 [11776/54000 (22%)] Loss: -429282.531250\n",
      "Train Epoch: 926 [23040/54000 (43%)] Loss: -423301.187500\n",
      "Train Epoch: 926 [34304/54000 (64%)] Loss: -429425.531250\n",
      "Train Epoch: 926 [45568/54000 (84%)] Loss: -551453.125000\n",
      "    epoch          : 926\n",
      "    loss           : -500809.6046875\n",
      "    val_loss       : -481513.4025390625\n",
      "Train Epoch: 927 [512/54000 (1%)] Loss: -568313.750000\n",
      "Train Epoch: 927 [11776/54000 (22%)] Loss: -472031.937500\n",
      "Train Epoch: 927 [23040/54000 (43%)] Loss: -545741.875000\n",
      "Train Epoch: 927 [34304/54000 (64%)] Loss: -470386.687500\n",
      "Train Epoch: 927 [45568/54000 (84%)] Loss: -544450.937500\n",
      "    epoch          : 927\n",
      "    loss           : -500922.0465625\n",
      "    val_loss       : -480687.31005859375\n",
      "Train Epoch: 928 [512/54000 (1%)] Loss: -464463.281250\n",
      "Train Epoch: 928 [11776/54000 (22%)] Loss: -551084.375000\n",
      "Train Epoch: 928 [23040/54000 (43%)] Loss: -432249.718750\n",
      "Train Epoch: 928 [34304/54000 (64%)] Loss: -551901.625000\n",
      "Train Epoch: 928 [45568/54000 (84%)] Loss: -546059.687500\n",
      "    epoch          : 928\n",
      "    loss           : -501316.1140625\n",
      "    val_loss       : -478297.52080078126\n",
      "Train Epoch: 929 [512/54000 (1%)] Loss: -433004.375000\n",
      "Train Epoch: 929 [11776/54000 (22%)] Loss: -523552.250000\n",
      "Train Epoch: 929 [23040/54000 (43%)] Loss: -592949.750000\n",
      "Train Epoch: 929 [34304/54000 (64%)] Loss: -463655.375000\n",
      "Train Epoch: 929 [45568/54000 (84%)] Loss: -524153.062500\n",
      "    epoch          : 929\n",
      "    loss           : -501307.258125\n",
      "    val_loss       : -483257.3119140625\n",
      "Train Epoch: 930 [512/54000 (1%)] Loss: -570969.875000\n",
      "Train Epoch: 930 [11776/54000 (22%)] Loss: -525747.000000\n",
      "Train Epoch: 930 [23040/54000 (43%)] Loss: -442019.218750\n",
      "Train Epoch: 930 [34304/54000 (64%)] Loss: -543501.125000\n",
      "Train Epoch: 930 [45568/54000 (84%)] Loss: -591883.250000\n",
      "    epoch          : 930\n",
      "    loss           : -501537.3321875\n",
      "    val_loss       : -480238.8732421875\n",
      "Train Epoch: 931 [512/54000 (1%)] Loss: -569353.562500\n",
      "Train Epoch: 931 [11776/54000 (22%)] Loss: -463501.625000\n",
      "Train Epoch: 931 [23040/54000 (43%)] Loss: -592005.875000\n",
      "Train Epoch: 931 [34304/54000 (64%)] Loss: -438937.937500\n",
      "Train Epoch: 931 [45568/54000 (84%)] Loss: -524221.312500\n",
      "    epoch          : 931\n",
      "    loss           : -501610.3290625\n",
      "    val_loss       : -476371.95185546874\n",
      "Train Epoch: 932 [512/54000 (1%)] Loss: -441134.687500\n",
      "Train Epoch: 932 [11776/54000 (22%)] Loss: -439255.406250\n",
      "Train Epoch: 932 [23040/54000 (43%)] Loss: -550834.875000\n",
      "Train Epoch: 932 [34304/54000 (64%)] Loss: -569469.125000\n",
      "Train Epoch: 932 [45568/54000 (84%)] Loss: -590404.437500\n",
      "    epoch          : 932\n",
      "    loss           : -501968.245625\n",
      "    val_loss       : -481344.90986328124\n",
      "Train Epoch: 933 [512/54000 (1%)] Loss: -544929.000000\n",
      "Train Epoch: 933 [11776/54000 (22%)] Loss: -435037.156250\n",
      "Train Epoch: 933 [23040/54000 (43%)] Loss: -462758.000000\n",
      "Train Epoch: 933 [34304/54000 (64%)] Loss: -436062.312500\n",
      "Train Epoch: 933 [45568/54000 (84%)] Loss: -568798.687500\n",
      "    epoch          : 933\n",
      "    loss           : -501837.9009375\n",
      "    val_loss       : -481342.29560546874\n",
      "Train Epoch: 934 [512/54000 (1%)] Loss: -568906.625000\n",
      "Train Epoch: 934 [11776/54000 (22%)] Loss: -569240.375000\n",
      "Train Epoch: 934 [23040/54000 (43%)] Loss: -474240.062500\n",
      "Train Epoch: 934 [34304/54000 (64%)] Loss: -429854.625000\n",
      "Train Epoch: 934 [45568/54000 (84%)] Loss: -476033.875000\n",
      "    epoch          : 934\n",
      "    loss           : -502172.9371875\n",
      "    val_loss       : -482357.39189453126\n",
      "Train Epoch: 935 [512/54000 (1%)] Loss: -591975.250000\n",
      "Train Epoch: 935 [11776/54000 (22%)] Loss: -591753.812500\n",
      "Train Epoch: 935 [23040/54000 (43%)] Loss: -591752.687500\n",
      "Train Epoch: 935 [34304/54000 (64%)] Loss: -469449.281250\n",
      "Train Epoch: 935 [45568/54000 (84%)] Loss: -554366.312500\n",
      "    epoch          : 935\n",
      "    loss           : -502377.500625\n",
      "    val_loss       : -482676.2208007813\n",
      "Train Epoch: 936 [512/54000 (1%)] Loss: -473736.406250\n",
      "Train Epoch: 936 [11776/54000 (22%)] Loss: -436903.031250\n",
      "Train Epoch: 936 [23040/54000 (43%)] Loss: -469645.687500\n",
      "Train Epoch: 936 [34304/54000 (64%)] Loss: -569173.937500\n",
      "Train Epoch: 936 [45568/54000 (84%)] Loss: -560401.687500\n",
      "    epoch          : 936\n",
      "    loss           : -502369.030625\n",
      "    val_loss       : -479698.2142578125\n",
      "Train Epoch: 937 [512/54000 (1%)] Loss: -591260.000000\n",
      "Train Epoch: 937 [11776/54000 (22%)] Loss: -548779.125000\n",
      "Train Epoch: 937 [23040/54000 (43%)] Loss: -427120.218750\n",
      "Train Epoch: 937 [34304/54000 (64%)] Loss: -525048.562500\n",
      "Train Epoch: 937 [45568/54000 (84%)] Loss: -525984.437500\n",
      "    epoch          : 937\n",
      "    loss           : -502612.2\n",
      "    val_loss       : -484254.772265625\n",
      "Train Epoch: 938 [512/54000 (1%)] Loss: -435254.062500\n",
      "Train Epoch: 938 [11776/54000 (22%)] Loss: -430579.187500\n",
      "Train Epoch: 938 [23040/54000 (43%)] Loss: -477824.062500\n",
      "Train Epoch: 938 [34304/54000 (64%)] Loss: -549194.625000\n",
      "Train Epoch: 938 [45568/54000 (84%)] Loss: -525808.562500\n",
      "    epoch          : 938\n",
      "    loss           : -502585.529375\n",
      "    val_loss       : -478693.7767578125\n",
      "Train Epoch: 939 [512/54000 (1%)] Loss: -592933.375000\n",
      "Train Epoch: 939 [11776/54000 (22%)] Loss: -467627.875000\n",
      "Train Epoch: 939 [23040/54000 (43%)] Loss: -554822.687500\n",
      "Train Epoch: 939 [34304/54000 (64%)] Loss: -546700.000000\n",
      "Train Epoch: 939 [45568/54000 (84%)] Loss: -435378.718750\n",
      "    epoch          : 939\n",
      "    loss           : -502622.5028125\n",
      "    val_loss       : -480598.73671875\n",
      "Train Epoch: 940 [512/54000 (1%)] Loss: -436123.937500\n",
      "Train Epoch: 940 [11776/54000 (22%)] Loss: -426768.750000\n",
      "Train Epoch: 940 [23040/54000 (43%)] Loss: -425180.437500\n",
      "Train Epoch: 940 [34304/54000 (64%)] Loss: -547568.500000\n",
      "Train Epoch: 940 [45568/54000 (84%)] Loss: -439151.187500\n",
      "    epoch          : 940\n",
      "    loss           : -503068.661875\n",
      "    val_loss       : -483918.52080078126\n",
      "Train Epoch: 941 [512/54000 (1%)] Loss: -433267.781250\n",
      "Train Epoch: 941 [11776/54000 (22%)] Loss: -529259.937500\n",
      "Train Epoch: 941 [23040/54000 (43%)] Loss: -438262.937500\n",
      "Train Epoch: 941 [34304/54000 (64%)] Loss: -568870.812500\n",
      "Train Epoch: 941 [45568/54000 (84%)] Loss: -473641.718750\n",
      "    epoch          : 941\n",
      "    loss           : -503128.5959375\n",
      "    val_loss       : -479504.1495117188\n",
      "Train Epoch: 942 [512/54000 (1%)] Loss: -568630.125000\n",
      "Train Epoch: 942 [11776/54000 (22%)] Loss: -546043.250000\n",
      "Train Epoch: 942 [23040/54000 (43%)] Loss: -527728.062500\n",
      "Train Epoch: 942 [34304/54000 (64%)] Loss: -527300.875000\n",
      "Train Epoch: 942 [45568/54000 (84%)] Loss: -545529.000000\n",
      "    epoch          : 942\n",
      "    loss           : -503308.294375\n",
      "    val_loss       : -481909.11005859374\n",
      "Train Epoch: 943 [512/54000 (1%)] Loss: -528747.187500\n",
      "Train Epoch: 943 [11776/54000 (22%)] Loss: -432076.062500\n",
      "Train Epoch: 943 [23040/54000 (43%)] Loss: -431292.250000\n",
      "Train Epoch: 943 [34304/54000 (64%)] Loss: -471559.531250\n",
      "Train Epoch: 943 [45568/54000 (84%)] Loss: -547726.062500\n",
      "    epoch          : 943\n",
      "    loss           : -503006.1109375\n",
      "    val_loss       : -483077.5704101563\n",
      "Train Epoch: 944 [512/54000 (1%)] Loss: -568877.500000\n",
      "Train Epoch: 944 [11776/54000 (22%)] Loss: -550539.250000\n",
      "Train Epoch: 944 [23040/54000 (43%)] Loss: -548930.500000\n",
      "Train Epoch: 944 [34304/54000 (64%)] Loss: -525446.875000\n",
      "Train Epoch: 944 [45568/54000 (84%)] Loss: -477867.625000\n",
      "    epoch          : 944\n",
      "    loss           : -503464.465\n",
      "    val_loss       : -485719.3915039062\n",
      "Train Epoch: 945 [512/54000 (1%)] Loss: -437912.031250\n",
      "Train Epoch: 945 [11776/54000 (22%)] Loss: -472973.312500\n",
      "Train Epoch: 945 [23040/54000 (43%)] Loss: -472875.562500\n",
      "Train Epoch: 945 [34304/54000 (64%)] Loss: -430918.312500\n",
      "Train Epoch: 945 [45568/54000 (84%)] Loss: -430165.187500\n",
      "    epoch          : 945\n",
      "    loss           : -503647.6346875\n",
      "    val_loss       : -482391.4759765625\n",
      "Train Epoch: 946 [512/54000 (1%)] Loss: -432474.750000\n",
      "Train Epoch: 946 [11776/54000 (22%)] Loss: -571465.500000\n",
      "Train Epoch: 946 [23040/54000 (43%)] Loss: -440253.781250\n",
      "Train Epoch: 946 [34304/54000 (64%)] Loss: -442409.343750\n",
      "Train Epoch: 946 [45568/54000 (84%)] Loss: -553312.187500\n",
      "    epoch          : 946\n",
      "    loss           : -503797.1409375\n",
      "    val_loss       : -477169.819140625\n",
      "Train Epoch: 947 [512/54000 (1%)] Loss: -553889.875000\n",
      "Train Epoch: 947 [11776/54000 (22%)] Loss: -553174.812500\n",
      "Train Epoch: 947 [23040/54000 (43%)] Loss: -570989.250000\n",
      "Train Epoch: 947 [34304/54000 (64%)] Loss: -431818.968750\n",
      "Train Epoch: 947 [45568/54000 (84%)] Loss: -546411.312500\n",
      "    epoch          : 947\n",
      "    loss           : -503684.2159375\n",
      "    val_loss       : -481029.39580078126\n",
      "Train Epoch: 948 [512/54000 (1%)] Loss: -439072.437500\n",
      "Train Epoch: 948 [11776/54000 (22%)] Loss: -594126.312500\n",
      "Train Epoch: 948 [23040/54000 (43%)] Loss: -570310.937500\n",
      "Train Epoch: 948 [34304/54000 (64%)] Loss: -440831.687500\n",
      "Train Epoch: 948 [45568/54000 (84%)] Loss: -553186.250000\n",
      "    epoch          : 948\n",
      "    loss           : -504005.796875\n",
      "    val_loss       : -482646.008984375\n",
      "Train Epoch: 949 [512/54000 (1%)] Loss: -428151.875000\n",
      "Train Epoch: 949 [11776/54000 (22%)] Loss: -479513.187500\n",
      "Train Epoch: 949 [23040/54000 (43%)] Loss: -469612.687500\n",
      "Train Epoch: 949 [34304/54000 (64%)] Loss: -593513.250000\n",
      "Train Epoch: 949 [45568/54000 (84%)] Loss: -519406.968750\n",
      "    epoch          : 949\n",
      "    loss           : -503995.7378125\n",
      "    val_loss       : -486256.15107421874\n",
      "Train Epoch: 950 [512/54000 (1%)] Loss: -594295.000000\n",
      "Train Epoch: 950 [11776/54000 (22%)] Loss: -571186.062500\n",
      "Train Epoch: 950 [23040/54000 (43%)] Loss: -599675.437500\n",
      "Train Epoch: 950 [34304/54000 (64%)] Loss: -433527.500000\n",
      "Train Epoch: 950 [45568/54000 (84%)] Loss: -429134.031250\n",
      "    epoch          : 950\n",
      "    loss           : -504411.7903125\n",
      "    val_loss       : -479586.77265625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch950.pth ...\n",
      "Train Epoch: 951 [512/54000 (1%)] Loss: -470756.250000\n",
      "Train Epoch: 951 [11776/54000 (22%)] Loss: -429891.125000\n",
      "Train Epoch: 951 [23040/54000 (43%)] Loss: -525620.937500\n",
      "Train Epoch: 951 [34304/54000 (64%)] Loss: -594477.000000\n",
      "Train Epoch: 951 [45568/54000 (84%)] Loss: -553888.062500\n",
      "    epoch          : 951\n",
      "    loss           : -504570.73875\n",
      "    val_loss       : -479757.8140625\n",
      "Train Epoch: 952 [512/54000 (1%)] Loss: -549422.687500\n",
      "Train Epoch: 952 [11776/54000 (22%)] Loss: -430187.281250\n",
      "Train Epoch: 952 [23040/54000 (43%)] Loss: -474817.312500\n",
      "Train Epoch: 952 [34304/54000 (64%)] Loss: -442130.375000\n",
      "Train Epoch: 952 [45568/54000 (84%)] Loss: -429086.875000\n",
      "    epoch          : 952\n",
      "    loss           : -504699.341875\n",
      "    val_loss       : -473993.3966796875\n",
      "Train Epoch: 953 [512/54000 (1%)] Loss: -552448.375000\n",
      "Train Epoch: 953 [11776/54000 (22%)] Loss: -558549.625000\n",
      "Train Epoch: 953 [23040/54000 (43%)] Loss: -572624.437500\n",
      "Train Epoch: 953 [34304/54000 (64%)] Loss: -431169.625000\n",
      "Train Epoch: 953 [45568/54000 (84%)] Loss: -423881.375000\n",
      "    epoch          : 953\n",
      "    loss           : -504681.6096875\n",
      "    val_loss       : -485406.622265625\n",
      "Train Epoch: 954 [512/54000 (1%)] Loss: -552690.750000\n",
      "Train Epoch: 954 [11776/54000 (22%)] Loss: -430162.125000\n",
      "Train Epoch: 954 [23040/54000 (43%)] Loss: -529448.562500\n",
      "Train Epoch: 954 [34304/54000 (64%)] Loss: -594320.562500\n",
      "Train Epoch: 954 [45568/54000 (84%)] Loss: -553531.562500\n",
      "    epoch          : 954\n",
      "    loss           : -504570.820625\n",
      "    val_loss       : -484814.47314453125\n",
      "Train Epoch: 955 [512/54000 (1%)] Loss: -427015.625000\n",
      "Train Epoch: 955 [11776/54000 (22%)] Loss: -462068.843750\n",
      "Train Epoch: 955 [23040/54000 (43%)] Loss: -439831.968750\n",
      "Train Epoch: 955 [34304/54000 (64%)] Loss: -527046.750000\n",
      "Train Epoch: 955 [45568/54000 (84%)] Loss: -528525.625000\n",
      "    epoch          : 955\n",
      "    loss           : -504807.43875\n",
      "    val_loss       : -487621.2305664063\n",
      "Train Epoch: 956 [512/54000 (1%)] Loss: -425272.218750\n",
      "Train Epoch: 956 [11776/54000 (22%)] Loss: -426825.125000\n",
      "Train Epoch: 956 [23040/54000 (43%)] Loss: -431041.156250\n",
      "Train Epoch: 956 [34304/54000 (64%)] Loss: -552856.437500\n",
      "Train Epoch: 956 [45568/54000 (84%)] Loss: -555357.250000\n",
      "    epoch          : 956\n",
      "    loss           : -505089.5259375\n",
      "    val_loss       : -485837.62939453125\n",
      "Train Epoch: 957 [512/54000 (1%)] Loss: -475790.750000\n",
      "Train Epoch: 957 [11776/54000 (22%)] Loss: -529912.000000\n",
      "Train Epoch: 957 [23040/54000 (43%)] Loss: -555082.812500\n",
      "Train Epoch: 957 [34304/54000 (64%)] Loss: -559679.500000\n",
      "Train Epoch: 957 [45568/54000 (84%)] Loss: -528644.750000\n",
      "    epoch          : 957\n",
      "    loss           : -505255.0859375\n",
      "    val_loss       : -480488.56806640624\n",
      "Train Epoch: 958 [512/54000 (1%)] Loss: -437726.781250\n",
      "Train Epoch: 958 [11776/54000 (22%)] Loss: -463606.531250\n",
      "Train Epoch: 958 [23040/54000 (43%)] Loss: -436299.906250\n",
      "Train Epoch: 958 [34304/54000 (64%)] Loss: -444212.906250\n",
      "Train Epoch: 958 [45568/54000 (84%)] Loss: -431876.843750\n",
      "    epoch          : 958\n",
      "    loss           : -505442.449375\n",
      "    val_loss       : -482482.94873046875\n",
      "Train Epoch: 959 [512/54000 (1%)] Loss: -448002.437500\n",
      "Train Epoch: 959 [11776/54000 (22%)] Loss: -573418.875000\n",
      "Train Epoch: 959 [23040/54000 (43%)] Loss: -594488.500000\n",
      "Train Epoch: 959 [34304/54000 (64%)] Loss: -476660.000000\n",
      "Train Epoch: 959 [45568/54000 (84%)] Loss: -554439.000000\n",
      "    epoch          : 959\n",
      "    loss           : -505499.7153125\n",
      "    val_loss       : -479167.7779785156\n",
      "Train Epoch: 960 [512/54000 (1%)] Loss: -441488.812500\n",
      "Train Epoch: 960 [11776/54000 (22%)] Loss: -528105.562500\n",
      "Train Epoch: 960 [23040/54000 (43%)] Loss: -434955.343750\n",
      "Train Epoch: 960 [34304/54000 (64%)] Loss: -549415.125000\n",
      "Train Epoch: 960 [45568/54000 (84%)] Loss: -530017.500000\n",
      "    epoch          : 960\n",
      "    loss           : -505772.6121875\n",
      "    val_loss       : -482167.85185546876\n",
      "Train Epoch: 961 [512/54000 (1%)] Loss: -526402.250000\n",
      "Train Epoch: 961 [11776/54000 (22%)] Loss: -433081.250000\n",
      "Train Epoch: 961 [23040/54000 (43%)] Loss: -446419.875000\n",
      "Train Epoch: 961 [34304/54000 (64%)] Loss: -596963.312500\n",
      "Train Epoch: 961 [45568/54000 (84%)] Loss: -551739.187500\n",
      "    epoch          : 961\n",
      "    loss           : -505857.6025\n",
      "    val_loss       : -483895.8165039063\n",
      "Train Epoch: 962 [512/54000 (1%)] Loss: -435354.875000\n",
      "Train Epoch: 962 [11776/54000 (22%)] Loss: -574368.437500\n",
      "Train Epoch: 962 [23040/54000 (43%)] Loss: -446948.250000\n",
      "Train Epoch: 962 [34304/54000 (64%)] Loss: -468807.031250\n",
      "Train Epoch: 962 [45568/54000 (84%)] Loss: -554858.500000\n",
      "    epoch          : 962\n",
      "    loss           : -505867.8096875\n",
      "    val_loss       : -482544.63740234374\n",
      "Train Epoch: 963 [512/54000 (1%)] Loss: -480885.687500\n",
      "Train Epoch: 963 [11776/54000 (22%)] Loss: -574802.437500\n",
      "Train Epoch: 963 [23040/54000 (43%)] Loss: -573114.875000\n",
      "Train Epoch: 963 [34304/54000 (64%)] Loss: -436388.937500\n",
      "Train Epoch: 963 [45568/54000 (84%)] Loss: -428970.375000\n",
      "    epoch          : 963\n",
      "    loss           : -505961.41\n",
      "    val_loss       : -485505.17255859374\n",
      "Train Epoch: 964 [512/54000 (1%)] Loss: -476718.093750\n",
      "Train Epoch: 964 [11776/54000 (22%)] Loss: -472231.812500\n",
      "Train Epoch: 964 [23040/54000 (43%)] Loss: -437150.937500\n",
      "Train Epoch: 964 [34304/54000 (64%)] Loss: -440984.906250\n",
      "Train Epoch: 964 [45568/54000 (84%)] Loss: -475637.281250\n",
      "    epoch          : 964\n",
      "    loss           : -506331.28625\n",
      "    val_loss       : -487107.6844726562\n",
      "Train Epoch: 965 [512/54000 (1%)] Loss: -573350.812500\n",
      "Train Epoch: 965 [11776/54000 (22%)] Loss: -428267.781250\n",
      "Train Epoch: 965 [23040/54000 (43%)] Loss: -598160.250000\n",
      "Train Epoch: 965 [34304/54000 (64%)] Loss: -473413.625000\n",
      "Train Epoch: 965 [45568/54000 (84%)] Loss: -553671.125000\n",
      "    epoch          : 965\n",
      "    loss           : -506447.341875\n",
      "    val_loss       : -480243.2115234375\n",
      "Train Epoch: 966 [512/54000 (1%)] Loss: -575860.937500\n",
      "Train Epoch: 966 [11776/54000 (22%)] Loss: -448465.531250\n",
      "Train Epoch: 966 [23040/54000 (43%)] Loss: -531254.250000\n",
      "Train Epoch: 966 [34304/54000 (64%)] Loss: -435712.187500\n",
      "Train Epoch: 966 [45568/54000 (84%)] Loss: -531960.750000\n",
      "    epoch          : 966\n",
      "    loss           : -506570.390625\n",
      "    val_loss       : -483885.0931640625\n",
      "Train Epoch: 967 [512/54000 (1%)] Loss: -435587.468750\n",
      "Train Epoch: 967 [11776/54000 (22%)] Loss: -481826.375000\n",
      "Train Epoch: 967 [23040/54000 (43%)] Loss: -573592.500000\n",
      "Train Epoch: 967 [34304/54000 (64%)] Loss: -482443.968750\n",
      "Train Epoch: 967 [45568/54000 (84%)] Loss: -550502.187500\n",
      "    epoch          : 967\n",
      "    loss           : -506536.24625\n",
      "    val_loss       : -488053.9540039062\n",
      "Train Epoch: 968 [512/54000 (1%)] Loss: -577175.250000\n",
      "Train Epoch: 968 [11776/54000 (22%)] Loss: -562428.375000\n",
      "Train Epoch: 968 [23040/54000 (43%)] Loss: -443537.187500\n",
      "Train Epoch: 968 [34304/54000 (64%)] Loss: -467224.906250\n",
      "Train Epoch: 968 [45568/54000 (84%)] Loss: -431871.062500\n",
      "    epoch          : 968\n",
      "    loss           : -506797.1615625\n",
      "    val_loss       : -479898.2455566406\n",
      "Train Epoch: 969 [512/54000 (1%)] Loss: -557584.687500\n",
      "Train Epoch: 969 [11776/54000 (22%)] Loss: -432723.125000\n",
      "Train Epoch: 969 [23040/54000 (43%)] Loss: -447687.562500\n",
      "Train Epoch: 969 [34304/54000 (64%)] Loss: -433248.437500\n",
      "Train Epoch: 969 [45568/54000 (84%)] Loss: -429443.812500\n",
      "    epoch          : 969\n",
      "    loss           : -506841.6559375\n",
      "    val_loss       : -480789.8504882812\n",
      "Train Epoch: 970 [512/54000 (1%)] Loss: -574469.562500\n",
      "Train Epoch: 970 [11776/54000 (22%)] Loss: -432467.625000\n",
      "Train Epoch: 970 [23040/54000 (43%)] Loss: -471630.031250\n",
      "Train Epoch: 970 [34304/54000 (64%)] Loss: -438626.718750\n",
      "Train Epoch: 970 [45568/54000 (84%)] Loss: -532470.062500\n",
      "    epoch          : 970\n",
      "    loss           : -506787.300625\n",
      "    val_loss       : -485571.9579101562\n",
      "Train Epoch: 971 [512/54000 (1%)] Loss: -552733.687500\n",
      "Train Epoch: 971 [11776/54000 (22%)] Loss: -535058.750000\n",
      "Train Epoch: 971 [23040/54000 (43%)] Loss: -428080.156250\n",
      "Train Epoch: 971 [34304/54000 (64%)] Loss: -598428.250000\n",
      "Train Epoch: 971 [45568/54000 (84%)] Loss: -553654.875000\n",
      "    epoch          : 971\n",
      "    loss           : -507115.98875\n",
      "    val_loss       : -485856.70732421876\n",
      "Train Epoch: 972 [512/54000 (1%)] Loss: -484107.062500\n",
      "Train Epoch: 972 [11776/54000 (22%)] Loss: -555089.500000\n",
      "Train Epoch: 972 [23040/54000 (43%)] Loss: -477991.468750\n",
      "Train Epoch: 972 [34304/54000 (64%)] Loss: -553599.875000\n",
      "Train Epoch: 972 [45568/54000 (84%)] Loss: -525634.750000\n",
      "    epoch          : 972\n",
      "    loss           : -507404.818125\n",
      "    val_loss       : -488362.079296875\n",
      "Train Epoch: 973 [512/54000 (1%)] Loss: -598751.625000\n",
      "Train Epoch: 973 [11776/54000 (22%)] Loss: -599948.625000\n",
      "Train Epoch: 973 [23040/54000 (43%)] Loss: -577404.875000\n",
      "Train Epoch: 973 [34304/54000 (64%)] Loss: -533434.875000\n",
      "Train Epoch: 973 [45568/54000 (84%)] Loss: -557573.437500\n",
      "    epoch          : 973\n",
      "    loss           : -507544.2334375\n",
      "    val_loss       : -487770.6250976563\n",
      "Train Epoch: 974 [512/54000 (1%)] Loss: -470206.593750\n",
      "Train Epoch: 974 [11776/54000 (22%)] Loss: -576516.875000\n",
      "Train Epoch: 974 [23040/54000 (43%)] Loss: -529584.000000\n",
      "Train Epoch: 974 [34304/54000 (64%)] Loss: -467672.250000\n",
      "Train Epoch: 974 [45568/54000 (84%)] Loss: -531147.125000\n",
      "    epoch          : 974\n",
      "    loss           : -507595.8646875\n",
      "    val_loss       : -482911.30537109374\n",
      "Train Epoch: 975 [512/54000 (1%)] Loss: -598636.000000\n",
      "Train Epoch: 975 [11776/54000 (22%)] Loss: -442469.218750\n",
      "Train Epoch: 975 [23040/54000 (43%)] Loss: -595546.937500\n",
      "Train Epoch: 975 [34304/54000 (64%)] Loss: -440664.781250\n",
      "Train Epoch: 975 [45568/54000 (84%)] Loss: -533289.812500\n",
      "    epoch          : 975\n",
      "    loss           : -507621.435625\n",
      "    val_loss       : -480293.21337890625\n",
      "Train Epoch: 976 [512/54000 (1%)] Loss: -559283.625000\n",
      "Train Epoch: 976 [11776/54000 (22%)] Loss: -479930.281250\n",
      "Train Epoch: 976 [23040/54000 (43%)] Loss: -529132.812500\n",
      "Train Epoch: 976 [34304/54000 (64%)] Loss: -555614.625000\n",
      "Train Epoch: 976 [45568/54000 (84%)] Loss: -530717.500000\n",
      "    epoch          : 976\n",
      "    loss           : -507989.883125\n",
      "    val_loss       : -484069.6805664062\n",
      "Train Epoch: 977 [512/54000 (1%)] Loss: -480102.031250\n",
      "Train Epoch: 977 [11776/54000 (22%)] Loss: -430820.562500\n",
      "Train Epoch: 977 [23040/54000 (43%)] Loss: -556941.562500\n",
      "Train Epoch: 977 [34304/54000 (64%)] Loss: -598097.250000\n",
      "Train Epoch: 977 [45568/54000 (84%)] Loss: -530692.625000\n",
      "    epoch          : 977\n",
      "    loss           : -507821.2028125\n",
      "    val_loss       : -484537.8669921875\n",
      "Train Epoch: 978 [512/54000 (1%)] Loss: -442469.312500\n",
      "Train Epoch: 978 [11776/54000 (22%)] Loss: -433156.375000\n",
      "Train Epoch: 978 [23040/54000 (43%)] Loss: -557115.125000\n",
      "Train Epoch: 978 [34304/54000 (64%)] Loss: -480741.125000\n",
      "Train Epoch: 978 [45568/54000 (84%)] Loss: -561036.500000\n",
      "    epoch          : 978\n",
      "    loss           : -508191.5121875\n",
      "    val_loss       : -485587.8240234375\n",
      "Train Epoch: 979 [512/54000 (1%)] Loss: -596231.000000\n",
      "Train Epoch: 979 [11776/54000 (22%)] Loss: -479927.437500\n",
      "Train Epoch: 979 [23040/54000 (43%)] Loss: -429460.187500\n",
      "Train Epoch: 979 [34304/54000 (64%)] Loss: -560342.875000\n",
      "Train Epoch: 979 [45568/54000 (84%)] Loss: -434033.718750\n",
      "    epoch          : 979\n",
      "    loss           : -508465.44375\n",
      "    val_loss       : -482826.6296875\n",
      "Train Epoch: 980 [512/54000 (1%)] Loss: -576801.750000\n",
      "Train Epoch: 980 [11776/54000 (22%)] Loss: -441142.125000\n",
      "Train Epoch: 980 [23040/54000 (43%)] Loss: -606804.562500\n",
      "Train Epoch: 980 [34304/54000 (64%)] Loss: -597949.312500\n",
      "Train Epoch: 980 [45568/54000 (84%)] Loss: -556651.062500\n",
      "    epoch          : 980\n",
      "    loss           : -508287.9728125\n",
      "    val_loss       : -485224.45\n",
      "Train Epoch: 981 [512/54000 (1%)] Loss: -596894.937500\n",
      "Train Epoch: 981 [11776/54000 (22%)] Loss: -600856.000000\n",
      "Train Epoch: 981 [23040/54000 (43%)] Loss: -557081.375000\n",
      "Train Epoch: 981 [34304/54000 (64%)] Loss: -555620.875000\n",
      "Train Epoch: 981 [45568/54000 (84%)] Loss: -557878.500000\n",
      "    epoch          : 981\n",
      "    loss           : -508546.954375\n",
      "    val_loss       : -485231.75927734375\n",
      "Train Epoch: 982 [512/54000 (1%)] Loss: -437710.843750\n",
      "Train Epoch: 982 [11776/54000 (22%)] Loss: -601089.937500\n",
      "Train Epoch: 982 [23040/54000 (43%)] Loss: -575799.500000\n",
      "Train Epoch: 982 [34304/54000 (64%)] Loss: -559678.500000\n",
      "Train Epoch: 982 [45568/54000 (84%)] Loss: -555430.187500\n",
      "    epoch          : 982\n",
      "    loss           : -508638.885\n",
      "    val_loss       : -484222.208984375\n",
      "Train Epoch: 983 [512/54000 (1%)] Loss: -575904.125000\n",
      "Train Epoch: 983 [11776/54000 (22%)] Loss: -554935.500000\n",
      "Train Epoch: 983 [23040/54000 (43%)] Loss: -479840.281250\n",
      "Train Epoch: 983 [34304/54000 (64%)] Loss: -469212.750000\n",
      "Train Epoch: 983 [45568/54000 (84%)] Loss: -468561.437500\n",
      "    epoch          : 983\n",
      "    loss           : -508777.1309375\n",
      "    val_loss       : -485882.36689453124\n",
      "Train Epoch: 984 [512/54000 (1%)] Loss: -575379.437500\n",
      "Train Epoch: 984 [11776/54000 (22%)] Loss: -557093.125000\n",
      "Train Epoch: 984 [23040/54000 (43%)] Loss: -467140.062500\n",
      "Train Epoch: 984 [34304/54000 (64%)] Loss: -554513.562500\n",
      "Train Epoch: 984 [45568/54000 (84%)] Loss: -531742.875000\n",
      "    epoch          : 984\n",
      "    loss           : -508783.94875\n",
      "    val_loss       : -485317.5126953125\n",
      "Train Epoch: 985 [512/54000 (1%)] Loss: -431414.812500\n",
      "Train Epoch: 985 [11776/54000 (22%)] Loss: -436380.187500\n",
      "Train Epoch: 985 [23040/54000 (43%)] Loss: -602533.625000\n",
      "Train Epoch: 985 [34304/54000 (64%)] Loss: -560188.562500\n",
      "Train Epoch: 985 [45568/54000 (84%)] Loss: -556432.312500\n",
      "    epoch          : 985\n",
      "    loss           : -509018.6265625\n",
      "    val_loss       : -487263.1514648438\n",
      "Train Epoch: 986 [512/54000 (1%)] Loss: -467571.125000\n",
      "Train Epoch: 986 [11776/54000 (22%)] Loss: -468848.937500\n",
      "Train Epoch: 986 [23040/54000 (43%)] Loss: -472911.687500\n",
      "Train Epoch: 986 [34304/54000 (64%)] Loss: -603854.000000\n",
      "Train Epoch: 986 [45568/54000 (84%)] Loss: -556888.187500\n",
      "    epoch          : 986\n",
      "    loss           : -509217.9965625\n",
      "    val_loss       : -483371.5252929687\n",
      "Train Epoch: 987 [512/54000 (1%)] Loss: -530581.250000\n",
      "Train Epoch: 987 [11776/54000 (22%)] Loss: -576758.875000\n",
      "Train Epoch: 987 [23040/54000 (43%)] Loss: -554261.187500\n",
      "Train Epoch: 987 [34304/54000 (64%)] Loss: -532258.562500\n",
      "Train Epoch: 987 [45568/54000 (84%)] Loss: -435961.062500\n",
      "    epoch          : 987\n",
      "    loss           : -509349.8634375\n",
      "    val_loss       : -485763.39873046876\n",
      "Train Epoch: 988 [512/54000 (1%)] Loss: -530319.875000\n",
      "Train Epoch: 988 [11776/54000 (22%)] Loss: -597003.375000\n",
      "Train Epoch: 988 [23040/54000 (43%)] Loss: -531550.312500\n",
      "Train Epoch: 988 [34304/54000 (64%)] Loss: -562269.750000\n",
      "Train Epoch: 988 [45568/54000 (84%)] Loss: -434984.312500\n",
      "    epoch          : 988\n",
      "    loss           : -509403.6753125\n",
      "    val_loss       : -488884.697265625\n",
      "Train Epoch: 989 [512/54000 (1%)] Loss: -555520.250000\n",
      "Train Epoch: 989 [11776/54000 (22%)] Loss: -474879.437500\n",
      "Train Epoch: 989 [23040/54000 (43%)] Loss: -433855.937500\n",
      "Train Epoch: 989 [34304/54000 (64%)] Loss: -450357.312500\n",
      "Train Epoch: 989 [45568/54000 (84%)] Loss: -433523.375000\n",
      "    epoch          : 989\n",
      "    loss           : -509482.3359375\n",
      "    val_loss       : -476038.06459960935\n",
      "Train Epoch: 990 [512/54000 (1%)] Loss: -482034.375000\n",
      "Train Epoch: 990 [11776/54000 (22%)] Loss: -580634.250000\n",
      "Train Epoch: 990 [23040/54000 (43%)] Loss: -477259.687500\n",
      "Train Epoch: 990 [34304/54000 (64%)] Loss: -480337.437500\n",
      "Train Epoch: 990 [45568/54000 (84%)] Loss: -555406.562500\n",
      "    epoch          : 990\n",
      "    loss           : -509465.78\n",
      "    val_loss       : -480314.86328125\n",
      "Train Epoch: 991 [512/54000 (1%)] Loss: -471353.406250\n",
      "Train Epoch: 991 [11776/54000 (22%)] Loss: -533712.000000\n",
      "Train Epoch: 991 [23040/54000 (43%)] Loss: -473867.812500\n",
      "Train Epoch: 991 [34304/54000 (64%)] Loss: -437099.093750\n",
      "Train Epoch: 991 [45568/54000 (84%)] Loss: -565735.000000\n",
      "    epoch          : 991\n",
      "    loss           : -509921.2778125\n",
      "    val_loss       : -484273.08720703126\n",
      "Train Epoch: 992 [512/54000 (1%)] Loss: -446710.625000\n",
      "Train Epoch: 992 [11776/54000 (22%)] Loss: -578614.687500\n",
      "Train Epoch: 992 [23040/54000 (43%)] Loss: -447225.781250\n",
      "Train Epoch: 992 [34304/54000 (64%)] Loss: -470001.562500\n",
      "Train Epoch: 992 [45568/54000 (84%)] Loss: -434810.031250\n",
      "    epoch          : 992\n",
      "    loss           : -509922.8753125\n",
      "    val_loss       : -488659.47890625\n",
      "Train Epoch: 993 [512/54000 (1%)] Loss: -440759.718750\n",
      "Train Epoch: 993 [11776/54000 (22%)] Loss: -467288.500000\n",
      "Train Epoch: 993 [23040/54000 (43%)] Loss: -472910.812500\n",
      "Train Epoch: 993 [34304/54000 (64%)] Loss: -597290.750000\n",
      "Train Epoch: 993 [45568/54000 (84%)] Loss: -529199.125000\n",
      "    epoch          : 993\n",
      "    loss           : -510024.8659375\n",
      "    val_loss       : -487392.64775390626\n",
      "Train Epoch: 994 [512/54000 (1%)] Loss: -532468.375000\n",
      "Train Epoch: 994 [11776/54000 (22%)] Loss: -605013.875000\n",
      "Train Epoch: 994 [23040/54000 (43%)] Loss: -444683.312500\n",
      "Train Epoch: 994 [34304/54000 (64%)] Loss: -436664.500000\n",
      "Train Epoch: 994 [45568/54000 (84%)] Loss: -434264.750000\n",
      "    epoch          : 994\n",
      "    loss           : -510207.0365625\n",
      "    val_loss       : -477851.888671875\n",
      "Train Epoch: 995 [512/54000 (1%)] Loss: -477359.437500\n",
      "Train Epoch: 995 [11776/54000 (22%)] Loss: -478143.812500\n",
      "Train Epoch: 995 [23040/54000 (43%)] Loss: -479589.562500\n",
      "Train Epoch: 995 [34304/54000 (64%)] Loss: -480837.312500\n",
      "Train Epoch: 995 [45568/54000 (84%)] Loss: -557074.125000\n",
      "    epoch          : 995\n",
      "    loss           : -510249.6534375\n",
      "    val_loss       : -490130.52890625\n",
      "Train Epoch: 996 [512/54000 (1%)] Loss: -479183.875000\n",
      "Train Epoch: 996 [11776/54000 (22%)] Loss: -534198.812500\n",
      "Train Epoch: 996 [23040/54000 (43%)] Loss: -448883.718750\n",
      "Train Epoch: 996 [34304/54000 (64%)] Loss: -533452.937500\n",
      "Train Epoch: 996 [45568/54000 (84%)] Loss: -437624.093750\n",
      "    epoch          : 996\n",
      "    loss           : -510153.106875\n",
      "    val_loss       : -490076.98193359375\n",
      "Train Epoch: 997 [512/54000 (1%)] Loss: -581008.750000\n",
      "Train Epoch: 997 [11776/54000 (22%)] Loss: -446987.593750\n",
      "Train Epoch: 997 [23040/54000 (43%)] Loss: -438093.218750\n",
      "Train Epoch: 997 [34304/54000 (64%)] Loss: -437661.906250\n",
      "Train Epoch: 997 [45568/54000 (84%)] Loss: -436771.968750\n",
      "    epoch          : 997\n",
      "    loss           : -510297.7725\n",
      "    val_loss       : -486924.573828125\n",
      "Train Epoch: 998 [512/54000 (1%)] Loss: -567336.750000\n",
      "Train Epoch: 998 [11776/54000 (22%)] Loss: -596253.562500\n",
      "Train Epoch: 998 [23040/54000 (43%)] Loss: -579630.562500\n",
      "Train Epoch: 998 [34304/54000 (64%)] Loss: -446753.375000\n",
      "Train Epoch: 998 [45568/54000 (84%)] Loss: -440289.937500\n",
      "    epoch          : 998\n",
      "    loss           : -510719.4675\n",
      "    val_loss       : -487086.9911132812\n",
      "Train Epoch: 999 [512/54000 (1%)] Loss: -579259.125000\n",
      "Train Epoch: 999 [11776/54000 (22%)] Loss: -448072.718750\n",
      "Train Epoch: 999 [23040/54000 (43%)] Loss: -561231.437500\n",
      "Train Epoch: 999 [34304/54000 (64%)] Loss: -603724.375000\n",
      "Train Epoch: 999 [45568/54000 (84%)] Loss: -535852.875000\n",
      "    epoch          : 999\n",
      "    loss           : -510595.55\n",
      "    val_loss       : -487623.1583984375\n",
      "Train Epoch: 1000 [512/54000 (1%)] Loss: -555980.687500\n",
      "Train Epoch: 1000 [11776/54000 (22%)] Loss: -439071.156250\n",
      "Train Epoch: 1000 [23040/54000 (43%)] Loss: -558600.625000\n",
      "Train Epoch: 1000 [34304/54000 (64%)] Loss: -604858.062500\n",
      "Train Epoch: 1000 [45568/54000 (84%)] Loss: -480543.187500\n",
      "    epoch          : 1000\n",
      "    loss           : -511015.32625\n",
      "    val_loss       : -488605.40673828125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0703_181447/checkpoint-epoch1000.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAECategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=64, bias=True)\n",
       "        (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=160, bias=True)\n",
       "        (1): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=64, bias=True)\n",
       "        (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=288, bias=True)\n",
       "        (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=64, bias=True)\n",
       "        (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_3_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=424, bias=True)\n",
       "        (1): BatchNorm1d(424, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): BatchNorm1d(424, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=64, bias=True)\n",
       "        (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=192, bias=True)\n",
       "        (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=128, bias=True)\n",
       "        (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=320, bias=True)\n",
       "        (1): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=128, bias=True)\n",
       "        (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=456, bias=True)\n",
       "        (1): BatchNorm1d(456, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): BatchNorm1d(456, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=128, bias=True)\n",
       "        (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=256, bias=True)\n",
       "        (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=520, bias=True)\n",
       "        (1): BatchNorm1d(520, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): BatchNorm1d(520, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=256, bias=True)\n",
       "        (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_9_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=648, bias=True)\n",
       "        (1): BatchNorm1d(648, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): BatchNorm1d(648, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=512, bias=True)\n",
       "        (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0_0): StandardNormal()\n",
       "    (global_element_1_0): StandardNormal()\n",
       "    (global_element_2_0): StandardNormal()\n",
       "    (global_element_3_0): StandardNormal()\n",
       "    (global_element_4_0): StandardNormal()\n",
       "  )\n",
       "  (guide_embedding): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=49, bias=True)\n",
       "    (1): BatchNorm1d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=49, out_features=49, bias=True)\n",
       "    (4): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (guide_confidences): Sequential(\n",
       "    (0): Linear(in_features=49, out_features=2, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
