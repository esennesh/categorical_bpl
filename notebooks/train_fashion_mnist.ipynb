{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 642694.000000\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: 633349.062500\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: 1012555.250000\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: 605585.687500\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: 759961.437500\n",
      "    epoch          : 1\n",
      "    loss           : 511732.68485351565\n",
      "    val_loss       : 226392.1412109375\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: 346792.906250\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: 502444.125000\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: 153418.171875\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: 224960.828125\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: 196167.937500\n",
      "    epoch          : 2\n",
      "    loss           : 76543.02665039063\n",
      "    val_loss       : -60328.2787109375\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: 186908.390625\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -13420.017578\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -318305.593750\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -81608.976562\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -397844.843750\n",
      "    epoch          : 3\n",
      "    loss           : -133294.88728027343\n",
      "    val_loss       : -197316.431640625\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -503405.437500\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -34760.128906\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -196550.562500\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -539293.437500\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -321158.187500\n",
      "    epoch          : 4\n",
      "    loss           : -287338.770703125\n",
      "    val_loss       : -326023.9837890625\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -203306.000000\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -263202.875000\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -535105.750000\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -498000.500000\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -510107.468750\n",
      "    epoch          : 5\n",
      "    loss           : -352391.3058984375\n",
      "    val_loss       : -376149.0890625\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -283031.281250\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -256293.171875\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -270757.062500\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -160926.890625\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -305212.750000\n",
      "    epoch          : 6\n",
      "    loss           : -377663.4544140625\n",
      "    val_loss       : -415299.9048828125\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -199454.968750\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -221033.015625\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -621453.750000\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -239603.218750\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -587429.250000\n",
      "    epoch          : 7\n",
      "    loss           : -420520.64015625\n",
      "    val_loss       : -432575.9439453125\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -558928.812500\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -237707.265625\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -306992.875000\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -405689.156250\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -587905.312500\n",
      "    epoch          : 8\n",
      "    loss           : -425177.2465625\n",
      "    val_loss       : -416238.937109375\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -558283.875000\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -545809.187500\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -412605.062500\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -604165.187500\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -596236.625000\n",
      "    epoch          : 9\n",
      "    loss           : -411642.82609375\n",
      "    val_loss       : -438441.8849609375\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -594078.875000\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -615072.187500\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -363659.500000\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -258296.250000\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -221168.375000\n",
      "    epoch          : 10\n",
      "    loss           : -431927.63546875\n",
      "    val_loss       : -420783.680078125\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -218825.843750\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -451523.000000\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -306493.187500\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -327871.562500\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -285939.718750\n",
      "    epoch          : 11\n",
      "    loss           : -441495.00890625\n",
      "    val_loss       : -460228.47734375\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -269997.687500\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -339927.125000\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -553432.812500\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -590187.000000\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -318486.312500\n",
      "    epoch          : 12\n",
      "    loss           : -440769.3946875\n",
      "    val_loss       : -460229.286328125\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -464345.625000\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -35843.484375\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -401922.000000\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -270173.937500\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -439727.093750\n",
      "    epoch          : 13\n",
      "    loss           : -355366.8442578125\n",
      "    val_loss       : -409571.4490234375\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -278311.500000\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -455523.406250\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -522471.250000\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: 288677.281250\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -287272.656250\n",
      "    epoch          : 14\n",
      "    loss           : -341842.7294921875\n",
      "    val_loss       : -357909.8423828125\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -481944.187500\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -253082.640625\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -530862.500000\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -545835.375000\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -428661.031250\n",
      "    epoch          : 15\n",
      "    loss           : -373195.1454296875\n",
      "    val_loss       : -432989.0537109375\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -608459.562500\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -353686.093750\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -580545.625000\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -248638.937500\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -236702.843750\n",
      "    epoch          : 16\n",
      "    loss           : -410997.63203125\n",
      "    val_loss       : -446789.135546875\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -373143.250000\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -549754.375000\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -262381.093750\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -592825.000000\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -416547.750000\n",
      "    epoch          : 17\n",
      "    loss           : -433262.58390625\n",
      "    val_loss       : -454792.7490234375\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -349183.062500\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -322947.843750\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -561435.625000\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -424040.281250\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -432102.625000\n",
      "    epoch          : 18\n",
      "    loss           : -429737.4790625\n",
      "    val_loss       : -438862.128515625\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -291979.687500\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -305006.531250\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -563277.250000\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -624331.750000\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -415092.843750\n",
      "    epoch          : 19\n",
      "    loss           : -425000.6890625\n",
      "    val_loss       : -442589.4357421875\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -341262.875000\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -675405.000000\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -410493.531250\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -552368.500000\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -324423.593750\n",
      "    epoch          : 20\n",
      "    loss           : -429867.3695703125\n",
      "    val_loss       : -467620.69609375\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -559319.750000\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -572273.062500\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -385637.437500\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -529487.875000\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -490403.406250\n",
      "    epoch          : 21\n",
      "    loss           : -462445.289375\n",
      "    val_loss       : -475756.880078125\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -496963.437500\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -568541.812500\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -480115.312500\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -677173.750000\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -616734.750000\n",
      "    epoch          : 22\n",
      "    loss           : -478926.093125\n",
      "    val_loss       : -481530.542578125\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -674230.312500\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -332482.750000\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -331024.500000\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -316154.937500\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -317158.093750\n",
      "    epoch          : 23\n",
      "    loss           : -467563.4609375\n",
      "    val_loss       : -456207.9978515625\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -568689.437500\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -643950.187500\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -397591.937500\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -353984.937500\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -608203.125000\n",
      "    epoch          : 24\n",
      "    loss           : -477744.663125\n",
      "    val_loss       : -498319.68203125\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -689670.250000\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -80932.273438\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -448732.875000\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -346202.968750\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -594767.875000\n",
      "    epoch          : 25\n",
      "    loss           : -398706.9268847656\n",
      "    val_loss       : -454141.6755859375\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -537292.875000\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -535456.312500\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -341603.031250\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -327533.093750\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -293259.812500\n",
      "    epoch          : 26\n",
      "    loss           : -456063.964375\n",
      "    val_loss       : -488634.229296875\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -354814.062500\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -320664.062500\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -619534.000000\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -638937.375000\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -376732.125000\n",
      "    epoch          : 27\n",
      "    loss           : -473879.09375\n",
      "    val_loss       : -495774.0740234375\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -503247.781250\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -380401.250000\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -702832.375000\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -421465.062500\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -340897.375000\n",
      "    epoch          : 28\n",
      "    loss           : -495321.73375\n",
      "    val_loss       : -482290.33984375\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -560923.875000\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -697501.375000\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -578058.437500\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -692271.750000\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -618613.562500\n",
      "    epoch          : 29\n",
      "    loss           : -467318.6576953125\n",
      "    val_loss       : -155926.89375\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -458847.312500\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -582183.500000\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -359992.531250\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -387216.750000\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -197297.312500\n",
      "    epoch          : 30\n",
      "    loss           : -388793.867734375\n",
      "    val_loss       : -423040.493359375\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -182082.750000\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -660531.000000\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -318362.625000\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -366302.031250\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -569847.875000\n",
      "    epoch          : 31\n",
      "    loss           : -442534.255625\n",
      "    val_loss       : -483574.7900390625\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -380068.687500\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -573202.875000\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -354727.062500\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -329554.687500\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -328111.718750\n",
      "    epoch          : 32\n",
      "    loss           : -477113.8259375\n",
      "    val_loss       : -484063.5947265625\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -590129.187500\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -669940.750000\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -338407.000000\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -524625.250000\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -280294.218750\n",
      "    epoch          : 33\n",
      "    loss           : -483548.78109375\n",
      "    val_loss       : -459501.0224609375\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -463511.375000\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -329824.718750\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -279958.750000\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -319977.750000\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -364906.875000\n",
      "    epoch          : 34\n",
      "    loss           : -487291.08484375\n",
      "    val_loss       : -483913.75078125\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -496413.781250\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -588213.437500\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -353053.937500\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -409354.250000\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -540382.937500\n",
      "    epoch          : 35\n",
      "    loss           : -493517.4015625\n",
      "    val_loss       : -505616.95078125\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -314839.937500\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -356000.875000\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -282779.812500\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -395851.000000\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -599182.125000\n",
      "    epoch          : 36\n",
      "    loss           : -479092.79796875\n",
      "    val_loss       : -504001.09453125\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -675911.812500\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -540098.625000\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -388286.687500\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -637626.375000\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -377765.312500\n",
      "    epoch          : 37\n",
      "    loss           : -500972.0309375\n",
      "    val_loss       : -506371.665234375\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -697229.125000\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -703359.625000\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -541721.875000\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -522535.500000\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -532084.187500\n",
      "    epoch          : 38\n",
      "    loss           : -503956.29\n",
      "    val_loss       : -502376.3640625\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -645187.875000\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -438957.312500\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -474983.187500\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -410312.750000\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -611905.312500\n",
      "    epoch          : 39\n",
      "    loss           : -484705.87328125\n",
      "    val_loss       : -499885.618359375\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -605911.000000\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -383099.687500\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -577547.375000\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -623744.500000\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -591777.250000\n",
      "    epoch          : 40\n",
      "    loss           : -498147.3421875\n",
      "    val_loss       : -491986.962109375\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -384011.343750\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -389826.781250\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -672716.750000\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -334917.375000\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -343369.593750\n",
      "    epoch          : 41\n",
      "    loss           : -503356.779375\n",
      "    val_loss       : -501986.5787109375\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -654839.937500\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -357416.218750\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -383278.125000\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -408342.125000\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -503353.906250\n",
      "    epoch          : 42\n",
      "    loss           : -495907.1346875\n",
      "    val_loss       : -485925.3525390625\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -355221.562500\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -374339.375000\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -391091.625000\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -346721.500000\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -626316.375000\n",
      "    epoch          : 43\n",
      "    loss           : -493398.8853125\n",
      "    val_loss       : -493442.26484375\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -696575.625000\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -590578.562500\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -434638.187500\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -383914.437500\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -419176.281250\n",
      "    epoch          : 44\n",
      "    loss           : -493092.8421875\n",
      "    val_loss       : -513379.28515625\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -383042.375000\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -433354.687500\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -389062.281250\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -442879.562500\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -636619.687500\n",
      "    epoch          : 45\n",
      "    loss           : -511493.3253125\n",
      "    val_loss       : -519794.185546875\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -448625.718750\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -607537.875000\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -431572.593750\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -710198.937500\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -600523.375000\n",
      "    epoch          : 46\n",
      "    loss           : -515250.44125\n",
      "    val_loss       : -521109.93828125\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -426466.093750\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -419087.062500\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -379374.562500\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -441944.281250\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -604168.625000\n",
      "    epoch          : 47\n",
      "    loss           : -512694.3590625\n",
      "    val_loss       : -528119.70859375\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -404754.500000\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -687916.750000\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -668524.312500\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -573919.000000\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -646535.750000\n",
      "    epoch          : 48\n",
      "    loss           : -509480.976875\n",
      "    val_loss       : -523833.5625\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -371205.031250\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -359762.906250\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -438877.875000\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -411770.125000\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -696547.500000\n",
      "    epoch          : 49\n",
      "    loss           : -506176.84375\n",
      "    val_loss       : -514298.906640625\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -638172.687500\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -395049.812500\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -542985.687500\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -650518.062500\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -596823.312500\n",
      "    epoch          : 50\n",
      "    loss           : -515331.880625\n",
      "    val_loss       : -520012.748828125\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0923_230151/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -343324.687500\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -342993.562500\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -559336.312500\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -450600.468750\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -386591.062500\n",
      "    epoch          : 51\n",
      "    loss           : -522501.234375\n",
      "    val_loss       : -515474.916796875\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -397726.562500\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -373725.312500\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -410310.812500\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -547596.875000\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -602462.812500\n",
      "    epoch          : 52\n",
      "    loss           : -510140.1234375\n",
      "    val_loss       : -529011.765625\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -367499.812500\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -387655.875000\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -703879.187500\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -697156.125000\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -551400.187500\n",
      "    epoch          : 53\n",
      "    loss           : -527922.725\n",
      "    val_loss       : -537649.580078125\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -700578.000000\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -605298.750000\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -557123.937500\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -364402.281250\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -661946.375000\n",
      "    epoch          : 54\n",
      "    loss           : -521822.2340625\n",
      "    val_loss       : -524176.996484375\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -666121.375000\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -710420.437500\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -455709.312500\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -345296.812500\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -400404.062500\n",
      "    epoch          : 55\n",
      "    loss           : -503378.7096875\n",
      "    val_loss       : -494615.698046875\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -354320.750000\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -354445.906250\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -348156.062500\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -656114.875000\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -494797.437500\n",
      "    epoch          : 56\n",
      "    loss           : -502216.7925\n",
      "    val_loss       : -516591.084375\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -391475.937500\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -353587.562500\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -575273.500000\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -548397.812500\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -394318.156250\n",
      "    epoch          : 57\n",
      "    loss           : -516609.3596875\n",
      "    val_loss       : -534407.809375\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -678744.562500\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -434004.437500\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -712034.437500\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -505294.062500\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -358016.406250\n",
      "    epoch          : 58\n",
      "    loss           : -492211.4446875\n",
      "    val_loss       : -471435.198046875\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -630413.875000\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -661519.250000\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -432399.437500\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -680463.375000\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -641857.000000\n",
      "    epoch          : 59\n",
      "    loss           : -500950.219375\n",
      "    val_loss       : -477935.93359375\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -310324.062500\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -318127.000000\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -433484.875000\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -372098.687500\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -375121.343750\n",
      "    epoch          : 60\n",
      "    loss           : -496251.8440625\n",
      "    val_loss       : -521250.24296875\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -402810.937500\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -711119.062500\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -690284.875000\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -698477.000000\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -370089.500000\n",
      "    epoch          : 61\n",
      "    loss           : -527066.1940625\n",
      "    val_loss       : -536135.073828125\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -457076.562500\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -627781.625000\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -710611.250000\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -377238.687500\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -662157.437500\n",
      "    epoch          : 62\n",
      "    loss           : -516918.18\n",
      "    val_loss       : -481958.129296875\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -390772.531250\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -679266.750000\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -709596.312500\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -375706.843750\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -550706.375000\n",
      "    epoch          : 63\n",
      "    loss           : -509698.7996875\n",
      "    val_loss       : -529465.212109375\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -457325.125000\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -597082.250000\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -721022.312500\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -373568.687500\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -566123.000000\n",
      "    epoch          : 64\n",
      "    loss           : -522142.7565625\n",
      "    val_loss       : -531351.923828125\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -560592.875000\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -714663.562500\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -363190.031250\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -559486.812500\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -602056.625000\n",
      "    epoch          : 65\n",
      "    loss           : -498528.39703125\n",
      "    val_loss       : -492612.935546875\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -685162.375000\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -430586.250000\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -438460.406250\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -679663.562500\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -367221.937500\n",
      "    epoch          : 66\n",
      "    loss           : -519844.8640625\n",
      "    val_loss       : -535440.453125\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -442475.687500\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -717001.125000\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -699763.500000\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -673399.500000\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -654541.250000\n",
      "    epoch          : 67\n",
      "    loss           : -532303.963125\n",
      "    val_loss       : -527381.890234375\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -447232.093750\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -562866.125000\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -385039.781250\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -697410.500000\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -446579.250000\n",
      "    epoch          : 68\n",
      "    loss           : -529511.8765625\n",
      "    val_loss       : -525844.723046875\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -614410.937500\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -375198.125000\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -537834.625000\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -548992.812500\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -661357.750000\n",
      "    epoch          : 69\n",
      "    loss           : -501012.67703125\n",
      "    val_loss       : -507709.79375\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -682994.375000\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -355104.937500\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -444479.343750\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -461089.625000\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -404927.218750\n",
      "    epoch          : 70\n",
      "    loss           : -524134.1990625\n",
      "    val_loss       : -538846.251953125\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -413009.375000\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -568613.750000\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -246451.359375\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -404045.625000\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -543805.375000\n",
      "    epoch          : 71\n",
      "    loss           : -488650.676953125\n",
      "    val_loss       : -523365.725390625\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -447372.093750\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -423559.062500\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -564822.375000\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -386428.625000\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -548499.000000\n",
      "    epoch          : 72\n",
      "    loss           : -522776.876875\n",
      "    val_loss       : -519545.31171875\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -375389.062500\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -661332.125000\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -393113.593750\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -345973.625000\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -557319.937500\n",
      "    epoch          : 73\n",
      "    loss           : -512058.2053125\n",
      "    val_loss       : -500342.6787109375\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -350152.750000\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -583437.937500\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -638962.875000\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -365487.906250\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -480000.812500\n",
      "    epoch          : 74\n",
      "    loss           : -416754.2004296875\n",
      "    val_loss       : -324790.5822265625\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -199614.812500\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -303770.062500\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -622886.750000\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -656988.687500\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -639333.750000\n",
      "    epoch          : 75\n",
      "    loss           : -458099.6734375\n",
      "    val_loss       : -504748.3607421875\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -582479.312500\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -653175.500000\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -554219.750000\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -439945.250000\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -353622.062500\n",
      "    epoch          : 76\n",
      "    loss           : -496610.801875\n",
      "    val_loss       : -488972.0044921875\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -364520.093750\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -511358.656250\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -366883.125000\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -353718.312500\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -325015.125000\n",
      "    epoch          : 77\n",
      "    loss           : -499166.7496875\n",
      "    val_loss       : -495819.1955078125\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -512382.375000\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -685455.250000\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -441891.937500\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -423121.562500\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -638912.937500\n",
      "    epoch          : 78\n",
      "    loss           : -507778.0221875\n",
      "    val_loss       : -487710.2115234375\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -677546.000000\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -544221.312500\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -543916.500000\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -374481.781250\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -657572.875000\n",
      "    epoch          : 79\n",
      "    loss           : -511589.06\n",
      "    val_loss       : -524968.638671875\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -443215.843750\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -535115.250000\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -266063.562500\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -506666.062500\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -449250.937500\n",
      "    epoch          : 80\n",
      "    loss           : -499173.9421875\n",
      "    val_loss       : -520227.46875\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -546211.937500\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -371095.437500\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -316502.937500\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -638616.312500\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -542334.125000\n",
      "    epoch          : 81\n",
      "    loss           : -518403.7053125\n",
      "    val_loss       : -526731.3734375\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -641475.500000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -706977.500000\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -358400.718750\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -618076.562500\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -386811.062500\n",
      "    epoch          : 82\n",
      "    loss           : -529752.96875\n",
      "    val_loss       : -515247.353515625\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -415465.656250\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -392128.312500\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -614499.875000\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -382333.875000\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -539387.875000\n",
      "    epoch          : 83\n",
      "    loss           : -521486.6225\n",
      "    val_loss       : -518081.708984375\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -681498.750000\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -384405.500000\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -328860.250000\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -374609.000000\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -661222.812500\n",
      "    epoch          : 84\n",
      "    loss           : -495333.9678125\n",
      "    val_loss       : -516660.3685546875\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -396848.093750\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -355800.218750\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -348573.062500\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -366440.968750\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -712573.125000\n",
      "    epoch          : 85\n",
      "    loss           : -522454.6953125\n",
      "    val_loss       : -505346.0953125\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -419872.937500\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -455510.062500\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -452632.875000\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -389658.375000\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -666344.312500\n",
      "    epoch          : 86\n",
      "    loss           : -528574.8684375\n",
      "    val_loss       : -544831.9421875\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -566541.375000\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -450385.312500\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -709191.125000\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -424592.156250\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -679419.812500\n",
      "    epoch          : 87\n",
      "    loss           : -514625.08171875\n",
      "    val_loss       : -302184.70234375\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -396157.937500\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -564537.562500\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -428713.343750\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -375079.625000\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -598470.250000\n",
      "    epoch          : 88\n",
      "    loss           : -484815.79109375\n",
      "    val_loss       : -533342.537890625\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -717382.500000\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -706222.000000\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -368896.125000\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -439011.312500\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -606073.937500\n",
      "    epoch          : 89\n",
      "    loss           : -520412.0209375\n",
      "    val_loss       : -528414.9861328125\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -420990.750000\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -714992.437500\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -637841.875000\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -686353.875000\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -633312.625000\n",
      "    epoch          : 90\n",
      "    loss           : -527957.01875\n",
      "    val_loss       : -530402.005078125\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -405075.093750\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -424384.375000\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -693374.000000\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -383537.562500\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -382826.468750\n",
      "    epoch          : 91\n",
      "    loss           : -525751.5365625\n",
      "    val_loss       : -288118.2046875\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -365182.562500\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -370881.000000\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -438682.375000\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -584481.500000\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -342516.343750\n",
      "    epoch          : 92\n",
      "    loss           : -372036.1746875\n",
      "    val_loss       : -480990.3609375\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -424825.656250\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -559057.750000\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -432292.937500\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -445318.031250\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -421738.593750\n",
      "    epoch          : 93\n",
      "    loss           : -495770.12375\n",
      "    val_loss       : -510427.7275390625\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -432763.781250\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -431423.375000\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -681391.625000\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -392822.281250\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -260847.390625\n",
      "    epoch          : 94\n",
      "    loss           : -497886.41609375\n",
      "    val_loss       : -469223.29453125\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -323623.781250\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -681373.250000\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -381915.937500\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -658102.250000\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -560244.625000\n",
      "    epoch          : 95\n",
      "    loss           : -512274.9303125\n",
      "    val_loss       : -531151.54453125\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -590679.750000\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -710865.375000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -565524.312500\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -447780.406250\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -394240.750000\n",
      "    epoch          : 96\n",
      "    loss           : -531174.8353125\n",
      "    val_loss       : -534104.996875\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -452321.437500\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -393694.375000\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -393651.000000\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -642951.312500\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -366803.531250\n",
      "    epoch          : 97\n",
      "    loss           : -528499.1065625\n",
      "    val_loss       : -536799.033203125\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -406900.218750\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -557080.375000\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -318956.281250\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -688306.750000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -614091.187500\n",
      "    epoch          : 98\n",
      "    loss           : -503466.4903125\n",
      "    val_loss       : -472959.2173828125\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -338090.750000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -431359.875000\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -350767.031250\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -587554.250000\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -560879.750000\n",
      "    epoch          : 99\n",
      "    loss           : -490127.813125\n",
      "    val_loss       : -461250.673828125\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -510562.312500\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -662685.250000\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -419859.187500\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -674607.875000\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -676601.125000\n",
      "    epoch          : 100\n",
      "    loss           : -516647.185\n",
      "    val_loss       : -526600.750390625\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0923_230151/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -555442.125000\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -695434.875000\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -698114.125000\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -440958.562500\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -396361.156250\n",
      "    epoch          : 101\n",
      "    loss           : -526175.17125\n",
      "    val_loss       : -536200.464453125\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -446553.187500\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -381059.937500\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -709113.250000\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -620080.375000\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -377621.312500\n",
      "    epoch          : 102\n",
      "    loss           : -516773.009375\n",
      "    val_loss       : -527781.22734375\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -432574.562500\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -571049.625000\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -400814.750000\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -436445.125000\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -648068.312500\n",
      "    epoch          : 103\n",
      "    loss           : -521822.2846875\n",
      "    val_loss       : -518057.3353515625\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -374285.281250\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -376751.500000\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -404315.718750\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -461383.000000\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -458067.906250\n",
      "    epoch          : 104\n",
      "    loss           : -513702.33125\n",
      "    val_loss       : -396044.9623046875\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -152636.453125\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -690994.062500\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -452866.875000\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -367387.875000\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -374507.062500\n",
      "    epoch          : 105\n",
      "    loss           : -498029.55546875\n",
      "    val_loss       : -505330.4650390625\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -326045.781250\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -534822.812500\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -391853.875000\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -663860.437500\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -663197.562500\n",
      "    epoch          : 106\n",
      "    loss           : -513930.665\n",
      "    val_loss       : -533117.18671875\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -448479.531250\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -378562.968750\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -701378.812500\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -700206.437500\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -620191.187500\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "    epoch          : 107\n",
      "    loss           : -531738.166875\n",
      "    val_loss       : -532077.6453125\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -378988.687500\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -694521.375000\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -566426.250000\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -376312.250000\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -663581.375000\n",
      "    epoch          : 108\n",
      "    loss           : -537233.129375\n",
      "    val_loss       : -543583.941015625\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -376715.000000\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -413417.250000\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -707508.875000\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -722093.000000\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -572603.687500\n",
      "    epoch          : 109\n",
      "    loss           : -542264.3359375\n",
      "    val_loss       : -541334.88359375\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -461249.062500\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -445795.031250\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -635911.125000\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -670183.750000\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -396456.031250\n",
      "    epoch          : 110\n",
      "    loss           : -544434.994375\n",
      "    val_loss       : -543505.874609375\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -715128.375000\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -621512.000000\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -384786.375000\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -718230.312500\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -633688.437500\n",
      "    epoch          : 111\n",
      "    loss           : -544214.4784375\n",
      "    val_loss       : -548986.572265625\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -464122.000000\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -475437.718750\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -717699.812500\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -572547.312500\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -576856.625000\n",
      "    epoch          : 112\n",
      "    loss           : -546482.72\n",
      "    val_loss       : -547795.8953125\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -391925.250000\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -730459.125000\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -398756.062500\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -634650.437500\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -703315.000000\n",
      "    epoch          : 113\n",
      "    loss           : -547061.725625\n",
      "    val_loss       : -546367.28984375\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -448450.000000\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -579725.250000\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -472875.343750\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -461085.343750\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -470368.125000\n",
      "    epoch          : 114\n",
      "    loss           : -546199.6375\n",
      "    val_loss       : -549094.5453125\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -574827.750000\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -469257.187500\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -395039.437500\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -706536.500000\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -454018.281250\n",
      "    epoch          : 115\n",
      "    loss           : -546603.715625\n",
      "    val_loss       : -550525.006640625\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -622680.875000\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -413923.000000\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -404865.843750\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -644226.562500\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -453170.062500\n",
      "    epoch          : 116\n",
      "    loss           : -545384.1746875\n",
      "    val_loss       : -549583.10078125\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -724817.250000\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -727562.625000\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -628445.875000\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -580418.500000\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -397893.468750\n",
      "    epoch          : 117\n",
      "    loss           : -548313.7025\n",
      "    val_loss       : -548939.133984375\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -462727.406250\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -703462.625000\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -417694.687500\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -683530.625000\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -679566.937500\n",
      "    epoch          : 118\n",
      "    loss           : -550190.3146875\n",
      "    val_loss       : -552094.844921875\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -420423.968750\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -722687.125000\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -727941.062500\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -632696.250000\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -682190.687500\n",
      "    epoch          : 119\n",
      "    loss           : -550131.244375\n",
      "    val_loss       : -550911.306640625\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -677623.000000\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -456956.625000\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -635454.750000\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -595091.125000\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -399977.156250\n",
      "    epoch          : 120\n",
      "    loss           : -541577.5203125\n",
      "    val_loss       : -541513.421484375\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -606987.250000\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -458130.343750\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -691731.500000\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -682537.375000\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -691123.187500\n",
      "    epoch          : 121\n",
      "    loss           : -546905.4921875\n",
      "    val_loss       : -548524.51484375\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -402499.812500\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -387134.812500\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -414847.093750\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -575181.437500\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -396911.718750\n",
      "    epoch          : 122\n",
      "    loss           : -548747.7271875\n",
      "    val_loss       : -551049.132421875\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -577038.625000\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -580822.125000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -695048.562500\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -387069.062500\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -679071.750000\n",
      "    epoch          : 123\n",
      "    loss           : -551096.320625\n",
      "    val_loss       : -551607.073046875\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -635419.250000\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -724180.500000\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -475993.156250\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -579949.187500\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -689705.812500\n",
      "    epoch          : 124\n",
      "    loss           : -549690.2021875\n",
      "    val_loss       : -550856.865234375\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -415901.250000\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -411825.406250\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -473017.875000\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -465879.750000\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -473466.968750\n",
      "    epoch          : 125\n",
      "    loss           : -550871.276875\n",
      "    val_loss       : -550750.919921875\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -715638.437500\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -720703.500000\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -407654.062500\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -391406.625000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -403557.375000\n",
      "    epoch          : 126\n",
      "    loss           : -550434.8715625\n",
      "    val_loss       : -552152.81796875\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -626815.062500\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -409910.562500\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -636885.312500\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -475480.812500\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -401533.718750\n",
      "    epoch          : 127\n",
      "    loss           : -551417.5109375\n",
      "    val_loss       : -551877.0232421875\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -720004.312500\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -480055.656250\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -456816.375000\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -394258.500000\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -410099.562500\n",
      "    epoch          : 128\n",
      "    loss           : -552895.490625\n",
      "    val_loss       : -551476.7578125\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -673754.375000\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -724422.687500\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -470267.531250\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -587905.562500\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -420673.625000\n",
      "    epoch          : 129\n",
      "    loss           : -552988.849375\n",
      "    val_loss       : -554575.523828125\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -401986.343750\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -477343.875000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -582197.562500\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -408143.000000\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -697089.000000\n",
      "    epoch          : 130\n",
      "    loss           : -553600.7740625\n",
      "    val_loss       : -555866.3904296875\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -683224.500000\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -472784.031250\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -464747.125000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -695009.062500\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -700670.375000\n",
      "    epoch          : 131\n",
      "    loss           : -553827.690625\n",
      "    val_loss       : -554921.6630859375\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -690654.250000\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -426794.000000\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -414708.375000\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -415342.812500\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -587704.625000\n",
      "    epoch          : 132\n",
      "    loss           : -552814.9246875\n",
      "    val_loss       : -557585.0373046875\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -489126.468750\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -713839.000000\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -714024.125000\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -421899.937500\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -691098.625000\n",
      "    epoch          : 133\n",
      "    loss           : -554119.989375\n",
      "    val_loss       : -558381.4412109375\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -631019.687500\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -730917.687500\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -692258.500000\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -696953.875000\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -403282.375000\n",
      "    epoch          : 134\n",
      "    loss           : -555401.0740625\n",
      "    val_loss       : -558946.380078125\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -469752.687500\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -399941.781250\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -628619.250000\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -713002.875000\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -587379.125000\n",
      "    epoch          : 135\n",
      "    loss           : -554344.001875\n",
      "    val_loss       : -556371.84609375\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -639476.375000\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -735094.500000\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -726249.937500\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -715264.937500\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -588539.812500\n",
      "    epoch          : 136\n",
      "    loss           : -555362.60125\n",
      "    val_loss       : -555516.41328125\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -735608.250000\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -635058.125000\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -582046.125000\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -732057.875000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -692212.250000\n",
      "    epoch          : 137\n",
      "    loss           : -555518.2003125\n",
      "    val_loss       : -557104.9548828125\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -480094.218750\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -730408.750000\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -577935.875000\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -641554.937500\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -696287.500000\n",
      "    epoch          : 138\n",
      "    loss           : -555041.1203125\n",
      "    val_loss       : -558630.741796875\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -590225.312500\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -724111.875000\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -490419.156250\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -405535.937500\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -619104.562500\n",
      "    epoch          : 139\n",
      "    loss           : -553850.9221875\n",
      "    val_loss       : -549999.4068359375\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -401843.687500\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -463161.375000\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -685824.875000\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -471592.781250\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -638817.187500\n",
      "    epoch          : 140\n",
      "    loss           : -554233.493125\n",
      "    val_loss       : -558450.2931640625\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -406035.000000\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -728191.000000\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -631648.625000\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -630425.625000\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -426344.406250\n",
      "    epoch          : 141\n",
      "    loss           : -555857.0584375\n",
      "    val_loss       : -558790.6537109375\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -405176.343750\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -480160.312500\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -687375.250000\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -726227.687500\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -487976.406250\n",
      "    epoch          : 142\n",
      "    loss           : -556688.1996875\n",
      "    val_loss       : -557840.228125\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -694751.125000\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -585272.187500\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -487026.375000\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -480488.375000\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -488024.812500\n",
      "    epoch          : 143\n",
      "    loss           : -556164.230625\n",
      "    val_loss       : -553792.455078125\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -476696.812500\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -426421.718750\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -739427.312500\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -584913.062500\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -634573.250000\n",
      "    epoch          : 144\n",
      "    loss           : -555698.87125\n",
      "    val_loss       : -558504.66875\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -723712.000000\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -722986.562500\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -474747.843750\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -710231.375000\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -427051.375000\n",
      "    epoch          : 145\n",
      "    loss           : -555728.23125\n",
      "    val_loss       : -556484.94296875\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -588382.250000\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -729615.625000\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -423637.343750\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -472240.625000\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -698154.375000\n",
      "    epoch          : 146\n",
      "    loss           : -555590.033125\n",
      "    val_loss       : -557412.1490234375\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -693080.625000\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -728388.500000\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -423607.343750\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -468052.937500\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -718233.250000\n",
      "    epoch          : 147\n",
      "    loss           : -555361.4396875\n",
      "    val_loss       : -556747.0052734375\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -422119.125000\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -400134.625000\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -479459.375000\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -480997.406250\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -590522.625000\n",
      "    epoch          : 148\n",
      "    loss           : -557220.9253125\n",
      "    val_loss       : -559868.2822265625\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -488889.500000\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -484978.437500\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -421213.187500\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -698686.625000\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -697281.437500\n",
      "    epoch          : 149\n",
      "    loss           : -557310.715625\n",
      "    val_loss       : -559264.0423828125\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -398610.156250\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -726064.312500\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -587924.562500\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -415483.250000\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -717112.250000\n",
      "    epoch          : 150\n",
      "    loss           : -557114.6440625\n",
      "    val_loss       : -559765.1068359375\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0923_230151/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -472314.937500\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -425674.937500\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -696469.625000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -468380.906250\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -630638.250000\n",
      "    epoch          : 151\n",
      "    loss           : -557128.97875\n",
      "    val_loss       : -559201.1361328125\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -482592.281250\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -419086.531250\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -595617.625000\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -729241.375000\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -590999.937500\n",
      "    epoch          : 152\n",
      "    loss           : -557959.8659375\n",
      "    val_loss       : -557558.0451171875\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -472577.343750\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -721790.625000\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -411043.687500\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -417914.343750\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -710716.250000\n",
      "    epoch          : 153\n",
      "    loss           : -556803.126875\n",
      "    val_loss       : -552098.87265625\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -706494.125000\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -635866.750000\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -485046.125000\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -636436.062500\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -401547.187500\n",
      "    epoch          : 154\n",
      "    loss           : -555749.6084375\n",
      "    val_loss       : -557721.9162109375\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -644554.500000\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -412261.343750\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -730363.812500\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -418944.187500\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -488324.812500\n",
      "    epoch          : 155\n",
      "    loss           : -557266.8909375\n",
      "    val_loss       : -562434.95\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -732513.312500\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -437274.281250\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -725581.625000\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -694582.625000\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -585224.000000\n",
      "    epoch          : 156\n",
      "    loss           : -558830.34875\n",
      "    val_loss       : -559380.7705078125\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -429651.656250\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -400408.875000\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -580066.937500\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -629577.875000\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -588480.250000\n",
      "    epoch          : 157\n",
      "    loss           : -557640.8778125\n",
      "    val_loss       : -558457.78125\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -424401.781250\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -592668.250000\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -694822.125000\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -480649.437500\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -698289.500000\n",
      "    epoch          : 158\n",
      "    loss           : -558552.566875\n",
      "    val_loss       : -559648.0990234375\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -589810.250000\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -405246.312500\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -408874.343750\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -636637.312500\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -590896.375000\n",
      "    epoch          : 159\n",
      "    loss           : -556004.9821875\n",
      "    val_loss       : -560785.8583984375\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -693730.937500\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -736351.375000\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -710612.687500\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -480316.343750\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -470926.687500\n",
      "    epoch          : 160\n",
      "    loss           : -557273.839375\n",
      "    val_loss       : -557216.651953125\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -726743.250000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -720518.812500\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -731206.000000\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -720102.000000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -692627.375000\n",
      "    epoch          : 161\n",
      "    loss           : -558844.1296875\n",
      "    val_loss       : -560745.5978515625\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -738336.750000\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -403772.500000\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -412810.156250\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -679780.812500\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -639813.875000\n",
      "    epoch          : 162\n",
      "    loss           : -552778.0634375\n",
      "    val_loss       : -557223.89375\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -734549.250000\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -643897.750000\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -632848.375000\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -696253.375000\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -590856.687500\n",
      "    epoch          : 163\n",
      "    loss           : -557725.1609375\n",
      "    val_loss       : -560026.8875\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -730985.500000\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -644956.312500\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -409941.687500\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -641317.125000\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -592069.750000\n",
      "    epoch          : 164\n",
      "    loss           : -558607.68\n",
      "    val_loss       : -560750.3171875\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -696423.375000\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -402739.093750\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -409810.375000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -588518.125000\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -590542.125000\n",
      "    epoch          : 165\n",
      "    loss           : -559258.113125\n",
      "    val_loss       : -561661.094140625\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -704219.375000\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -729271.312500\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -713525.875000\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -722804.750000\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -407413.437500\n",
      "    epoch          : 166\n",
      "    loss           : -557730.9115625\n",
      "    val_loss       : -559354.8154296875\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -631852.375000\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -719877.687500\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -436187.125000\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -639747.500000\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -412854.093750\n",
      "    epoch          : 167\n",
      "    loss           : -559313.785625\n",
      "    val_loss       : -564298.1748046875\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -432228.375000\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -428402.718750\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -728706.562500\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -410834.406250\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -682011.750000\n",
      "    epoch          : 168\n",
      "    loss           : -560449.173125\n",
      "    val_loss       : -562121.8453125\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -489376.093750\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -646694.562500\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -490272.156250\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -642010.562500\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -418835.062500\n",
      "    epoch          : 169\n",
      "    loss           : -560059.786875\n",
      "    val_loss       : -564797.08203125\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -416105.875000\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -417776.781250\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -486482.156250\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -414624.312500\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -585421.375000\n",
      "    epoch          : 170\n",
      "    loss           : -561024.7321875\n",
      "    val_loss       : -562850.2103515625\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -422505.125000\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -411035.500000\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -484492.281250\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -483922.750000\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -480137.875000\n",
      "    epoch          : 171\n",
      "    loss           : -560053.8153125\n",
      "    val_loss       : -563810.425390625\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -425654.593750\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -724721.937500\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -419896.375000\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -418139.187500\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -473374.781250\n",
      "    epoch          : 172\n",
      "    loss           : -561452.685625\n",
      "    val_loss       : -562716.972265625\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -406720.406250\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -596514.250000\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -715002.750000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -587360.875000\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -420301.312500\n",
      "    epoch          : 173\n",
      "    loss           : -560849.043125\n",
      "    val_loss       : -562267.271484375\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -413529.125000\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -488248.968750\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -491656.125000\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -645901.562500\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -696572.500000\n",
      "    epoch          : 174\n",
      "    loss           : -560585.85875\n",
      "    val_loss       : -562344.869921875\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -643350.000000\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -732210.000000\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -652752.687500\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -589745.562500\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -710824.062500\n",
      "    epoch          : 175\n",
      "    loss           : -561945.7953125\n",
      "    val_loss       : -563489.1078125\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -700026.000000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -474392.468750\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -491933.718750\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -468245.750000\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -653527.312500\n",
      "    epoch          : 176\n",
      "    loss           : -558625.8934375\n",
      "    val_loss       : -561772.4548828125\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -590506.750000\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -713993.250000\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -710467.000000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -413334.750000\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -584428.125000\n",
      "    epoch          : 177\n",
      "    loss           : -556004.3309375\n",
      "    val_loss       : -558596.3734375\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -653867.687500\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -420325.937500\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -701150.125000\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -468319.406250\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -706362.125000\n",
      "    epoch          : 178\n",
      "    loss           : -560371.5853125\n",
      "    val_loss       : -563131.884375\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -707002.062500\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -598987.375000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -693114.875000\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -705782.625000\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -683161.500000\n",
      "    epoch          : 179\n",
      "    loss           : -562104.84375\n",
      "    val_loss       : -562503.19609375\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -722031.812500\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -595526.562500\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -415261.625000\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -488865.062500\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -482363.156250\n",
      "    epoch          : 180\n",
      "    loss           : -561452.50375\n",
      "    val_loss       : -563840.983984375\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -638060.875000\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -643967.875000\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -640261.062500\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -482161.937500\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -686207.250000\n",
      "    epoch          : 181\n",
      "    loss           : -557422.360625\n",
      "    val_loss       : -558254.1685546875\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -717898.250000\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -643391.625000\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -477382.343750\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -696871.000000\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -702338.875000\n",
      "    epoch          : 182\n",
      "    loss           : -559313.65125\n",
      "    val_loss       : -563013.014453125\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -488704.531250\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -593337.812500\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -700808.562500\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -650390.187500\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -645888.937500\n",
      "    epoch          : 183\n",
      "    loss           : -560493.4571875\n",
      "    val_loss       : -563829.475\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -588741.750000\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -588256.375000\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -424589.656250\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -411781.687500\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -421643.812500\n",
      "    epoch          : 184\n",
      "    loss           : -562108.939375\n",
      "    val_loss       : -562756.9298828125\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -698602.312500\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -485980.656250\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -427517.718750\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -727567.000000\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -482980.906250\n",
      "    epoch          : 185\n",
      "    loss           : -561783.9678125\n",
      "    val_loss       : -558472.1201171875\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -471580.812500\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -737454.687500\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -697771.812500\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -727097.500000\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -600303.375000\n",
      "    epoch          : 186\n",
      "    loss           : -561890.6959375\n",
      "    val_loss       : -563982.469921875\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -414442.750000\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -410252.187500\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -690111.125000\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -704839.750000\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -594052.750000\n",
      "    epoch          : 187\n",
      "    loss           : -561125.7696875\n",
      "    val_loss       : -566092.63046875\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -435738.250000\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -729776.625000\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -588964.125000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -633882.375000\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -698770.312500\n",
      "    epoch          : 188\n",
      "    loss           : -562045.1859375\n",
      "    val_loss       : -563256.0560546875\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -648077.687500\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -639967.750000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -650448.125000\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -421190.500000\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -418088.250000\n",
      "    epoch          : 189\n",
      "    loss           : -562697.2821875\n",
      "    val_loss       : -563068.515234375\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -437131.625000\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -439447.812500\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -735127.187500\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -472880.437500\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -416304.125000\n",
      "    epoch          : 190\n",
      "    loss           : -562872.3628125\n",
      "    val_loss       : -562969.2779296875\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -720366.375000\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -637699.062500\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -421421.500000\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -638258.125000\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -646767.875000\n",
      "    epoch          : 191\n",
      "    loss           : -562281.425625\n",
      "    val_loss       : -564623.7048828125\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -408715.500000\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -424540.937500\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -492047.937500\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -644685.250000\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -645888.437500\n",
      "    epoch          : 192\n",
      "    loss           : -563088.6115625\n",
      "    val_loss       : -559849.6841796875\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -471301.187500\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -428563.062500\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -464789.187500\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -418498.062500\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -484080.875000\n",
      "    epoch          : 193\n",
      "    loss           : -560777.406875\n",
      "    val_loss       : -566129.333984375\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -444522.343750\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -414827.812500\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -430190.937500\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -416436.031250\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -416038.968750\n",
      "    epoch          : 194\n",
      "    loss           : -562927.4259375\n",
      "    val_loss       : -564807.9630859375\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -717142.000000\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -427263.375000\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -734052.000000\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -430948.312500\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -590442.187500\n",
      "    epoch          : 195\n",
      "    loss           : -562467.99375\n",
      "    val_loss       : -566861.49765625\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -431692.437500\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -735782.937500\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -725824.000000\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -650107.750000\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -639219.750000\n",
      "    epoch          : 196\n",
      "    loss           : -563312.7828125\n",
      "    val_loss       : -564768.77890625\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -413895.156250\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -416554.937500\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -585173.312500\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -410736.343750\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -483941.812500\n",
      "    epoch          : 197\n",
      "    loss           : -564165.7459375\n",
      "    val_loss       : -565208.885546875\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -426766.812500\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -488846.187500\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -702725.562500\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -705844.625000\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -597368.187500\n",
      "    epoch          : 198\n",
      "    loss           : -563158.44875\n",
      "    val_loss       : -566601.56171875\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -490506.937500\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -712773.562500\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -479818.531250\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -464485.812500\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -411890.531250\n",
      "    epoch          : 199\n",
      "    loss           : -563712.0675\n",
      "    val_loss       : -564755.317578125\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -736657.125000\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -644385.750000\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -652268.625000\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -427186.968750\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -595009.187500\n",
      "    epoch          : 200\n",
      "    loss           : -562907.36875\n",
      "    val_loss       : -563975.344140625\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0923_230151/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -727522.750000\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -471302.312500\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -478164.781250\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -643421.250000\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -594077.375000\n",
      "    epoch          : 201\n",
      "    loss           : -563490.6096875\n",
      "    val_loss       : -566030.4314453125\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -477895.500000\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -424526.406250\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -598586.750000\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -416770.843750\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -595522.250000\n",
      "    epoch          : 202\n",
      "    loss           : -563817.4715625\n",
      "    val_loss       : -566259.3646484375\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -485174.062500\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -596649.625000\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -475539.000000\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -731110.187500\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -705897.625000\n",
      "    epoch          : 203\n",
      "    loss           : -562732.45125\n",
      "    val_loss       : -562022.1017578125\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -694710.250000\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -411928.781250\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -645534.250000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -486829.062500\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -643594.312500\n",
      "    epoch          : 204\n",
      "    loss           : -560580.031875\n",
      "    val_loss       : -563574.7498046875\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -436251.562500\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -732215.937500\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -490522.875000\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -436532.062500\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -597743.812500\n",
      "    epoch          : 205\n",
      "    loss           : -563283.3803125\n",
      "    val_loss       : -563863.144140625\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -706070.000000\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -483473.875000\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -724661.062500\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -704576.062500\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -415485.500000\n",
      "    epoch          : 206\n",
      "    loss           : -563782.421875\n",
      "    val_loss       : -566617.2146484375\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -421509.812500\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -428188.562500\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -473354.656250\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -491213.937500\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -598216.375000\n",
      "    epoch          : 207\n",
      "    loss           : -564826.24625\n",
      "    val_loss       : -567139.5998046875\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -596806.250000\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -635577.562500\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -715550.437500\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -411901.062500\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -640997.250000\n",
      "    epoch          : 208\n",
      "    loss           : -562707.636875\n",
      "    val_loss       : -564933.99609375\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -731166.250000\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -648524.250000\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -489932.718750\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -652550.562500\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -698573.562500\n",
      "    epoch          : 209\n",
      "    loss           : -563426.769375\n",
      "    val_loss       : -565054.962109375\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -416448.875000\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -406852.156250\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -407648.656250\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -647488.125000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -591500.437500\n",
      "    epoch          : 210\n",
      "    loss           : -563677.2496875\n",
      "    val_loss       : -567416.8626953125\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -730764.875000\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -487939.250000\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -648692.875000\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -486318.250000\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -710136.750000\n",
      "    epoch          : 211\n",
      "    loss           : -562950.894375\n",
      "    val_loss       : -563508.50390625\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -429405.593750\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -415955.750000\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -712231.875000\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -697161.750000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -414618.593750\n",
      "    epoch          : 212\n",
      "    loss           : -561334.1359375\n",
      "    val_loss       : -562634.3818359375\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -724492.625000\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -719693.750000\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -470676.781250\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -729109.312500\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -408996.625000\n",
      "    epoch          : 213\n",
      "    loss           : -563439.6865625\n",
      "    val_loss       : -566808.99609375\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -490091.437500\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -709801.687500\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -479547.000000\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -739285.937500\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -420046.875000\n",
      "    epoch          : 214\n",
      "    loss           : -564184.39875\n",
      "    val_loss       : -561358.5953125\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -711079.625000\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -425822.437500\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -732305.000000\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -639936.625000\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -586967.937500\n",
      "    epoch          : 215\n",
      "    loss           : -563524.950625\n",
      "    val_loss       : -565052.76484375\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -739896.437500\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -496577.062500\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -587032.312500\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -431543.562500\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -636866.375000\n",
      "    epoch          : 216\n",
      "    loss           : -563046.5840625\n",
      "    val_loss       : -555357.6107421875\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -705875.250000\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -478693.531250\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -419798.312500\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -644914.500000\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -492509.062500\n",
      "    epoch          : 217\n",
      "    loss           : -560528.426875\n",
      "    val_loss       : -563014.3388671875\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -423395.625000\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -417761.062500\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -486480.000000\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -642511.375000\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -589371.250000\n",
      "    epoch          : 218\n",
      "    loss           : -564138.275625\n",
      "    val_loss       : -566176.480859375\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -710837.125000\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -421635.437500\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -423875.156250\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -479517.781250\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -421355.437500\n",
      "    epoch          : 219\n",
      "    loss           : -563733.9765625\n",
      "    val_loss       : -563922.403125\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -723282.062500\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -478405.062500\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -420494.500000\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -436891.437500\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -420448.593750\n",
      "    epoch          : 220\n",
      "    loss           : -564707.7003125\n",
      "    val_loss       : -567268.5255859375\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -411331.718750\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -705280.062500\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -725260.250000\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -422481.250000\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -599776.125000\n",
      "    epoch          : 221\n",
      "    loss           : -564085.87625\n",
      "    val_loss       : -564560.5869140625\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -639062.875000\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -645993.500000\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -480981.437500\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -489469.750000\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -697454.125000\n",
      "    epoch          : 222\n",
      "    loss           : -564239.3696875\n",
      "    val_loss       : -566617.9869140625\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -740132.625000\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -736151.937500\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -423712.375000\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -437244.000000\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -706661.562500\n",
      "    epoch          : 223\n",
      "    loss           : -564678.695\n",
      "    val_loss       : -567208.7060546875\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -733530.125000\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -595855.750000\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -646941.125000\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -487418.187500\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -420107.281250\n",
      "    epoch          : 224\n",
      "    loss           : -564717.4340625\n",
      "    val_loss       : -567037.890625\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -485672.531250\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -734577.750000\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -489963.093750\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -732502.937500\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -719933.375000\n",
      "    epoch          : 225\n",
      "    loss           : -564720.2109375\n",
      "    val_loss       : -565955.2884765625\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -702284.750000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -718590.625000\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -423739.187500\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -645291.187500\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -482982.125000\n",
      "    epoch          : 226\n",
      "    loss           : -563783.651875\n",
      "    val_loss       : -565935.9552734375\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -596105.500000\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -430842.625000\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -422532.406250\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -415952.281250\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -415922.062500\n",
      "    epoch          : 227\n",
      "    loss           : -565921.240625\n",
      "    val_loss       : -568127.21953125\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -724842.750000\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -636179.625000\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -421390.781250\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -488105.312500\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -486693.125000\n",
      "    epoch          : 228\n",
      "    loss           : -564918.07625\n",
      "    val_loss       : -566130.911328125\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -639455.000000\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -594938.812500\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -734926.875000\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -648678.000000\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -705565.000000\n",
      "    epoch          : 229\n",
      "    loss           : -565565.79\n",
      "    val_loss       : -567790.246484375\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -719718.000000\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -487738.406250\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -594437.937500\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -481799.437500\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -590151.187500\n",
      "    epoch          : 230\n",
      "    loss           : -564586.815625\n",
      "    val_loss       : -567065.337109375\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -735620.750000\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -498132.937500\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -597393.437500\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -594197.062500\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -592612.312500\n",
      "    epoch          : 231\n",
      "    loss           : -563009.449375\n",
      "    val_loss       : -566831.7044921875\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -737041.562500\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -741862.562500\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -424315.375000\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -435722.812500\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -480663.812500\n",
      "    epoch          : 232\n",
      "    loss           : -565611.1953125\n",
      "    val_loss       : -567218.265234375\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -732603.687500\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -427359.343750\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -494925.500000\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -737556.375000\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -647034.375000\n",
      "    epoch          : 233\n",
      "    loss           : -566045.6046875\n",
      "    val_loss       : -565857.410546875\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -485069.812500\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -725455.500000\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -648928.250000\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -704568.625000\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -730164.187500\n",
      "    epoch          : 234\n",
      "    loss           : -562789.2803125\n",
      "    val_loss       : -567229.6794921875\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -733354.250000\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -419301.625000\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -488492.375000\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -643910.062500\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -416185.062500\n",
      "    epoch          : 235\n",
      "    loss           : -564372.8946875\n",
      "    val_loss       : -562921.99375\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -725452.750000\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -420952.656250\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -646896.500000\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -475576.250000\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -414139.125000\n",
      "    epoch          : 236\n",
      "    loss           : -565038.6546875\n",
      "    val_loss       : -565940.7197265625\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -475052.562500\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -471945.656250\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -437867.375000\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -432850.343750\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -417846.687500\n",
      "    epoch          : 237\n",
      "    loss           : -564725.4578125\n",
      "    val_loss       : -567702.8220703125\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -419092.687500\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -421669.625000\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -730929.437500\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -691391.125000\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -425122.781250\n",
      "    epoch          : 238\n",
      "    loss           : -563119.2090625\n",
      "    val_loss       : -566691.4396484375\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -481883.500000\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -475636.093750\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -735785.562500\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -598361.125000\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -705771.625000\n",
      "    epoch          : 239\n",
      "    loss           : -564172.5440625\n",
      "    val_loss       : -568599.2671875\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -432271.531250\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -483652.812500\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -415363.468750\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -588644.000000\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -708130.687500\n",
      "    epoch          : 240\n",
      "    loss           : -565202.9215625\n",
      "    val_loss       : -568845.7359375\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -646228.562500\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -417420.937500\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -421851.468750\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -490815.281250\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -424633.312500\n",
      "    epoch          : 241\n",
      "    loss           : -565158.3575\n",
      "    val_loss       : -564149.8751953125\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -485999.000000\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -476423.562500\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -434606.562500\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -436306.281250\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -700752.750000\n",
      "    epoch          : 242\n",
      "    loss           : -565615.0490625\n",
      "    val_loss       : -568244.85234375\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -594152.000000\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -644944.062500\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -497785.218750\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -648049.437500\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -599318.375000\n",
      "    epoch          : 243\n",
      "    loss           : -565900.194375\n",
      "    val_loss       : -569237.593359375\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -705661.562500\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -731663.875000\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -432277.593750\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -708324.625000\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -592258.875000\n",
      "    epoch          : 244\n",
      "    loss           : -566525.9196875\n",
      "    val_loss       : -569373.05546875\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -642484.500000\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -438021.000000\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -645322.375000\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -426981.218750\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -702193.750000\n",
      "    epoch          : 245\n",
      "    loss           : -564761.3965625\n",
      "    val_loss       : -569387.47265625\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -433765.375000\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -654561.812500\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -497921.187500\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -426824.593750\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -647342.875000\n",
      "    epoch          : 246\n",
      "    loss           : -565435.6425\n",
      "    val_loss       : -566163.835546875\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -418769.906250\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -433716.906250\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -488539.437500\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -672977.312500\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -584130.000000\n",
      "    epoch          : 247\n",
      "    loss           : -554518.048125\n",
      "    val_loss       : -557976.8599609375\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -414378.562500\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -473455.656250\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -589768.750000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -484054.625000\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -421429.062500\n",
      "    epoch          : 248\n",
      "    loss           : -559613.1146875\n",
      "    val_loss       : -565535.4712890625\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -476699.656250\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -427534.187500\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -423005.062500\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -489918.218750\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -633858.000000\n",
      "    epoch          : 249\n",
      "    loss           : -562770.86875\n",
      "    val_loss       : -551322.878125\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -465075.812500\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -587972.000000\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -600713.375000\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -492167.656250\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -597403.375000\n",
      "    epoch          : 250\n",
      "    loss           : -560987.824375\n",
      "    val_loss       : -566555.8576171875\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0923_230151/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -421365.562500\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -421358.875000\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -497684.062500\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -415921.843750\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -703544.250000\n",
      "    epoch          : 251\n",
      "    loss           : -565100.865\n",
      "    val_loss       : -567942.6888671875\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -420658.937500\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -704972.000000\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -740334.312500\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -440119.031250\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -411474.375000\n",
      "    epoch          : 252\n",
      "    loss           : -565943.88625\n",
      "    val_loss       : -567507.7111328125\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -425595.125000\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -644772.437500\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -719206.000000\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -641355.125000\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -705459.187500\n",
      "    epoch          : 253\n",
      "    loss           : -565389.4990625\n",
      "    val_loss       : -566642.7921875\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -420837.500000\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -725325.125000\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -415760.625000\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -423997.781250\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -600237.687500\n",
      "    epoch          : 254\n",
      "    loss           : -565425.3796875\n",
      "    val_loss       : -567545.21171875\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -702465.125000\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -442595.687500\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -740212.687500\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -736667.000000\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -487468.218750\n",
      "    epoch          : 255\n",
      "    loss           : -566943.4428125\n",
      "    val_loss       : -569156.6509765625\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -423565.968750\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -476066.625000\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -439852.125000\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -421866.812500\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -439758.312500\n",
      "    epoch          : 256\n",
      "    loss           : -566901.4753125\n",
      "    val_loss       : -568483.43359375\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -470555.500000\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -638953.562500\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -429327.531250\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -477789.906250\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -708185.187500\n",
      "    epoch          : 257\n",
      "    loss           : -566408.6415625\n",
      "    val_loss       : -569913.0533203125\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -730573.000000\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -433238.937500\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -410505.937500\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -714641.937500\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -422113.250000\n",
      "    epoch          : 258\n",
      "    loss           : -567093.52875\n",
      "    val_loss       : -568378.2201171875\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -731820.250000\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -438881.468750\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -710039.937500\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -442004.875000\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -601318.937500\n",
      "    epoch          : 259\n",
      "    loss           : -567646.836875\n",
      "    val_loss       : -567232.8560546875\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -421124.406250\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -487981.750000\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -440738.500000\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -597234.937500\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -587356.500000\n",
      "    epoch          : 260\n",
      "    loss           : -566960.854375\n",
      "    val_loss       : -568086.6455078125\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -638158.562500\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -473775.718750\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -426638.437500\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -725620.187500\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -701336.937500\n",
      "    epoch          : 261\n",
      "    loss           : -566761.7334375\n",
      "    val_loss       : -569014.4875\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -431515.187500\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -702334.875000\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -429187.375000\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -603027.750000\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -421193.656250\n",
      "    epoch          : 262\n",
      "    loss           : -567249.6428125\n",
      "    val_loss       : -568699.13984375\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -497473.625000\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -441861.625000\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -740722.000000\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -480218.468750\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -433988.781250\n",
      "    epoch          : 263\n",
      "    loss           : -567403.391875\n",
      "    val_loss       : -567384.7009765625\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -482427.937500\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -474652.875000\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -492463.312500\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -599514.187500\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -414368.187500\n",
      "    epoch          : 264\n",
      "    loss           : -566816.356875\n",
      "    val_loss       : -570929.99375\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -439738.500000\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -424986.437500\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -641817.500000\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -593244.750000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -738759.625000\n",
      "    epoch          : 265\n",
      "    loss           : -567273.73125\n",
      "    val_loss       : -568778.43203125\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -433376.375000\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -421545.468750\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -728304.062500\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -731645.125000\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -428319.031250\n",
      "    epoch          : 266\n",
      "    loss           : -567850.135\n",
      "    val_loss       : -569591.1748046875\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -738204.937500\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -649035.437500\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -438000.750000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -485366.343750\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -480529.562500\n",
      "    epoch          : 267\n",
      "    loss           : -568297.9871875\n",
      "    val_loss       : -571070.53515625\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -732229.375000\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -470962.468750\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -709570.125000\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -600625.625000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -712738.562500\n",
      "    epoch          : 268\n",
      "    loss           : -567686.430625\n",
      "    val_loss       : -569186.11328125\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -711941.312500\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -733818.500000\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -495737.656250\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -476266.250000\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -619761.625000\n",
      "    epoch          : 269\n",
      "    loss           : -561916.071875\n",
      "    val_loss       : -563655.4115234375\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -425717.250000\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -488739.062500\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -490060.125000\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -480406.343750\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -598357.125000\n",
      "    epoch          : 270\n",
      "    loss           : -566163.931875\n",
      "    val_loss       : -567558.791015625\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -438997.843750\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -477565.437500\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -478746.750000\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -596204.312500\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -648853.000000\n",
      "    epoch          : 271\n",
      "    loss           : -567219.5596875\n",
      "    val_loss       : -568661.333984375\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -472489.281250\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -740369.125000\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -712855.500000\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -440980.750000\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -653434.437500\n",
      "    epoch          : 272\n",
      "    loss           : -568002.520625\n",
      "    val_loss       : -571429.7310546875\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -709771.500000\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -736288.437500\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -477378.656250\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -644673.437500\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -495417.093750\n",
      "    epoch          : 273\n",
      "    loss           : -566114.51625\n",
      "    val_loss       : -566848.9537109375\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -491095.437500\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -651826.062500\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -734961.000000\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -422712.031250\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -691386.750000\n",
      "    epoch          : 274\n",
      "    loss           : -566231.0128125\n",
      "    val_loss       : -568001.9177734375\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -595581.687500\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -696339.937500\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -706865.875000\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -416655.093750\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -491246.625000\n",
      "    epoch          : 275\n",
      "    loss           : -568310.090625\n",
      "    val_loss       : -569565.8751953125\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -481823.468750\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -418831.687500\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -649929.125000\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -647282.000000\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -421588.312500\n",
      "    epoch          : 276\n",
      "    loss           : -566124.5325\n",
      "    val_loss       : -568126.084765625\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -705125.125000\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -478217.031250\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -597704.437500\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -640478.812500\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -650487.625000\n",
      "    epoch          : 277\n",
      "    loss           : -567270.1434375\n",
      "    val_loss       : -569243.83671875\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -475927.531250\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -417837.687500\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -443404.125000\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -653988.187500\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -593088.750000\n",
      "    epoch          : 278\n",
      "    loss           : -568278.5\n",
      "    val_loss       : -570547.251953125\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -475984.062500\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -472929.468750\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -729351.812500\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -481469.000000\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -644846.437500\n",
      "    epoch          : 279\n",
      "    loss           : -568160.7996875\n",
      "    val_loss       : -570309.153515625\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -737658.250000\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -738929.000000\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -412484.000000\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -420757.343750\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -425959.500000\n",
      "    epoch          : 280\n",
      "    loss           : -567838.7125\n",
      "    val_loss       : -567796.7013671875\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -425198.625000\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -419184.531250\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -429320.937500\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -601230.500000\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -596970.750000\n",
      "    epoch          : 281\n",
      "    loss           : -565961.936875\n",
      "    val_loss       : -563113.1640625\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -415072.687500\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -471012.218750\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -713074.375000\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -420018.750000\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -601790.500000\n",
      "    epoch          : 282\n",
      "    loss           : -567337.3990625\n",
      "    val_loss       : -570053.8720703125\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -448024.062500\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -437735.875000\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -648546.000000\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -645666.187500\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -488651.468750\n",
      "    epoch          : 283\n",
      "    loss           : -568238.124375\n",
      "    val_loss       : -570670.64921875\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -597177.812500\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -697867.125000\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -442989.906250\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -733155.750000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -407571.156250\n",
      "    epoch          : 284\n",
      "    loss           : -562924.7325\n",
      "    val_loss       : -558888.3857421875\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -431018.750000\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -598561.437500\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -493605.437500\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -639265.062500\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -498464.906250\n",
      "    epoch          : 285\n",
      "    loss           : -566297.4034375\n",
      "    val_loss       : -570438.517578125\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -437950.375000\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -597783.250000\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -648389.875000\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -431170.062500\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -655734.500000\n",
      "    epoch          : 286\n",
      "    loss           : -566971.0353125\n",
      "    val_loss       : -568614.9662109375\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -441431.875000\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -435437.468750\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -707234.062500\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -600452.937500\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -640502.687500\n",
      "    epoch          : 287\n",
      "    loss           : -567357.1521875\n",
      "    val_loss       : -569873.788671875\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -441961.750000\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -739166.312500\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -741139.562500\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -737632.000000\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -427188.937500\n",
      "    epoch          : 288\n",
      "    loss           : -569326.154375\n",
      "    val_loss       : -570613.948828125\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -416995.125000\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -490608.625000\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -422444.093750\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -421608.000000\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -647445.125000\n",
      "    epoch          : 289\n",
      "    loss           : -568694.43\n",
      "    val_loss       : -571358.52421875\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -494198.843750\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -418505.687500\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -425933.093750\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -728345.500000\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -644375.125000\n",
      "    epoch          : 290\n",
      "    loss           : -568988.8484375\n",
      "    val_loss       : -570832.294921875\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -415868.812500\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -409557.281250\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -431743.781250\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -491055.718750\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -660028.562500\n",
      "    epoch          : 291\n",
      "    loss           : -560265.4359375\n",
      "    val_loss       : -567015.47109375\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -606444.000000\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -599187.625000\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -482175.562500\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -424954.125000\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -655525.875000\n",
      "    epoch          : 292\n",
      "    loss           : -567655.76\n",
      "    val_loss       : -570847.7228515625\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -718090.500000\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -429497.343750\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -423503.843750\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -498205.250000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -650565.375000\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "    epoch          : 293\n",
      "    loss           : -568435.9465625\n",
      "    val_loss       : -568839.23984375\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -649503.937500\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -735474.875000\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -479380.906250\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -710180.000000\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -655943.437500\n",
      "    epoch          : 294\n",
      "    loss           : -568492.87125\n",
      "    val_loss       : -569836.0330078125\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -480831.250000\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -705833.437500\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -479158.500000\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -736274.812500\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -709390.687500\n",
      "    epoch          : 295\n",
      "    loss           : -568880.6753125\n",
      "    val_loss       : -569740.71640625\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -433969.968750\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -735059.250000\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -735615.125000\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -654463.750000\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -421103.500000\n",
      "    epoch          : 296\n",
      "    loss           : -569854.965625\n",
      "    val_loss       : -571269.7109375\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -657597.312500\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -743233.437500\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -737812.687500\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -727078.250000\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -649120.250000\n",
      "    epoch          : 297\n",
      "    loss           : -570171.7721875\n",
      "    val_loss       : -568154.8087890625\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -702723.062500\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -650676.687500\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -608538.875000\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -432757.812500\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -709839.812500\n",
      "    epoch          : 298\n",
      "    loss           : -569716.0471875\n",
      "    val_loss       : -571091.9158203125\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -479988.375000\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -604177.000000\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -441173.437500\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -740952.062500\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -718999.500000\n",
      "    epoch          : 299\n",
      "    loss           : -569589.2090625\n",
      "    val_loss       : -571722.0861328125\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -732191.000000\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -491285.406250\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -659629.750000\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -433647.156250\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -423011.625000\n",
      "    epoch          : 300\n",
      "    loss           : -569993.58875\n",
      "    val_loss       : -572095.9208984375\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0923_230151/checkpoint-epoch300.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -479596.468750\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -419543.187500\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -603152.375000\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -600623.750000\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -705227.625000\n",
      "    epoch          : 301\n",
      "    loss           : -569726.406875\n",
      "    val_loss       : -569104.3251953125\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -413451.343750\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -653403.250000\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -436733.343750\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -597471.687500\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -600521.125000\n",
      "    epoch          : 302\n",
      "    loss           : -569754.253125\n",
      "    val_loss       : -570864.7111328125\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -495042.500000\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -654415.125000\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -645550.187500\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -597580.562500\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -424237.500000\n",
      "    epoch          : 303\n",
      "    loss           : -570214.25375\n",
      "    val_loss       : -571659.9515625\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -731612.125000\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -436224.343750\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -646556.000000\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -592968.750000\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -421476.062500\n",
      "    epoch          : 304\n",
      "    loss           : -569732.0315625\n",
      "    val_loss       : -571187.8140625\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -658038.125000\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -430164.125000\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -610963.625000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -417500.156250\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -710979.125000\n",
      "    epoch          : 305\n",
      "    loss           : -569471.99\n",
      "    val_loss       : -573300.92578125\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -602484.125000\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -739423.062500\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -732914.750000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -739291.000000\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -489408.562500\n",
      "    epoch          : 306\n",
      "    loss           : -569647.266875\n",
      "    val_loss       : -571945.923046875\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -651444.312500\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -433606.531250\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -641717.500000\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -431068.218750\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -425771.156250\n",
      "    epoch          : 307\n",
      "    loss           : -569913.1375\n",
      "    val_loss       : -572842.4041015625\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -727416.375000\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -488256.187500\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -491438.406250\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -429494.062500\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -651506.875000\n",
      "    epoch          : 308\n",
      "    loss           : -570109.2571875\n",
      "    val_loss       : -571790.5080078125\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -742566.562500\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -429407.312500\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -438618.437500\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -648858.875000\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -608036.437500\n",
      "    epoch          : 309\n",
      "    loss           : -570157.12625\n",
      "    val_loss       : -570217.0771484375\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -606497.562500\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -601364.687500\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -734656.062500\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -654827.875000\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -652576.062500\n",
      "    epoch          : 310\n",
      "    loss           : -570131.7990625\n",
      "    val_loss       : -570831.753125\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -590844.312500\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -725767.750000\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -422330.312500\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -427081.468750\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -438375.093750\n",
      "    epoch          : 311\n",
      "    loss           : -569868.5296875\n",
      "    val_loss       : -572054.9564453125\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -436272.031250\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -446750.500000\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -427169.156250\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -649717.812500\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -428534.500000\n",
      "    epoch          : 312\n",
      "    loss           : -570063.636875\n",
      "    val_loss       : -572865.493359375\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -438375.406250\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -490435.750000\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -505322.250000\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -736506.375000\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -435251.000000\n",
      "    epoch          : 313\n",
      "    loss           : -570342.3665625\n",
      "    val_loss       : -572375.0732421875\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -734635.437500\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -738258.812500\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -415383.218750\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -721512.812500\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -601134.062500\n",
      "    epoch          : 314\n",
      "    loss           : -570058.7828125\n",
      "    val_loss       : -572447.4884765625\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -436118.781250\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -731831.125000\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -440923.187500\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -445664.531250\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -703760.312500\n",
      "    epoch          : 315\n",
      "    loss           : -570672.636875\n",
      "    val_loss       : -572388.462890625\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -481666.718750\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -603731.125000\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -484602.562500\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -426572.750000\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -712958.750000\n",
      "    epoch          : 316\n",
      "    loss           : -570152.308125\n",
      "    val_loss       : -574444.4837890625\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -433852.156250\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -432529.312500\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -411127.562500\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -498030.093750\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -707064.687500\n",
      "    epoch          : 317\n",
      "    loss           : -570404.6296875\n",
      "    val_loss       : -571310.0638671875\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -425388.812500\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -432423.093750\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -419284.000000\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -739064.125000\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -420641.468750\n",
      "    epoch          : 318\n",
      "    loss           : -570133.848125\n",
      "    val_loss       : -571899.2708984375\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -487066.312500\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -479985.468750\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -490484.531250\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -433480.687500\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -596348.812500\n",
      "    epoch          : 319\n",
      "    loss           : -570199.7578125\n",
      "    val_loss       : -573341.7947265625\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -602365.250000\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -645882.375000\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -453127.093750\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -431469.843750\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -651284.625000\n",
      "    epoch          : 320\n",
      "    loss           : -570736.568125\n",
      "    val_loss       : -573421.2369140625\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -739219.000000\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -420974.625000\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -605410.750000\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -422120.843750\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -599630.500000\n",
      "    epoch          : 321\n",
      "    loss           : -570314.296875\n",
      "    val_loss       : -572989.8900390625\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -498694.625000\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -432333.562500\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -596643.625000\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -653397.812500\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -426619.218750\n",
      "    epoch          : 322\n",
      "    loss           : -570385.6321875\n",
      "    val_loss       : -573615.331640625\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -482146.437500\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -439529.687500\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -488707.687500\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -441610.531250\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -643214.500000\n",
      "    epoch          : 323\n",
      "    loss           : -570100.8646875\n",
      "    val_loss       : -573858.9626953125\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -421794.937500\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -504385.750000\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -712755.187500\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -481153.875000\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -709284.750000\n",
      "    epoch          : 324\n",
      "    loss           : -570529.366875\n",
      "    val_loss       : -570538.348046875\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -643312.500000\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -653058.250000\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -609061.312500\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -735140.375000\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -659736.187500\n",
      "    epoch          : 325\n",
      "    loss           : -569852.5865625\n",
      "    val_loss       : -570875.567578125\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -429222.500000\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -473382.875000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -439141.312500\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -710692.625000\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -422512.125000\n",
      "    epoch          : 326\n",
      "    loss           : -570671.79\n",
      "    val_loss       : -570538.7396484375\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -739991.062500\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -426112.218750\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -429158.500000\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -703422.000000\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -429509.156250\n",
      "    epoch          : 327\n",
      "    loss           : -570316.9671875\n",
      "    val_loss       : -572689.4451171875\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -736125.500000\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -714580.875000\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -487936.343750\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -427763.593750\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -490347.343750\n",
      "    epoch          : 328\n",
      "    loss           : -569739.2540625\n",
      "    val_loss       : -570226.376953125\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -718502.875000\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -497225.000000\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -732261.437500\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -710712.562500\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -598397.625000\n",
      "    epoch          : 329\n",
      "    loss           : -569756.0046875\n",
      "    val_loss       : -571015.885546875\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -443544.250000\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -485266.031250\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -430444.375000\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -605916.062500\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -429032.843750\n",
      "    epoch          : 330\n",
      "    loss           : -570330.4903125\n",
      "    val_loss       : -572134.622265625\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -704922.687500\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -739918.000000\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -495285.218750\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -446259.000000\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -483870.468750\n",
      "    epoch          : 331\n",
      "    loss           : -570851.2665625\n",
      "    val_loss       : -572247.5767578125\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -423310.000000\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -486944.875000\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -732026.937500\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -433346.687500\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -648130.187500\n",
      "    epoch          : 332\n",
      "    loss           : -570994.98625\n",
      "    val_loss       : -571385.2310546875\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -424866.000000\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -733781.250000\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -728061.312500\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -707724.125000\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -604120.875000\n",
      "    epoch          : 333\n",
      "    loss           : -570593.145\n",
      "    val_loss       : -572404.6921875\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -645744.250000\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -743256.312500\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -480740.468750\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -711005.500000\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -640642.750000\n",
      "    epoch          : 334\n",
      "    loss           : -570292.7221875\n",
      "    val_loss       : -571203.88359375\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -735989.875000\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -733560.500000\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -430252.187500\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -708624.812500\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -606793.000000\n",
      "    epoch          : 335\n",
      "    loss           : -570753.4965625\n",
      "    val_loss       : -570898.7578125\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -424085.187500\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -444167.500000\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -649951.812500\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -697582.750000\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -424700.687500\n",
      "    epoch          : 336\n",
      "    loss           : -570220.6834375\n",
      "    val_loss       : -571488.6611328125\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -498258.437500\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -740926.500000\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -434432.125000\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -599459.687500\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -431627.718750\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   336: reducing learning rate of group 0 to 1.0000e-06.\n",
      "    epoch          : 337\n",
      "    loss           : -570786.771875\n",
      "    val_loss       : -572731.9322265625\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -429046.906250\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -442529.625000\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -439092.750000\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -601137.062500\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -700263.000000\n",
      "    epoch          : 338\n",
      "    loss           : -570738.428125\n",
      "    val_loss       : -571019.39609375\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -657945.125000\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -710963.187500\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -487314.968750\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -739162.375000\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -431302.687500\n",
      "    epoch          : 339\n",
      "    loss           : -569937.4321875\n",
      "    val_loss       : -570505.8603515625\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -645077.062500\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -435767.093750\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -712184.812500\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -656304.375000\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -497689.031250\n",
      "    epoch          : 340\n",
      "    loss           : -570773.11875\n",
      "    val_loss       : -571394.4048828125\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -429555.312500\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -710490.375000\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -473578.281250\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -439714.093750\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -705853.250000\n",
      "    epoch          : 341\n",
      "    loss           : -570261.4\n",
      "    val_loss       : -572114.5349609375\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -648340.875000\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -715236.125000\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -500286.031250\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -493452.656250\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -709982.937500\n",
      "    epoch          : 342\n",
      "    loss           : -570669.0578125\n",
      "    val_loss       : -571759.8955078125\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -484226.593750\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -484032.437500\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -425914.468750\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -422649.875000\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -602656.875000\n",
      "    epoch          : 343\n",
      "    loss           : -570289.3071875\n",
      "    val_loss       : -571211.2423828125\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -717567.000000\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -425265.312500\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -595284.062500\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -485080.687500\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -498326.312500\n",
      "    epoch          : 344\n",
      "    loss           : -570478.7171875\n",
      "    val_loss       : -571352.6697265625\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -726550.750000\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -705064.750000\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -707955.000000\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -495934.343750\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -493266.468750\n",
      "    epoch          : 345\n",
      "    loss           : -570400.1621875\n",
      "    val_loss       : -571315.762109375\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -495355.187500\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -492603.812500\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -597236.750000\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -437578.562500\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -602200.625000\n",
      "    epoch          : 346\n",
      "    loss           : -570449.546875\n",
      "    val_loss       : -572686.8568359375\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -740134.250000\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -735751.125000\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -420857.156250\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -732672.250000\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -709744.375000\n",
      "    epoch          : 347\n",
      "    loss           : -570253.56375\n",
      "    val_loss       : -571367.6515625\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -602955.125000\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -487815.625000\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -742302.875000\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -598282.562500\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -642608.375000\n",
      "    epoch          : 348\n",
      "    loss           : -570440.025\n",
      "    val_loss       : -571749.103125\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -483743.000000\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -741314.000000\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -604478.812500\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -496262.625000\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -433499.937500\n",
      "    epoch          : 349\n",
      "    loss           : -570786.078125\n",
      "    val_loss       : -569421.9748046875\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -450528.000000\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -434005.250000\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -742447.625000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -421419.031250\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -650797.750000\n",
      "    epoch          : 350\n",
      "    loss           : -570630.5209375\n",
      "    val_loss       : -570288.164453125\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0923_230151/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -499267.218750\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -645472.125000\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -450364.687500\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -743706.437500\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -492292.406250\n",
      "    epoch          : 351\n",
      "    loss           : -570435.9284375\n",
      "    val_loss       : -571216.0138671875\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -448322.062500\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -488707.062500\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -733735.375000\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -472830.437500\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -597377.875000\n",
      "    epoch          : 352\n",
      "    loss           : -570545.5134375\n",
      "    val_loss       : -573696.1244140625\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -444529.937500\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -430641.187500\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -414730.375000\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -648708.625000\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -433349.937500\n",
      "    epoch          : 353\n",
      "    loss           : -570308.811875\n",
      "    val_loss       : -570725.320703125\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -490734.906250\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -597043.937500\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -502282.437500\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -430591.437500\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -499184.062500\n",
      "    epoch          : 354\n",
      "    loss           : -570501.8746875\n",
      "    val_loss       : -574005.782421875\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -738509.875000\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -736273.562500\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -748806.375000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -658732.687500\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -728978.812500\n",
      "    epoch          : 355\n",
      "    loss           : -570521.8090625\n",
      "    val_loss       : -573053.187890625\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -421799.562500\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -430819.031250\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -655268.250000\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -422353.187500\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -604278.812500\n",
      "    epoch          : 356\n",
      "    loss           : -570890.168125\n",
      "    val_loss       : -572315.305078125\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -425937.000000\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -740502.125000\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -599251.125000\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -660417.500000\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -654420.750000\n",
      "    epoch          : 357\n",
      "    loss           : -570391.050625\n",
      "    val_loss       : -571064.1494140625\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -441593.500000\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -738405.625000\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -737703.937500\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -604576.875000\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -715154.625000\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   357: reducing learning rate of group 0 to 1.0000e-07.\n",
      "    epoch          : 358\n",
      "    loss           : -570682.0346875\n",
      "    val_loss       : -571891.625390625\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -425674.750000\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -467125.156250\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -716196.875000\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -439706.562500\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -651645.125000\n",
      "    epoch          : 359\n",
      "    loss           : -570463.5909375\n",
      "    val_loss       : -574599.15390625\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -433672.968750\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -741615.250000\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -434320.312500\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -658753.312500\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -647162.125000\n",
      "    epoch          : 360\n",
      "    loss           : -570656.6978125\n",
      "    val_loss       : -573174.21171875\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -595208.875000\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -495543.187500\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -490957.187500\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -422621.468750\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -659394.687500\n",
      "    epoch          : 361\n",
      "    loss           : -570839.9921875\n",
      "    val_loss       : -572585.617578125\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -743166.500000\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -431360.468750\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -604920.125000\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -711655.187500\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -711264.125000\n",
      "    epoch          : 362\n",
      "    loss           : -570291.4365625\n",
      "    val_loss       : -571743.950390625\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -494372.000000\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -493749.687500\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -594767.875000\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -604818.187500\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -425191.218750\n",
      "    epoch          : 363\n",
      "    loss           : -570601.24125\n",
      "    val_loss       : -571921.8630859375\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -421265.875000\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -496524.000000\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -429790.375000\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -487224.812500\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -605605.625000\n",
      "    epoch          : 364\n",
      "    loss           : -570601.93875\n",
      "    val_loss       : -572918.5189453125\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -431574.281250\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -433402.156250\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -427198.687500\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -730841.375000\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -649297.125000\n",
      "    epoch          : 365\n",
      "    loss           : -571074.3465625\n",
      "    val_loss       : -572560.7373046875\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -648748.937500\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -610160.875000\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -437506.687500\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -598349.000000\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -721979.500000\n",
      "    epoch          : 366\n",
      "    loss           : -570839.755\n",
      "    val_loss       : -570201.858984375\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -741321.312500\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -440981.562500\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -481396.656250\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -602369.375000\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -738427.250000\n",
      "    epoch          : 367\n",
      "    loss           : -570900.013125\n",
      "    val_loss       : -573033.9509765625\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -711395.437500\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -419342.062500\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -743007.750000\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -435651.125000\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -716275.000000\n",
      "    epoch          : 368\n",
      "    loss           : -571062.5309375\n",
      "    val_loss       : -572899.5642578125\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -606099.625000\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -736160.625000\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -485296.000000\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -499335.718750\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -428015.875000\n",
      "    epoch          : 369\n",
      "    loss           : -570408.6478125\n",
      "    val_loss       : -572531.2830078125\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -483444.062500\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -492403.562500\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -733033.562500\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -492709.906250\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -500056.906250\n",
      "    epoch          : 370\n",
      "    loss           : -570733.8971875\n",
      "    val_loss       : -572753.59453125\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -711054.250000\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -738893.187500\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -714074.625000\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -707933.125000\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -602493.625000\n",
      "    epoch          : 371\n",
      "    loss           : -570663.15875\n",
      "    val_loss       : -572165.00859375\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -603035.875000\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -709660.250000\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -422355.062500\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -645952.500000\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -423402.593750\n",
      "    epoch          : 372\n",
      "    loss           : -570795.84625\n",
      "    val_loss       : -573335.83828125\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -433309.437500\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -741248.562500\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -500131.625000\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -731336.937500\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -711792.375000\n",
      "    epoch          : 373\n",
      "    loss           : -570771.6103125\n",
      "    val_loss       : -573890.32734375\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -491253.625000\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -424343.593750\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -727992.500000\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -421136.312500\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -504428.687500\n",
      "    epoch          : 374\n",
      "    loss           : -570385.7371875\n",
      "    val_loss       : -574460.1548828125\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -596762.625000\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -650505.375000\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -426063.281250\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -603354.500000\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -440261.156250\n",
      "    epoch          : 375\n",
      "    loss           : -570619.6978125\n",
      "    val_loss       : -571358.8767578125\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -437378.562500\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -444644.687500\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -742305.250000\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -425049.468750\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -649061.625000\n",
      "    epoch          : 376\n",
      "    loss           : -570760.4309375\n",
      "    val_loss       : -572466.7890625\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -715166.812500\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -737490.500000\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -738590.312500\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -506079.312500\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -428758.187500\n",
      "    epoch          : 377\n",
      "    loss           : -570602.04625\n",
      "    val_loss       : -574017.332421875\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -735812.812500\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -449455.500000\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -428925.687500\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -477400.468750\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -421621.218750\n",
      "    epoch          : 378\n",
      "    loss           : -570175.87375\n",
      "    val_loss       : -570818.3169921875\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -740496.125000\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -729465.000000\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -737400.875000\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -733050.312500\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -496223.031250\n",
      "    epoch          : 379\n",
      "    loss           : -570470.929375\n",
      "    val_loss       : -571961.514453125\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -725990.562500\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -650146.625000\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -435017.218750\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -483093.687500\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -713617.437500\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "    epoch          : 380\n",
      "    loss           : -570397.414375\n",
      "    val_loss       : -571756.584375\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -715537.750000\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -498165.062500\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -740023.562500\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -733859.125000\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -604536.250000\n",
      "    epoch          : 381\n",
      "    loss           : -571020.2128125\n",
      "    val_loss       : -572166.9927734375\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -650022.812500\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -707944.875000\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -427502.687500\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -707028.062500\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -704067.187500\n",
      "    epoch          : 382\n",
      "    loss           : -570435.9834375\n",
      "    val_loss       : -572170.708984375\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -477728.062500\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -599941.062500\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -652333.937500\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -432515.625000\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -431527.937500\n",
      "    epoch          : 383\n",
      "    loss           : -570778.53875\n",
      "    val_loss       : -573814.2806640625\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -642228.187500\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -434075.562500\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -651036.875000\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -425020.812500\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -422616.968750\n",
      "    epoch          : 384\n",
      "    loss           : -570754.145625\n",
      "    val_loss       : -573477.746484375\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -738985.000000\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -650848.000000\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -731273.375000\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -713261.125000\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -419906.406250\n",
      "    epoch          : 385\n",
      "    loss           : -570248.1846875\n",
      "    val_loss       : -572320.7296875\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -648887.250000\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -739164.000000\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -440308.625000\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -601823.312500\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -422441.875000\n",
      "    epoch          : 386\n",
      "    loss           : -570986.6540625\n",
      "    val_loss       : -573200.91875\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -485446.312500\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -482539.343750\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -599478.312500\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -713881.687500\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -486265.406250\n",
      "    epoch          : 387\n",
      "    loss           : -570428.0665625\n",
      "    val_loss       : -573137.461328125\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -739335.125000\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -743472.812500\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -652116.500000\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -708072.437500\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -714391.750000\n",
      "    epoch          : 388\n",
      "    loss           : -570595.785625\n",
      "    val_loss       : -571068.376171875\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -703757.375000\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -481849.156250\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -430170.593750\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -499301.406250\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -604831.312500\n",
      "    epoch          : 389\n",
      "    loss           : -571037.9996875\n",
      "    val_loss       : -571180.072265625\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -600409.500000\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -444754.375000\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -424987.437500\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -652547.875000\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -709813.375000\n",
      "    epoch          : 390\n",
      "    loss           : -570615.5596875\n",
      "    val_loss       : -571534.6232421875\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -414067.062500\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -720489.750000\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -714409.250000\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -483979.125000\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -426159.875000\n",
      "    epoch          : 391\n",
      "    loss           : -570743.2\n",
      "    val_loss       : -572395.469921875\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -423256.031250\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -716551.125000\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -599950.937500\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -736754.437500\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -734617.500000\n",
      "    epoch          : 392\n",
      "    loss           : -570991.9665625\n",
      "    val_loss       : -573117.201171875\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -424186.625000\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -641069.375000\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -492871.812500\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -486345.406250\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -441221.437500\n",
      "    epoch          : 393\n",
      "    loss           : -570156.025625\n",
      "    val_loss       : -573574.398828125\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -727241.437500\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -604093.000000\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -604975.187500\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -652331.250000\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -493231.281250\n",
      "    epoch          : 394\n",
      "    loss           : -570698.5571875\n",
      "    val_loss       : -573827.254296875\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -742060.937500\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -428927.062500\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -729987.937500\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -652379.312500\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -599750.500000\n",
      "    epoch          : 395\n",
      "    loss           : -570969.9634375\n",
      "    val_loss       : -571715.9263671875\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -493222.750000\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -421179.250000\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -435412.875000\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -500113.750000\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -503421.812500\n",
      "    epoch          : 396\n",
      "    loss           : -570249.8753125\n",
      "    val_loss       : -571441.932421875\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -599711.250000\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -732852.000000\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -433059.968750\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -732731.187500\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -602062.687500\n",
      "    epoch          : 397\n",
      "    loss           : -570905.4478125\n",
      "    val_loss       : -572636.7220703125\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -424046.468750\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -439499.312500\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -724145.250000\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -436764.343750\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -716632.812500\n",
      "    epoch          : 398\n",
      "    loss           : -571003.2228125\n",
      "    val_loss       : -570265.3154296875\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -435960.781250\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -433431.562500\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -433006.968750\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -653574.125000\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -478654.906250\n",
      "    epoch          : 399\n",
      "    loss           : -570522.415625\n",
      "    val_loss       : -573036.29140625\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -442582.125000\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -714895.125000\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -428079.437500\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -490737.750000\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -705183.750000\n",
      "    epoch          : 400\n",
      "    loss           : -570857.5515625\n",
      "    val_loss       : -570273.83984375\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0923_230151/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -729332.000000\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -641539.125000\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -742471.750000\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -500929.812500\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -650462.500000\n",
      "    epoch          : 401\n",
      "    loss           : -570946.209375\n",
      "    val_loss       : -570729.6921875\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -427392.843750\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -725881.000000\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -430931.562500\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -486197.468750\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -496610.375000\n",
      "    epoch          : 402\n",
      "    loss           : -570360.4659375\n",
      "    val_loss       : -570481.6333984375\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -741943.062500\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -427410.562500\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -597341.687500\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -648137.062500\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -497738.562500\n",
      "    epoch          : 403\n",
      "    loss           : -571095.14625\n",
      "    val_loss       : -571098.8140625\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -655384.250000\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -738905.437500\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -725170.562500\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -427645.875000\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -430464.187500\n",
      "    epoch          : 404\n",
      "    loss           : -570552.62125\n",
      "    val_loss       : -573182.6080078125\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -702885.312500\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -602336.375000\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -439947.875000\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -429509.031250\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -652413.500000\n",
      "    epoch          : 405\n",
      "    loss           : -570394.27\n",
      "    val_loss       : -572631.8490234375\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -647058.125000\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -595416.625000\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -424805.437500\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -719671.000000\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -658352.750000\n",
      "    epoch          : 406\n",
      "    loss           : -570616.6346875\n",
      "    val_loss       : -571109.39921875\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -443277.500000\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -485832.656250\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -419384.093750\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -493295.625000\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -652112.812500\n",
      "    epoch          : 407\n",
      "    loss           : -570507.7459375\n",
      "    val_loss       : -570843.7732421875\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -744136.000000\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -430511.687500\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -733866.000000\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -434884.000000\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -435050.000000\n",
      "    epoch          : 408\n",
      "    loss           : -570297.6646875\n",
      "    val_loss       : -572467.040234375\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -489812.687500\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -604039.125000\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -492986.687500\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -492007.468750\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -707398.500000\n",
      "    epoch          : 409\n",
      "    loss           : -571046.77125\n",
      "    val_loss       : -573580.7046875\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -650514.937500\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -735023.750000\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -487010.562500\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -644239.937500\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -430479.437500\n",
      "    epoch          : 410\n",
      "    loss           : -571050.5090625\n",
      "    val_loss       : -571977.2486328125\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -430456.968750\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -496864.531250\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -488079.031250\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -422120.593750\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -606004.375000\n",
      "    epoch          : 411\n",
      "    loss           : -571231.0875\n",
      "    val_loss       : -572481.0212890625\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -489294.000000\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -429207.625000\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -438588.531250\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -492961.125000\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -484309.687500\n",
      "    epoch          : 412\n",
      "    loss           : -570737.973125\n",
      "    val_loss       : -572495.4603515625\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -730927.375000\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -715808.750000\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -735703.500000\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -493181.875000\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -736556.000000\n",
      "    epoch          : 413\n",
      "    loss           : -570636.0465625\n",
      "    val_loss       : -572238.2888671875\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -472724.250000\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -605994.625000\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -701443.187500\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -714787.312500\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -730099.750000\n",
      "    epoch          : 414\n",
      "    loss           : -570444.47375\n",
      "    val_loss       : -570070.7287109375\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -731890.812500\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -445442.000000\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -437346.437500\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -650918.000000\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -489645.718750\n",
      "    epoch          : 415\n",
      "    loss           : -570820.2996875\n",
      "    val_loss       : -572551.9810546875\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -741051.937500\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -595924.500000\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -486498.875000\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -735496.500000\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -653439.250000\n",
      "    epoch          : 416\n",
      "    loss           : -570263.8628125\n",
      "    val_loss       : -571716.0369140625\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -597011.250000\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -496720.781250\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -436356.187500\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -600814.625000\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -601096.562500\n",
      "    epoch          : 417\n",
      "    loss           : -570542.36125\n",
      "    val_loss       : -571137.1244140625\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -445818.281250\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -733571.125000\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -738707.687500\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -493918.375000\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -612693.750000\n",
      "    epoch          : 418\n",
      "    loss           : -570784.8\n",
      "    val_loss       : -571474.725390625\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -494889.312500\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -422611.250000\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -737508.375000\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -737454.625000\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -422075.937500\n",
      "    epoch          : 419\n",
      "    loss           : -570507.95\n",
      "    val_loss       : -572639.4951171875\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -732848.187500\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -709473.250000\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -703028.750000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -736493.000000\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -709069.812500\n",
      "    epoch          : 420\n",
      "    loss           : -570936.155625\n",
      "    val_loss       : -570878.451171875\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -479062.875000\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -487781.031250\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -736582.375000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -732911.125000\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -704383.312500\n",
      "    epoch          : 421\n",
      "    loss           : -571109.998125\n",
      "    val_loss       : -571270.338671875\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -428845.781250\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -491432.062500\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -740958.937500\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -736469.500000\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -652448.625000\n",
      "    epoch          : 422\n",
      "    loss           : -570677.6665625\n",
      "    val_loss       : -571315.3326171875\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -488134.312500\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -495160.937500\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -443054.562500\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -425688.187500\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -430146.312500\n",
      "    epoch          : 423\n",
      "    loss           : -570394.9084375\n",
      "    val_loss       : -569522.8896484375\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -427226.656250\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -734889.437500\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -422828.281250\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -437417.312500\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -423518.750000\n",
      "    epoch          : 424\n",
      "    loss           : -570759.7028125\n",
      "    val_loss       : -572836.9767578125\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -739155.625000\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -426062.781250\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -654890.937500\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -745892.562500\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -431478.187500\n",
      "    epoch          : 425\n",
      "    loss           : -570616.0859375\n",
      "    val_loss       : -571311.259375\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -657765.812500\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -447306.593750\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -729103.937500\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -610391.937500\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -604743.375000\n",
      "    epoch          : 426\n",
      "    loss           : -570956.9859375\n",
      "    val_loss       : -572602.8357421875\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -711251.562500\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -605242.750000\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -737442.125000\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -603565.312500\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -714434.250000\n",
      "    epoch          : 427\n",
      "    loss           : -570138.6971875\n",
      "    val_loss       : -571983.275390625\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -603492.812500\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -426139.093750\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -650789.312500\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -710950.937500\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -419843.062500\n",
      "    epoch          : 428\n",
      "    loss           : -570940.369375\n",
      "    val_loss       : -572113.6533203125\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -601288.000000\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -428013.437500\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -425594.375000\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -483739.500000\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -740365.687500\n",
      "    epoch          : 429\n",
      "    loss           : -570964.215625\n",
      "    val_loss       : -572225.7578125\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -704484.625000\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -735110.687500\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -731963.875000\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -730357.750000\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -651856.000000\n",
      "    epoch          : 430\n",
      "    loss           : -570704.853125\n",
      "    val_loss       : -572225.2845703125\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -601643.750000\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -650309.000000\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -436360.406250\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -420686.937500\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -650631.187500\n",
      "    epoch          : 431\n",
      "    loss           : -570782.198125\n",
      "    val_loss       : -574295.438671875\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -442045.718750\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -440407.343750\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -429520.656250\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -413376.250000\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -595302.250000\n",
      "    epoch          : 432\n",
      "    loss           : -570757.6575\n",
      "    val_loss       : -572547.149609375\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -436480.562500\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -506463.625000\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -441896.375000\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -703821.000000\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -437868.718750\n",
      "    epoch          : 433\n",
      "    loss           : -570536.6765625\n",
      "    val_loss       : -572338.90078125\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -495174.468750\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -738367.875000\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -440616.937500\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -433334.562500\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -739275.562500\n",
      "    epoch          : 434\n",
      "    loss           : -570844.62125\n",
      "    val_loss       : -572314.3240234375\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -598145.312500\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -443052.468750\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -597532.750000\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -643280.250000\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -651819.000000\n",
      "    epoch          : 435\n",
      "    loss           : -570625.1521875\n",
      "    val_loss       : -572108.4068359375\n",
      "Validation performance didn't improve for 75 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VlaeCategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (7): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=196, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): LadderDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=392, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (noise_linear): Linear(in_features=2744, out_features=4, bias=True)\n",
       "      (ladder_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (ladder_linear): Linear(in_features=2744, out_features=392, bias=True)\n",
       "    )\n",
       "    (generator_5): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_5_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_6_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_7_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (7): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=196, out_features=392, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_8_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (7): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=196, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_9_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=16, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=42, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
