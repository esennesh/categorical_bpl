{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 569224.125000\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: 358179.937500\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: 276281.875000\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: 240193.750000\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: 176271.250000\n",
      "    epoch          : 1\n",
      "    loss           : 289875.71515625\n",
      "    val_loss       : 163790.17675392627\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: 160747.437500\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: 132181.437500\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: 110954.710938\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: 100028.437500\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: 96068.437500\n",
      "    epoch          : 2\n",
      "    loss           : 118715.603515625\n",
      "    val_loss       : 72281.76645600796\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: 71647.296875\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: 53307.667969\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: 51117.804688\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: 31027.921875\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: 29912.169922\n",
      "    epoch          : 3\n",
      "    loss           : 49663.08920410156\n",
      "    val_loss       : 21392.564913606642\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: 33482.273438\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -7297.458008\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -9030.557617\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: 9067.423828\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -6135.886230\n",
      "    epoch          : 4\n",
      "    loss           : 13312.113273925781\n",
      "    val_loss       : -4630.1540850400925\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -38302.515625\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -42035.531250\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -50483.261719\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -9576.408203\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -20850.710938\n",
      "    epoch          : 5\n",
      "    loss           : -12706.96231689453\n",
      "    val_loss       : -36132.90549480915\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -25617.994141\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -18008.666016\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -57550.480469\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -11661.909180\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: 73065.921875\n",
      "    epoch          : 6\n",
      "    loss           : -17835.898571777343\n",
      "    val_loss       : -16526.154086434843\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: 87015.781250\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -71845.640625\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -40975.781250\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -78006.304688\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -78211.937500\n",
      "    epoch          : 7\n",
      "    loss           : -46178.5313671875\n",
      "    val_loss       : -53734.80682106018\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: 40880.359375\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -44258.960938\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -106016.929688\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -68520.125000\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -59683.859375\n",
      "    epoch          : 8\n",
      "    loss           : -62194.91048828125\n",
      "    val_loss       : -54678.05115351677\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -48474.183594\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -70621.882812\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -118363.054688\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -56437.484375\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -714.218750\n",
      "    epoch          : 9\n",
      "    loss           : -53385.33518554687\n",
      "    val_loss       : -45242.004727494714\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -38395.093750\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -43080.386719\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -47271.703125\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -127508.234375\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -115458.679688\n",
      "    epoch          : 10\n",
      "    loss           : -67364.2059375\n",
      "    val_loss       : -76573.45687884092\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: 20703.001953\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -56950.882812\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -131376.000000\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -87917.617188\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -70694.390625\n",
      "    epoch          : 11\n",
      "    loss           : -81179.76568359375\n",
      "    val_loss       : -80387.13158552647\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -81112.335938\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -121704.484375\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -49435.089844\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -36682.914062\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -121199.820312\n",
      "    epoch          : 12\n",
      "    loss           : -82652.6521875\n",
      "    val_loss       : -106813.00764242411\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -116882.937500\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -104177.781250\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -160270.968750\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -81346.390625\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -99560.890625\n",
      "    epoch          : 13\n",
      "    loss           : -102801.6910546875\n",
      "    val_loss       : -107756.2167623043\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -172995.656250\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -106553.398438\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -155165.500000\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -162608.781250\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -84062.929688\n",
      "    epoch          : 14\n",
      "    loss           : -98746.51017578125\n",
      "    val_loss       : -107097.23127586841\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -156222.140625\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -97774.953125\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -85457.804688\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -110657.375000\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -114358.890625\n",
      "    epoch          : 15\n",
      "    loss           : -102985.151171875\n",
      "    val_loss       : -107708.58507355451\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -164182.125000\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -171114.000000\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -180834.968750\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -92260.765625\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: 19194.607422\n",
      "    epoch          : 16\n",
      "    loss           : -100486.39545898438\n",
      "    val_loss       : -46009.560057651994\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -108949.250000\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -137250.125000\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -114822.117188\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -111641.476562\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -112824.921875\n",
      "    epoch          : 17\n",
      "    loss           : -100030.8807421875\n",
      "    val_loss       : -128012.15784566998\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -140115.718750\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: 4619.570312\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -178171.500000\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -121838.789062\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -122216.632812\n",
      "    epoch          : 18\n",
      "    loss           : -111738.73751953125\n",
      "    val_loss       : -124449.68047064543\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -108364.859375\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -139558.609375\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -160908.781250\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -84307.812500\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -146075.625000\n",
      "    epoch          : 19\n",
      "    loss           : -122740.69086914063\n",
      "    val_loss       : -124132.44406226874\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -123544.156250\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -127842.578125\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -123578.515625\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -143826.375000\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -67621.101562\n",
      "    epoch          : 20\n",
      "    loss           : -121786.83087890624\n",
      "    val_loss       : -114257.31218031645\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: 3638.263672\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: 10733.761719\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -131308.531250\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -121147.750000\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -103738.625000\n",
      "    epoch          : 21\n",
      "    loss           : -127534.04077148438\n",
      "    val_loss       : -128811.4764328003\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -127748.265625\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -165446.218750\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -138871.234375\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -133537.156250\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -126532.328125\n",
      "    epoch          : 22\n",
      "    loss           : -131935.53095703124\n",
      "    val_loss       : -138001.12326378823\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -137763.187500\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -146754.484375\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -192242.921875\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -137305.406250\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -145251.781250\n",
      "    epoch          : 23\n",
      "    loss           : -130944.64225585938\n",
      "    val_loss       : -138919.36139882804\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -126565.164062\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -148986.000000\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -136154.250000\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -133210.390625\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -131173.765625\n",
      "    epoch          : 24\n",
      "    loss           : -134722.8480859375\n",
      "    val_loss       : -142169.3863672912\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -208969.531250\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -13748.363281\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -7892.767578\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -149565.656250\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: 11067.875000\n",
      "    epoch          : 25\n",
      "    loss           : -128016.775\n",
      "    val_loss       : -133018.89862737656\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -124712.585938\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -119387.218750\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -224095.062500\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -156350.906250\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -1618.941406\n",
      "    epoch          : 26\n",
      "    loss           : -139871.32033203126\n",
      "    val_loss       : -149030.99583749176\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -156729.531250\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -186642.046875\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -167451.140625\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -155946.468750\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -183003.343750\n",
      "    epoch          : 27\n",
      "    loss           : -158252.8615234375\n",
      "    val_loss       : -159337.57285785675\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -133189.234375\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -221728.406250\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -17375.949219\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -241443.593750\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -148648.187500\n",
      "    epoch          : 28\n",
      "    loss           : -157238.31022460936\n",
      "    val_loss       : -131765.4105203867\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -125286.164062\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -144636.125000\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -193757.593750\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -127684.156250\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -102171.765625\n",
      "    epoch          : 29\n",
      "    loss           : -106850.81474609375\n",
      "    val_loss       : -131277.30885724426\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: 2172.531250\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -185996.578125\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -152782.531250\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -216804.062500\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -130781.906250\n",
      "    epoch          : 30\n",
      "    loss           : -153551.3302734375\n",
      "    val_loss       : -150304.3601773858\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -242541.562500\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -154582.328125\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: 47104.070312\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -145719.484375\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -161728.968750\n",
      "    epoch          : 31\n",
      "    loss           : -155057.92282226562\n",
      "    val_loss       : -174280.81826848985\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -249879.359375\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -257335.984375\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -174768.375000\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -158362.593750\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -113290.046875\n",
      "    epoch          : 32\n",
      "    loss           : -166165.6050292969\n",
      "    val_loss       : -161239.46117474436\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -154480.062500\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -136522.343750\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -231075.281250\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -139831.859375\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -144805.968750\n",
      "    epoch          : 33\n",
      "    loss           : -147296.42102539062\n",
      "    val_loss       : -148988.17286601066\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -216334.437500\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -137667.046875\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -148866.671875\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -140852.625000\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -169823.875000\n",
      "    epoch          : 34\n",
      "    loss           : -155501.62397460936\n",
      "    val_loss       : -166012.24195158482\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -156385.968750\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -124109.773438\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -9065.961914\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -149421.546875\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -147969.312500\n",
      "    epoch          : 35\n",
      "    loss           : -152228.20088867188\n",
      "    val_loss       : -156645.6017598927\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -217236.468750\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -202938.718750\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -175951.500000\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -209850.140625\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -203019.796875\n",
      "    epoch          : 36\n",
      "    loss           : -173370.849765625\n",
      "    val_loss       : -183215.13260313869\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -183208.328125\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -30679.757812\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -17137.203125\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -117292.968750\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -126190.867188\n",
      "    epoch          : 37\n",
      "    loss           : -151520.01334960936\n",
      "    val_loss       : -166336.9367689967\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -159260.875000\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -178474.937500\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -25761.445312\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -169868.531250\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -15063.943359\n",
      "    epoch          : 38\n",
      "    loss           : -147838.83939453124\n",
      "    val_loss       : -122227.75081063509\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -134315.812500\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -239423.859375\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -237088.921875\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -163507.078125\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -161254.593750\n",
      "    epoch          : 39\n",
      "    loss           : -163705.4673828125\n",
      "    val_loss       : -185937.11389917135\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -168755.156250\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -187006.406250\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -33931.429688\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -152408.656250\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -143463.937500\n",
      "    epoch          : 40\n",
      "    loss           : -173464.56587890626\n",
      "    val_loss       : -176590.66695241927\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -201266.125000\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -184407.218750\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -186704.203125\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -14971.889648\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -190141.687500\n",
      "    epoch          : 41\n",
      "    loss           : -181446.06016601564\n",
      "    val_loss       : -191177.21353228093\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -200245.906250\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -44192.265625\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -278742.250000\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -137322.843750\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -139816.859375\n",
      "    epoch          : 42\n",
      "    loss           : -181962.9069921875\n",
      "    val_loss       : -131634.08920292853\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -191126.046875\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -200933.921875\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -185653.750000\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -260868.265625\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -165963.328125\n",
      "    epoch          : 43\n",
      "    loss           : -163554.23119140626\n",
      "    val_loss       : -120746.75494928956\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -157968.703125\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -172279.187500\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -13525.554688\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -168797.687500\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -185241.343750\n",
      "    epoch          : 44\n",
      "    loss           : -157153.25529296874\n",
      "    val_loss       : -187842.20009741784\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -185478.890625\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -269162.843750\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -195376.750000\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -269661.093750\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -201149.968750\n",
      "    epoch          : 45\n",
      "    loss           : -193025.2469921875\n",
      "    val_loss       : -196235.78472669126\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -178270.265625\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -189541.250000\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -209764.281250\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -195478.000000\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -218228.281250\n",
      "    epoch          : 46\n",
      "    loss           : -188100.5516015625\n",
      "    val_loss       : -196733.76996178628\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -39203.156250\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -279252.687500\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -157760.484375\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -146303.312500\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -167365.265625\n",
      "    epoch          : 47\n",
      "    loss           : -152542.11302734376\n",
      "    val_loss       : -171202.61166762112\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -14530.342773\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -23705.382812\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -226450.406250\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -216923.296875\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -203513.953125\n",
      "    epoch          : 48\n",
      "    loss           : -184595.0604394531\n",
      "    val_loss       : -198625.80055943728\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -189808.750000\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -53397.851562\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -226163.625000\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -182820.406250\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -185176.562500\n",
      "    epoch          : 49\n",
      "    loss           : -196938.71421875\n",
      "    val_loss       : -192271.011627388\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -194197.250000\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -233854.406250\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -278598.000000\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -283792.000000\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -174342.531250\n",
      "    epoch          : 50\n",
      "    loss           : -198581.1705859375\n",
      "    val_loss       : -185057.50323758126\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1230_200319/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -190293.734375\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -162173.421875\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -147184.687500\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -4525.324219\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -112234.875000\n",
      "    epoch          : 51\n",
      "    loss           : -142223.64314453126\n",
      "    val_loss       : -135252.59532520772\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -131631.437500\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -160245.875000\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -177032.843750\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -196607.796875\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -200314.718750\n",
      "    epoch          : 52\n",
      "    loss           : -189444.09500976562\n",
      "    val_loss       : -202220.8258302152\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -231715.390625\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -197541.515625\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -147064.562500\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -12859.918945\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -177818.781250\n",
      "    epoch          : 53\n",
      "    loss           : -193205.462890625\n",
      "    val_loss       : -204732.36665239334\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -202877.359375\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -205431.875000\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -291844.968750\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -295362.750000\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -191164.250000\n",
      "    epoch          : 54\n",
      "    loss           : -189406.84548828125\n",
      "    val_loss       : -182884.40054065586\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -192325.109375\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -246908.687500\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -259590.843750\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -37612.710938\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -198772.218750\n",
      "    epoch          : 55\n",
      "    loss           : -182920.57912109376\n",
      "    val_loss       : -207345.45175465345\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -234000.875000\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -54341.250000\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -198205.031250\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -211905.296875\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -216239.343750\n",
      "    epoch          : 56\n",
      "    loss           : -209831.8958984375\n",
      "    val_loss       : -200861.91491838693\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -199639.718750\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -194406.875000\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -87274.125000\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -110279.750000\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -194495.875000\n",
      "    epoch          : 57\n",
      "    loss           : -153216.1229296875\n",
      "    val_loss       : -182034.06308428646\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -180349.656250\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -40121.812500\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -206174.296875\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -197524.468750\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -217949.453125\n",
      "    epoch          : 58\n",
      "    loss           : -200068.7316015625\n",
      "    val_loss       : -201009.63685492874\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -210804.656250\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -279582.125000\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -195305.265625\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -299188.437500\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -206338.312500\n",
      "    epoch          : 59\n",
      "    loss           : -198417.858828125\n",
      "    val_loss       : -199766.76226269602\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -203980.484375\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -283749.281250\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -259234.281250\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -119319.148438\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -131012.070312\n",
      "    epoch          : 60\n",
      "    loss           : -166833.61462890625\n",
      "    val_loss       : -166127.5247664243\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -13085.514648\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -170612.671875\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -42878.191406\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -211294.343750\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -284571.468750\n",
      "    epoch          : 61\n",
      "    loss           : -196792.90416992188\n",
      "    val_loss       : -210230.29498310387\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -185737.640625\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -201982.843750\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -307245.093750\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -216261.328125\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -201330.625000\n",
      "    epoch          : 62\n",
      "    loss           : -203586.88193359374\n",
      "    val_loss       : -198587.78206102253\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -201242.781250\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -39305.527344\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -132024.265625\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -81373.062500\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -161902.796875\n",
      "    epoch          : 63\n",
      "    loss           : -172039.97759765625\n",
      "    val_loss       : -194928.2423290372\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -191857.500000\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -43469.949219\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -291289.750000\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -200775.656250\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -186600.750000\n",
      "    epoch          : 64\n",
      "    loss           : -212438.16671875\n",
      "    val_loss       : -218785.966475904\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -303500.468750\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -219680.625000\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -220433.546875\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -217575.390625\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -306053.625000\n",
      "    epoch          : 65\n",
      "    loss           : -221468.5860546875\n",
      "    val_loss       : -222089.53495129943\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -317849.312500\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -273695.343750\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -233985.281250\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -179225.171875\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -166376.718750\n",
      "    epoch          : 66\n",
      "    loss           : -183324.4666308594\n",
      "    val_loss       : -190269.06476204394\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -231365.781250\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -43677.671875\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -238770.781250\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -313293.000000\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -224610.937500\n",
      "    epoch          : 67\n",
      "    loss           : -214267.989765625\n",
      "    val_loss       : -224141.8850693822\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -259335.171875\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -210069.343750\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -227048.656250\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -158693.578125\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -109780.625000\n",
      "    epoch          : 68\n",
      "    loss           : -178350.61048828124\n",
      "    val_loss       : -118981.25609637499\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -72246.296875\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -294313.062500\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -45683.152344\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -241037.437500\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -199077.875000\n",
      "    epoch          : 69\n",
      "    loss           : -191083.40150390624\n",
      "    val_loss       : -199823.6581683755\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -181125.187500\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -318309.125000\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -217236.687500\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -187487.734375\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -220056.734375\n",
      "    epoch          : 70\n",
      "    loss           : -208471.3327734375\n",
      "    val_loss       : -209375.11236009002\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -200496.453125\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -207790.343750\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -191246.703125\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -50952.921875\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -37886.156250\n",
      "    epoch          : 71\n",
      "    loss           : -201386.38748046875\n",
      "    val_loss       : -212789.12591984868\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -204304.781250\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -291369.750000\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -196742.187500\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -228248.343750\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -324687.125000\n",
      "    epoch          : 72\n",
      "    loss           : -206239.25690429687\n",
      "    val_loss       : -204867.26246640086\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -191068.281250\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -296896.500000\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -216737.937500\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -42826.757812\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -207673.921875\n",
      "    epoch          : 73\n",
      "    loss           : -204968.61734375\n",
      "    val_loss       : -215629.94385933876\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -201387.343750\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -49219.914062\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -190557.500000\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -202355.281250\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -245458.968750\n",
      "    epoch          : 74\n",
      "    loss           : -218294.34046875\n",
      "    val_loss       : -204907.69596613347\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -192797.843750\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -112006.757812\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: 122956.062500\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: 180585.968750\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -195340.703125\n",
      "    epoch          : 75\n",
      "    loss           : -154685.65744140625\n",
      "    val_loss       : -202464.08695182204\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -232055.015625\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -189473.453125\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -297316.250000\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -247400.234375\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -238914.031250\n",
      "    epoch          : 76\n",
      "    loss           : -214867.09192382812\n",
      "    val_loss       : -218602.42280382515\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -195453.484375\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -251280.203125\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -221700.046875\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -184514.953125\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -166430.625000\n",
      "    epoch          : 77\n",
      "    loss           : -201652.35345703125\n",
      "    val_loss       : -200586.58279409408\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -221631.343750\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -207304.312500\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -187730.468750\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -200382.843750\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -218674.531250\n",
      "    epoch          : 78\n",
      "    loss           : -209430.775546875\n",
      "    val_loss       : -211758.1561332345\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -319389.312500\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -162527.328125\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -198632.250000\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -303117.062500\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -161864.093750\n",
      "    epoch          : 79\n",
      "    loss           : -202488.05638671876\n",
      "    val_loss       : -209463.68875688314\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -172635.375000\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -312183.750000\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -224143.593750\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -174017.718750\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -164463.140625\n",
      "    epoch          : 80\n",
      "    loss           : -197918.313671875\n",
      "    val_loss       : -175927.2935643196\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -220064.562500\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -172975.093750\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -249269.218750\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -299523.437500\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -188089.062500\n",
      "    epoch          : 81\n",
      "    loss           : -197377.12923828125\n",
      "    val_loss       : -226303.03289502262\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -314285.718750\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -217696.250000\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -252776.468750\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -223415.546875\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -236370.875000\n",
      "    epoch          : 82\n",
      "    loss           : -225630.763515625\n",
      "    val_loss       : -213438.39075837733\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -213997.875000\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -200431.531250\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -221262.406250\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -282738.875000\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -139791.421875\n",
      "    epoch          : 83\n",
      "    loss           : -196119.67709960937\n",
      "    val_loss       : -165477.97346413136\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -3793.302734\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -190989.703125\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -190359.187500\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -309368.750000\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -231579.562500\n",
      "    epoch          : 84\n",
      "    loss           : -209011.78771484375\n",
      "    val_loss       : -233730.49290795327\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -320449.812500\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -212485.859375\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -231061.281250\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -261090.609375\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -213514.843750\n",
      "    epoch          : 85\n",
      "    loss           : -227968.32971679687\n",
      "    val_loss       : -222809.66467028856\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -317451.781250\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -223476.875000\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -313037.125000\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -51663.449219\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -178657.968750\n",
      "    epoch          : 86\n",
      "    loss           : -215565.29206054687\n",
      "    val_loss       : -229154.3027057141\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -212174.000000\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -213337.625000\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -207716.515625\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -183373.234375\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -230801.343750\n",
      "    epoch          : 87\n",
      "    loss           : -213903.33494140624\n",
      "    val_loss       : -222445.93777236342\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -215027.593750\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -176144.781250\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -195527.750000\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -289579.750000\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -200653.531250\n",
      "    epoch          : 88\n",
      "    loss           : -187704.85666015625\n",
      "    val_loss       : -215109.23036707938\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -216705.859375\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -208458.062500\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -221978.734375\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -230113.968750\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -190497.156250\n",
      "    epoch          : 89\n",
      "    loss           : -219146.75765625\n",
      "    val_loss       : -202802.24733430744\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -56380.906250\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -185200.718750\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -293701.718750\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -216848.593750\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -249652.171875\n",
      "    epoch          : 90\n",
      "    loss           : -198246.65069335938\n",
      "    val_loss       : -225012.1386661172\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -205320.203125\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -217890.500000\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -230061.265625\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -246099.437500\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -151993.828125\n",
      "    epoch          : 91\n",
      "    loss           : -224249.8926953125\n",
      "    val_loss       : -196996.52587100863\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -226909.968750\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -224817.609375\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -203338.578125\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -216393.437500\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -210213.453125\n",
      "    epoch          : 92\n",
      "    loss           : -203148.52294921875\n",
      "    val_loss       : -235508.1282283008\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -269746.843750\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -342144.562500\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -246036.875000\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -239652.218750\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -219801.250000\n",
      "    epoch          : 93\n",
      "    loss           : -239488.353984375\n",
      "    val_loss       : -238662.8604059875\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -81983.406250\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -69525.460938\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -207451.828125\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -184660.968750\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -213834.750000\n",
      "    epoch          : 94\n",
      "    loss           : -206419.18322265625\n",
      "    val_loss       : -216913.47537478805\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -253662.734375\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -301999.031250\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -213557.250000\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -246893.000000\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -262499.625000\n",
      "    epoch          : 95\n",
      "    loss           : -217277.6425390625\n",
      "    val_loss       : -220903.98370035886\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -310988.062500\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -249186.843750\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -235788.187500\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -310343.468750\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -202694.140625\n",
      "    epoch          : 96\n",
      "    loss           : -225449.959765625\n",
      "    val_loss       : -231280.62650393246\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -56086.199219\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -257333.984375\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -321648.156250\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -249823.078125\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -255892.781250\n",
      "    epoch          : 97\n",
      "    loss           : -219480.1059375\n",
      "    val_loss       : -232376.27623919846\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -271373.250000\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -228818.781250\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -345465.750000\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -231568.156250\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -242622.359375\n",
      "    epoch          : 98\n",
      "    loss           : -238130.6761328125\n",
      "    val_loss       : -239725.44963380098\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -232988.000000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -225916.187500\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -209099.593750\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -192089.390625\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -222209.375000\n",
      "    epoch          : 99\n",
      "    loss           : -215065.1891796875\n",
      "    val_loss       : -187778.87945811154\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -172873.000000\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -214348.375000\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -102962.812500\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -170965.515625\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -226890.812500\n",
      "    epoch          : 100\n",
      "    loss           : -177235.80346679688\n",
      "    val_loss       : -217302.9457463622\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1230_200319/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -57132.527344\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -30768.281250\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -229108.625000\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -155803.421875\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -192875.468750\n",
      "    epoch          : 101\n",
      "    loss           : -216717.41212890626\n",
      "    val_loss       : -210285.44682867528\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -327955.718750\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -221929.234375\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -230599.953125\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -211574.546875\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -273252.656250\n",
      "    epoch          : 102\n",
      "    loss           : -228633.7556640625\n",
      "    val_loss       : -228170.03285684585\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -252225.187500\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -211978.343750\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -278623.468750\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -222599.234375\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -273681.875000\n",
      "    epoch          : 103\n",
      "    loss           : -242134.93033203125\n",
      "    val_loss       : -244080.18410660623\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -215674.312500\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -278142.375000\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -81791.640625\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -241126.625000\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -280274.156250\n",
      "    epoch          : 104\n",
      "    loss           : -249175.416640625\n",
      "    val_loss       : -246094.74400904178\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -240131.125000\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -355995.375000\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -210482.421875\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -293919.875000\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -114757.312500\n",
      "    epoch          : 105\n",
      "    loss           : -231667.65642578126\n",
      "    val_loss       : -175755.46818050146\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -301767.218750\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -9336.390625\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -162249.218750\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -222970.218750\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -258569.250000\n",
      "    epoch          : 106\n",
      "    loss           : -203084.5917578125\n",
      "    val_loss       : -240907.1256296456\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -272224.906250\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -256345.171875\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -239910.953125\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -204556.500000\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -194709.671875\n",
      "    epoch          : 107\n",
      "    loss           : -237025.56728515626\n",
      "    val_loss       : -162107.73982435465\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -170436.828125\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -315708.187500\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -254073.125000\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -219644.484375\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -247940.218750\n",
      "    epoch          : 108\n",
      "    loss           : -224055.44182617188\n",
      "    val_loss       : -234713.96052948237\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -246493.937500\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -318527.843750\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -62457.562500\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -247384.671875\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -217095.937500\n",
      "    epoch          : 109\n",
      "    loss           : -235323.0387109375\n",
      "    val_loss       : -218033.2934992969\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -173892.656250\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -320703.656250\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -299066.562500\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -187189.281250\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -233807.125000\n",
      "    epoch          : 110\n",
      "    loss           : -206968.7482128906\n",
      "    val_loss       : -221554.739083004\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -311222.218750\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -222608.937500\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -302742.468750\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -110545.601562\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -214814.843750\n",
      "    epoch          : 111\n",
      "    loss           : -205635.77188476562\n",
      "    val_loss       : -223607.97164006234\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -197018.843750\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -338763.906250\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -81181.765625\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -345038.187500\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -261217.421875\n",
      "    epoch          : 112\n",
      "    loss           : -233922.789765625\n",
      "    val_loss       : -218408.92831714748\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -40697.234375\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -205419.625000\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -202292.375000\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -248156.250000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -212772.031250\n",
      "    epoch          : 113\n",
      "    loss           : -231140.7734765625\n",
      "    val_loss       : -201713.90571273566\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -241425.593750\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -203005.406250\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -223822.953125\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -339577.156250\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -205932.828125\n",
      "    epoch          : 114\n",
      "    loss           : -234786.7173046875\n",
      "    val_loss       : -235485.83891907334\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -66350.890625\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -220185.125000\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -204596.250000\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -312551.625000\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -223415.875000\n",
      "    epoch          : 115\n",
      "    loss           : -225471.8768359375\n",
      "    val_loss       : -238696.80932465792\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -231936.593750\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -241637.156250\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -275144.812500\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -76031.054688\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -197875.656250\n",
      "    epoch          : 116\n",
      "    loss           : -237757.7709375\n",
      "    val_loss       : -221624.73681060076\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -333774.750000\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -161797.187500\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -274072.562500\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -223707.156250\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -222028.406250\n",
      "    epoch          : 117\n",
      "    loss           : -196023.2122265625\n",
      "    val_loss       : -239422.3422953367\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -72094.703125\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -348104.250000\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -282688.875000\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -214035.937500\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -222316.546875\n",
      "    epoch          : 118\n",
      "    loss           : -242960.146015625\n",
      "    val_loss       : -247907.93272636534\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -246797.937500\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -360950.531250\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -69358.390625\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -231052.625000\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -148853.500000\n",
      "    epoch          : 119\n",
      "    loss           : -233131.91908203124\n",
      "    val_loss       : -241024.410947299\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -333661.250000\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -348595.625000\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -200182.500000\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -234083.218750\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -248981.859375\n",
      "    epoch          : 120\n",
      "    loss           : -244954.31171875\n",
      "    val_loss       : -252205.36720827222\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -246808.218750\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -249855.234375\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -100541.367188\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -347375.812500\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -210867.281250\n",
      "    epoch          : 121\n",
      "    loss           : -250861.8421875\n",
      "    val_loss       : -240228.7391586125\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -282471.500000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -326440.031250\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -301775.062500\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -201663.000000\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -126748.992188\n",
      "    epoch          : 122\n",
      "    loss           : -195101.72749023436\n",
      "    val_loss       : -203455.59568440914\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -296091.281250\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -140019.250000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -173143.562500\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -261612.531250\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -221086.781250\n",
      "    epoch          : 123\n",
      "    loss           : -171842.8275390625\n",
      "    val_loss       : -194809.63052976728\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -67977.851562\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -61703.093750\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -280205.312500\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -226956.203125\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -212334.046875\n",
      "    epoch          : 124\n",
      "    loss           : -234026.98453125\n",
      "    val_loss       : -243544.3259348482\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -225130.281250\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -238705.375000\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -229864.359375\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -255230.437500\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -265459.687500\n",
      "    epoch          : 125\n",
      "    loss           : -245460.87298828125\n",
      "    val_loss       : -220356.868662256\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -75187.968750\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -207037.140625\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: 45587.527344\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -142273.062500\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -261613.000000\n",
      "    epoch          : 126\n",
      "    loss           : -215690.866953125\n",
      "    val_loss       : -243655.21241813301\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -222021.390625\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -239086.390625\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -263507.656250\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -230907.500000\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -245441.484375\n",
      "    epoch          : 127\n",
      "    loss           : -253208.679921875\n",
      "    val_loss       : -253666.20564669967\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -345701.250000\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -91219.000000\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -250084.312500\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -207331.140625\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -235795.843750\n",
      "    epoch          : 128\n",
      "    loss           : -248657.983203125\n",
      "    val_loss       : -241458.8682109833\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -359451.406250\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -149006.171875\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -228095.046875\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -272720.500000\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -243836.000000\n",
      "    epoch          : 129\n",
      "    loss           : -230628.565625\n",
      "    val_loss       : -226450.30915952922\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -234861.687500\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -236217.453125\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -239399.796875\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -272709.656250\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -344119.812500\n",
      "    epoch          : 130\n",
      "    loss           : -240364.1741015625\n",
      "    val_loss       : -240223.38755154313\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -222233.937500\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -230611.718750\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -288286.750000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -257508.593750\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -282391.500000\n",
      "    epoch          : 131\n",
      "    loss           : -240709.4927734375\n",
      "    val_loss       : -248528.02933635114\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -284372.593750\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -326579.625000\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -233768.906250\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -257261.375000\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -250978.984375\n",
      "    epoch          : 132\n",
      "    loss           : -240806.14390625\n",
      "    val_loss       : -223742.39200471342\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -193302.625000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -346013.687500\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -259895.687500\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -292818.062500\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -216200.640625\n",
      "    epoch          : 133\n",
      "    loss           : -238694.5134375\n",
      "    val_loss       : -241420.91257634162\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -264699.187500\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -257871.390625\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -248273.343750\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -225701.062500\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -218479.140625\n",
      "    epoch          : 134\n",
      "    loss           : -239482.7798046875\n",
      "    val_loss       : -175988.98331055642\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -218646.578125\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -216218.734375\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -255492.140625\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -85048.453125\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -228564.937500\n",
      "    epoch          : 135\n",
      "    loss           : -227985.934921875\n",
      "    val_loss       : -251881.0582190454\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -216490.937500\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -353255.000000\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -223177.343750\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -166685.625000\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -88513.679688\n",
      "    epoch          : 136\n",
      "    loss           : -222913.59415039062\n",
      "    val_loss       : -183295.64184300305\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -137520.859375\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -245257.593750\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: 26501.882812\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -204364.812500\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -216959.328125\n",
      "    epoch          : 137\n",
      "    loss           : -189252.39345703126\n",
      "    val_loss       : -242893.26946935058\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -222696.937500\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -227540.062500\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -221501.093750\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -222162.562500\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -280128.125000\n",
      "    epoch          : 138\n",
      "    loss           : -247129.55609375\n",
      "    val_loss       : -256978.67800064088\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -355073.625000\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -232023.859375\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -90373.656250\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -237299.750000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -271633.031250\n",
      "    epoch          : 139\n",
      "    loss           : -256145.111875\n",
      "    val_loss       : -244867.9608083397\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -196534.531250\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -186536.546875\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -231397.750000\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -232227.828125\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -286952.562500\n",
      "    epoch          : 140\n",
      "    loss           : -239782.6419140625\n",
      "    val_loss       : -257206.41672417225\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -217393.031250\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -297745.687500\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -240788.562500\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -272223.312500\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -220798.234375\n",
      "    epoch          : 141\n",
      "    loss           : -260830.00875\n",
      "    val_loss       : -250204.50026388763\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -222378.125000\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -370720.968750\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -352081.312500\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -197123.656250\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -279838.687500\n",
      "    epoch          : 142\n",
      "    loss           : -232118.68642578126\n",
      "    val_loss       : -145240.97477451563\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -149448.906250\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -200820.828125\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -272457.531250\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -93070.890625\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -186865.812500\n",
      "    epoch          : 143\n",
      "    loss           : -199726.29880859374\n",
      "    val_loss       : -185745.4331720531\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -180140.312500\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -348608.062500\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -91545.054688\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -240218.968750\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -253098.562500\n",
      "    epoch          : 144\n",
      "    loss           : -249472.18485351562\n",
      "    val_loss       : -258739.2534654051\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -296615.187500\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -293357.406250\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -292937.312500\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -349578.625000\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -154852.250000\n",
      "    epoch          : 145\n",
      "    loss           : -239996.480390625\n",
      "    val_loss       : -216595.1811436206\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -155845.296875\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -215290.703125\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -231836.031250\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -209685.390625\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -222587.171875\n",
      "    epoch          : 146\n",
      "    loss           : -250743.912578125\n",
      "    val_loss       : -260142.4200227529\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -362148.562500\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -251037.203125\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -60194.132812\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -122771.359375\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -186936.453125\n",
      "    epoch          : 147\n",
      "    loss           : -240269.7938671875\n",
      "    val_loss       : -218413.20489834546\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -69707.968750\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -257383.562500\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -367341.062500\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -281339.250000\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -263248.218750\n",
      "    epoch          : 148\n",
      "    loss           : -254625.2597265625\n",
      "    val_loss       : -265212.0188932985\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -382241.562500\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -225856.500000\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -375011.625000\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -228961.859375\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -263418.468750\n",
      "    epoch          : 149\n",
      "    loss           : -267127.904375\n",
      "    val_loss       : -261704.66639398635\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -253271.781250\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -217066.109375\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -218209.421875\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -240512.046875\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -259267.109375\n",
      "    epoch          : 150\n",
      "    loss           : -234969.55888671876\n",
      "    val_loss       : -240815.66347993613\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1230_200319/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -248721.375000\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -242027.500000\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -376376.375000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -298016.187500\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -283787.687500\n",
      "    epoch          : 151\n",
      "    loss           : -259457.721171875\n",
      "    val_loss       : -263441.9612998009\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -253968.812500\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -239875.500000\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -212083.359375\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -140100.546875\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -160711.468750\n",
      "    epoch          : 152\n",
      "    loss           : -224863.40390625\n",
      "    val_loss       : -235306.4851919651\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -234614.937500\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -224815.062500\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -94518.140625\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -92580.750000\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -368334.062500\n",
      "    epoch          : 153\n",
      "    loss           : -258505.10796875\n",
      "    val_loss       : -263693.2993569613\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -358035.718750\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -370477.500000\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -248332.671875\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -259756.718750\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -235407.250000\n",
      "    epoch          : 154\n",
      "    loss           : -269543.26734375\n",
      "    val_loss       : -269373.24317008257\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -307456.625000\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -297293.156250\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -368841.375000\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -292676.468750\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -218003.968750\n",
      "    epoch          : 155\n",
      "    loss           : -245248.10255859376\n",
      "    val_loss       : -204698.12453030943\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -212897.531250\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -213126.562500\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -317035.781250\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -85771.171875\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -264720.843750\n",
      "    epoch          : 156\n",
      "    loss           : -233514.18431640626\n",
      "    val_loss       : -227877.0299959302\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -186548.031250\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -331098.750000\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -291650.937500\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -210083.562500\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -225389.921875\n",
      "    epoch          : 157\n",
      "    loss           : -249197.21280273437\n",
      "    val_loss       : -253867.95146948696\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -95910.562500\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -253788.703125\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -230943.156250\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -263485.750000\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -298655.625000\n",
      "    epoch          : 158\n",
      "    loss           : -263787.769609375\n",
      "    val_loss       : -264111.3085552454\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -85330.070312\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -379401.093750\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -244407.796875\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -188289.359375\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -199808.625000\n",
      "    epoch          : 159\n",
      "    loss           : -245928.56248046874\n",
      "    val_loss       : -92540.44128929377\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -125480.031250\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -276236.062500\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -259518.781250\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -235817.125000\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -225625.531250\n",
      "    epoch          : 160\n",
      "    loss           : -225523.27400390626\n",
      "    val_loss       : -216284.1108388573\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -201790.781250\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -222416.281250\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -129784.515625\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -76276.406250\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -230503.750000\n",
      "    epoch          : 161\n",
      "    loss           : -181695.36435546874\n",
      "    val_loss       : -204154.77604981363\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -367427.687500\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -187268.000000\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -223208.250000\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -247901.468750\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -297671.375000\n",
      "    epoch          : 162\n",
      "    loss           : -241582.2482421875\n",
      "    val_loss       : -267127.3073576987\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -370822.843750\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -385796.031250\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -249225.468750\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -266624.093750\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -284414.937500\n",
      "    epoch          : 163\n",
      "    loss           : -272152.47734375\n",
      "    val_loss       : -269007.9669441044\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -229419.781250\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -302887.187500\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -106120.570312\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -256969.625000\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -257318.546875\n",
      "    epoch          : 164\n",
      "    loss           : -266908.457578125\n",
      "    val_loss       : -259283.4846003175\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -237231.125000\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -371638.125000\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -252869.703125\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -269994.187500\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -270530.500000\n",
      "    epoch          : 165\n",
      "    loss           : -264291.21171875\n",
      "    val_loss       : -260938.34111121894\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -237779.859375\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -241528.500000\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -201605.406250\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -278252.250000\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -302737.375000\n",
      "    epoch          : 166\n",
      "    loss           : -240214.63592773437\n",
      "    val_loss       : -267351.50218638184\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -374772.062500\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -242139.703125\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -259719.203125\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -257748.453125\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -239701.750000\n",
      "    epoch          : 167\n",
      "    loss           : -272064.202890625\n",
      "    val_loss       : -270806.9640664935\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -224585.984375\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -234652.593750\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -285122.218750\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -96192.664062\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -266105.156250\n",
      "    epoch          : 168\n",
      "    loss           : -262558.730703125\n",
      "    val_loss       : -262889.3360900283\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -278817.687500\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -297773.687500\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -187787.421875\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -88240.468750\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -261950.312500\n",
      "    epoch          : 169\n",
      "    loss           : -225745.84421875\n",
      "    val_loss       : -247708.7532820046\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -68532.273438\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -254813.421875\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -299451.500000\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -381632.500000\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -383566.250000\n",
      "    epoch          : 170\n",
      "    loss           : -264854.7778125\n",
      "    val_loss       : -267698.73399471643\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -247492.875000\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -309943.812500\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -282303.562500\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -286075.562500\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -227829.312500\n",
      "    epoch          : 171\n",
      "    loss           : -263049.30125\n",
      "    val_loss       : -256641.6027017057\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -71206.812500\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -79231.687500\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -381789.875000\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -379305.406250\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -309072.812500\n",
      "    epoch          : 172\n",
      "    loss           : -267795.94015625\n",
      "    val_loss       : -261526.50497820377\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -282445.937500\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -250684.843750\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -92134.132812\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -280726.812500\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -287150.906250\n",
      "    epoch          : 173\n",
      "    loss           : -252581.1293359375\n",
      "    val_loss       : -196323.75895619392\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: 16071.453125\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -238950.875000\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -274810.468750\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -181733.375000\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -260343.250000\n",
      "    epoch          : 174\n",
      "    loss           : -195406.454375\n",
      "    val_loss       : -238664.95402287244\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -209845.500000\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -301632.375000\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -245880.250000\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -271514.000000\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -257126.578125\n",
      "    epoch          : 175\n",
      "    loss           : -262659.7453515625\n",
      "    val_loss       : -269261.4624233961\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -253920.000000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -261018.796875\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -245775.906250\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -310808.906250\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -225889.781250\n",
      "    epoch          : 176\n",
      "    loss           : -275566.739375\n",
      "    val_loss       : -273256.6705486417\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -239591.171875\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -393261.562500\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -262993.875000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -314718.093750\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -214989.203125\n",
      "    epoch          : 177\n",
      "    loss           : -267553.3654296875\n",
      "    val_loss       : -227681.05185943842\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -182329.750000\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -264579.906250\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -292659.625000\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -93006.718750\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -253163.312500\n",
      "    epoch          : 178\n",
      "    loss           : -239869.93822265626\n",
      "    val_loss       : -245322.5858020067\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -288303.375000\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -168541.234375\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -206469.140625\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -298631.062500\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -182830.218750\n",
      "    epoch          : 179\n",
      "    loss           : -226937.31771484375\n",
      "    val_loss       : -168780.2865329027\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -133547.187500\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -180405.359375\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -170438.656250\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -153429.187500\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -217119.000000\n",
      "    epoch          : 180\n",
      "    loss           : -212839.18021484374\n",
      "    val_loss       : -256195.15778095127\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -248002.375000\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -104049.765625\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -98322.820312\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -295725.750000\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -307742.437500\n",
      "    epoch          : 181\n",
      "    loss           : -270901.81078125\n",
      "    val_loss       : -277395.99851884844\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -255877.078125\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -388489.562500\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -318725.500000\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -258687.093750\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -277926.312500\n",
      "    epoch          : 182\n",
      "    loss           : -279429.892890625\n",
      "    val_loss       : -276514.1525819242\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -306446.687500\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -103892.265625\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -120586.843750\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -226163.843750\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -309242.093750\n",
      "    epoch          : 183\n",
      "    loss           : -273841.676328125\n",
      "    val_loss       : -208462.8236763239\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -165051.046875\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -198132.437500\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -227227.000000\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -185804.593750\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -230408.250000\n",
      "    epoch          : 184\n",
      "    loss           : -214468.37796875\n",
      "    val_loss       : -256940.12360486985\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -244088.187500\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -233102.156250\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -103905.031250\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -267840.562500\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -263542.968750\n",
      "    epoch          : 185\n",
      "    loss           : -265747.38203125\n",
      "    val_loss       : -271587.1319436043\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -305829.562500\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -317725.875000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -101409.476562\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -288608.500000\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -305367.687500\n",
      "    epoch          : 186\n",
      "    loss           : -269315.018046875\n",
      "    val_loss       : -260784.64216606022\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -240451.312500\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -294276.125000\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -212645.937500\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -206570.546875\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -205268.250000\n",
      "    epoch          : 187\n",
      "    loss           : -258820.5442578125\n",
      "    val_loss       : -236390.42172563076\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -210207.734375\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -147651.781250\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -213979.375000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -319540.875000\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -227451.890625\n",
      "    epoch          : 188\n",
      "    loss           : -200214.54487304686\n",
      "    val_loss       : -262307.275408724\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -377898.406250\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -312190.875000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -307672.812500\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -273355.093750\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -289372.625000\n",
      "    epoch          : 189\n",
      "    loss           : -274299.606953125\n",
      "    val_loss       : -276730.8871060759\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -112587.789062\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -319781.843750\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -234810.781250\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -391537.687500\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -288010.062500\n",
      "    epoch          : 190\n",
      "    loss           : -273226.08828125\n",
      "    val_loss       : -237923.05384771526\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -256179.140625\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -298940.812500\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -382325.562500\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -240327.515625\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -103401.609375\n",
      "    epoch          : 191\n",
      "    loss           : -245065.226484375\n",
      "    val_loss       : -220714.4232903421\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -220673.531250\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -345874.062500\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -189349.781250\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -250585.625000\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -256395.046875\n",
      "    epoch          : 192\n",
      "    loss           : -244654.9598046875\n",
      "    val_loss       : -272751.8881129861\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -233927.843750\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -271985.250000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -322154.750000\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -316075.562500\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -379157.593750\n",
      "    epoch          : 193\n",
      "    loss           : -279130.0428125\n",
      "    val_loss       : -277417.7672394991\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -392660.312500\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -375173.281250\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -222721.046875\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -232101.593750\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -204538.656250\n",
      "    epoch          : 194\n",
      "    loss           : -253741.632890625\n",
      "    val_loss       : -230035.86947775484\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -265820.687500\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -240600.375000\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -368235.000000\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -190809.437500\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -230220.843750\n",
      "    epoch          : 195\n",
      "    loss           : -237190.991875\n",
      "    val_loss       : -265660.6383308351\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -236987.937500\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -265241.843750\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -313798.437500\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -229281.453125\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -240514.812500\n",
      "    epoch          : 196\n",
      "    loss           : -272051.257421875\n",
      "    val_loss       : -272702.9738440931\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -264093.625000\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -241888.828125\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -389972.062500\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -395450.218750\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -273202.687500\n",
      "    epoch          : 197\n",
      "    loss           : -274716.8174609375\n",
      "    val_loss       : -266454.7103066325\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -257552.312500\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -246731.531250\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -222451.359375\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -306071.000000\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -217535.921875\n",
      "    epoch          : 198\n",
      "    loss           : -217135.67092773438\n",
      "    val_loss       : -220628.7834619999\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -346877.406250\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -268144.843750\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -276622.343750\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -259205.906250\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -233644.812500\n",
      "    epoch          : 199\n",
      "    loss           : -254899.5176953125\n",
      "    val_loss       : -278111.8514075011\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -104141.421875\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -288013.718750\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -252023.125000\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -261180.781250\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -260137.375000\n",
      "    epoch          : 200\n",
      "    loss           : -277123.29859375\n",
      "    val_loss       : -263621.91872441175\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1230_200319/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -367618.937500\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -396706.062500\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -317469.750000\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -242555.906250\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -260407.000000\n",
      "    epoch          : 201\n",
      "    loss           : -278232.1509375\n",
      "    val_loss       : -271739.79150180815\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -248185.906250\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -307618.281250\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -259556.531250\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -214662.937500\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -258534.218750\n",
      "    epoch          : 202\n",
      "    loss           : -269723.643515625\n",
      "    val_loss       : -262783.3835605681\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -239010.593750\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -328052.375000\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -193919.781250\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -211702.312500\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -270052.875000\n",
      "    epoch          : 203\n",
      "    loss           : -235224.04395507812\n",
      "    val_loss       : -271809.2641503811\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -90726.265625\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -244694.250000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -234905.218750\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -214868.656250\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -181754.796875\n",
      "    epoch          : 204\n",
      "    loss           : -243661.840234375\n",
      "    val_loss       : -146046.822836262\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -216162.812500\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -248992.843750\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -231185.656250\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -209923.906250\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -263830.843750\n",
      "    epoch          : 205\n",
      "    loss           : -211476.60431640624\n",
      "    val_loss       : -239127.78224573136\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -37500.062500\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -81806.898438\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -379444.406250\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -231370.562500\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -302916.500000\n",
      "    epoch          : 206\n",
      "    loss           : -268423.054609375\n",
      "    val_loss       : -269898.1735042691\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -394987.437500\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -384029.750000\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -274295.812500\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -379584.781250\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -236579.578125\n",
      "    epoch          : 207\n",
      "    loss           : -266337.26953125\n",
      "    val_loss       : -212497.9798412621\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -65979.507812\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -328066.625000\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -281223.937500\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -240027.937500\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -219681.421875\n",
      "    epoch          : 208\n",
      "    loss           : -221313.73413085938\n",
      "    val_loss       : -265999.7410143077\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -272094.125000\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -294544.937500\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -291279.562500\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -302907.593750\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -247658.593750\n",
      "    epoch          : 209\n",
      "    loss           : -266171.183046875\n",
      "    val_loss       : -273862.1256488383\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -294599.312500\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -280349.812500\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -114752.164062\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -323668.187500\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -233987.187500\n",
      "    epoch          : 210\n",
      "    loss           : -272084.01734375\n",
      "    val_loss       : -258227.00869812965\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -296224.125000\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -241599.500000\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -379939.437500\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -244057.281250\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -281413.000000\n",
      "    epoch          : 211\n",
      "    loss           : -272435.99703125\n",
      "    val_loss       : -275553.32922514976\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -107278.734375\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -257162.265625\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -233383.156250\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -255729.000000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -179381.156250\n",
      "    epoch          : 212\n",
      "    loss           : -272847.1179296875\n",
      "    val_loss       : -256487.55819624066\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -200525.265625\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -235906.562500\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -390914.687500\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -233630.281250\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -233423.593750\n",
      "    epoch          : 213\n",
      "    loss           : -257038.4095703125\n",
      "    val_loss       : -236662.18072404264\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -186723.750000\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -232434.000000\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -354561.437500\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -375119.906250\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -293096.531250\n",
      "    epoch          : 214\n",
      "    loss           : -243369.3505859375\n",
      "    val_loss       : -242096.3529145956\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -357801.968750\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -370497.343750\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -295205.531250\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -314746.281250\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -245988.296875\n",
      "    epoch          : 215\n",
      "    loss           : -265915.67609375\n",
      "    val_loss       : -282241.64187993406\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -268788.125000\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -277415.375000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -279516.937500\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -262367.312500\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -325793.187500\n",
      "    epoch          : 216\n",
      "    loss           : -288642.351171875\n",
      "    val_loss       : -285409.7985405683\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -256598.625000\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -240455.171875\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -311953.062500\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -292380.000000\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -298747.062500\n",
      "    epoch          : 217\n",
      "    loss           : -288016.81296875\n",
      "    val_loss       : -281617.98181760014\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -288645.218750\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -287244.312500\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -290614.593750\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -154946.000000\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -110904.046875\n",
      "    epoch          : 218\n",
      "    loss           : -169894.33090820312\n",
      "    val_loss       : -65357.59351881147\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -42969.691406\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -124318.914062\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -283260.656250\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -262842.656250\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -275756.250000\n",
      "    epoch          : 219\n",
      "    loss           : -217317.71116210936\n",
      "    val_loss       : -277651.1941259384\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -262715.312500\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -314140.406250\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -261957.343750\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -244579.984375\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -286079.437500\n",
      "    epoch          : 220\n",
      "    loss           : -280253.708515625\n",
      "    val_loss       : -282078.4643780589\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -249637.890625\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -328035.687500\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -262679.093750\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -118546.828125\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -241582.875000\n",
      "    epoch          : 221\n",
      "    loss           : -285848.24390625\n",
      "    val_loss       : -281174.099788028\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -267784.093750\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -246684.250000\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -399184.125000\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -244037.718750\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -260548.453125\n",
      "    epoch          : 222\n",
      "    loss           : -256271.6068359375\n",
      "    val_loss       : -245585.42573105692\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -236700.031250\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -394001.218750\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -256901.687500\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -379421.250000\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -293340.437500\n",
      "    epoch          : 223\n",
      "    loss           : -273603.04703125\n",
      "    val_loss       : -281925.6224423647\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -289384.250000\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -264429.656250\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -117494.406250\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -311244.500000\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -384401.875000\n",
      "    epoch          : 224\n",
      "    loss           : -282412.583671875\n",
      "    val_loss       : -276507.69751963916\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -302355.593750\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -264221.187500\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -410500.687500\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -262834.187500\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -252778.875000\n",
      "    epoch          : 225\n",
      "    loss           : -264197.589453125\n",
      "    val_loss       : -221338.18072669805\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -253539.937500\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -65733.726562\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -228768.312500\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -311277.312500\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -251370.734375\n",
      "    epoch          : 226\n",
      "    loss           : -266836.57359375\n",
      "    val_loss       : -283395.9397930056\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -255640.562500\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -406140.968750\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -279178.312500\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -268175.156250\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -293837.281250\n",
      "    epoch          : 227\n",
      "    loss           : -285335.6903125\n",
      "    val_loss       : -270916.0621578753\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -387336.562500\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -385371.812500\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -217749.406250\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -252061.468750\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -243650.531250\n",
      "    epoch          : 228\n",
      "    loss           : -227625.8414453125\n",
      "    val_loss       : -264876.07670171856\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -96161.640625\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -233548.312500\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -260821.812500\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -286488.031250\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -309050.125000\n",
      "    epoch          : 229\n",
      "    loss           : -282920.65609375\n",
      "    val_loss       : -286120.8819188714\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -292412.687500\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -387570.437500\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -273692.375000\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -244952.156250\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -157218.578125\n",
      "    epoch          : 230\n",
      "    loss           : -275307.47390625\n",
      "    val_loss       : -273017.9633181125\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -388494.437500\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -246050.281250\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -310275.062500\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -274714.500000\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -265333.187500\n",
      "    epoch          : 231\n",
      "    loss           : -281681.223046875\n",
      "    val_loss       : -283996.4956545889\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -393794.562500\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -274132.062500\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -393821.343750\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -256693.031250\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -324574.750000\n",
      "    epoch          : 232\n",
      "    loss           : -284191.7646875\n",
      "    val_loss       : -281241.7356997907\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -92868.156250\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -409297.625000\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -267438.281250\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -212166.250000\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -114087.804688\n",
      "    epoch          : 233\n",
      "    loss           : -183014.3912109375\n",
      "    val_loss       : -195910.3121171564\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -154114.484375\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -201597.359375\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -250469.843750\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -295960.593750\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -246855.890625\n",
      "    epoch          : 234\n",
      "    loss           : -236286.094296875\n",
      "    val_loss       : -267038.02490148845\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -252086.437500\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -237116.109375\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -400457.781250\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -309716.437500\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -90457.468750\n",
      "    epoch          : 235\n",
      "    loss           : -272027.56453125\n",
      "    val_loss       : -273698.48436240555\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -248501.875000\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -248254.343750\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -265688.125000\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -373778.312500\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -219239.078125\n",
      "    epoch          : 236\n",
      "    loss           : -274631.0478125\n",
      "    val_loss       : -271990.606287539\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -242844.343750\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -120655.601562\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -234165.421875\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -254961.953125\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -296710.250000\n",
      "    epoch          : 237\n",
      "    loss           : -269828.408515625\n",
      "    val_loss       : -269075.2234135509\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -244233.125000\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -394328.093750\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -244680.328125\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -326861.625000\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -306971.187500\n",
      "    epoch          : 238\n",
      "    loss           : -280572.318984375\n",
      "    val_loss       : -277520.1984196484\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -380625.187500\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -272435.718750\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -383037.093750\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -237319.062500\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -387018.906250\n",
      "    epoch          : 239\n",
      "    loss           : -265473.2424609375\n",
      "    val_loss       : -285620.3970282376\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -270423.968750\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -243899.765625\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -239364.562500\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -188102.546875\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -255183.328125\n",
      "    epoch          : 240\n",
      "    loss           : -249078.2637109375\n",
      "    val_loss       : 121803.96789932848\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: 534066.375000\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -280862.500000\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -206768.468750\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -200535.375000\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -301974.343750\n",
      "    epoch          : 241\n",
      "    loss           : -170668.55129882813\n",
      "    val_loss       : -274153.2974338591\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -245628.000000\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -303233.531250\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -409747.156250\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -404693.187500\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -313940.437500\n",
      "    epoch          : 242\n",
      "    loss           : -287343.06546875\n",
      "    val_loss       : -286881.9772644341\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -409121.562500\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -259393.531250\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -292074.937500\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -266127.281250\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -307240.562500\n",
      "    epoch          : 243\n",
      "    loss           : -290143.166015625\n",
      "    val_loss       : -282620.93281729816\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -244205.640625\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -306973.656250\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -204410.687500\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -164567.156250\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -297670.250000\n",
      "    epoch          : 244\n",
      "    loss           : -265302.155546875\n",
      "    val_loss       : -244113.45812962056\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -205367.984375\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -237245.546875\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -272873.093750\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -302898.843750\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -120594.859375\n",
      "    epoch          : 245\n",
      "    loss           : -279525.425\n",
      "    val_loss       : -283262.75826749206\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -243046.109375\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -397405.843750\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -395976.937500\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -398830.625000\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -261403.875000\n",
      "    epoch          : 246\n",
      "    loss           : -282113.5778125\n",
      "    val_loss       : -276644.6285244346\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -299664.937500\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -95267.328125\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -350015.781250\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -309098.687500\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -235569.718750\n",
      "    epoch          : 247\n",
      "    loss           : -248075.0228125\n",
      "    val_loss       : -286447.6663464963\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -328906.750000\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -331245.156250\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -407117.500000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -414023.937500\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -335000.062500\n",
      "    epoch          : 248\n",
      "    loss           : -296338.68875\n",
      "    val_loss       : -293824.50748439133\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -417132.187500\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -285128.625000\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -266390.562500\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -290352.281250\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -248623.218750\n",
      "    epoch          : 249\n",
      "    loss           : -289645.06375\n",
      "    val_loss       : -265390.99284271\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -290337.937500\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -287645.187500\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -320502.093750\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -320148.187500\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -330456.593750\n",
      "    epoch          : 250\n",
      "    loss           : -275025.199453125\n",
      "    val_loss       : -292277.94572108984\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1230_200319/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -401585.937500\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -264983.125000\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -286261.687500\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -240234.875000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -249049.093750\n",
      "    epoch          : 251\n",
      "    loss           : -296913.085\n",
      "    val_loss       : -289079.80887572764\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -337017.593750\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -198538.843750\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -209600.812500\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -227624.359375\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -265834.562500\n",
      "    epoch          : 252\n",
      "    loss           : -254715.734453125\n",
      "    val_loss       : -269069.5781789102\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -304549.968750\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -406780.218750\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -319490.562500\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -291205.500000\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -403194.625000\n",
      "    epoch          : 253\n",
      "    loss           : -282820.4584375\n",
      "    val_loss       : -292969.5902874082\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -253814.875000\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -297600.843750\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -275251.812500\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -256992.156250\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -261120.421875\n",
      "    epoch          : 254\n",
      "    loss           : -290391.02953125\n",
      "    val_loss       : -281638.3487554729\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -297281.187500\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -20910.238281\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -294845.500000\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -253047.031250\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -224710.406250\n",
      "    epoch          : 255\n",
      "    loss           : -256755.99921875\n",
      "    val_loss       : -278602.09359087946\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -101513.703125\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -259818.781250\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -305586.625000\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -327659.062500\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -326839.625000\n",
      "    epoch          : 256\n",
      "    loss           : -294064.88453125\n",
      "    val_loss       : -294148.2277727008\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -276374.000000\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -271459.593750\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -276643.281250\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -261138.656250\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -258206.687500\n",
      "    epoch          : 257\n",
      "    loss           : -274484.146171875\n",
      "    val_loss       : -263052.8601817906\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -223919.203125\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -403520.125000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -402730.468750\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -260087.359375\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -310475.562500\n",
      "    epoch          : 258\n",
      "    loss           : -274813.975078125\n",
      "    val_loss       : -278492.3377689064\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -272057.312500\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -388850.062500\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -287582.062500\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -163339.906250\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -234441.875000\n",
      "    epoch          : 259\n",
      "    loss           : -262319.440078125\n",
      "    val_loss       : -258054.34958304762\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -77141.953125\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -182130.078125\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -402904.875000\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -220265.250000\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -240900.890625\n",
      "    epoch          : 260\n",
      "    loss           : -244134.2376171875\n",
      "    val_loss       : -183015.2560437143\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -199753.515625\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -96380.687500\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -405578.875000\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -283806.406250\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -251228.906250\n",
      "    epoch          : 261\n",
      "    loss           : -276263.373046875\n",
      "    val_loss       : -290082.944333002\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -417224.562500\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -328872.656250\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -295844.281250\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -121868.687500\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -329766.125000\n",
      "    epoch          : 262\n",
      "    loss           : -294174.34953125\n",
      "    val_loss       : -286659.1417141914\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -258771.187500\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -331962.687500\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -276285.656250\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -222664.343750\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -243686.250000\n",
      "    epoch          : 263\n",
      "    loss           : -254859.71724609376\n",
      "    val_loss       : -273493.12042710185\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -283500.468750\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -406229.812500\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -323191.437500\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -329231.593750\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -263306.406250\n",
      "    epoch          : 264\n",
      "    loss           : -289040.278828125\n",
      "    val_loss       : -292947.7933800042\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -267925.343750\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -413977.687500\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -341744.312500\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -260524.000000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -348598.156250\n",
      "    epoch          : 265\n",
      "    loss           : -264259.629765625\n",
      "    val_loss       : -224451.18233071268\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -237684.812500\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -159364.000000\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -221696.546875\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -41987.796875\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -303958.875000\n",
      "    epoch          : 266\n",
      "    loss           : -202488.86481445312\n",
      "    val_loss       : -253987.37344089747\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -299968.062500\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -274130.250000\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -327067.000000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -406657.750000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -291083.625000\n",
      "    epoch          : 267\n",
      "    loss           : -284788.6684375\n",
      "    val_loss       : -287846.40244846046\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -328438.750000\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -399751.062500\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -293229.093750\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -246348.812500\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -328013.500000\n",
      "    epoch          : 268\n",
      "    loss           : -281643.364453125\n",
      "    val_loss       : -286435.27437090874\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -335205.281250\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -335122.500000\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -299728.156250\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -251692.078125\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -315296.937500\n",
      "    epoch          : 269\n",
      "    loss           : -297931.237734375\n",
      "    val_loss       : -289966.2780127913\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -269704.093750\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -422269.187500\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -337033.812500\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -109763.000000\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -237530.312500\n",
      "    epoch          : 270\n",
      "    loss           : -271952.7472265625\n",
      "    val_loss       : -268877.07799552084\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -400672.562500\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -259016.093750\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -269424.437500\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -245653.703125\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -316280.812500\n",
      "    epoch          : 271\n",
      "    loss           : -280725.924921875\n",
      "    val_loss       : -284783.03603792784\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -287395.375000\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -216792.343750\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -21507.835938\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -306605.437500\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -295584.718750\n",
      "    epoch          : 272\n",
      "    loss           : -265450.2615332031\n",
      "    val_loss       : -253563.5701967597\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -263648.500000\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -312318.375000\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -264103.531250\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -412992.687500\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -334796.656250\n",
      "    epoch          : 273\n",
      "    loss           : -286230.86859375\n",
      "    val_loss       : -295112.35756685137\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -290102.250000\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -275450.437500\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -123973.500000\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -341992.625000\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -250686.625000\n",
      "    epoch          : 274\n",
      "    loss           : -301010.8290625\n",
      "    val_loss       : -297085.8809363842\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -337133.531250\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -125679.109375\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -415340.718750\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -325541.781250\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -221336.984375\n",
      "    epoch          : 275\n",
      "    loss           : -298509.81515625\n",
      "    val_loss       : -293539.792030555\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -281182.500000\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -123487.945312\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -256745.812500\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -400027.000000\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -323571.750000\n",
      "    epoch          : 276\n",
      "    loss           : -286441.99109375\n",
      "    val_loss       : -281767.76932388544\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -258636.750000\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -317417.500000\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -300614.156250\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -321274.437500\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -238803.109375\n",
      "    epoch          : 277\n",
      "    loss           : -289961.28921875\n",
      "    val_loss       : -289352.11572209\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -256551.015625\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -264031.875000\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -241555.468750\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -248769.921875\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -327566.531250\n",
      "    epoch          : 278\n",
      "    loss           : -272024.02794921875\n",
      "    val_loss       : -291390.78187964857\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -252200.062500\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -284548.125000\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -272518.656250\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -126257.281250\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -297601.062500\n",
      "    epoch          : 279\n",
      "    loss           : -298314.95671875\n",
      "    val_loss       : -294902.3167201042\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -124092.359375\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -252319.125000\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -275444.437500\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -118810.843750\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: 16981.160156\n",
      "    epoch          : 280\n",
      "    loss           : -85083.93759765624\n",
      "    val_loss       : -142908.87613199948\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -269824.250000\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -348310.875000\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -254504.000000\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -270657.062500\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -327187.093750\n",
      "    epoch          : 281\n",
      "    loss           : -258786.3919140625\n",
      "    val_loss       : -287477.3391605437\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -268026.875000\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -406668.875000\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -407420.468750\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -256209.625000\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -335766.062500\n",
      "    epoch          : 282\n",
      "    loss           : -297104.19625\n",
      "    val_loss       : -296294.485935992\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -252150.281250\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -245693.953125\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -423012.531250\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -271706.062500\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -319159.937500\n",
      "    epoch          : 283\n",
      "    loss           : -297527.652578125\n",
      "    val_loss       : -281546.5717411965\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -313896.562500\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -222268.656250\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -247469.500000\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -376479.125000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -317350.156250\n",
      "    epoch          : 284\n",
      "    loss           : -279581.2990625\n",
      "    val_loss       : -287965.57681610284\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -259404.203125\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -263283.062500\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -290264.000000\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -331729.562500\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -231210.156250\n",
      "    epoch          : 285\n",
      "    loss           : -288636.1660546875\n",
      "    val_loss       : -273853.0064057767\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -321698.750000\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -321529.250000\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -111112.593750\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -112739.031250\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -291013.937500\n",
      "    epoch          : 286\n",
      "    loss           : -283650.9553125\n",
      "    val_loss       : -292766.246169883\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -411161.937500\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -287702.750000\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -290618.687500\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -285860.406250\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -344097.062500\n",
      "    epoch          : 287\n",
      "    loss           : -302508.327734375\n",
      "    val_loss       : -300696.17756842973\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -127212.179688\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -130110.312500\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -292027.750000\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -336613.562500\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -419364.562500\n",
      "    epoch          : 288\n",
      "    loss           : -306647.175703125\n",
      "    val_loss       : -298334.3818076849\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -268254.437500\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -285380.062500\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -315558.500000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -276465.656250\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -405495.156250\n",
      "    epoch          : 289\n",
      "    loss           : -294919.51296875\n",
      "    val_loss       : -293037.5667114019\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -309813.156250\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -106275.312500\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -334378.000000\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -311893.687500\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -307443.656250\n",
      "    epoch          : 290\n",
      "    loss           : -296937.68453125\n",
      "    val_loss       : -297736.4460671723\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -340055.468750\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -126852.367188\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -283736.906250\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -417255.468750\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -128281.859375\n",
      "    epoch          : 291\n",
      "    loss           : -261430.7346484375\n",
      "    val_loss       : -208816.34561273752\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -373733.562500\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -71851.875000\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -245807.187500\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -266271.781250\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -324904.875000\n",
      "    epoch          : 292\n",
      "    loss           : -257602.0472265625\n",
      "    val_loss       : -296266.12208812835\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -405139.562500\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -279233.187500\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -424055.437500\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -341581.750000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -335586.250000\n",
      "    epoch          : 293\n",
      "    loss           : -296532.508828125\n",
      "    val_loss       : -283269.63460178376\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -247290.968750\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -354882.937500\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -305221.812500\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -192139.156250\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -301874.843750\n",
      "    epoch          : 294\n",
      "    loss           : -214114.19431640624\n",
      "    val_loss       : -283501.26658127905\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -251307.109375\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -300851.437500\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -330666.093750\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -418544.375000\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -334111.750000\n",
      "    epoch          : 295\n",
      "    loss           : -293620.494609375\n",
      "    val_loss       : -295537.43616369367\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -412393.781250\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -255438.062500\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -301491.187500\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -258475.171875\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -286599.562500\n",
      "    epoch          : 296\n",
      "    loss           : -301039.366640625\n",
      "    val_loss       : -299658.00277099165\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -141135.937500\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -268783.375000\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -418857.062500\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -250556.171875\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -306266.062500\n",
      "    epoch          : 297\n",
      "    loss           : -299800.142734375\n",
      "    val_loss       : -290329.2589142203\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -276711.062500\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -249764.406250\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -256206.093750\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -117569.218750\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -295693.531250\n",
      "    epoch          : 298\n",
      "    loss           : -291535.5940625\n",
      "    val_loss       : -289116.8005408615\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -272019.812500\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -243821.765625\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -404478.875000\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -310390.687500\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -257740.921875\n",
      "    epoch          : 299\n",
      "    loss           : -290647.0378125\n",
      "    val_loss       : -296853.81655893923\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -286771.562500\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -282994.531250\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -145526.093750\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -264855.187500\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -319965.187500\n",
      "    epoch          : 300\n",
      "    loss           : -288142.9913085938\n",
      "    val_loss       : -276899.5043034375\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1230_200319/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -400681.343750\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -413995.218750\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -277536.812500\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -397934.000000\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -244740.593750\n",
      "    epoch          : 301\n",
      "    loss           : -288804.3266796875\n",
      "    val_loss       : -299079.26522170904\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -283747.062500\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -411100.062500\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -350942.062500\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -333153.937500\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -216401.609375\n",
      "    epoch          : 302\n",
      "    loss           : -298305.0278125\n",
      "    val_loss       : -268047.1037736535\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -315327.500000\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -122103.968750\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -409899.437500\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -276856.562500\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -244541.531250\n",
      "    epoch          : 303\n",
      "    loss           : -284494.141640625\n",
      "    val_loss       : -282877.7290924847\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -408854.187500\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -122960.203125\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -295595.437500\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -248019.375000\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -246214.531250\n",
      "    epoch          : 304\n",
      "    loss           : -294974.834375\n",
      "    val_loss       : -294301.11954211595\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -264279.437500\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -103626.414062\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -297724.000000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -411144.531250\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -192174.125000\n",
      "    epoch          : 305\n",
      "    loss           : -258413.4198828125\n",
      "    val_loss       : -195062.5287394464\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -53487.460938\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -226961.921875\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -231432.906250\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -428740.437500\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -296945.250000\n",
      "    epoch          : 306\n",
      "    loss           : -274454.26390625\n",
      "    val_loss       : -301032.71460231545\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -415552.343750\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -131583.109375\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -345096.000000\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -267218.468750\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -238242.734375\n",
      "    epoch          : 307\n",
      "    loss           : -293424.11515625\n",
      "    val_loss       : -281127.8764631391\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -82301.046875\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -79626.601562\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -180707.281250\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -345339.781250\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -255479.625000\n",
      "    epoch          : 308\n",
      "    loss           : -262814.0681640625\n",
      "    val_loss       : -285284.1049595624\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -412217.687500\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -274425.156250\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -269604.687500\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -327401.875000\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -327098.531250\n",
      "    epoch          : 309\n",
      "    loss           : -296517.193203125\n",
      "    val_loss       : -290544.0175337136\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -326047.250000\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -311041.156250\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -275108.875000\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -427385.250000\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -301171.343750\n",
      "    epoch          : 310\n",
      "    loss           : -293983.42859375\n",
      "    val_loss       : -295941.44670695066\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -283910.000000\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -328360.656250\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -235285.937500\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -269851.375000\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -329683.781250\n",
      "    epoch          : 311\n",
      "    loss           : -283278.42875\n",
      "    val_loss       : -300232.90085333586\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -307696.000000\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -418138.906250\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -220465.734375\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -432315.593750\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -132858.031250\n",
      "    epoch          : 312\n",
      "    loss           : -300309.989765625\n",
      "    val_loss       : -304290.0974684775\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -277445.312500\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -291978.437500\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -420234.125000\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -305645.062500\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -259754.250000\n",
      "    epoch          : 313\n",
      "    loss           : -296200.113125\n",
      "    val_loss       : -295921.0902208209\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -324913.687500\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -329192.781250\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -321719.687500\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -336081.062500\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -15448.394531\n",
      "    epoch          : 314\n",
      "    loss           : -218325.15759765625\n",
      "    val_loss       : -149254.68980309664\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -209181.250000\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -194268.562500\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -216639.281250\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -242201.031250\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -245435.125000\n",
      "    epoch          : 315\n",
      "    loss           : -232814.0689453125\n",
      "    val_loss       : -278419.49837738276\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -316727.187500\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -309812.687500\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -423924.875000\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -112575.398438\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -240776.968750\n",
      "    epoch          : 316\n",
      "    loss           : -285187.5790625\n",
      "    val_loss       : -265729.39851924183\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -226566.234375\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -265733.718750\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -257877.125000\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -224479.031250\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -277501.812500\n",
      "    epoch          : 317\n",
      "    loss           : -273790.994375\n",
      "    val_loss       : -286732.0564181447\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -287745.312500\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -424636.562500\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -427128.312500\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -160286.578125\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -149750.546875\n",
      "    epoch          : 318\n",
      "    loss           : -232049.7121484375\n",
      "    val_loss       : -245994.04205728174\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -350735.625000\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -246484.578125\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -426860.937500\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -258250.625000\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -243912.281250\n",
      "    epoch          : 319\n",
      "    loss           : -288437.7759375\n",
      "    val_loss       : -290311.25844733714\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -258720.218750\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -423402.343750\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -344107.968750\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -276108.562500\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -415249.125000\n",
      "    epoch          : 320\n",
      "    loss           : -304460.0096875\n",
      "    val_loss       : -294669.5092769057\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -304312.187500\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -204408.234375\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -418225.843750\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -340284.062500\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -260750.375000\n",
      "    epoch          : 321\n",
      "    loss           : -299530.568359375\n",
      "    val_loss       : -298763.35272195935\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -131656.000000\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -283864.500000\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -283502.875000\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -277605.625000\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -305121.062500\n",
      "    epoch          : 322\n",
      "    loss           : -291764.136875\n",
      "    val_loss       : -298109.89341225626\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -278461.468750\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -249340.968750\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -95184.757812\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -356111.031250\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -321614.000000\n",
      "    epoch          : 323\n",
      "    loss           : -263011.9812109375\n",
      "    val_loss       : -204376.83183564246\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -195357.687500\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -88639.054688\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -287635.875000\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -309951.468750\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -321755.312500\n",
      "    epoch          : 324\n",
      "    loss           : -201829.767421875\n",
      "    val_loss       : -294088.19278359116\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -283479.187500\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -283276.000000\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -117560.187500\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -335194.062500\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -297033.593750\n",
      "    epoch          : 325\n",
      "    loss           : -294349.364140625\n",
      "    val_loss       : -291541.59612481\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -333955.718750\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -262390.125000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -430127.312500\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -138330.328125\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -324651.937500\n",
      "    epoch          : 326\n",
      "    loss           : -307381.6084375\n",
      "    val_loss       : -305988.02809196414\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -435733.812500\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -352057.562500\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -290015.437500\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -294337.187500\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -323409.093750\n",
      "    epoch          : 327\n",
      "    loss           : -311474.30875\n",
      "    val_loss       : -303359.50271897914\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -274288.468750\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -260310.703125\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -255754.171875\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -222265.843750\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -278090.187500\n",
      "    epoch          : 328\n",
      "    loss           : -295249.771171875\n",
      "    val_loss       : -278257.2249502003\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -87768.671875\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -255575.656250\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -256615.406250\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -260354.031250\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -275631.062500\n",
      "    epoch          : 329\n",
      "    loss           : -293322.515078125\n",
      "    val_loss       : -302925.4287193596\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -431141.031250\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -295932.250000\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -253901.828125\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -432687.281250\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -263273.625000\n",
      "    epoch          : 330\n",
      "    loss           : -311062.729453125\n",
      "    val_loss       : -306301.70278257725\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -259950.250000\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -308974.812500\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -284812.562500\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -251529.656250\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -340596.812500\n",
      "    epoch          : 331\n",
      "    loss           : -292933.628984375\n",
      "    val_loss       : -297400.17125034926\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -117462.492188\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -275854.437500\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -117186.789062\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -276801.156250\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -328269.875000\n",
      "    epoch          : 332\n",
      "    loss           : -293717.450859375\n",
      "    val_loss       : -286499.6287900627\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -261580.562500\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -291741.531250\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -273536.718750\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -224704.062500\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -299383.093750\n",
      "    epoch          : 333\n",
      "    loss           : -281921.4676171875\n",
      "    val_loss       : -275678.4666220695\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -277748.000000\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -246466.578125\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -307395.468750\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -333284.750000\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -259002.718750\n",
      "    epoch          : 334\n",
      "    loss           : -286451.11078125\n",
      "    val_loss       : -273262.4178213656\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -108529.414062\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -423706.312500\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -263166.312500\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -294720.125000\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -293550.500000\n",
      "    epoch          : 335\n",
      "    loss           : -298601.3875\n",
      "    val_loss       : -300914.3363642901\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -268669.062500\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -280219.750000\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -348992.000000\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -225631.250000\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -341658.562500\n",
      "    epoch          : 336\n",
      "    loss           : -302611.573984375\n",
      "    val_loss       : -298166.03223855497\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -261537.875000\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -262669.562500\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -290549.750000\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -180674.687500\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -222024.656250\n",
      "    epoch          : 337\n",
      "    loss           : -259077.03736328124\n",
      "    val_loss       : -273335.3489263117\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -327322.437500\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -413793.968750\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -333747.875000\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -257438.750000\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -289527.718750\n",
      "    epoch          : 338\n",
      "    loss           : -286541.3996484375\n",
      "    val_loss       : -295985.7947103709\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -275706.375000\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -271039.031250\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -427448.125000\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -440360.687500\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -275795.875000\n",
      "    epoch          : 339\n",
      "    loss           : -303968.291015625\n",
      "    val_loss       : -293954.03219460545\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -128617.218750\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -138083.562500\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -277323.187500\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -264660.312500\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -262227.500000\n",
      "    epoch          : 340\n",
      "    loss           : -302871.234375\n",
      "    val_loss       : -288674.1915064156\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -427970.250000\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -259913.359375\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -112219.195312\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -428061.843750\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -233524.156250\n",
      "    epoch          : 341\n",
      "    loss           : -296952.333203125\n",
      "    val_loss       : -285071.54761743546\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -260759.562500\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -435890.812500\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -410240.125000\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -267132.343750\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -330430.562500\n",
      "    epoch          : 342\n",
      "    loss           : -285948.166484375\n",
      "    val_loss       : -289212.65425387025\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -269995.562500\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -231990.812500\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -258857.937500\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -257910.531250\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -308036.687500\n",
      "    epoch          : 343\n",
      "    loss           : -302220.943671875\n",
      "    val_loss       : -304908.863724491\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -284006.437500\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -124071.929688\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -349599.781250\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -345430.531250\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -321061.625000\n",
      "    epoch          : 344\n",
      "    loss           : -309991.0534375\n",
      "    val_loss       : -289327.73068330885\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -343516.125000\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -305863.312500\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -276229.906250\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -13922.760742\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -229887.875000\n",
      "    epoch          : 345\n",
      "    loss           : -207911.08991210937\n",
      "    val_loss       : -273141.41998612287\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -94944.179688\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -384076.500000\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -429804.406250\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -299258.062500\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -295491.000000\n",
      "    epoch          : 346\n",
      "    loss           : -303616.959375\n",
      "    val_loss       : -304983.0875143617\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -285405.875000\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -354374.562500\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -274519.406250\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -237581.250000\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -273890.750000\n",
      "    epoch          : 347\n",
      "    loss           : -299111.637421875\n",
      "    val_loss       : -289143.06819328666\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -346948.687500\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -273664.125000\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -281924.156250\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -438372.250000\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -138550.640625\n",
      "    epoch          : 348\n",
      "    loss           : -311666.305625\n",
      "    val_loss       : -312506.9852175951\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -358391.437500\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -354348.062500\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -286857.687500\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -128581.640625\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -350542.437500\n",
      "    epoch          : 349\n",
      "    loss           : -307048.30984375\n",
      "    val_loss       : -305852.09468623996\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -433339.875000\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -288492.187500\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -332788.375000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -126442.328125\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -211833.703125\n",
      "    epoch          : 350\n",
      "    loss           : -296956.9690625\n",
      "    val_loss       : -306768.2283867478\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1230_200319/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -261529.953125\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -310890.562500\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -432310.062500\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -77836.437500\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -224418.968750\n",
      "    epoch          : 351\n",
      "    loss           : -252688.82271484376\n",
      "    val_loss       : -231393.58849959372\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -282659.562500\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -400218.906250\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -75208.078125\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -290886.187500\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -285833.593750\n",
      "    epoch          : 352\n",
      "    loss           : -265922.937265625\n",
      "    val_loss       : -306374.3216422081\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -135826.562500\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -255823.437500\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -141661.281250\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -443416.343750\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -340947.375000\n",
      "    epoch          : 353\n",
      "    loss           : -316287.10984375\n",
      "    val_loss       : -310799.02890561224\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -432504.562500\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -282256.156250\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -293085.375000\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -269169.500000\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -354518.531250\n",
      "    epoch          : 354\n",
      "    loss           : -317049.57875\n",
      "    val_loss       : -312578.2345541775\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -343343.562500\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -303196.125000\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -353255.187500\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -356306.375000\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -316146.000000\n",
      "    epoch          : 355\n",
      "    loss           : -317163.26828125\n",
      "    val_loss       : -305453.42183478025\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -294041.625000\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -350987.437500\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -428686.500000\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -233902.828125\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -123392.156250\n",
      "    epoch          : 356\n",
      "    loss           : -293477.05961914064\n",
      "    val_loss       : -277088.9433128595\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -416963.187500\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -228311.375000\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -249984.328125\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -430373.250000\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -263244.812500\n",
      "    epoch          : 357\n",
      "    loss           : -293828.149375\n",
      "    val_loss       : -303339.3241350025\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -429618.093750\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -430270.218750\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -339152.125000\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -277217.656250\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -276902.937500\n",
      "    epoch          : 358\n",
      "    loss           : -300269.607109375\n",
      "    val_loss       : -297448.416589877\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -275416.437500\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -286159.531250\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -401187.500000\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -238830.953125\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -237395.296875\n",
      "    epoch          : 359\n",
      "    loss           : -281105.486328125\n",
      "    val_loss       : -239043.47645484805\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -349819.468750\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -230026.906250\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -263103.187500\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -255510.750000\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -310123.968750\n",
      "    epoch          : 360\n",
      "    loss           : -294194.474453125\n",
      "    val_loss       : -301723.0085663319\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -118077.945312\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -437113.875000\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -360297.218750\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -235082.343750\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -233536.593750\n",
      "    epoch          : 361\n",
      "    loss           : -277152.44015625\n",
      "    val_loss       : -179424.46822904944\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: 1616.632812\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -263281.718750\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -326688.125000\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -242869.312500\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -300073.406250\n",
      "    epoch          : 362\n",
      "    loss           : -262414.680234375\n",
      "    val_loss       : -267149.0005656481\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -244664.125000\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -319315.781250\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -343129.343750\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -439839.750000\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -347017.625000\n",
      "    epoch          : 363\n",
      "    loss           : -299248.93453125\n",
      "    val_loss       : -297856.3268317461\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -291227.531250\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -123748.468750\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -263532.968750\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -433927.625000\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -311928.531250\n",
      "    epoch          : 364\n",
      "    loss           : -313828.70921875\n",
      "    val_loss       : -304698.030469203\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -341527.593750\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -395951.218750\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -99786.250000\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -244270.093750\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -244638.718750\n",
      "    epoch          : 365\n",
      "    loss           : -278777.5695703125\n",
      "    val_loss       : -278419.4336622864\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -301399.687500\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -420597.968750\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -269816.593750\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -139236.062500\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -354227.812500\n",
      "    epoch          : 366\n",
      "    loss           : -300793.650234375\n",
      "    val_loss       : -310478.27128330467\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -348910.406250\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -346284.468750\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -308926.593750\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -310068.593750\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -343399.875000\n",
      "    epoch          : 367\n",
      "    loss           : -311570.938046875\n",
      "    val_loss       : -292943.74494515656\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -315313.750000\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -264914.406250\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -270775.562500\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -424614.500000\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -97632.648438\n",
      "    epoch          : 368\n",
      "    loss           : -279351.1614453125\n",
      "    val_loss       : -301882.16838707327\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -439293.875000\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -131044.468750\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -295940.500000\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -266851.812500\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -332118.687500\n",
      "    epoch          : 369\n",
      "    loss           : -313573.4378125\n",
      "    val_loss       : -306175.2855851889\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -261561.640625\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -317158.156250\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -268588.093750\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -260503.656250\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -350179.812500\n",
      "    epoch          : 370\n",
      "    loss           : -316738.477890625\n",
      "    val_loss       : -309200.8336377352\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -132466.015625\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -289825.375000\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -320966.500000\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -320846.625000\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -357017.812500\n",
      "    epoch          : 371\n",
      "    loss           : -317268.698125\n",
      "    val_loss       : -293410.7895133674\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -255145.093750\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -323033.687500\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -288914.875000\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -287793.812500\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -302445.437500\n",
      "    epoch          : 372\n",
      "    loss           : -289160.64828125\n",
      "    val_loss       : -135669.24873563944\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -65355.632812\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -333408.250000\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -327468.562500\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -438677.875000\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -256871.718750\n",
      "    epoch          : 373\n",
      "    loss           : -290651.796171875\n",
      "    val_loss       : -260926.4424935162\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -222798.640625\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -290662.375000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -437904.000000\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -351463.687500\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -439788.062500\n",
      "    epoch          : 374\n",
      "    loss           : -306537.25609375\n",
      "    val_loss       : -289776.5755499482\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -133642.031250\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -346562.843750\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -340762.343750\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -319540.062500\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -350923.437500\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   375: reducing learning rate of group 0 to 1.0000e-04.\n",
      "    epoch          : 375\n",
      "    loss           : -305815.41671875\n",
      "    val_loss       : -302592.71732260584\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -257366.984375\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -435542.437500\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -446412.687500\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -441667.375000\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -143825.437500\n",
      "    epoch          : 376\n",
      "    loss           : -324002.5403125\n",
      "    val_loss       : -317158.7506901383\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -269594.250000\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -324788.406250\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -449431.000000\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -348507.250000\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -306297.937500\n",
      "    epoch          : 377\n",
      "    loss           : -325839.52609375\n",
      "    val_loss       : -317719.28967286943\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -444654.218750\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -293999.968750\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -309272.625000\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -329390.406250\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -323150.343750\n",
      "    epoch          : 378\n",
      "    loss           : -326326.54390625\n",
      "    val_loss       : -318889.13286253216\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -359860.000000\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -297417.750000\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -361155.250000\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -270746.125000\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -308733.250000\n",
      "    epoch          : 379\n",
      "    loss           : -326356.8884375\n",
      "    val_loss       : -316712.3198528111\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -311478.156250\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -366163.375000\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -276399.718750\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -443973.031250\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -270325.312500\n",
      "    epoch          : 380\n",
      "    loss           : -326805.58140625\n",
      "    val_loss       : -317725.2393677294\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -352140.500000\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -272002.250000\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -328597.156250\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -361159.000000\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -281750.781250\n",
      "    epoch          : 381\n",
      "    loss           : -326549.02484375\n",
      "    val_loss       : -319465.7744700432\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -143117.281250\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -293012.562500\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -333497.906250\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -361376.437500\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -350201.468750\n",
      "    epoch          : 382\n",
      "    loss           : -326903.83421875\n",
      "    val_loss       : -318443.9426679254\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -312787.125000\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -273206.437500\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -448920.250000\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -360997.375000\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -352087.218750\n",
      "    epoch          : 383\n",
      "    loss           : -327300.39\n",
      "    val_loss       : -318644.4171469331\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -303185.937500\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -302485.937500\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -148279.203125\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -361846.500000\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -360575.312500\n",
      "    epoch          : 384\n",
      "    loss           : -327338.57046875\n",
      "    val_loss       : -319295.93571312726\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -360137.250000\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -350830.562500\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -309761.218750\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -441422.031250\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -302980.125000\n",
      "    epoch          : 385\n",
      "    loss           : -327128.4171875\n",
      "    val_loss       : -318700.86110591295\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -157731.156250\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -308109.250000\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -283083.312500\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -355371.187500\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -336936.406250\n",
      "    epoch          : 386\n",
      "    loss           : -327277.44859375\n",
      "    val_loss       : -320396.9103728056\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -304200.125000\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -312163.468750\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -274724.812500\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -282953.500000\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -333243.062500\n",
      "    epoch          : 387\n",
      "    loss           : -327567.86859375\n",
      "    val_loss       : -320311.47916500864\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -288366.312500\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -295992.281250\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -274732.937500\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -363212.500000\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -315059.125000\n",
      "    epoch          : 388\n",
      "    loss           : -327461.6540625\n",
      "    val_loss       : -319550.3063990146\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -442282.937500\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -288138.437500\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -153732.328125\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -273449.125000\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -362041.718750\n",
      "    epoch          : 389\n",
      "    loss           : -327483.41859375\n",
      "    val_loss       : -319277.8040069699\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -450303.843750\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -311873.156250\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -445307.125000\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -451347.656250\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -356066.281250\n",
      "    epoch          : 390\n",
      "    loss           : -327837.7015625\n",
      "    val_loss       : -317976.3542249858\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -368206.031250\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -274325.156250\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -443305.812500\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -282315.187500\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -361587.250000\n",
      "    epoch          : 391\n",
      "    loss           : -327946.57125\n",
      "    val_loss       : -317828.3329887241\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -283691.625000\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -443459.062500\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -282310.562500\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -455719.156250\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -275598.062500\n",
      "    epoch          : 392\n",
      "    loss           : -327915.56078125\n",
      "    val_loss       : -318641.60213227867\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -151158.953125\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -364173.281250\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -446516.937500\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -285541.187500\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -357866.218750\n",
      "    epoch          : 393\n",
      "    loss           : -328118.85734375\n",
      "    val_loss       : -319680.5985137671\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -452509.500000\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -278986.468750\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -453159.250000\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -299265.406250\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -357080.750000\n",
      "    epoch          : 394\n",
      "    loss           : -327996.29609375\n",
      "    val_loss       : -319955.36178194283\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -442699.687500\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -322116.218750\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -306501.812500\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -286212.843750\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -450521.000000\n",
      "    epoch          : 395\n",
      "    loss           : -328290.68171875\n",
      "    val_loss       : -318789.7503089577\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -145869.562500\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -157317.625000\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -285143.375000\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -343491.625000\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -363177.187500\n",
      "    epoch          : 396\n",
      "    loss           : -328387.74390625\n",
      "    val_loss       : -319597.68953658047\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -283860.625000\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -309304.093750\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -306065.687500\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -276893.750000\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -279252.593750\n",
      "    epoch          : 397\n",
      "    loss           : -328480.15875\n",
      "    val_loss       : -319721.9812089562\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -287784.406250\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -313606.875000\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -301314.437500\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -452065.468750\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -451812.968750\n",
      "    epoch          : 398\n",
      "    loss           : -328471.85765625\n",
      "    val_loss       : -318732.56542619166\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -346842.031250\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -353895.593750\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -329174.812500\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -285898.218750\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -272266.750000\n",
      "    epoch          : 399\n",
      "    loss           : -328741.8496875\n",
      "    val_loss       : -319120.5743457198\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -372921.125000\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -280962.312500\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -278696.968750\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -357063.375000\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -286694.875000\n",
      "    epoch          : 400\n",
      "    loss           : -328716.3446875\n",
      "    val_loss       : -319433.33222206833\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1230_200319/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -332728.312500\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -302761.500000\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -342588.625000\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -361459.968750\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -299361.625000\n",
      "    epoch          : 401\n",
      "    loss           : -328542.9984375\n",
      "    val_loss       : -319393.7646085009\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -151263.906250\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -291978.843750\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -305809.125000\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -370823.500000\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -279168.843750\n",
      "    epoch          : 402\n",
      "    loss           : -328999.50671875\n",
      "    val_loss       : -318548.18220763205\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -365240.000000\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -451338.312500\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -281576.562500\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -357991.718750\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -355938.500000\n",
      "    epoch          : 403\n",
      "    loss           : -328767.0221875\n",
      "    val_loss       : -319115.7302461326\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -453870.250000\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -368565.718750\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -303883.375000\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -152566.250000\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -344043.093750\n",
      "    epoch          : 404\n",
      "    loss           : -329194.9578125\n",
      "    val_loss       : -320069.0736017406\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -292374.062500\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -363662.125000\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -448134.781250\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -367832.375000\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -330448.531250\n",
      "    epoch          : 405\n",
      "    loss           : -329170.60421875\n",
      "    val_loss       : -318807.48389021755\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -158102.312500\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -311491.562500\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -328074.187500\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -275241.906250\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -274676.468750\n",
      "    epoch          : 406\n",
      "    loss           : -329229.60953125\n",
      "    val_loss       : -321649.81850595475\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -362796.375000\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -295474.312500\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -454902.125000\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -312730.250000\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -337142.781250\n",
      "    epoch          : 407\n",
      "    loss           : -329462.3790625\n",
      "    val_loss       : -320302.67305458785\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -454858.312500\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -444763.406250\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -450284.812500\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -363306.281250\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -326681.812500\n",
      "    epoch          : 408\n",
      "    loss           : -329320.11765625\n",
      "    val_loss       : -320721.4095934659\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -328963.375000\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -329990.218750\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -282074.062500\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -272083.312500\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -446430.187500\n",
      "    epoch          : 409\n",
      "    loss           : -329345.37421875\n",
      "    val_loss       : -321225.33496895136\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -451138.875000\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -350653.281250\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -306617.343750\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -450956.843750\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -356283.250000\n",
      "    epoch          : 410\n",
      "    loss           : -329450.435625\n",
      "    val_loss       : -321285.7878657967\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -311505.687500\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -358219.937500\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -282323.625000\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -459290.812500\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -368728.406250\n",
      "    epoch          : 411\n",
      "    loss           : -329877.70953125\n",
      "    val_loss       : -320588.89158009586\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -314774.906250\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -156047.375000\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -330934.625000\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -356696.250000\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -345463.187500\n",
      "    epoch          : 412\n",
      "    loss           : -329780.21453125\n",
      "    val_loss       : -321304.5431276172\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -282807.687500\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -451425.312500\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -449688.687500\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -308204.125000\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -282180.062500\n",
      "    epoch          : 413\n",
      "    loss           : -329762.07109375\n",
      "    val_loss       : -320335.8363053143\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -368886.531250\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -311184.875000\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -292458.187500\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -281876.875000\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -336903.437500\n",
      "    epoch          : 414\n",
      "    loss           : -329676.044375\n",
      "    val_loss       : -321102.48769341706\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -300944.375000\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -148047.640625\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -462128.812500\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -372383.812500\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -334240.687500\n",
      "    epoch          : 415\n",
      "    loss           : -330130.31265625\n",
      "    val_loss       : -320592.97347877023\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -306372.218750\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -453936.593750\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -276699.593750\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -286920.468750\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -350698.187500\n",
      "    epoch          : 416\n",
      "    loss           : -329825.91234375\n",
      "    val_loss       : -320576.4633308351\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -290674.718750\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -358542.312500\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -453822.281250\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -368382.437500\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -310770.250000\n",
      "    epoch          : 417\n",
      "    loss           : -330205.7134375\n",
      "    val_loss       : -322292.033431077\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -313775.937500\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -153321.531250\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -298780.156250\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -312630.437500\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -308871.312500\n",
      "    epoch          : 418\n",
      "    loss           : -330309.4103125\n",
      "    val_loss       : -320354.76212516427\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -287657.687500\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -293875.375000\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -368047.093750\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -153481.687500\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -356376.125000\n",
      "    epoch          : 419\n",
      "    loss           : -330424.69484375\n",
      "    val_loss       : -319798.50514673593\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -300546.906250\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -450922.968750\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -304444.750000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -368079.562500\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -376048.718750\n",
      "    epoch          : 420\n",
      "    loss           : -330271.0109375\n",
      "    val_loss       : -320893.7901865512\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -342592.031250\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -369208.125000\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -300431.250000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -369649.625000\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -271617.500000\n",
      "    epoch          : 421\n",
      "    loss           : -330561.12546875\n",
      "    val_loss       : -322251.92988111975\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -307579.875000\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -315784.968750\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -157198.578125\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -364786.187500\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -277949.562500\n",
      "    epoch          : 422\n",
      "    loss           : -330469.2715625\n",
      "    val_loss       : -319855.8485778153\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -314682.000000\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -359934.812500\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -449277.562500\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -343961.000000\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -439460.781250\n",
      "    epoch          : 423\n",
      "    loss           : -330565.423125\n",
      "    val_loss       : -321205.7206805766\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -455982.781250\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -458083.250000\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -462121.562500\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -343883.875000\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -167521.578125\n",
      "    epoch          : 424\n",
      "    loss           : -330602.716875\n",
      "    val_loss       : -322883.13192178606\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -153810.437500\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -306932.468750\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -309379.218750\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -371622.187500\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -309107.375000\n",
      "    epoch          : 425\n",
      "    loss           : -330523.93453125\n",
      "    val_loss       : -321392.8722980678\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -291989.125000\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -301051.250000\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -331650.000000\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -158719.562500\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -284474.281250\n",
      "    epoch          : 426\n",
      "    loss           : -330982.60671875\n",
      "    val_loss       : -321093.9663737357\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -373418.875000\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -307162.562500\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -308860.906250\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -294323.125000\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -313747.125000\n",
      "    epoch          : 427\n",
      "    loss           : -330557.80328125\n",
      "    val_loss       : -321541.2311163306\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -299857.593750\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -370993.625000\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -283413.000000\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -366441.031250\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -345941.437500\n",
      "    epoch          : 428\n",
      "    loss           : -330919.31703125\n",
      "    val_loss       : -320523.5856342435\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -351722.781250\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -362140.281250\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -352317.093750\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -308011.062500\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -372209.062500\n",
      "    epoch          : 429\n",
      "    loss           : -331046.218125\n",
      "    val_loss       : -322210.16603914497\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -153526.531250\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -161473.437500\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -308124.000000\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -162665.500000\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -277469.187500\n",
      "    epoch          : 430\n",
      "    loss           : -331119.0278125\n",
      "    val_loss       : -320360.2137182653\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -158163.843750\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -307537.125000\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -301459.875000\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -333366.250000\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -373905.000000\n",
      "    epoch          : 431\n",
      "    loss           : -331093.17875\n",
      "    val_loss       : -322136.62277941406\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -276492.625000\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -274062.312500\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -449061.000000\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -313963.843750\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -362707.500000\n",
      "    epoch          : 432\n",
      "    loss           : -331280.585625\n",
      "    val_loss       : -321979.99996949435\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -300470.281250\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -284844.375000\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -307553.875000\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -363503.750000\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -368430.562500\n",
      "    epoch          : 433\n",
      "    loss           : -331245.29671875\n",
      "    val_loss       : -322196.5982475579\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -277102.250000\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -163321.968750\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -306797.875000\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -373395.562500\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -373648.656250\n",
      "    epoch          : 434\n",
      "    loss           : -331241.961875\n",
      "    val_loss       : -322268.2438521713\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -300038.375000\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -375077.562500\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -304549.375000\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -163204.281250\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -277629.593750\n",
      "    epoch          : 435\n",
      "    loss           : -331400.85953125\n",
      "    val_loss       : -322666.56971027853\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -165322.890625\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -334427.812500\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -355432.125000\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -275236.125000\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -368533.031250\n",
      "    epoch          : 436\n",
      "    loss           : -331548.81640625\n",
      "    val_loss       : -322564.0579360485\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -372843.781250\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -453755.781250\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -456514.187500\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -335730.656250\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -280251.156250\n",
      "    epoch          : 437\n",
      "    loss           : -331692.3959375\n",
      "    val_loss       : -322897.7754114449\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -310482.031250\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -352569.937500\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -287367.031250\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -293464.250000\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -358695.062500\n",
      "    epoch          : 438\n",
      "    loss           : -332034.50375\n",
      "    val_loss       : -322990.030142349\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -368642.906250\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -278341.812500\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -452141.250000\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -304989.093750\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -286150.875000\n",
      "    epoch          : 439\n",
      "    loss           : -331707.9553125\n",
      "    val_loss       : -322136.6889533639\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -453326.375000\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -305682.656250\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -331196.125000\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -451839.562500\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -366644.437500\n",
      "    epoch          : 440\n",
      "    loss           : -331979.56015625\n",
      "    val_loss       : -321313.8689756006\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -290670.000000\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -168415.312500\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -369091.187500\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -331841.031250\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -329388.750000\n",
      "    epoch          : 441\n",
      "    loss           : -331842.36125\n",
      "    val_loss       : -322266.4548049122\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -161411.671875\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -287268.437500\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -367881.093750\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -306868.125000\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -305379.968750\n",
      "    epoch          : 442\n",
      "    loss           : -331987.01484375\n",
      "    val_loss       : -322858.29552892444\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -310781.031250\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -455838.187500\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -301520.750000\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -368253.312500\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -311656.562500\n",
      "    epoch          : 443\n",
      "    loss           : -331966.77203125\n",
      "    val_loss       : -323648.099964714\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -299441.343750\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -452551.812500\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -313707.437500\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -333879.187500\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -274635.218750\n",
      "    epoch          : 444\n",
      "    loss           : -331895.876875\n",
      "    val_loss       : -322950.56192107796\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -165931.531250\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -309810.031250\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -291207.593750\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -281528.031250\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -375349.593750\n",
      "    epoch          : 445\n",
      "    loss           : -332225.14890625\n",
      "    val_loss       : -323823.9011033565\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -288737.625000\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -372407.375000\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -457549.687500\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -459201.218750\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -337639.875000\n",
      "    epoch          : 446\n",
      "    loss           : -332211.926875\n",
      "    val_loss       : -323150.7343597114\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -365258.937500\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -285330.625000\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -360087.906250\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -459709.687500\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -305469.187500\n",
      "    epoch          : 447\n",
      "    loss           : -332098.37125\n",
      "    val_loss       : -322998.64860210416\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -370066.937500\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -303755.187500\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -302243.937500\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -376021.687500\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -279117.937500\n",
      "    epoch          : 448\n",
      "    loss           : -332143.23640625\n",
      "    val_loss       : -323020.3829054564\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -453117.656250\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -455315.093750\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -285762.312500\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -310351.187500\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -365684.937500\n",
      "    epoch          : 449\n",
      "    loss           : -332355.358125\n",
      "    val_loss       : -325030.16206090746\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -328934.625000\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -341102.843750\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -303851.625000\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -359183.125000\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -282746.875000\n",
      "    epoch          : 450\n",
      "    loss           : -332218.92578125\n",
      "    val_loss       : -322504.6073622346\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1230_200319/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -454932.843750\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -285257.187500\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -161451.625000\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -150676.593750\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -336591.343750\n",
      "    epoch          : 451\n",
      "    loss           : -332707.28578125\n",
      "    val_loss       : -324162.4303578198\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -344135.562500\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -284178.031250\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -459639.875000\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -457202.562500\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -325507.500000\n",
      "    epoch          : 452\n",
      "    loss           : -332579.1609375\n",
      "    val_loss       : -322287.57712803484\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -456675.562500\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -351690.500000\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -161664.750000\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -458253.156250\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -334023.437500\n",
      "    epoch          : 453\n",
      "    loss           : -332703.45859375\n",
      "    val_loss       : -323499.48759747745\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -296548.187500\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -464053.000000\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -464998.812500\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -329634.781250\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -281467.812500\n",
      "    epoch          : 454\n",
      "    loss           : -332875.409375\n",
      "    val_loss       : -321883.63350675104\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -329722.125000\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -284933.093750\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -304615.718750\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -307390.093750\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -163177.781250\n",
      "    epoch          : 455\n",
      "    loss           : -332782.17828125\n",
      "    val_loss       : -322565.65827034117\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -307292.000000\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -369999.875000\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -363665.812500\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -373659.437500\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -309253.187500\n",
      "    epoch          : 456\n",
      "    loss           : -332844.2396875\n",
      "    val_loss       : -323130.6183441222\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -149811.625000\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -456732.718750\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -451468.437500\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -372493.750000\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -365403.375000\n",
      "    epoch          : 457\n",
      "    loss           : -332872.1825\n",
      "    val_loss       : -322979.1001000583\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -284162.187500\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -288088.500000\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -308383.875000\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -295731.062500\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -282932.937500\n",
      "    epoch          : 458\n",
      "    loss           : -333024.40421875\n",
      "    val_loss       : -322986.7070923984\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -338291.843750\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -291807.125000\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -272422.406250\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -375703.312500\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -309917.312500\n",
      "    epoch          : 459\n",
      "    loss           : -333109.32859375\n",
      "    val_loss       : -323153.8674191594\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -463366.375000\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -450179.781250\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -461796.031250\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -163646.062500\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -369569.656250\n",
      "    epoch          : 460\n",
      "    loss           : -333145.28421875\n",
      "    val_loss       : -323785.8630248457\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -277384.562500\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -315208.937500\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -341104.937500\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -148799.578125\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -290589.593750\n",
      "    epoch          : 461\n",
      "    loss           : -333386.06265625\n",
      "    val_loss       : -321897.6341846377\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -376517.750000\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -281897.593750\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -456992.062500\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -306024.000000\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -336834.656250\n",
      "    epoch          : 462\n",
      "    loss           : -333165.81875\n",
      "    val_loss       : -324622.7172680378\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -311003.656250\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -340519.250000\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -463018.312500\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -376125.250000\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -273315.468750\n",
      "    epoch          : 463\n",
      "    loss           : -333377.173125\n",
      "    val_loss       : -323280.5313588738\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -363698.656250\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -450446.500000\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -152701.609375\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -316882.468750\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -336059.937500\n",
      "    epoch          : 464\n",
      "    loss           : -333489.46203125\n",
      "    val_loss       : -324544.2274015456\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -378799.875000\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -160802.546875\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -454565.625000\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -309190.968750\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -281830.875000\n",
      "    epoch          : 465\n",
      "    loss           : -333549.79203125\n",
      "    val_loss       : -323003.9180736542\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -453569.312500\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -276460.312500\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -163390.718750\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -308115.531250\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -336288.125000\n",
      "    epoch          : 466\n",
      "    loss           : -333544.65109375\n",
      "    val_loss       : -324307.00787163374\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -314648.625000\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -461167.875000\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -307426.625000\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -333138.750000\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -362596.875000\n",
      "    epoch          : 467\n",
      "    loss           : -333812.01703125\n",
      "    val_loss       : -323821.9413144618\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -291007.750000\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -462670.593750\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -358005.250000\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -173382.031250\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -328070.437500\n",
      "    epoch          : 468\n",
      "    loss           : -333649.06640625\n",
      "    val_loss       : -324435.43097726704\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -363582.062500\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -356439.406250\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -337390.812500\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -359562.375000\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -363087.687500\n",
      "    epoch          : 469\n",
      "    loss           : -333734.2725\n",
      "    val_loss       : -323127.30630531907\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -303108.687500\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -342064.937500\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -358720.125000\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -333509.031250\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -280004.812500\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   470: reducing learning rate of group 0 to 1.0000e-05.\n",
      "    epoch          : 470\n",
      "    loss           : -333690.39125\n",
      "    val_loss       : -323551.0184384823\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -299773.000000\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -313925.500000\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -456218.750000\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -314630.750000\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -373310.625000\n",
      "    epoch          : 471\n",
      "    loss           : -334045.33484375\n",
      "    val_loss       : -323631.56492011546\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -280172.500000\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -361145.750000\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -277685.062500\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -355243.312500\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -310301.125000\n",
      "    epoch          : 472\n",
      "    loss           : -333730.4828125\n",
      "    val_loss       : -322585.4690950215\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -460751.062500\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -277609.687500\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -337318.125000\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -459655.906250\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -160848.781250\n",
      "    epoch          : 473\n",
      "    loss           : -333957.9165625\n",
      "    val_loss       : -323780.8204020768\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -282448.468750\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -311024.125000\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -298466.750000\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -158650.843750\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -282640.625000\n",
      "    epoch          : 474\n",
      "    loss           : -334190.9425\n",
      "    val_loss       : -323701.5627001196\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -461132.250000\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -458554.656250\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -277397.937500\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -333038.687500\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -162298.500000\n",
      "    epoch          : 475\n",
      "    loss           : -334175.72\n",
      "    val_loss       : -323559.3479059696\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -166584.218750\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -463842.218750\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -313946.625000\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -443708.875000\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -281370.906250\n",
      "    epoch          : 476\n",
      "    loss           : -333979.85234375\n",
      "    val_loss       : -324031.4611822188\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -462530.843750\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -458616.656250\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -319643.968750\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -448845.406250\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -369476.250000\n",
      "    epoch          : 477\n",
      "    loss           : -333971.21515625\n",
      "    val_loss       : -323735.5565268874\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -447687.625000\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -146455.843750\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -158780.609375\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -279352.500000\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -367519.375000\n",
      "    epoch          : 478\n",
      "    loss           : -334028.838125\n",
      "    val_loss       : -324762.024838233\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -369479.312500\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -461921.406250\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -465415.093750\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -337155.250000\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -375193.281250\n",
      "    epoch          : 479\n",
      "    loss           : -333962.88453125\n",
      "    val_loss       : -323499.94459278585\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -311700.562500\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -161983.234375\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -378112.562500\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -337714.500000\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -285152.218750\n",
      "    epoch          : 480\n",
      "    loss           : -333983.7775\n",
      "    val_loss       : -323515.324896881\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -275374.750000\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -166829.234375\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -454561.937500\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -330725.437500\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -288359.218750\n",
      "    epoch          : 481\n",
      "    loss           : -333830.48125\n",
      "    val_loss       : -324825.5584922135\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -339828.187500\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -154716.500000\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -336817.562500\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -315864.750000\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -372408.937500\n",
      "    epoch          : 482\n",
      "    loss           : -334025.75921875\n",
      "    val_loss       : -324518.8231004059\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -459240.562500\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -171879.781250\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -370939.500000\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -376974.062500\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -354936.687500\n",
      "    epoch          : 483\n",
      "    loss           : -334158.80390625\n",
      "    val_loss       : -324773.7443751812\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -333826.093750\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -292091.750000\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -298946.531250\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -160013.703125\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -292475.312500\n",
      "    epoch          : 484\n",
      "    loss           : -334003.3034375\n",
      "    val_loss       : -323849.842644006\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -366470.125000\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -287129.656250\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -469192.281250\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -362191.062500\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -274040.562500\n",
      "    epoch          : 485\n",
      "    loss           : -334317.49921875\n",
      "    val_loss       : -323160.25803223846\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -300024.937500\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -155280.812500\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -314174.375000\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -299653.562500\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -355090.437500\n",
      "    epoch          : 486\n",
      "    loss           : -333968.74203125\n",
      "    val_loss       : -323922.79572864773\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -318449.812500\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -306674.062500\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -161152.703125\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -310549.468750\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -337257.437500\n",
      "    epoch          : 487\n",
      "    loss           : -334149.83984375\n",
      "    val_loss       : -324411.533390215\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -340138.531250\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -300425.625000\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -329598.500000\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -341659.625000\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -461144.250000\n",
      "    epoch          : 488\n",
      "    loss           : -333959.1215625\n",
      "    val_loss       : -324783.55058889685\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -158703.312500\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -155584.531250\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -462437.406250\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -455410.281250\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -344274.343750\n",
      "    epoch          : 489\n",
      "    loss           : -334152.13671875\n",
      "    val_loss       : -324563.87810410856\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -159296.296875\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -158055.250000\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -291141.343750\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -282953.125000\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -265679.125000\n",
      "    epoch          : 490\n",
      "    loss           : -334074.044375\n",
      "    val_loss       : -323770.68568123877\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -303949.906250\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -301607.875000\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -293103.250000\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -368433.125000\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -359205.531250\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "    epoch          : 491\n",
      "    loss           : -334045.13359375\n",
      "    val_loss       : -324293.41591219604\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -310818.656250\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -372930.187500\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -299821.187500\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -311893.000000\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -313912.937500\n",
      "    epoch          : 492\n",
      "    loss           : -334194.760625\n",
      "    val_loss       : -324611.23244065046\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -297769.500000\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -286701.593750\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -327062.812500\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -345803.843750\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -357660.812500\n",
      "    epoch          : 493\n",
      "    loss           : -333913.73171875\n",
      "    val_loss       : -323774.4291216314\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -300860.625000\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -461589.125000\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -463503.750000\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -353092.375000\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -279827.937500\n",
      "    epoch          : 494\n",
      "    loss           : -334216.79921875\n",
      "    val_loss       : -324196.14695366623\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -165563.765625\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -158402.187500\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -454837.062500\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -284579.625000\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -346158.625000\n",
      "    epoch          : 495\n",
      "    loss           : -334138.6896875\n",
      "    val_loss       : -323339.41892738343\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -460272.062500\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -157374.078125\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -447571.687500\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -369208.750000\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -286212.562500\n",
      "    epoch          : 496\n",
      "    loss           : -333909.4596875\n",
      "    val_loss       : -324073.7269236922\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -353988.125000\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -309119.031250\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -367890.593750\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -294593.062500\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -380112.375000\n",
      "    epoch          : 497\n",
      "    loss           : -334001.400625\n",
      "    val_loss       : -323714.1495757252\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -335010.312500\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -317532.000000\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -462755.562500\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -357930.125000\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -287678.125000\n",
      "    epoch          : 498\n",
      "    loss           : -334139.151875\n",
      "    val_loss       : -323997.3271875858\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -455949.187500\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -317606.437500\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -337953.812500\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -458681.312500\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -278524.750000\n",
      "    epoch          : 499\n",
      "    loss           : -334232.19484375\n",
      "    val_loss       : -324622.8308166981\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -278079.750000\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -316573.031250\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -455809.343750\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -279235.625000\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -374968.187500\n",
      "    epoch          : 500\n",
      "    loss           : -334181.93421875\n",
      "    val_loss       : -324865.7799750924\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1230_200319/checkpoint-epoch500.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VlaeCategoryModel(\n",
       "  (_category): FreeCategory(\n",
       "    (generator_0): LadderDecoder(\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): LadderDecoder(\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): LadderDecoder(\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): LadderDecoder(\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (7): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=196, out_features=196, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=392, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_5): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=98, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (7): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=196, out_features=196, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "  )\n",
       "  (_dagger_category): FreeCategory(\n",
       "    (generator_0): LadderEncoder(\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): LadderEncoder(\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): LadderEncoder(\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): LadderEncoder(\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): LadderEncoder(\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (noise_linear): Linear(in_features=2744, out_features=4, bias=True)\n",
       "      (ladder_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (ladder_linear): Linear(in_features=2744, out_features=196, bias=True)\n",
       "    )\n",
       "    (generator_5): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (7): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=196, out_features=196, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=32, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
