{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 138887.156250\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -45559.789062\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -36528.597656\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -52554.671875\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -43068.246094\n",
      "    epoch          : 1\n",
      "    loss           : -25273.207529296877\n",
      "    val_loss       : -59844.7046875\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -27159.632812\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -54413.480469\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -34927.320312\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -17072.132812\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -93111.656250\n",
      "    epoch          : 2\n",
      "    loss           : -61861.573071289065\n",
      "    val_loss       : -64059.73515625\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -76179.093750\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -41305.382812\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -40224.421875\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -57996.390625\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -60286.515625\n",
      "    epoch          : 3\n",
      "    loss           : -64214.36083984375\n",
      "    val_loss       : -65508.7984375\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -72120.976562\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -79234.984375\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -41711.851562\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -80378.843750\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -76889.757812\n",
      "    epoch          : 4\n",
      "    loss           : -65745.431015625\n",
      "    val_loss       : -67086.01484375\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -62476.722656\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -43045.929688\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -74802.257812\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -23312.892578\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -81577.304688\n",
      "    epoch          : 5\n",
      "    loss           : -67291.12044921875\n",
      "    val_loss       : -68665.5546875\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -45933.273438\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -73787.023438\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -65062.109375\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -26644.191406\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -83658.343750\n",
      "    epoch          : 6\n",
      "    loss           : -68560.2012890625\n",
      "    val_loss       : -69798.2484375\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -81685.335938\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -80883.734375\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -28153.044922\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -84755.132812\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -77692.953125\n",
      "    epoch          : 7\n",
      "    loss           : -69989.52255859374\n",
      "    val_loss       : -71093.9046875\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -47354.667969\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -87107.523438\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -66135.398438\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -66701.890625\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -87173.367188\n",
      "    epoch          : 8\n",
      "    loss           : -71348.53169921874\n",
      "    val_loss       : -72669.5234375\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -65039.960938\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -79619.187500\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -78017.593750\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -50543.140625\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -91055.953125\n",
      "    epoch          : 9\n",
      "    loss           : -72640.655390625\n",
      "    val_loss       : -73859.43984375\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -52629.820312\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -64530.960938\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -103562.750000\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -68195.757812\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -86185.750000\n",
      "    epoch          : 10\n",
      "    loss           : -73945.37640625\n",
      "    val_loss       : -75464.63515625\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -86023.484375\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -52775.632812\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -90836.546875\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -82916.203125\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -68838.828125\n",
      "    epoch          : 11\n",
      "    loss           : -75238.34841796875\n",
      "    val_loss       : -76386.728125\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -83366.437500\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -84313.882812\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -105038.820312\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -86700.210938\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -86613.046875\n",
      "    epoch          : 12\n",
      "    loss           : -76489.0913671875\n",
      "    val_loss       : -77644.4484375\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -104966.218750\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -72657.515625\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -85701.328125\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -37785.367188\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -89808.703125\n",
      "    epoch          : 13\n",
      "    loss           : -77753.3167578125\n",
      "    val_loss       : -78899.06171875\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -72331.312500\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -87354.593750\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -72949.703125\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -90271.226562\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -87113.632812\n",
      "    epoch          : 14\n",
      "    loss           : -79016.6442578125\n",
      "    val_loss       : -80434.0875\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -87703.593750\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -98122.937500\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -88141.929688\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -41534.191406\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -96569.585938\n",
      "    epoch          : 15\n",
      "    loss           : -80245.6825390625\n",
      "    val_loss       : -81580.1984375\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -72293.390625\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -109113.234375\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -108062.742188\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -99865.484375\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -43746.304688\n",
      "    epoch          : 16\n",
      "    loss           : -81484.2440625\n",
      "    val_loss       : -82807.27421875\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -109230.046875\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -88264.421875\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -91865.531250\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -98845.289062\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -75855.390625\n",
      "    epoch          : 17\n",
      "    loss           : -82753.6338671875\n",
      "    val_loss       : -84001.575\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -87761.437500\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -87121.156250\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -77669.039062\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -76618.664062\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -93115.203125\n",
      "    epoch          : 18\n",
      "    loss           : -83952.097890625\n",
      "    val_loss       : -85164.9328125\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -102058.539062\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -48376.515625\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -104010.070312\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -77127.078125\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -95010.859375\n",
      "    epoch          : 19\n",
      "    loss           : -85186.666015625\n",
      "    val_loss       : -86324.26328125\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -67850.625000\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -111901.906250\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -113495.203125\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -89057.835938\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -74990.156250\n",
      "    epoch          : 20\n",
      "    loss           : -86409.9071484375\n",
      "    val_loss       : -87779.990625\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -96339.687500\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -98188.773438\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -75681.742188\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -52915.355469\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -106212.968750\n",
      "    epoch          : 21\n",
      "    loss           : -87558.581953125\n",
      "    val_loss       : -88918.871875\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -77336.187500\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -96981.000000\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -93382.718750\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -80336.617188\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -97572.460938\n",
      "    epoch          : 22\n",
      "    loss           : -88749.13859375\n",
      "    val_loss       : -89694.09140625\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -71103.023438\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -101273.859375\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -99370.664062\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -109002.570312\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -97232.398438\n",
      "    epoch          : 23\n",
      "    loss           : -89990.6172265625\n",
      "    val_loss       : -91049.69296875\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -81989.062500\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -96454.640625\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -93927.671875\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -99527.125000\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -96175.875000\n",
      "    epoch          : 24\n",
      "    loss           : -91140.9151953125\n",
      "    val_loss       : -92199.69765625\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -117441.078125\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -74151.070312\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -57739.261719\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -74160.929688\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -84538.281250\n",
      "    epoch          : 25\n",
      "    loss           : -92324.1194140625\n",
      "    val_loss       : -93573.7359375\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -77435.718750\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -59536.062500\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -99833.531250\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -118894.781250\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -85128.804688\n",
      "    epoch          : 26\n",
      "    loss           : -93514.6783984375\n",
      "    val_loss       : -94705.609375\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -98636.046875\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -77986.187500\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -106368.617188\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -114507.382812\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -113952.156250\n",
      "    epoch          : 27\n",
      "    loss           : -94675.41046875\n",
      "    val_loss       : -95933.51796875\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -120198.132812\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -86798.406250\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -81743.992188\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -103550.156250\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -86822.687500\n",
      "    epoch          : 28\n",
      "    loss           : -95811.1904296875\n",
      "    val_loss       : -97046.44921875\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -102524.179688\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -80170.437500\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -89183.296875\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -103502.171875\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -88082.062500\n",
      "    epoch          : 29\n",
      "    loss           : -97007.7502734375\n",
      "    val_loss       : -98090.46640625\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -103018.984375\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -102586.187500\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -102196.648438\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -121548.226562\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -89400.046875\n",
      "    epoch          : 30\n",
      "    loss           : -98131.307734375\n",
      "    val_loss       : -99378.9359375\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -85020.375000\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -103353.515625\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -83819.492188\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -90446.984375\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -90870.804688\n",
      "    epoch          : 31\n",
      "    loss           : -99226.4596875\n",
      "    val_loss       : -100323.5578125\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -105572.414062\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -119966.031250\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -124706.546875\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -112827.250000\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -124010.476562\n",
      "    epoch          : 32\n",
      "    loss           : -100343.142734375\n",
      "    val_loss       : -101471.56796875\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -87201.398438\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -70233.828125\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -69843.359375\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -114470.257812\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -70902.375000\n",
      "    epoch          : 33\n",
      "    loss           : -101531.953515625\n",
      "    val_loss       : -102703.24375\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -116954.296875\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -115554.632812\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -106667.851562\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -108956.406250\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -107904.140625\n",
      "    epoch          : 34\n",
      "    loss           : -102631.6340625\n",
      "    val_loss       : -103736.25546875\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -106557.507812\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -94992.265625\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -94926.156250\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -106911.921875\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -93565.140625\n",
      "    epoch          : 35\n",
      "    loss           : -103710.39890625\n",
      "    val_loss       : -104916.853125\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -127362.078125\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -91940.875000\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -106991.164062\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -105126.609375\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -74565.375000\n",
      "    epoch          : 36\n",
      "    loss           : -104878.172734375\n",
      "    val_loss       : -105878.77109375\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -129484.671875\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -108486.703125\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -77835.960938\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -110365.875000\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -88324.039062\n",
      "    epoch          : 37\n",
      "    loss           : -105903.170546875\n",
      "    val_loss       : -106854.15\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -110712.656250\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -98233.554688\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -89756.773438\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -108539.007812\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -114074.234375\n",
      "    epoch          : 38\n",
      "    loss           : -107013.10140625\n",
      "    val_loss       : -108078.790625\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -131278.468750\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -95693.226562\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -111282.671875\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -121909.093750\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -111645.320312\n",
      "    epoch          : 39\n",
      "    loss           : -108033.5271875\n",
      "    val_loss       : -109243.7078125\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -112025.460938\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -111474.640625\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -112648.890625\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -81353.312500\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -113512.195312\n",
      "    epoch          : 40\n",
      "    loss           : -109099.56109375\n",
      "    val_loss       : -110146.5734375\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -92641.718750\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -125123.750000\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -99700.765625\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -99029.601562\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -113220.296875\n",
      "    epoch          : 41\n",
      "    loss           : -110174.892734375\n",
      "    val_loss       : -111250.93515625\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -113738.546875\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -131470.281250\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -98929.992188\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -114353.890625\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -101715.687500\n",
      "    epoch          : 42\n",
      "    loss           : -111215.83609375\n",
      "    val_loss       : -112066.6921875\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -114934.570312\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -132330.890625\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -101706.976562\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -115716.734375\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -114314.648438\n",
      "    epoch          : 43\n",
      "    loss           : -112228.3096875\n",
      "    val_loss       : -113387.6546875\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -129333.156250\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -114993.968750\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -129447.562500\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -134436.593750\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -103090.289062\n",
      "    epoch          : 44\n",
      "    loss           : -113309.393671875\n",
      "    val_loss       : -114414.28671875\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -116712.187500\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -130417.679688\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -94428.671875\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -136567.750000\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -136677.093750\n",
      "    epoch          : 45\n",
      "    loss           : -114359.255390625\n",
      "    val_loss       : -115535.85703125\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -105637.453125\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -117229.953125\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -96356.703125\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -96550.890625\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -118948.187500\n",
      "    epoch          : 46\n",
      "    loss           : -115342.39765625\n",
      "    val_loss       : -116161.28359375\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -96837.156250\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -138414.750000\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -107452.304688\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -137646.265625\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -104805.265625\n",
      "    epoch          : 47\n",
      "    loss           : -116352.5246875\n",
      "    val_loss       : -117605.11875\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -118738.781250\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -119655.437500\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -139646.203125\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -119564.960938\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -140650.156250\n",
      "    epoch          : 48\n",
      "    loss           : -117363.928046875\n",
      "    val_loss       : -118516.97890625\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -92941.703125\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -97720.601562\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -94097.218750\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -140336.937500\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -141482.703125\n",
      "    epoch          : 49\n",
      "    loss           : -118345.684296875\n",
      "    val_loss       : -119485.4640625\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -140146.187500\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -136598.171875\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -137152.093750\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -97706.421875\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -94659.539062\n",
      "    epoch          : 50\n",
      "    loss           : -119355.723515625\n",
      "    val_loss       : -120412.20390625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -140878.750000\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -111341.968750\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -97470.312500\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -120367.421875\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -112894.640625\n",
      "    epoch          : 51\n",
      "    loss           : -120297.51203125\n",
      "    val_loss       : -121467.65859375\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -99869.000000\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -144264.671875\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -108944.500000\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -99475.734375\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -139949.593750\n",
      "    epoch          : 52\n",
      "    loss           : -121259.03453125\n",
      "    val_loss       : -122179.484375\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -98693.343750\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -98028.156250\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -123431.679688\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -110847.953125\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -100000.500000\n",
      "    epoch          : 53\n",
      "    loss           : -122222.981171875\n",
      "    val_loss       : -123278.83671875\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -100146.984375\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -145297.156250\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -115760.726562\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -116455.968750\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -124421.093750\n",
      "    epoch          : 54\n",
      "    loss           : -123200.9425\n",
      "    val_loss       : -124152.81484375\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -123752.968750\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -143719.359375\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -112986.078125\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -112473.312500\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -113086.859375\n",
      "    epoch          : 55\n",
      "    loss           : -124134.02953125\n",
      "    val_loss       : -125221.86796875\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -149949.468750\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -103343.703125\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -143184.000000\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -143417.281250\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -125599.609375\n",
      "    epoch          : 56\n",
      "    loss           : -125064.519453125\n",
      "    val_loss       : -126071.28203125\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -125211.937500\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -144776.500000\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -113580.328125\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -102818.484375\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -114489.398438\n",
      "    epoch          : 57\n",
      "    loss           : -126043.679296875\n",
      "    val_loss       : -127038.4578125\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -126870.992188\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -128431.312500\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -145952.218750\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -127555.140625\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -115082.593750\n",
      "    epoch          : 58\n",
      "    loss           : -126903.132890625\n",
      "    val_loss       : -127769.296875\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -103752.656250\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -128542.000000\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -147251.312500\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -152166.562500\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -107400.914062\n",
      "    epoch          : 59\n",
      "    loss           : -127798.855234375\n",
      "    val_loss       : -128719.853125\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -123775.281250\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -128154.679688\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -127835.875000\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -129992.250000\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -108991.625000\n",
      "    epoch          : 60\n",
      "    loss           : -128759.1903125\n",
      "    val_loss       : -129713.22109375\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -148063.281250\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -124325.578125\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -104711.843750\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -149331.843750\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -117821.031250\n",
      "    epoch          : 61\n",
      "    loss           : -129648.149609375\n",
      "    val_loss       : -130610.8765625\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -126971.953125\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -129674.625000\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -127226.046875\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -148174.843750\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -147348.078125\n",
      "    epoch          : 62\n",
      "    loss           : -130558.446484375\n",
      "    val_loss       : -131530.60859375\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -147916.359375\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -155941.875000\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -132662.421875\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -153679.031250\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -128510.890625\n",
      "    epoch          : 63\n",
      "    loss           : -131389.02328125\n",
      "    val_loss       : -132260.15078125\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -155538.656250\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -153224.703125\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -134223.781250\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -108052.125000\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -151352.312500\n",
      "    epoch          : 64\n",
      "    loss           : -132297.9009375\n",
      "    val_loss       : -133215.88984375\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -153945.953125\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -131988.359375\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -135138.812500\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -132926.625000\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -158756.281250\n",
      "    epoch          : 65\n",
      "    loss           : -133196.788984375\n",
      "    val_loss       : -134126.90703125\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -130648.804688\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -155785.218750\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -132740.843750\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -133574.656250\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -131028.265625\n",
      "    epoch          : 66\n",
      "    loss           : -134063.758125\n",
      "    val_loss       : -134987.790625\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -156484.578125\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -134984.796875\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -151243.015625\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -131321.203125\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -134305.656250\n",
      "    epoch          : 67\n",
      "    loss           : -134875.009765625\n",
      "    val_loss       : -136020.8515625\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -133009.093750\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -133556.703125\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -162018.343750\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -151521.687500\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -162450.562500\n",
      "    epoch          : 68\n",
      "    loss           : -135751.7559375\n",
      "    val_loss       : -136822.503125\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -110443.625000\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -151962.796875\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -134252.843750\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -134565.593750\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -134608.890625\n",
      "    epoch          : 69\n",
      "    loss           : -136571.4296875\n",
      "    val_loss       : -137635.68046875\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -122590.296875\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -124455.539062\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -111247.914062\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -120249.078125\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -160886.234375\n",
      "    epoch          : 70\n",
      "    loss           : -137382.084765625\n",
      "    val_loss       : -138558.5203125\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -136825.421875\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -154825.484375\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -137079.187500\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -137508.750000\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -123189.046875\n",
      "    epoch          : 71\n",
      "    loss           : -138271.655\n",
      "    val_loss       : -139280.97578125\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -138114.781250\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -137399.500000\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -139094.843750\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -123160.179688\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -153739.875000\n",
      "    epoch          : 72\n",
      "    loss           : -139090.867578125\n",
      "    val_loss       : -140094.3046875\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -139667.468750\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -162180.796875\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -138519.125000\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -135583.687500\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -123484.992188\n",
      "    epoch          : 73\n",
      "    loss           : -139882.33328125\n",
      "    val_loss       : -140963.86640625\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -155330.546875\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -140009.906250\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -164920.562500\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -127201.789062\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -167596.171875\n",
      "    epoch          : 74\n",
      "    loss           : -140675.58890625\n",
      "    val_loss       : -141558.0453125\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -111922.562500\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -112134.835938\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -112769.703125\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -140975.734375\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -167882.531250\n",
      "    epoch          : 75\n",
      "    loss           : -141481.6709375\n",
      "    val_loss       : -142470.67890625\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -156633.468750\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -142687.687500\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -155677.312500\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -136195.296875\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -143132.296875\n",
      "    epoch          : 76\n",
      "    loss           : -142277.749609375\n",
      "    val_loss       : -143145.7046875\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -128728.921875\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -115239.289062\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -158180.812500\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -137714.187500\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -170179.593750\n",
      "    epoch          : 77\n",
      "    loss           : -143083.59640625\n",
      "    val_loss       : -143996.990625\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -114874.882812\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -129459.750000\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -170073.187500\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -130279.625000\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -129071.062500\n",
      "    epoch          : 78\n",
      "    loss           : -143865.445078125\n",
      "    val_loss       : -144821.0875\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -169631.937500\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -143657.906250\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -140711.953125\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -143952.046875\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -158580.750000\n",
      "    epoch          : 79\n",
      "    loss           : -144662.969296875\n",
      "    val_loss       : -145928.31484375\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -115085.773438\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -143703.000000\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -142994.343750\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -159613.968750\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -128924.156250\n",
      "    epoch          : 80\n",
      "    loss           : -145418.183125\n",
      "    val_loss       : -146362.95078125\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -160087.671875\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -144332.890625\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -144055.187500\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -116946.593750\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -131852.031250\n",
      "    epoch          : 81\n",
      "    loss           : -146202.99390625\n",
      "    val_loss       : -147052.53359375\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -139843.250000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -172295.765625\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -171984.718750\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -144969.375000\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -144234.234375\n",
      "    epoch          : 82\n",
      "    loss           : -146947.51640625\n",
      "    val_loss       : -147960.01484375\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -161925.937500\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -149618.968750\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -150143.468750\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -133414.437500\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -149751.500000\n",
      "    epoch          : 83\n",
      "    loss           : -147695.336171875\n",
      "    val_loss       : -148493.16875\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -144193.796875\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -117361.546875\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -160561.609375\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -140309.484375\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -176849.625000\n",
      "    epoch          : 84\n",
      "    loss           : -148425.6375\n",
      "    val_loss       : -149463.32734375\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -147715.125000\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -146679.500000\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -144863.468750\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -144714.468750\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -178632.187500\n",
      "    epoch          : 85\n",
      "    loss           : -149157.744375\n",
      "    val_loss       : -150028.453125\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -153181.875000\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -118493.703125\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -176227.734375\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -141334.093750\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -134161.718750\n",
      "    epoch          : 86\n",
      "    loss           : -149892.31609375\n",
      "    val_loss       : -150818.49375\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -178444.968750\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -153936.218750\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -148478.484375\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -135705.781250\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -135347.812500\n",
      "    epoch          : 87\n",
      "    loss           : -150677.29296875\n",
      "    val_loss       : -151620.740625\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -179006.031250\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -155778.343750\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -178685.343750\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -135084.781250\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -140961.765625\n",
      "    epoch          : 88\n",
      "    loss           : -151379.713984375\n",
      "    val_loss       : -152344.3234375\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -181962.625000\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -180857.000000\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -120816.492188\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -144575.843750\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -135739.750000\n",
      "    epoch          : 89\n",
      "    loss           : -152138.674453125\n",
      "    val_loss       : -153078.32890625\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -118824.640625\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -139061.203125\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -137535.500000\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -152411.718750\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -143624.250000\n",
      "    epoch          : 90\n",
      "    loss           : -152780.298359375\n",
      "    val_loss       : -153854.41328125\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -150284.906250\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -164170.015625\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -143679.000000\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -144233.375000\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -144031.125000\n",
      "    epoch          : 91\n",
      "    loss           : -153528.338046875\n",
      "    val_loss       : -154583.371875\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -183108.296875\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -145950.656250\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -143804.437500\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -145683.187500\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -165432.562500\n",
      "    epoch          : 92\n",
      "    loss           : -154224.9671875\n",
      "    val_loss       : -155372.80234375\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -167333.250000\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -145817.343750\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -183435.781250\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -150116.031250\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -143474.593750\n",
      "    epoch          : 93\n",
      "    loss           : -154921.9784375\n",
      "    val_loss       : -155883.490625\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -118793.968750\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -149641.312500\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -121990.492188\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -152039.000000\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -162527.625000\n",
      "    epoch          : 94\n",
      "    loss           : -155583.75359375\n",
      "    val_loss       : -156508.2171875\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -151960.843750\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -146318.859375\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -185061.687500\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -123254.093750\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -166434.312500\n",
      "    epoch          : 95\n",
      "    loss           : -156278.518671875\n",
      "    val_loss       : -157139.13359375\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -163617.656250\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -154300.000000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -142321.687500\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -187275.562500\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -145151.328125\n",
      "    epoch          : 96\n",
      "    loss           : -156943.160625\n",
      "    val_loss       : -157894.409375\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -145746.296875\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -188928.640625\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -165011.671875\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -168050.531250\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -186912.578125\n",
      "    epoch          : 97\n",
      "    loss           : -157611.47703125\n",
      "    val_loss       : -158492.67734375\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -153859.265625\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -188750.703125\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -124999.843750\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -190342.156250\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -123156.585938\n",
      "    epoch          : 98\n",
      "    loss           : -158306.59296875\n",
      "    val_loss       : -159136.6484375\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -154267.265625\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -153627.687500\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -168993.406250\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -125139.585938\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -152666.781250\n",
      "    epoch          : 99\n",
      "    loss           : -158952.223125\n",
      "    val_loss       : -159864.2203125\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -146687.937500\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -145257.031250\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -144482.343750\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -153553.062500\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -146611.687500\n",
      "    epoch          : 100\n",
      "    loss           : -159645.6090625\n",
      "    val_loss       : -160608.7875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -190521.359375\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -167678.531250\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -147779.968750\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -155405.640625\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -168695.906250\n",
      "    epoch          : 101\n",
      "    loss           : -160299.226328125\n",
      "    val_loss       : -160965.00078125\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -191997.078125\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -169183.781250\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -146559.000000\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -145788.875000\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -149382.171875\n",
      "    epoch          : 102\n",
      "    loss           : -160941.036328125\n",
      "    val_loss       : -161879.30859375\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -123781.343750\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -193461.218750\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -156177.531250\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -155931.031250\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -155830.375000\n",
      "    epoch          : 103\n",
      "    loss           : -161530.02921875\n",
      "    val_loss       : -162761.78046875\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -144834.812500\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -194679.578125\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -146174.187500\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -170849.562500\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -158055.015625\n",
      "    epoch          : 104\n",
      "    loss           : -162222.776640625\n",
      "    val_loss       : -163214.50546875\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -171978.750000\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -172863.609375\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -157262.562500\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -157780.781250\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -147154.328125\n",
      "    epoch          : 105\n",
      "    loss           : -162889.54703125\n",
      "    val_loss       : -163661.05234375\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -172148.656250\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -195235.078125\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -161049.281250\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -124177.507812\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -145327.703125\n",
      "    epoch          : 106\n",
      "    loss           : -163530.038828125\n",
      "    val_loss       : -164435.9140625\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -196227.343750\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -196685.375000\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -195879.156250\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -149147.218750\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -149608.750000\n",
      "    epoch          : 107\n",
      "    loss           : -164081.69171875\n",
      "    val_loss       : -164834.69375\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -197116.593750\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -157405.156250\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -130079.187500\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -128349.015625\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -171423.609375\n",
      "    epoch          : 108\n",
      "    loss           : -164707.017109375\n",
      "    val_loss       : -165735.8625\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -196503.750000\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -161444.843750\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -162416.468750\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -147728.187500\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -198488.906250\n",
      "    epoch          : 109\n",
      "    loss           : -165290.748359375\n",
      "    val_loss       : -166225.39140625\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -162627.328125\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -199793.000000\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -161436.093750\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -198482.156250\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -149671.687500\n",
      "    epoch          : 110\n",
      "    loss           : -165934.200234375\n",
      "    val_loss       : -167074.82421875\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -160008.937500\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -160880.500000\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -177895.593750\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -199792.812500\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -150742.687500\n",
      "    epoch          : 111\n",
      "    loss           : -166521.20515625\n",
      "    val_loss       : -167715.15859375\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -151054.734375\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -162832.296875\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -199963.656250\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -161355.125000\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -199815.687500\n",
      "    epoch          : 112\n",
      "    loss           : -167129.057734375\n",
      "    val_loss       : -168228.67421875\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -175341.968750\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -173907.968750\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -161986.750000\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -202144.125000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -131296.609375\n",
      "    epoch          : 113\n",
      "    loss           : -167777.11828125\n",
      "    val_loss       : -168403.1140625\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -201347.906250\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -202836.296875\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -167531.031250\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -175038.750000\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -130130.296875\n",
      "    epoch          : 114\n",
      "    loss           : -168389.18359375\n",
      "    val_loss       : -169286.095703125\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -174332.671875\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -155342.843750\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -163994.625000\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -176255.468750\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -150835.031250\n",
      "    epoch          : 115\n",
      "    loss           : -168963.513203125\n",
      "    val_loss       : -169781.62890625\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -150532.281250\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -161286.437500\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -154157.968750\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -168209.593750\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -152563.765625\n",
      "    epoch          : 116\n",
      "    loss           : -169487.8184375\n",
      "    val_loss       : -170468.792578125\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -182129.781250\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -181812.812500\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -182557.093750\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -167855.968750\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -203731.546875\n",
      "    epoch          : 117\n",
      "    loss           : -170088.551875\n",
      "    val_loss       : -171088.16015625\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -174772.437500\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -205140.093750\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -152740.453125\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -153516.406250\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -204220.125000\n",
      "    epoch          : 118\n",
      "    loss           : -170689.709375\n",
      "    val_loss       : -171474.280859375\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -164545.312500\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -154904.296875\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -153129.890625\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -164880.781250\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -153854.375000\n",
      "    epoch          : 119\n",
      "    loss           : -171195.133203125\n",
      "    val_loss       : -172098.15546875\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -178228.781250\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -129147.398438\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -177662.531250\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -171658.187500\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -156268.875000\n",
      "    epoch          : 120\n",
      "    loss           : -171831.99796875\n",
      "    val_loss       : -172435.48828125\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -166657.859375\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -165727.687500\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -157017.031250\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -164716.546875\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -206952.671875\n",
      "    epoch          : 121\n",
      "    loss           : -172355.0203125\n",
      "    val_loss       : -173458.908203125\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -171597.562500\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -176681.390625\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -171969.828125\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -155405.468750\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -186724.921875\n",
      "    epoch          : 122\n",
      "    loss           : -172906.22875\n",
      "    val_loss       : -173887.14765625\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -132359.125000\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -155979.687500\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -155828.093750\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -166387.562500\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -155309.921875\n",
      "    epoch          : 123\n",
      "    loss           : -173514.991640625\n",
      "    val_loss       : -174360.204296875\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -134254.062500\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -157847.734375\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -157474.156250\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -133583.187500\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -154720.562500\n",
      "    epoch          : 124\n",
      "    loss           : -174052.2221875\n",
      "    val_loss       : -175018.7875\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -177956.437500\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -208467.687500\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -166462.859375\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -165489.343750\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -166516.546875\n",
      "    epoch          : 125\n",
      "    loss           : -174567.85515625\n",
      "    val_loss       : -175403.428515625\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -209841.812500\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -131360.125000\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -210363.890625\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -189422.406250\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -179706.781250\n",
      "    epoch          : 126\n",
      "    loss           : -175125.21234375\n",
      "    val_loss       : -176134.938671875\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -210698.906250\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -133529.562500\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -179245.500000\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -209606.093750\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -177903.390625\n",
      "    epoch          : 127\n",
      "    loss           : -175662.15046875\n",
      "    val_loss       : -176311.20859375\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -169963.843750\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -158602.500000\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -158298.921875\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -210999.109375\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -156181.093750\n",
      "    epoch          : 128\n",
      "    loss           : -176233.6671875\n",
      "    val_loss       : -177054.454296875\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -159057.343750\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -133015.078125\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -168704.000000\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -168775.796875\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -177683.906250\n",
      "    epoch          : 129\n",
      "    loss           : -176740.804453125\n",
      "    val_loss       : -177689.8234375\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -169889.140625\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -214559.593750\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -212718.140625\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -213459.031250\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -159207.375000\n",
      "    epoch          : 130\n",
      "    loss           : -177199.10234375\n",
      "    val_loss       : -178073.078515625\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -194432.578125\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -194723.718750\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -214247.390625\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -215103.296875\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -134765.187500\n",
      "    epoch          : 131\n",
      "    loss           : -177786.869140625\n",
      "    val_loss       : -178669.46640625\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -194245.718750\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -182440.062500\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -137027.265625\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -215483.406250\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -159553.437500\n",
      "    epoch          : 132\n",
      "    loss           : -178279.325625\n",
      "    val_loss       : -179154.570703125\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -195669.140625\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -157774.218750\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -182440.812500\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -181451.406250\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -156301.468750\n",
      "    epoch          : 133\n",
      "    loss           : -178798.64578125\n",
      "    val_loss       : -179694.27578125\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -196666.453125\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -184707.703125\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -163272.125000\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -170376.656250\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -214487.109375\n",
      "    epoch          : 134\n",
      "    loss           : -179333.65046875\n",
      "    val_loss       : -180531.54921875\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -218356.453125\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -182274.875000\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -216111.515625\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -214917.906250\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -160692.125000\n",
      "    epoch          : 135\n",
      "    loss           : -179860.10796875\n",
      "    val_loss       : -180611.35078125\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -161138.812500\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -134705.968750\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -162838.843750\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -185730.640625\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -173035.250000\n",
      "    epoch          : 136\n",
      "    loss           : -180367.11828125\n",
      "    val_loss       : -180951.88828125\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -138659.656250\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -159784.375000\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -217380.187500\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -217799.437500\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -172545.078125\n",
      "    epoch          : 137\n",
      "    loss           : -180884.3221875\n",
      "    val_loss       : -181858.830078125\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -135910.406250\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -184344.687500\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -158542.328125\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -185128.437500\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -161768.687500\n",
      "    epoch          : 138\n",
      "    loss           : -181372.7540625\n",
      "    val_loss       : -182475.1859375\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -163336.859375\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -171683.078125\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -222400.375000\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -219060.375000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -159774.953125\n",
      "    epoch          : 139\n",
      "    loss           : -181839.96234375\n",
      "    val_loss       : -182703.430078125\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -162365.218750\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -221806.812500\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -183576.250000\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -171468.546875\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -187212.281250\n",
      "    epoch          : 140\n",
      "    loss           : -182323.84890625\n",
      "    val_loss       : -183196.811328125\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -173227.937500\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -173655.312500\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -184253.109375\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -202540.390625\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -159899.875000\n",
      "    epoch          : 141\n",
      "    loss           : -182787.5\n",
      "    val_loss       : -183840.511328125\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -173814.218750\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -202116.031250\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -157417.437500\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -174831.031250\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -161375.468750\n",
      "    epoch          : 142\n",
      "    loss           : -183297.95734375\n",
      "    val_loss       : -184273.005859375\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -221161.328125\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -183648.937500\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -138519.906250\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -220814.562500\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -162281.562500\n",
      "    epoch          : 143\n",
      "    loss           : -183780.8003125\n",
      "    val_loss       : -184693.341796875\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -138313.468750\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -162108.937500\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -174067.500000\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -174881.812500\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -222204.828125\n",
      "    epoch          : 144\n",
      "    loss           : -184234.53015625\n",
      "    val_loss       : -185104.170703125\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -173702.312500\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -165334.875000\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -225186.984375\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -189619.281250\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -223279.281250\n",
      "    epoch          : 145\n",
      "    loss           : -184729.1509375\n",
      "    val_loss       : -185852.46875\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -174103.625000\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -186600.156250\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -226692.531250\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -173028.437500\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -192483.312500\n",
      "    epoch          : 146\n",
      "    loss           : -185220.36765625\n",
      "    val_loss       : -186000.785546875\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -163334.109375\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -206704.500000\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -194278.093750\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -166074.093750\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -167109.406250\n",
      "    epoch          : 147\n",
      "    loss           : -185664.73609375\n",
      "    val_loss       : -186298.34921875\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -225965.515625\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -167341.500000\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -137052.281250\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -207399.718750\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -160377.593750\n",
      "    epoch          : 148\n",
      "    loss           : -186169.99875\n",
      "    val_loss       : -187085.61171875\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -208097.000000\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -177358.250000\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -168665.437500\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -162585.203125\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -192778.609375\n",
      "    epoch          : 149\n",
      "    loss           : -186598.06328125\n",
      "    val_loss       : -187616.0671875\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -207343.375000\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -176817.953125\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -164725.593750\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -162614.640625\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -168058.312500\n",
      "    epoch          : 150\n",
      "    loss           : -187076.0603125\n",
      "    val_loss       : -187907.0015625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch150.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -209728.125000\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -225055.484375\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -230318.875000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -178019.500000\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -225886.437500\n",
      "    epoch          : 151\n",
      "    loss           : -187505.36546875\n",
      "    val_loss       : -188296.737890625\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -175386.156250\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -196337.796875\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -161812.046875\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -168934.109375\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -191874.281250\n",
      "    epoch          : 152\n",
      "    loss           : -187948.82140625\n",
      "    val_loss       : -188920.307421875\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -194132.500000\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -178542.078125\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -230265.984375\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -226675.500000\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -169052.625000\n",
      "    epoch          : 153\n",
      "    loss           : -188419.02375\n",
      "    val_loss       : -189485.069140625\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -164387.515625\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -169961.468750\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -179922.968750\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -177162.546875\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -198090.406250\n",
      "    epoch          : 154\n",
      "    loss           : -188810.5178125\n",
      "    val_loss       : -190065.05703125\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -227123.171875\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -164489.187500\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -165929.265625\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -163406.421875\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -165146.593750\n",
      "    epoch          : 155\n",
      "    loss           : -189205.4290625\n",
      "    val_loss       : -190275.257421875\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -190137.031250\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -188795.093750\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -231347.703125\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -178469.156250\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -169720.218750\n",
      "    epoch          : 156\n",
      "    loss           : -189732.8315625\n",
      "    val_loss       : -190354.869140625\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -213773.375000\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -178615.515625\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -178586.890625\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -180361.468750\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -166579.546875\n",
      "    epoch          : 157\n",
      "    loss           : -190216.64734375\n",
      "    val_loss       : -190971.475\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -230734.250000\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -213689.093750\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -234601.078125\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -171007.656250\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -179294.593750\n",
      "    epoch          : 158\n",
      "    loss           : -190564.17953125\n",
      "    val_loss       : -191556.09453125\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -214076.625000\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -143502.171875\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -234614.781250\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -181833.093750\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -234577.062500\n",
      "    epoch          : 159\n",
      "    loss           : -191077.97734375\n",
      "    val_loss       : -192332.310546875\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -202469.328125\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -141998.781250\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -182205.156250\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -183204.140625\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -231966.218750\n",
      "    epoch          : 160\n",
      "    loss           : -191427.9171875\n",
      "    val_loss       : -192302.4453125\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -179788.343750\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -172184.265625\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -139909.937500\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -231538.750000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -166447.921875\n",
      "    epoch          : 161\n",
      "    loss           : -191867.53203125\n",
      "    val_loss       : -192845.616796875\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -141884.687500\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -176809.312500\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -171663.453125\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -166446.515625\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -173005.937500\n",
      "    epoch          : 162\n",
      "    loss           : -192284.2021875\n",
      "    val_loss       : -193310.47890625\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -183382.906250\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -163940.625000\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -183323.187500\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -216905.375000\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -168401.375000\n",
      "    epoch          : 163\n",
      "    loss           : -192684.169375\n",
      "    val_loss       : -193708.400390625\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -181908.359375\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -181143.062500\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -238713.359375\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -217973.265625\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -233877.781250\n",
      "    epoch          : 164\n",
      "    loss           : -193095.74859375\n",
      "    val_loss       : -193890.0953125\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -193315.250000\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -142583.187500\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -185186.250000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -188513.921875\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -204495.734375\n",
      "    epoch          : 165\n",
      "    loss           : -193502.33421875\n",
      "    val_loss       : -194391.455859375\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -184147.031250\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -219085.250000\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -233847.375000\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -168360.937500\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -143633.765625\n",
      "    epoch          : 166\n",
      "    loss           : -193876.70515625\n",
      "    val_loss       : -194978.14609375\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -189051.437500\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -232500.875000\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -237877.500000\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -235609.843750\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -235374.937500\n",
      "    epoch          : 167\n",
      "    loss           : -194325.47984375\n",
      "    val_loss       : -195275.458203125\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -235548.125000\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -142868.203125\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -220807.281250\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -142341.187500\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -164961.968750\n",
      "    epoch          : 168\n",
      "    loss           : -194692.58984375\n",
      "    val_loss       : -195689.78671875\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -181828.250000\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -184092.984375\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -180804.468750\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -235221.062500\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -167173.125000\n",
      "    epoch          : 169\n",
      "    loss           : -195162.79890625\n",
      "    val_loss       : -196034.598828125\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -166121.812500\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -143250.718750\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -164126.718750\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -182385.203125\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -170467.171875\n",
      "    epoch          : 170\n",
      "    loss           : -195597.9453125\n",
      "    val_loss       : -196565.703125\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -208039.015625\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -184641.359375\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -146707.906250\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -145386.375000\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -184834.109375\n",
      "    epoch          : 171\n",
      "    loss           : -195954.91703125\n",
      "    val_loss       : -197123.185546875\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -149992.375000\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -191974.906250\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -176879.484375\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -242770.546875\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -209493.750000\n",
      "    epoch          : 172\n",
      "    loss           : -196271.8771875\n",
      "    val_loss       : -197148.569140625\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -208469.750000\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -208476.781250\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -186418.375000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -182992.546875\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -209243.812500\n",
      "    epoch          : 173\n",
      "    loss           : -196708.61921875\n",
      "    val_loss       : -197613.220703125\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -194833.031250\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -210075.625000\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -194649.265625\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -183665.968750\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -209288.765625\n",
      "    epoch          : 174\n",
      "    loss           : -197162.3196875\n",
      "    val_loss       : -197881.356640625\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -243745.828125\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -243804.640625\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -223298.953125\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -187540.156250\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -174538.281250\n",
      "    epoch          : 175\n",
      "    loss           : -197572.57078125\n",
      "    val_loss       : -198150.37890625\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -185509.203125\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -244251.671875\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -194958.687500\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -239845.890625\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -171760.796875\n",
      "    epoch          : 176\n",
      "    loss           : -197909.8546875\n",
      "    val_loss       : -198852.91640625\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -188714.265625\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -209207.968750\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -224801.093750\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -174808.140625\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -240140.140625\n",
      "    epoch          : 177\n",
      "    loss           : -198311.35859375\n",
      "    val_loss       : -199117.81171875\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -185754.171875\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -178991.062500\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -226013.625000\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -176364.625000\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -170391.796875\n",
      "    epoch          : 178\n",
      "    loss           : -198646.301875\n",
      "    val_loss       : -199517.719140625\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -213977.937500\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -146120.468750\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -186959.015625\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -187930.187500\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -240855.000000\n",
      "    epoch          : 179\n",
      "    loss           : -199126.62921875\n",
      "    val_loss       : -199838.065234375\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -148389.640625\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -187073.234375\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -196036.750000\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -240785.500000\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -242592.765625\n",
      "    epoch          : 180\n",
      "    loss           : -199447.7609375\n",
      "    val_loss       : -200518.26328125\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -146855.500000\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -185963.656250\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -194690.000000\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -247757.156250\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -176975.531250\n",
      "    epoch          : 181\n",
      "    loss           : -199789.47125\n",
      "    val_loss       : -200545.078515625\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -173024.375000\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -228225.078125\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -181877.437500\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -179057.046875\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -243332.531250\n",
      "    epoch          : 182\n",
      "    loss           : -200195.833125\n",
      "    val_loss       : -200826.51171875\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -197964.484375\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -183072.843750\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -215197.171875\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -247479.359375\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -171127.468750\n",
      "    epoch          : 183\n",
      "    loss           : -200563.856875\n",
      "    val_loss       : -201431.193359375\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -248372.968750\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -186652.031250\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -230914.812500\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -197073.703125\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -192076.421875\n",
      "    epoch          : 184\n",
      "    loss           : -200953.54546875\n",
      "    val_loss       : -201397.32890625\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -181917.468750\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -243642.437500\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -230672.609375\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -199140.343750\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -179646.343750\n",
      "    epoch          : 185\n",
      "    loss           : -201266.52640625\n",
      "    val_loss       : -202367.769921875\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -193781.625000\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -231157.875000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -231216.609375\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -246422.156250\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -171750.031250\n",
      "    epoch          : 186\n",
      "    loss           : -201684.665\n",
      "    val_loss       : -202554.813671875\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -245604.328125\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -219164.562500\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -171703.140625\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -144433.156250\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -215024.406250\n",
      "    epoch          : 187\n",
      "    loss           : -201957.92703125\n",
      "    val_loss       : -203214.840625\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -231054.328125\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -252029.093750\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -186854.687500\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -198642.109375\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -215674.453125\n",
      "    epoch          : 188\n",
      "    loss           : -202350.51265625\n",
      "    val_loss       : -203094.547265625\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -187274.796875\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -246620.156250\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -216624.093750\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -196260.859375\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -184760.875000\n",
      "    epoch          : 189\n",
      "    loss           : -202676.4559375\n",
      "    val_loss       : -203704.688671875\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -189581.421875\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -187205.500000\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -144202.593750\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -192326.265625\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -184247.375000\n",
      "    epoch          : 190\n",
      "    loss           : -203022.99046875\n",
      "    val_loss       : -204159.394921875\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -199732.359375\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -181552.781250\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -146672.906250\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -219844.140625\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -175424.828125\n",
      "    epoch          : 191\n",
      "    loss           : -203380.04671875\n",
      "    val_loss       : -204037.05859375\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -248239.765625\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -245035.593750\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -247258.250000\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -247055.203125\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -182039.500000\n",
      "    epoch          : 192\n",
      "    loss           : -203663.84109375\n",
      "    val_loss       : -204482.55390625\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -233489.437500\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -235099.218750\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -191232.125000\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -190177.656250\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -224499.156250\n",
      "    epoch          : 193\n",
      "    loss           : -204134.2403125\n",
      "    val_loss       : -204781.803125\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -186226.125000\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -255166.125000\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -173809.093750\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -235241.875000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -184733.218750\n",
      "    epoch          : 194\n",
      "    loss           : -204402.86109375\n",
      "    val_loss       : -205140.200390625\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -173666.359375\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -173034.328125\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -189198.031250\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -195525.468750\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -221078.156250\n",
      "    epoch          : 195\n",
      "    loss           : -204773.08484375\n",
      "    val_loss       : -205496.211328125\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -198500.875000\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -223077.421875\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -184954.343750\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -219885.234375\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -189656.343750\n",
      "    epoch          : 196\n",
      "    loss           : -205106.3346875\n",
      "    val_loss       : -206013.00546875\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -236573.046875\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -190576.859375\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -200451.453125\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -250246.218750\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -250104.171875\n",
      "    epoch          : 197\n",
      "    loss           : -205419.091875\n",
      "    val_loss       : -206503.61875\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -255839.531250\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -200455.937500\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -256910.203125\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -222634.062500\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -256613.906250\n",
      "    epoch          : 198\n",
      "    loss           : -205738.78265625\n",
      "    val_loss       : -206335.321875\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -152418.906250\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -151313.343750\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -185394.343750\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -252052.468750\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -224920.171875\n",
      "    epoch          : 199\n",
      "    loss           : -206088.71109375\n",
      "    val_loss       : -206650.015234375\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -192426.609375\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -194383.531250\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -150764.671875\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -250945.156250\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -174657.671875\n",
      "    epoch          : 200\n",
      "    loss           : -206422.21609375\n",
      "    val_loss       : -207366.717578125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch200.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -173126.281250\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -249890.640625\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -223628.468750\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -250781.718750\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -200373.296875\n",
      "    epoch          : 201\n",
      "    loss           : -206749.15453125\n",
      "    val_loss       : -207369.868359375\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -175570.625000\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -195276.843750\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -194416.187500\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -175233.234375\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -223317.359375\n",
      "    epoch          : 202\n",
      "    loss           : -207148.59\n",
      "    val_loss       : -207725.363671875\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -240261.156250\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -252536.359375\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -153903.359375\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -239556.734375\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -175050.921875\n",
      "    epoch          : 203\n",
      "    loss           : -207329.0290625\n",
      "    val_loss       : -207878.4171875\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -184694.421875\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -227044.250000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -227038.125000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -200386.500000\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -200088.031250\n",
      "    epoch          : 204\n",
      "    loss           : -207748.5159375\n",
      "    val_loss       : -208668.502734375\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -191915.187500\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -148800.078125\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -201248.828125\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -253560.750000\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -227093.406250\n",
      "    epoch          : 205\n",
      "    loss           : -208075.8909375\n",
      "    val_loss       : -209144.55234375\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -198876.281250\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -254656.796875\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -175934.281250\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -150047.906250\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -174299.390625\n",
      "    epoch          : 206\n",
      "    loss           : -208385.67640625\n",
      "    val_loss       : -209037.712109375\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -241617.843750\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -201379.125000\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -172707.515625\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -261825.687500\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -174762.046875\n",
      "    epoch          : 207\n",
      "    loss           : -208657.6215625\n",
      "    val_loss       : -209203.38515625\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -202084.562500\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -260992.875000\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -225051.734375\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -202483.031250\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -198058.109375\n",
      "    epoch          : 208\n",
      "    loss           : -208984.18984375\n",
      "    val_loss       : -210139.023046875\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -149919.312500\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -195676.125000\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -187420.171875\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -186065.937500\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -260111.750000\n",
      "    epoch          : 209\n",
      "    loss           : -209292.428125\n",
      "    val_loss       : -209960.365234375\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -176519.656250\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -243435.718750\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -194532.750000\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -151682.187500\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -174270.593750\n",
      "    epoch          : 210\n",
      "    loss           : -209650.12203125\n",
      "    val_loss       : -210435.5859375\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -177562.765625\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -175704.437500\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -262317.500000\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -243940.984375\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -180781.234375\n",
      "    epoch          : 211\n",
      "    loss           : -209917.56484375\n",
      "    val_loss       : -211133.205078125\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -190206.562500\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -175079.546875\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -230216.687500\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -193613.281250\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -229505.187500\n",
      "    epoch          : 212\n",
      "    loss           : -210198.51171875\n",
      "    val_loss       : -211204.691796875\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -230907.500000\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -154079.531250\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -244764.515625\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -197622.031250\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -175792.984375\n",
      "    epoch          : 213\n",
      "    loss           : -210628.4721875\n",
      "    val_loss       : -212084.92421875\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -245405.640625\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -152363.078125\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -191270.984375\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -262892.312500\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -229907.531250\n",
      "    epoch          : 214\n",
      "    loss           : -210910.0609375\n",
      "    val_loss       : -211786.337890625\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -231979.531250\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -174974.265625\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -199326.343750\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -231095.250000\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -178522.671875\n",
      "    epoch          : 215\n",
      "    loss           : -211120.8290625\n",
      "    val_loss       : -211938.21484375\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -189534.359375\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -246824.921875\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -263583.812500\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -256695.734375\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -257161.343750\n",
      "    epoch          : 216\n",
      "    loss           : -211463.35609375\n",
      "    val_loss       : -212328.401171875\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -246973.859375\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -150645.796875\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -230939.156250\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -197273.156250\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -196472.234375\n",
      "    epoch          : 217\n",
      "    loss           : -211669.6103125\n",
      "    val_loss       : -212866.59296875\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -265422.093750\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -203483.046875\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -206012.875000\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -204432.921875\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -191370.281250\n",
      "    epoch          : 218\n",
      "    loss           : -211992.26921875\n",
      "    val_loss       : -213059.536328125\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -196684.421875\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -181706.921875\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -257807.500000\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -198431.000000\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -171928.718750\n",
      "    epoch          : 219\n",
      "    loss           : -212302.0778125\n",
      "    val_loss       : -213096.758984375\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -236450.437500\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -148835.937500\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -247737.531250\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -259110.328125\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -203647.625000\n",
      "    epoch          : 220\n",
      "    loss           : -212689.55015625\n",
      "    val_loss       : -213276.21640625\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -176183.125000\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -178452.031250\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -248991.187500\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -156927.843750\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -205022.500000\n",
      "    epoch          : 221\n",
      "    loss           : -212931.859375\n",
      "    val_loss       : -213733.426953125\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -248552.468750\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -266316.843750\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -203684.203125\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -188856.093750\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -178961.328125\n",
      "    epoch          : 222\n",
      "    loss           : -213253.81125\n",
      "    val_loss       : -213799.68359375\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -260940.875000\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -158112.578125\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -192804.984375\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -204625.140625\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -207146.953125\n",
      "    epoch          : 223\n",
      "    loss           : -213557.4409375\n",
      "    val_loss       : -214700.475\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -196299.375000\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -201512.375000\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -153054.656250\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -188918.562500\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -235745.031250\n",
      "    epoch          : 224\n",
      "    loss           : -213776.89765625\n",
      "    val_loss       : -214640.34921875\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -159537.562500\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -202651.515625\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -197954.984375\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -259939.156250\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -263488.375000\n",
      "    epoch          : 225\n",
      "    loss           : -214104.965625\n",
      "    val_loss       : -214824.26328125\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -200225.328125\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -193544.140625\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -205578.109375\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -270007.062500\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -237080.250000\n",
      "    epoch          : 226\n",
      "    loss           : -214375.73109375\n",
      "    val_loss       : -215258.5609375\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -183134.890625\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -235011.531250\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -262632.500000\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -197861.906250\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -190393.718750\n",
      "    epoch          : 227\n",
      "    loss           : -214641.488125\n",
      "    val_loss       : -215777.481640625\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -261926.640625\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -236458.109375\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -251858.765625\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -200693.828125\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -261924.406250\n",
      "    epoch          : 228\n",
      "    loss           : -214910.37390625\n",
      "    val_loss       : -215664.516796875\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -261181.156250\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -263895.125000\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -205026.890625\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -262435.093750\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -176524.750000\n",
      "    epoch          : 229\n",
      "    loss           : -215202.81375\n",
      "    val_loss       : -215977.821875\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -263469.000000\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -176154.687500\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -251448.187500\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -206345.875000\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -265188.468750\n",
      "    epoch          : 230\n",
      "    loss           : -215436.4946875\n",
      "    val_loss       : -216194.671484375\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -270459.187500\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -198762.046875\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -193992.281250\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -271118.437500\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -199722.250000\n",
      "    epoch          : 231\n",
      "    loss           : -215801.046875\n",
      "    val_loss       : -216609.78125\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -192788.984375\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -194633.906250\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -253728.062500\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -194023.031250\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -235603.500000\n",
      "    epoch          : 232\n",
      "    loss           : -215980.8203125\n",
      "    val_loss       : -216853.05546875\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -264037.750000\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -265163.000000\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -200555.765625\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -197978.296875\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -195852.781250\n",
      "    epoch          : 233\n",
      "    loss           : -216355.9684375\n",
      "    val_loss       : -216903.168359375\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -253987.125000\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -195159.281250\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -193673.359375\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -271167.687500\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -264887.000000\n",
      "    epoch          : 234\n",
      "    loss           : -216553.09140625\n",
      "    val_loss       : -217349.47421875\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -206352.500000\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -241029.468750\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -193929.062500\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -201781.187500\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -195614.656250\n",
      "    epoch          : 235\n",
      "    loss           : -216877.1284375\n",
      "    val_loss       : -217547.48046875\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -266022.406250\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -265796.562500\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -203994.609375\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -195462.281250\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -239226.062500\n",
      "    epoch          : 236\n",
      "    loss           : -217127.66484375\n",
      "    val_loss       : -217894.3203125\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -205536.437500\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -255055.218750\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -272841.281250\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -153837.812500\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -179292.156250\n",
      "    epoch          : 237\n",
      "    loss           : -217354.74765625\n",
      "    val_loss       : -217993.837890625\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -181738.890625\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -275417.812500\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -201019.640625\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -266622.750000\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -175458.812500\n",
      "    epoch          : 238\n",
      "    loss           : -217619.69234375\n",
      "    val_loss       : -218602.96640625\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -266437.812500\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -255030.406250\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -267538.843750\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -193929.046875\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -242085.906250\n",
      "    epoch          : 239\n",
      "    loss           : -217866.79125\n",
      "    val_loss       : -218410.534375\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -265841.656250\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -266802.031250\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -256533.625000\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -267067.687500\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -195732.250000\n",
      "    epoch          : 240\n",
      "    loss           : -218235.63125\n",
      "    val_loss       : -218734.691796875\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -195702.312500\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -147689.359375\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -180406.812500\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -196650.703125\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -274639.031250\n",
      "    epoch          : 241\n",
      "    loss           : -218464.46640625\n",
      "    val_loss       : -219183.732421875\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -241585.562500\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -180173.437500\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -152698.656250\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -240991.906250\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -265869.218750\n",
      "    epoch          : 242\n",
      "    loss           : -218643.90875\n",
      "    val_loss       : -219428.77109375\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -202115.296875\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -208870.546875\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -181236.593750\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -153818.125000\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -267297.343750\n",
      "    epoch          : 243\n",
      "    loss           : -218916.5603125\n",
      "    val_loss       : -219739.4640625\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -201486.453125\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -277910.156250\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -177924.453125\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -200069.406250\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -162984.093750\n",
      "    epoch          : 244\n",
      "    loss           : -219217.95046875\n",
      "    val_loss       : -220081.245703125\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -275399.968750\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -155021.437500\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -205423.031250\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -181732.750000\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -176601.890625\n",
      "    epoch          : 245\n",
      "    loss           : -219490.03953125\n",
      "    val_loss       : -220193.26484375\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -242714.640625\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -178142.953125\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -183397.140625\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -176918.531250\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -185776.531250\n",
      "    epoch          : 246\n",
      "    loss           : -219810.32171875\n",
      "    val_loss       : -220454.927734375\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -156365.890625\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -210782.859375\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -195504.906250\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -181386.656250\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -194015.843750\n",
      "    epoch          : 247\n",
      "    loss           : -220025.64796875\n",
      "    val_loss       : -220711.20390625\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -196162.531250\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -207463.843750\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -279174.375000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -204631.718750\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -270664.125000\n",
      "    epoch          : 248\n",
      "    loss           : -220212.01625\n",
      "    val_loss       : -221100.897265625\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -157549.578125\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -185099.265625\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -203471.031250\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -271441.750000\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -276725.312500\n",
      "    epoch          : 249\n",
      "    loss           : -220481.0490625\n",
      "    val_loss       : -221017.59140625\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -181560.796875\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -204074.187500\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -259508.203125\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -202300.218750\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -197643.203125\n",
      "    epoch          : 250\n",
      "    loss           : -220733.71296875\n",
      "    val_loss       : -221393.54140625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch250.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -156948.046875\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -157516.203125\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -278519.875000\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -246039.593750\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -271316.031250\n",
      "    epoch          : 251\n",
      "    loss           : -220968.41328125\n",
      "    val_loss       : -221661.6453125\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -200806.937500\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -260760.468750\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -205345.375000\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -195994.062500\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -200199.562500\n",
      "    epoch          : 252\n",
      "    loss           : -221192.614375\n",
      "    val_loss       : -222479.5015625\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -246297.406250\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -202317.687500\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -201643.156250\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -248657.906250\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -179240.250000\n",
      "    epoch          : 253\n",
      "    loss           : -221451.12296875\n",
      "    val_loss       : -222791.943359375\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -182583.468750\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -156708.453125\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -209261.953125\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -203111.593750\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -199518.843750\n",
      "    epoch          : 254\n",
      "    loss           : -221684.99078125\n",
      "    val_loss       : -222319.2171875\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -280520.843750\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -206793.531250\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -271696.093750\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -273796.500000\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -154027.703125\n",
      "    epoch          : 255\n",
      "    loss           : -221892.4209375\n",
      "    val_loss       : -222978.68359375\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -271531.343750\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -280453.812500\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -201865.687500\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -271830.062500\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -198614.968750\n",
      "    epoch          : 256\n",
      "    loss           : -222218.08546875\n",
      "    val_loss       : -223474.800390625\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -207113.906250\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -283340.312500\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -205465.984375\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -198329.781250\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -247280.125000\n",
      "    epoch          : 257\n",
      "    loss           : -222393.3496875\n",
      "    val_loss       : -223063.675390625\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -162450.453125\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -272128.031250\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -206999.703125\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -199304.796875\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -201592.437500\n",
      "    epoch          : 258\n",
      "    loss           : -222671.74390625\n",
      "    val_loss       : -223716.315234375\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -249676.328125\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -247224.093750\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -272802.562500\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -271846.687500\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -200922.859375\n",
      "    epoch          : 259\n",
      "    loss           : -222922.78171875\n",
      "    val_loss       : -223830.85546875\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -159061.109375\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -282347.375000\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -211095.078125\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -183304.968750\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -274736.125000\n",
      "    epoch          : 260\n",
      "    loss           : -223133.648125\n",
      "    val_loss       : -223705.055859375\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -184970.750000\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -283732.312500\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -264624.343750\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -197728.812500\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -251618.906250\n",
      "    epoch          : 261\n",
      "    loss           : -223363.961875\n",
      "    val_loss       : -223944.59296875\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -252131.781250\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -183311.531250\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -273308.625000\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -152534.187500\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -164733.781250\n",
      "    epoch          : 262\n",
      "    loss           : -223613.68828125\n",
      "    val_loss       : -224121.68203125\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -209947.453125\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -265971.875000\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -184067.515625\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -251927.328125\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -248149.843750\n",
      "    epoch          : 263\n",
      "    loss           : -223829.06671875\n",
      "    val_loss       : -224599.58203125\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -283948.125000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -154269.343750\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -265564.125000\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -207134.234375\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -185854.125000\n",
      "    epoch          : 264\n",
      "    loss           : -224156.7215625\n",
      "    val_loss       : -224790.51328125\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -202375.281250\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -208110.375000\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -265494.031250\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -183986.796875\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -285176.218750\n",
      "    epoch          : 265\n",
      "    loss           : -224301.06703125\n",
      "    val_loss       : -225274.853125\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -201571.625000\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -266588.812500\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -206035.859375\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -206841.968750\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -182204.718750\n",
      "    epoch          : 266\n",
      "    loss           : -224475.029375\n",
      "    val_loss       : -225602.8765625\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -283555.250000\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -276150.500000\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -250762.250000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -276381.812500\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -208917.328125\n",
      "    epoch          : 267\n",
      "    loss           : -224725.5784375\n",
      "    val_loss       : -225687.06328125\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -184894.687500\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -285752.906250\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -253933.953125\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -284933.875000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -202243.625000\n",
      "    epoch          : 268\n",
      "    loss           : -224970.490625\n",
      "    val_loss       : -225942.915234375\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -285873.031250\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -274687.781250\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -285250.687500\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -205651.250000\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -185338.062500\n",
      "    epoch          : 269\n",
      "    loss           : -225161.0059375\n",
      "    val_loss       : -225821.438671875\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -208221.281250\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -210214.250000\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -199640.109375\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -253106.171875\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -253835.109375\n",
      "    epoch          : 270\n",
      "    loss           : -225483.22078125\n",
      "    val_loss       : -226193.148828125\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -286221.156250\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -212895.812500\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -269236.937500\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -256999.906250\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -202143.984375\n",
      "    epoch          : 271\n",
      "    loss           : -225727.87921875\n",
      "    val_loss       : -226412.37109375\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -211744.171875\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -209166.218750\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -187729.906250\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -154520.156250\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -203007.250000\n",
      "    epoch          : 272\n",
      "    loss           : -225911.89140625\n",
      "    val_loss       : -227242.523046875\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -207390.406250\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -256333.328125\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -255945.968750\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -285654.937500\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -180926.640625\n",
      "    epoch          : 273\n",
      "    loss           : -226150.681875\n",
      "    val_loss       : -226922.317578125\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -255511.156250\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -251929.500000\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -207398.500000\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -184377.609375\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -202720.546875\n",
      "    epoch          : 274\n",
      "    loss           : -226292.14625\n",
      "    val_loss       : -226825.738671875\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -161029.250000\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -286557.062500\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -287513.500000\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -208531.515625\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -255568.156250\n",
      "    epoch          : 275\n",
      "    loss           : -226459.30015625\n",
      "    val_loss       : -227623.889453125\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -207099.875000\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -269865.906250\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -207513.281250\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -214499.125000\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -256627.250000\n",
      "    epoch          : 276\n",
      "    loss           : -226714.07515625\n",
      "    val_loss       : -227328.614453125\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -209348.125000\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -208733.687500\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -160321.671875\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -253081.343750\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -213844.468750\n",
      "    epoch          : 277\n",
      "    loss           : -226965.82953125\n",
      "    val_loss       : -228093.031640625\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -208850.843750\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -213029.000000\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -209113.843750\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -210319.015625\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -255251.250000\n",
      "    epoch          : 278\n",
      "    loss           : -227182.95859375\n",
      "    val_loss       : -228161.043359375\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -211469.359375\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -214265.593750\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -212540.140625\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -211952.937500\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -256187.281250\n",
      "    epoch          : 279\n",
      "    loss           : -227425.24234375\n",
      "    val_loss       : -228147.5640625\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -271867.375000\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -213003.328125\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -211724.234375\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -159563.687500\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -204841.281250\n",
      "    epoch          : 280\n",
      "    loss           : -227635.10625\n",
      "    val_loss       : -228661.592578125\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -158334.390625\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -210500.515625\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -217418.265625\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -206706.718750\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -186479.078125\n",
      "    epoch          : 281\n",
      "    loss           : -227807.18359375\n",
      "    val_loss       : -228407.66171875\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -270356.468750\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -207035.546875\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -216061.125000\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -280889.125000\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -258268.218750\n",
      "    epoch          : 282\n",
      "    loss           : -228053.368125\n",
      "    val_loss       : -228800.15546875\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -185964.187500\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -255567.437500\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -201429.500000\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -280787.125000\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -200485.656250\n",
      "    epoch          : 283\n",
      "    loss           : -228257.79953125\n",
      "    val_loss       : -228890.736328125\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -205739.171875\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -279586.000000\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -214608.593750\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -217594.015625\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -184995.531250\n",
      "    epoch          : 284\n",
      "    loss           : -228397.261875\n",
      "    val_loss       : -229009.242578125\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -281566.687500\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -208391.843750\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -212369.437500\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -217542.171875\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -289935.937500\n",
      "    epoch          : 285\n",
      "    loss           : -228697.809375\n",
      "    val_loss       : -229804.062109375\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -206262.187500\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -291338.312500\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -208159.093750\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -187858.078125\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -212614.453125\n",
      "    epoch          : 286\n",
      "    loss           : -228942.6521875\n",
      "    val_loss       : -229909.047265625\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -212172.281250\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -258989.437500\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -207111.625000\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -210778.812500\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -206347.531250\n",
      "    epoch          : 287\n",
      "    loss           : -229064.03828125\n",
      "    val_loss       : -230085.7203125\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -273669.625000\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -290474.375000\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -283488.437500\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -212929.031250\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -186509.859375\n",
      "    epoch          : 288\n",
      "    loss           : -229351.27328125\n",
      "    val_loss       : -230245.312890625\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -281906.687500\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -258802.578125\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -218036.468750\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -213431.812500\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -208742.093750\n",
      "    epoch          : 289\n",
      "    loss           : -229540.7084375\n",
      "    val_loss       : -230431.69765625\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -220523.687500\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -292647.437500\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -206039.406250\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -211559.234375\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -283277.468750\n",
      "    epoch          : 290\n",
      "    loss           : -229673.6059375\n",
      "    val_loss       : -230800.08046875\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -292147.687500\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -274278.531250\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -292865.093750\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -290921.125000\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -259418.187500\n",
      "    epoch          : 291\n",
      "    loss           : -229988.54734375\n",
      "    val_loss       : -230571.347265625\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -284168.750000\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -293763.843750\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -275650.625000\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -276515.625000\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -259903.875000\n",
      "    epoch          : 292\n",
      "    loss           : -229964.07734375\n",
      "    val_loss       : -230757.358984375\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -290915.750000\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -210075.968750\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -209430.687500\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -293162.281250\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -212555.312500\n",
      "    epoch          : 293\n",
      "    loss           : -230358.65875\n",
      "    val_loss       : -231514.58125\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -208421.281250\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -291993.343750\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -214862.703125\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -217401.750000\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -261507.218750\n",
      "    epoch          : 294\n",
      "    loss           : -230494.61421875\n",
      "    val_loss       : -231871.145703125\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -206432.500000\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -160161.562500\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -212016.203125\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -283856.000000\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -260277.609375\n",
      "    epoch          : 295\n",
      "    loss           : -230727.526875\n",
      "    val_loss       : -231459.590234375\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -284208.562500\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -285957.843750\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -283471.562500\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -285616.625000\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -205288.656250\n",
      "    epoch          : 296\n",
      "    loss           : -230955.14515625\n",
      "    val_loss       : -232093.6328125\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -209055.421875\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -257699.812500\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -160196.296875\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -160462.500000\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -161998.453125\n",
      "    epoch          : 297\n",
      "    loss           : -231093.14984375\n",
      "    val_loss       : -231502.826171875\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -276946.562500\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -295894.531250\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -203930.421875\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -262903.125000\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -259183.875000\n",
      "    epoch          : 298\n",
      "    loss           : -231266.56421875\n",
      "    val_loss       : -232559.138671875\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -218493.156250\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -188124.500000\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -206555.843750\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -261648.250000\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -184956.062500\n",
      "    epoch          : 299\n",
      "    loss           : -231473.479375\n",
      "    val_loss       : -232391.181640625\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -276912.250000\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -216731.093750\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -161914.687500\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -261797.937500\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -189287.296875\n",
      "    epoch          : 300\n",
      "    loss           : -231651.8025\n",
      "    val_loss       : -232684.23046875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch300.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -212024.125000\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -210949.218750\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -208267.562500\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -286351.625000\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -284511.000000\n",
      "    epoch          : 301\n",
      "    loss           : -231950.7346875\n",
      "    val_loss       : -232847.68125\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -156408.718750\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -211830.578125\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -261293.031250\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -188975.062500\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -263343.687500\n",
      "    epoch          : 302\n",
      "    loss           : -232094.35671875\n",
      "    val_loss       : -233031.989453125\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -208184.531250\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -164916.515625\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -164613.953125\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -207170.359375\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -188447.750000\n",
      "    epoch          : 303\n",
      "    loss           : -232268.05828125\n",
      "    val_loss       : -232926.64140625\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -207963.984375\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -215829.468750\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -216090.953125\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -217630.937500\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -184130.000000\n",
      "    epoch          : 304\n",
      "    loss           : -232501.9521875\n",
      "    val_loss       : -233361.07890625\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -298227.656250\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -207294.609375\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -165628.171875\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -285804.687500\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -210947.921875\n",
      "    epoch          : 305\n",
      "    loss           : -232664.76203125\n",
      "    val_loss       : -233626.584375\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -297521.343750\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -217042.656250\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -165053.000000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -296305.750000\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -284775.062500\n",
      "    epoch          : 306\n",
      "    loss           : -232811.33375\n",
      "    val_loss       : -233473.89375\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -158859.859375\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -212295.625000\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -287691.312500\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -265306.343750\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -264311.875000\n",
      "    epoch          : 307\n",
      "    loss           : -233058.19390625\n",
      "    val_loss       : -234026.114453125\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -153914.984375\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -214076.859375\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -211942.140625\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -213011.312500\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -267054.375000\n",
      "    epoch          : 308\n",
      "    loss           : -233272.46859375\n",
      "    val_loss       : -233765.62578125\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -296932.281250\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -193192.546875\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -162939.812500\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -162311.468750\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -288695.437500\n",
      "    epoch          : 309\n",
      "    loss           : -233411.84203125\n",
      "    val_loss       : -234879.305859375\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -208699.343750\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -265061.968750\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -280710.125000\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -281421.125000\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -265856.156250\n",
      "    epoch          : 310\n",
      "    loss           : -233571.83640625\n",
      "    val_loss       : -234888.8328125\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -288456.843750\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -288517.500000\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -220568.062500\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -216426.375000\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -189897.187500\n",
      "    epoch          : 311\n",
      "    loss           : -233745.2103125\n",
      "    val_loss       : -234297.960546875\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -215574.218750\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -187006.468750\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -281596.437500\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -156926.328125\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -289984.750000\n",
      "    epoch          : 312\n",
      "    loss           : -233910.30046875\n",
      "    val_loss       : -235313.665625\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -281681.781250\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -208039.718750\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -282757.687500\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -298654.156250\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -267297.625000\n",
      "    epoch          : 313\n",
      "    loss           : -234197.79859375\n",
      "    val_loss       : -234451.671875\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -298754.250000\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -219553.468750\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -214558.140625\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -213257.593750\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -221145.812500\n",
      "    epoch          : 314\n",
      "    loss           : -234254.04296875\n",
      "    val_loss       : -235304.278125\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -218412.218750\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -218081.609375\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -282067.625000\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -163383.093750\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -210048.406250\n",
      "    epoch          : 315\n",
      "    loss           : -234534.48953125\n",
      "    val_loss       : -235360.412890625\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -166921.234375\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -159271.875000\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -283501.812500\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -290890.687500\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -288098.562500\n",
      "    epoch          : 316\n",
      "    loss           : -234727.0275\n",
      "    val_loss       : -235808.85234375\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -216754.578125\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -288164.500000\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -215268.000000\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -213122.718750\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -269820.187500\n",
      "    epoch          : 317\n",
      "    loss           : -234862.69453125\n",
      "    val_loss       : -235777.49140625\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -186882.515625\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -187297.343750\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -165909.750000\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -282384.906250\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -291663.937500\n",
      "    epoch          : 318\n",
      "    loss           : -235031.1321875\n",
      "    val_loss       : -235974.41328125\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -297679.562500\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -222128.312500\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -270485.000000\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -219539.703125\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -210525.953125\n",
      "    epoch          : 319\n",
      "    loss           : -235340.765\n",
      "    val_loss       : -236252.8640625\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -290723.312500\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -213532.265625\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -301784.500000\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -300600.937500\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -289590.750000\n",
      "    epoch          : 320\n",
      "    loss           : -235414.046875\n",
      "    val_loss       : -236528.498828125\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -217863.875000\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -266433.562500\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -213254.531250\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -213138.890625\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -217502.031250\n",
      "    epoch          : 321\n",
      "    loss           : -235663.725625\n",
      "    val_loss       : -236823.021875\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -219988.781250\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -220271.531250\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -269581.593750\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -270585.562500\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -188543.390625\n",
      "    epoch          : 322\n",
      "    loss           : -235722.68125\n",
      "    val_loss       : -236270.680859375\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -186300.187500\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -284239.312500\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -161162.921875\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -293466.687500\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -193372.984375\n",
      "    epoch          : 323\n",
      "    loss           : -235956.29328125\n",
      "    val_loss       : -237144.417578125\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -284947.593750\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -213553.390625\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -211276.125000\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -300346.875000\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -268200.968750\n",
      "    epoch          : 324\n",
      "    loss           : -236164.40046875\n",
      "    val_loss       : -237097.326953125\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -285095.875000\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -217734.843750\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -291503.343750\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -291976.343750\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -158906.562500\n",
      "    epoch          : 325\n",
      "    loss           : -236316.63125\n",
      "    val_loss       : -237211.455859375\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -302999.000000\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -293659.437500\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -302813.125000\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -163504.890625\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -209757.890625\n",
      "    epoch          : 326\n",
      "    loss           : -236483.46765625\n",
      "    val_loss       : -237379.52421875\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -212310.656250\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -215390.343750\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -213828.046875\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -163078.375000\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -216881.968750\n",
      "    epoch          : 327\n",
      "    loss           : -236697.93\n",
      "    val_loss       : -237592.24921875\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -303052.187500\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -286361.937500\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -219904.406250\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -301257.812500\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -271100.656250\n",
      "    epoch          : 328\n",
      "    loss           : -236867.34109375\n",
      "    val_loss       : -238090.0328125\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -286722.437500\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -303745.656250\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -191501.250000\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -215488.593750\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -187267.703125\n",
      "    epoch          : 329\n",
      "    loss           : -236970.86546875\n",
      "    val_loss       : -237581.007421875\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -212383.765625\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -219339.125000\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -220335.265625\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -292319.593750\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -291836.281250\n",
      "    epoch          : 330\n",
      "    loss           : -237263.72\n",
      "    val_loss       : -238546.441796875\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -213367.796875\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -302345.031250\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -216554.312500\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -191439.843750\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -221900.343750\n",
      "    epoch          : 331\n",
      "    loss           : -237397.4878125\n",
      "    val_loss       : -238223.7078125\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -304668.812500\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -217759.031250\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -287331.375000\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -211668.109375\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -188783.593750\n",
      "    epoch          : 332\n",
      "    loss           : -237572.50859375\n",
      "    val_loss       : -238387.497265625\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -196336.000000\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -267587.687500\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -213282.937500\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -292569.093750\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -191738.375000\n",
      "    epoch          : 333\n",
      "    loss           : -237650.81453125\n",
      "    val_loss       : -238831.8640625\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -306623.187500\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -291488.937500\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -212485.312500\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -291812.187500\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -211781.453125\n",
      "    epoch          : 334\n",
      "    loss           : -237781.09\n",
      "    val_loss       : -238855.19375\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -222319.250000\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -303345.843750\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -218102.375000\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -294663.593750\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -273792.375000\n",
      "    epoch          : 335\n",
      "    loss           : -238053.94046875\n",
      "    val_loss       : -239458.3546875\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -304326.406250\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -221259.390625\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -286774.875000\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -214839.421875\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -271362.000000\n",
      "    epoch          : 336\n",
      "    loss           : -238241.99703125\n",
      "    val_loss       : -238781.683984375\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -294064.375000\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -286728.562500\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -217392.421875\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -274088.812500\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -218262.343750\n",
      "    epoch          : 337\n",
      "    loss           : -238385.66578125\n",
      "    val_loss       : -239231.461328125\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -220851.593750\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -167184.218750\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -214408.625000\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -158940.875000\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -184353.906250\n",
      "    epoch          : 338\n",
      "    loss           : -238503.073125\n",
      "    val_loss       : -240200.95625\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -218219.703125\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -217887.125000\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -288774.312500\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -167512.250000\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -295128.781250\n",
      "    epoch          : 339\n",
      "    loss           : -238754.0959375\n",
      "    val_loss       : -239713.5015625\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -218143.656250\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -223005.843750\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -289417.875000\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -189908.796875\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -215666.406250\n",
      "    epoch          : 340\n",
      "    loss           : -238902.1840625\n",
      "    val_loss       : -239377.538671875\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -160558.000000\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -190273.656250\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -214015.734375\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -211064.453125\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -190111.140625\n",
      "    epoch          : 341\n",
      "    loss           : -239026.1259375\n",
      "    val_loss       : -240071.394140625\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -166553.375000\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -217644.500000\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -214085.156250\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -214987.515625\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -190774.625000\n",
      "    epoch          : 342\n",
      "    loss           : -239306.44828125\n",
      "    val_loss       : -239929.083203125\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -189629.156250\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -222474.937500\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -214704.687500\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -215683.031250\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -192194.937500\n",
      "    epoch          : 343\n",
      "    loss           : -239263.018125\n",
      "    val_loss       : -239862.885546875\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -165914.718750\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -220625.796875\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -277645.531250\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -190948.640625\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -224511.187500\n",
      "    epoch          : 344\n",
      "    loss           : -239526.66046875\n",
      "    val_loss       : -240890.834375\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -212700.734375\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -289477.187500\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -159825.468750\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -222243.390625\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -218141.437500\n",
      "    epoch          : 345\n",
      "    loss           : -239720.8075\n",
      "    val_loss       : -240032.803125\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -307623.562500\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -305300.187500\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -309384.187500\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -160036.718750\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -220900.593750\n",
      "    epoch          : 346\n",
      "    loss           : -239833.2415625\n",
      "    val_loss       : -240674.281640625\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -191658.250000\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -215875.531250\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -222623.984375\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -309234.312500\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -276726.937500\n",
      "    epoch          : 347\n",
      "    loss           : -239997.08390625\n",
      "    val_loss       : -241070.38359375\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -291261.625000\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -189961.468750\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -224390.281250\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -190815.812500\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -222604.250000\n",
      "    epoch          : 348\n",
      "    loss           : -240108.69015625\n",
      "    val_loss       : -241010.767578125\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -291873.250000\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -218156.046875\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -221708.781250\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -215030.843750\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -192414.265625\n",
      "    epoch          : 349\n",
      "    loss           : -240349.1034375\n",
      "    val_loss       : -241017.923046875\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -290840.312500\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -163193.437500\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -295913.937500\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -214906.171875\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -213251.281250\n",
      "    epoch          : 350\n",
      "    loss           : -240554.6059375\n",
      "    val_loss       : -241597.055078125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch350.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -274861.281250\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -292550.062500\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -310831.562500\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -273609.281250\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -298625.875000\n",
      "    epoch          : 351\n",
      "    loss           : -240610.88015625\n",
      "    val_loss       : -241532.991796875\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -276039.125000\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -214179.937500\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -225061.750000\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -216392.906250\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -217353.750000\n",
      "    epoch          : 352\n",
      "    loss           : -240822.7571875\n",
      "    val_loss       : -241316.881640625\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -221106.843750\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -216291.093750\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -278750.656250\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -164901.750000\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -186533.468750\n",
      "    epoch          : 353\n",
      "    loss           : -240952.051875\n",
      "    val_loss       : -241739.41171875\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -219262.671875\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -218023.265625\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -297562.562500\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -293220.625000\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -273127.781250\n",
      "    epoch          : 354\n",
      "    loss           : -241155.385\n",
      "    val_loss       : -241476.89296875\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -292336.156250\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -167001.890625\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -165687.625000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -225244.859375\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -300458.000000\n",
      "    epoch          : 355\n",
      "    loss           : -241234.13234375\n",
      "    val_loss       : -242050.9046875\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -216606.453125\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -225027.312500\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -275176.406250\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -223178.359375\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -279634.437500\n",
      "    epoch          : 356\n",
      "    loss           : -241378.29765625\n",
      "    val_loss       : -242287.89765625\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -309053.937500\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -298045.093750\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -276902.812500\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -297480.000000\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -301372.375000\n",
      "    epoch          : 357\n",
      "    loss           : -241578.21984375\n",
      "    val_loss       : -242014.271875\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -163003.140625\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -297455.500000\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -311101.312500\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -300827.968750\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -191677.890625\n",
      "    epoch          : 358\n",
      "    loss           : -241686.47625\n",
      "    val_loss       : -243130.341796875\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -217233.265625\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -293714.312500\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -293791.843750\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -198518.750000\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -219240.406250\n",
      "    epoch          : 359\n",
      "    loss           : -241845.60421875\n",
      "    val_loss       : -242824.55703125\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -221152.312500\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -215076.359375\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -168828.953125\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -218194.937500\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -164556.046875\n",
      "    epoch          : 360\n",
      "    loss           : -242027.70578125\n",
      "    val_loss       : -243115.52734375\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -215970.703125\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -224456.375000\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -292992.656250\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -300789.437500\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -172070.531250\n",
      "    epoch          : 361\n",
      "    loss           : -242191.06265625\n",
      "    val_loss       : -243043.319921875\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -279310.687500\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -169045.187500\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -214930.890625\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -278515.750000\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -188559.640625\n",
      "    epoch          : 362\n",
      "    loss           : -242251.37328125\n",
      "    val_loss       : -243774.915234375\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -312721.062500\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -300145.937500\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -162088.343750\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -294654.250000\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -226459.718750\n",
      "    epoch          : 363\n",
      "    loss           : -242501.1771875\n",
      "    val_loss       : -243234.440234375\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -312535.531250\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -312437.062500\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -163556.203125\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -225552.203125\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -279395.781250\n",
      "    epoch          : 364\n",
      "    loss           : -242619.07671875\n",
      "    val_loss       : -243880.450390625\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -218795.062500\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -166908.500000\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -296960.750000\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -279978.812500\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -218355.968750\n",
      "    epoch          : 365\n",
      "    loss           : -242737.81\n",
      "    val_loss       : -243600.1296875\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -300321.937500\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -171633.031250\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -224029.062500\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -221512.968750\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -278959.750000\n",
      "    epoch          : 366\n",
      "    loss           : -242899.69328125\n",
      "    val_loss       : -243698.503125\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -294140.437500\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -302154.187500\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -218934.718750\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -188039.968750\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -216348.593750\n",
      "    epoch          : 367\n",
      "    loss           : -243072.92109375\n",
      "    val_loss       : -244302.937890625\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -280258.218750\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -219667.375000\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -312198.750000\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -225307.937500\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -281451.687500\n",
      "    epoch          : 368\n",
      "    loss           : -243250.7959375\n",
      "    val_loss       : -243760.78125\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -296039.750000\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -295233.562500\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -158483.656250\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -311010.312500\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -196641.062500\n",
      "    epoch          : 369\n",
      "    loss           : -243340.7665625\n",
      "    val_loss       : -244308.727734375\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -219824.437500\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -311878.281250\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -227023.625000\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -217247.796875\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -300389.781250\n",
      "    epoch          : 370\n",
      "    loss           : -243598.6246875\n",
      "    val_loss       : -244309.640234375\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -223280.906250\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -313060.062500\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -221167.718750\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -302591.968750\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -189205.531250\n",
      "    epoch          : 371\n",
      "    loss           : -243608.77796875\n",
      "    val_loss       : -244326.916796875\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -226667.421875\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -225292.625000\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -277653.500000\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -192111.375000\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -197787.593750\n",
      "    epoch          : 372\n",
      "    loss           : -243798.27046875\n",
      "    val_loss       : -245023.05625\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -161018.500000\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -221301.843750\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -224052.250000\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -224523.875000\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -278027.625000\n",
      "    epoch          : 373\n",
      "    loss           : -244078.5384375\n",
      "    val_loss       : -244470.540625\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -160844.703125\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -314180.250000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -226734.968750\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -170590.375000\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -217698.984375\n",
      "    epoch          : 374\n",
      "    loss           : -244140.85578125\n",
      "    val_loss       : -245134.269140625\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -314394.062500\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -168424.375000\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -225270.625000\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -281697.937500\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -193938.468750\n",
      "    epoch          : 375\n",
      "    loss           : -244312.3825\n",
      "    val_loss       : -245546.43203125\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -221942.750000\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -197192.187500\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -222716.093750\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -303690.812500\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -219391.812500\n",
      "    epoch          : 376\n",
      "    loss           : -244481.22109375\n",
      "    val_loss       : -245652.211328125\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -168032.687500\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -279570.312500\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -281970.375000\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -305594.625000\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -302402.031250\n",
      "    epoch          : 377\n",
      "    loss           : -244550.2596875\n",
      "    val_loss       : -245648.314453125\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -226797.062500\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -316001.687500\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -217672.953125\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -297134.281250\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -298211.375000\n",
      "    epoch          : 378\n",
      "    loss           : -244626.118125\n",
      "    val_loss       : -245330.38515625\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -223079.437500\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -226410.484375\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -221637.625000\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -226057.906250\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -280886.968750\n",
      "    epoch          : 379\n",
      "    loss           : -244703.65984375\n",
      "    val_loss       : -246473.64609375\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -303660.281250\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -314252.125000\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -315030.031250\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -304737.468750\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -218509.750000\n",
      "    epoch          : 380\n",
      "    loss           : -244973.56453125\n",
      "    val_loss       : -245480.56796875\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -299227.812500\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -163679.609375\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -281356.312500\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -218087.140625\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -222815.609375\n",
      "    epoch          : 381\n",
      "    loss           : -245093.25609375\n",
      "    val_loss       : -246073.8484375\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -226711.296875\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -299888.687500\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -315539.437500\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -195593.343750\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -219888.312500\n",
      "    epoch          : 382\n",
      "    loss           : -245207.34796875\n",
      "    val_loss       : -245571.962109375\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -299050.875000\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -315564.937500\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -220341.015625\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -167729.093750\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -305956.687500\n",
      "    epoch          : 383\n",
      "    loss           : -245354.48921875\n",
      "    val_loss       : -247064.372265625\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -305068.250000\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -230304.937500\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -304701.406250\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -200039.625000\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -197200.406250\n",
      "    epoch          : 384\n",
      "    loss           : -245467.604375\n",
      "    val_loss       : -246448.691796875\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -226345.218750\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -228338.937500\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -226484.312500\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -226032.046875\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -284016.812500\n",
      "    epoch          : 385\n",
      "    loss           : -245740.78453125\n",
      "    val_loss       : -246708.56875\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -300768.593750\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -315382.750000\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -224775.062500\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -223859.187500\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -283579.562500\n",
      "    epoch          : 386\n",
      "    loss           : -245789.88203125\n",
      "    val_loss       : -246627.824609375\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -316751.156250\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -318205.500000\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -304535.906250\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -317156.125000\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -194635.218750\n",
      "    epoch          : 387\n",
      "    loss           : -245980.24390625\n",
      "    val_loss       : -246315.0046875\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -219695.671875\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -298116.625000\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -191201.734375\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -216638.953125\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -221771.250000\n",
      "    epoch          : 388\n",
      "    loss           : -246027.99421875\n",
      "    val_loss       : -246932.925\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -194014.531250\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -228297.562500\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -222223.937500\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -220113.984375\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -305340.562500\n",
      "    epoch          : 389\n",
      "    loss           : -246136.39890625\n",
      "    val_loss       : -247251.191796875\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -222243.250000\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -283400.687500\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -226585.375000\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -302019.156250\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -193189.937500\n",
      "    epoch          : 390\n",
      "    loss           : -246271.21078125\n",
      "    val_loss       : -247081.330859375\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -219980.671875\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -306979.125000\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -223114.921875\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -197857.875000\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -315000.312500\n",
      "    epoch          : 391\n",
      "    loss           : -246493.834375\n",
      "    val_loss       : -247231.82578125\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -300740.312500\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -284796.906250\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -305235.187500\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -169215.468750\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -306347.187500\n",
      "    epoch          : 392\n",
      "    loss           : -246510.34203125\n",
      "    val_loss       : -247316.466796875\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -194186.031250\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -300763.937500\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -305165.562500\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -287737.812500\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -221603.906250\n",
      "    epoch          : 393\n",
      "    loss           : -246678.7453125\n",
      "    val_loss       : -247675.232421875\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -283998.250000\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -164715.437500\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -223445.578125\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -224923.828125\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -305609.906250\n",
      "    epoch          : 394\n",
      "    loss           : -246872.4596875\n",
      "    val_loss       : -248377.7265625\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -167935.281250\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -222814.453125\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -285682.062500\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -219850.203125\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -216735.140625\n",
      "    epoch          : 395\n",
      "    loss           : -247071.81078125\n",
      "    val_loss       : -247940.969921875\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -221549.890625\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -221296.484375\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -165571.500000\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -285858.562500\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -225264.546875\n",
      "    epoch          : 396\n",
      "    loss           : -247210.45140625\n",
      "    val_loss       : -247610.283984375\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -300215.125000\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -226796.812500\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -316995.375000\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -283962.437500\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -308283.937500\n",
      "    epoch          : 397\n",
      "    loss           : -247182.1803125\n",
      "    val_loss       : -248324.948828125\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -302148.312500\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -230242.281250\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -305191.656250\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -317679.125000\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -162501.421875\n",
      "    epoch          : 398\n",
      "    loss           : -247383.19984375\n",
      "    val_loss       : -248312.609765625\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -195353.156250\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -221048.750000\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -222674.312500\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -288087.687500\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -217879.109375\n",
      "    epoch          : 399\n",
      "    loss           : -247653.22625\n",
      "    val_loss       : -248601.2109375\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -320585.531250\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -230178.109375\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -301636.125000\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -220064.296875\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -191433.234375\n",
      "    epoch          : 400\n",
      "    loss           : -247651.3453125\n",
      "    val_loss       : -248134.44140625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -221694.187500\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -219311.109375\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -285404.500000\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -223683.343750\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -285178.906250\n",
      "    epoch          : 401\n",
      "    loss           : -247793.22859375\n",
      "    val_loss       : -248937.312890625\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -191165.781250\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -220343.625000\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -318843.875000\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -196955.093750\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -214576.375000\n",
      "    epoch          : 402\n",
      "    loss           : -247910.18734375\n",
      "    val_loss       : -248971.09140625\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -223720.031250\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -222277.953125\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -308721.656250\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -197206.187500\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -321335.562500\n",
      "    epoch          : 403\n",
      "    loss           : -248113.0978125\n",
      "    val_loss       : -248853.68359375\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -303327.875000\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -230516.312500\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -303684.875000\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -286338.968750\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -288014.312500\n",
      "    epoch          : 404\n",
      "    loss           : -248119.53875\n",
      "    val_loss       : -249728.80234375\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -200703.625000\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -231289.437500\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -308486.968750\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -162508.312500\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -199618.640625\n",
      "    epoch          : 405\n",
      "    loss           : -248396.6821875\n",
      "    val_loss       : -249430.81796875\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -223964.062500\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -304458.593750\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -320916.687500\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -175090.015625\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -307897.687500\n",
      "    epoch          : 406\n",
      "    loss           : -248343.8803125\n",
      "    val_loss       : -249504.765625\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -309956.812500\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -168313.625000\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -225966.093750\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -227748.328125\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -196548.484375\n",
      "    epoch          : 407\n",
      "    loss           : -248493.6871875\n",
      "    val_loss       : -249181.409375\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -320056.343750\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -221032.828125\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -323288.187500\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -322166.781250\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -230585.156250\n",
      "    epoch          : 408\n",
      "    loss           : -248697.42484375\n",
      "    val_loss       : -249816.27890625\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -304550.187500\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -226359.828125\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -288842.000000\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -193977.562500\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -226721.468750\n",
      "    epoch          : 409\n",
      "    loss           : -248957.94515625\n",
      "    val_loss       : -249409.51796875\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -168364.296875\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -221989.796875\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -228637.125000\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -223741.625000\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -199834.437500\n",
      "    epoch          : 410\n",
      "    loss           : -249112.53078125\n",
      "    val_loss       : -250622.612890625\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -305370.718750\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -309212.812500\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -168872.671875\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -196224.687500\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -225468.359375\n",
      "    epoch          : 411\n",
      "    loss           : -249059.4725\n",
      "    val_loss       : -250125.793359375\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -193396.906250\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -228582.125000\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -308798.375000\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -230426.562500\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -236324.187500\n",
      "    epoch          : 412\n",
      "    loss           : -249382.89390625\n",
      "    val_loss       : -250062.095703125\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -167593.718750\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -304646.687500\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -224372.093750\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -310293.718750\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -311215.250000\n",
      "    epoch          : 413\n",
      "    loss           : -249423.62328125\n",
      "    val_loss       : -250416.008984375\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -228601.281250\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -175525.750000\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -231208.500000\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -291513.000000\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -223060.687500\n",
      "    epoch          : 414\n",
      "    loss           : -249517.67234375\n",
      "    val_loss       : -250424.48046875\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -309899.937500\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -167548.937500\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -223097.234375\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -305807.656250\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -289248.937500\n",
      "    epoch          : 415\n",
      "    loss           : -249712.6871875\n",
      "    val_loss       : -250129.475390625\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -309640.562500\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -162788.593750\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -305385.562500\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -195545.421875\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -223546.281250\n",
      "    epoch          : 416\n",
      "    loss           : -249734.4775\n",
      "    val_loss       : -250501.25859375\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -198157.796875\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -218681.171875\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -304564.750000\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -173666.828125\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -200391.281250\n",
      "    epoch          : 417\n",
      "    loss           : -250025.70140625\n",
      "    val_loss       : -250667.2640625\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -224459.875000\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -229427.062500\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -324584.718750\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -222075.625000\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -224807.546875\n",
      "    epoch          : 418\n",
      "    loss           : -249989.011875\n",
      "    val_loss       : -250461.27578125\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -289387.250000\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -311594.875000\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -321247.562500\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -232160.062500\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -313160.125000\n",
      "    epoch          : 419\n",
      "    loss           : -250105.91515625\n",
      "    val_loss       : -251279.6453125\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -226566.500000\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -323311.000000\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -286237.750000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -222063.640625\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -294508.250000\n",
      "    epoch          : 420\n",
      "    loss           : -250244.41375\n",
      "    val_loss       : -251256.725390625\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -307870.343750\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -310934.812500\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -289662.250000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -290531.531250\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -288038.000000\n",
      "    epoch          : 421\n",
      "    loss           : -250340.585625\n",
      "    val_loss       : -250910.666796875\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -170916.578125\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -288757.937500\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -225610.125000\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -223210.093750\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -223719.421875\n",
      "    epoch          : 422\n",
      "    loss           : -250467.15921875\n",
      "    val_loss       : -251799.243359375\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -221462.843750\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -321649.343750\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -226221.093750\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -292663.156250\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -289430.562500\n",
      "    epoch          : 423\n",
      "    loss           : -250663.96484375\n",
      "    val_loss       : -251626.508984375\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -230719.078125\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -224205.000000\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -321567.750000\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -200868.250000\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -291594.750000\n",
      "    epoch          : 424\n",
      "    loss           : -250788.82953125\n",
      "    val_loss       : -251351.06796875\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -230967.750000\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -223096.750000\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -290089.875000\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -324986.437500\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -193201.625000\n",
      "    epoch          : 425\n",
      "    loss           : -250834.956875\n",
      "    val_loss       : -251636.928515625\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -223743.328125\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -324045.218750\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -323152.875000\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -169315.500000\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -313837.468750\n",
      "    epoch          : 426\n",
      "    loss           : -250903.9909375\n",
      "    val_loss       : -252287.157421875\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -311484.968750\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -323885.687500\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -311353.125000\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -166800.125000\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -291003.250000\n",
      "    epoch          : 427\n",
      "    loss           : -251038.63078125\n",
      "    val_loss       : -252387.132421875\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -325276.062500\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -313036.343750\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -228200.593750\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -226482.281250\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -313310.468750\n",
      "    epoch          : 428\n",
      "    loss           : -251142.23484375\n",
      "    val_loss       : -252456.463671875\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -229720.500000\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -170397.250000\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -287648.593750\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -311399.687500\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -291560.062500\n",
      "    epoch          : 429\n",
      "    loss           : -251357.43703125\n",
      "    val_loss       : -252047.71640625\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -221274.156250\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -325138.250000\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -307609.625000\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -307463.031250\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -313211.500000\n",
      "    epoch          : 430\n",
      "    loss           : -251432.91578125\n",
      "    val_loss       : -252394.30703125\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -197017.859375\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -194195.312500\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -308429.125000\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -308255.343750\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -198023.562500\n",
      "    epoch          : 431\n",
      "    loss           : -251510.760625\n",
      "    val_loss       : -252051.366796875\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -326426.531250\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -324694.250000\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -294522.062500\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -174220.968750\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -315163.125000\n",
      "    epoch          : 432\n",
      "    loss           : -251636.37359375\n",
      "    val_loss       : -252076.852734375\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -314123.000000\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -229236.031250\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -223824.000000\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -290443.781250\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -229067.812500\n",
      "    epoch          : 433\n",
      "    loss           : -251652.03671875\n",
      "    val_loss       : -252743.6171875\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -326117.937500\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -327913.937500\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -324687.187500\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -170468.593750\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -224083.437500\n",
      "    epoch          : 434\n",
      "    loss           : -251949.91421875\n",
      "    val_loss       : -252582.180859375\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -309951.812500\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -308664.250000\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -326449.593750\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -312989.375000\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -290491.062500\n",
      "    epoch          : 435\n",
      "    loss           : -252150.29640625\n",
      "    val_loss       : -252683.391796875\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -289550.343750\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -291099.656250\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -225523.562500\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -294155.000000\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -309030.562500\n",
      "    epoch          : 436\n",
      "    loss           : -252244.876875\n",
      "    val_loss       : -253311.87890625\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -308673.781250\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -325751.937500\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -233011.468750\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -309048.343750\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -290963.906250\n",
      "    epoch          : 437\n",
      "    loss           : -252294.00859375\n",
      "    val_loss       : -253317.023828125\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -224136.531250\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -310865.531250\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -235012.453125\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -314465.812500\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -197863.500000\n",
      "    epoch          : 438\n",
      "    loss           : -252356.26296875\n",
      "    val_loss       : -253578.131640625\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -196593.968750\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -229525.375000\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -294950.656250\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -312707.406250\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -231652.281250\n",
      "    epoch          : 439\n",
      "    loss           : -252443.88265625\n",
      "    val_loss       : -253320.49140625\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -230205.328125\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -232058.593750\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -327276.250000\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -234615.234375\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -315224.156250\n",
      "    epoch          : 440\n",
      "    loss           : -252706.66234375\n",
      "    val_loss       : -253304.44921875\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -235122.406250\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -203387.562500\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -229201.671875\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -230769.265625\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -314916.500000\n",
      "    epoch          : 441\n",
      "    loss           : -252651.418125\n",
      "    val_loss       : -253645.859765625\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -166240.453125\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -310713.093750\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -324253.343750\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -310548.531250\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -222467.265625\n",
      "    epoch          : 442\n",
      "    loss           : -252833.31015625\n",
      "    val_loss       : -253423.16796875\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -231953.843750\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -165462.390625\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -225330.781250\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -199273.515625\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -315985.812500\n",
      "    epoch          : 443\n",
      "    loss           : -253033.6253125\n",
      "    val_loss       : -253301.105859375\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -316446.093750\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -315538.000000\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -228433.015625\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -199845.125000\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -202710.562500\n",
      "    epoch          : 444\n",
      "    loss           : -253225.34640625\n",
      "    val_loss       : -254673.77734375\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -315411.500000\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -225159.562500\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -168307.750000\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -311301.656250\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -312288.468750\n",
      "    epoch          : 445\n",
      "    loss           : -253313.880625\n",
      "    val_loss       : -253577.291796875\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -233233.531250\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -229037.265625\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -327245.937500\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -206017.062500\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -237786.062500\n",
      "    epoch          : 446\n",
      "    loss           : -253135.63546875\n",
      "    val_loss       : -253782.997265625\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -225171.500000\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -226947.703125\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -170076.500000\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -193454.109375\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -197461.250000\n",
      "    epoch          : 447\n",
      "    loss           : -253431.3878125\n",
      "    val_loss       : -253652.36796875\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -231406.546875\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -314267.062500\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -290641.375000\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -233935.437500\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -327319.562500\n",
      "    epoch          : 448\n",
      "    loss           : -253576.194375\n",
      "    val_loss       : -254191.633203125\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -231690.937500\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -233348.062500\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -202796.656250\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -234301.515625\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -294023.625000\n",
      "    epoch          : 449\n",
      "    loss           : -253612.18375\n",
      "    val_loss       : -254384.461328125\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -233309.921875\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -169057.687500\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -226879.000000\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -311168.562500\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -292876.562500\n",
      "    epoch          : 450\n",
      "    loss           : -253742.19015625\n",
      "    val_loss       : -254958.420703125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch450.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -328710.125000\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -327965.656250\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -326470.500000\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -309650.562500\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -317451.187500\n",
      "    epoch          : 451\n",
      "    loss           : -253863.521875\n",
      "    val_loss       : -254988.029296875\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -169376.609375\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -234034.937500\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -293663.250000\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -317668.562500\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -297839.187500\n",
      "    epoch          : 452\n",
      "    loss           : -254028.7496875\n",
      "    val_loss       : -255351.8640625\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -165678.078125\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -315548.562500\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -198635.218750\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -326305.625000\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -233761.781250\n",
      "    epoch          : 453\n",
      "    loss           : -254127.2165625\n",
      "    val_loss       : -255147.419140625\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -316313.437500\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -226555.687500\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -165743.031250\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -224906.968750\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -296784.156250\n",
      "    epoch          : 454\n",
      "    loss           : -254131.4075\n",
      "    val_loss       : -255132.0265625\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -316335.687500\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -231731.593750\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -233891.390625\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -200546.421875\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -167371.906250\n",
      "    epoch          : 455\n",
      "    loss           : -254433.7890625\n",
      "    val_loss       : -255792.231640625\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -311766.750000\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -226434.468750\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -226463.453125\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -201729.109375\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -227123.781250\n",
      "    epoch          : 456\n",
      "    loss           : -254472.01078125\n",
      "    val_loss       : -255707.321875\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -229799.234375\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -171024.593750\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -232311.000000\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -227189.843750\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -198271.906250\n",
      "    epoch          : 457\n",
      "    loss           : -254543.9925\n",
      "    val_loss       : -255273.09296875\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -329262.375000\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -298677.312500\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -233062.156250\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -173501.578125\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -224984.703125\n",
      "    epoch          : 458\n",
      "    loss           : -254648.60375\n",
      "    val_loss       : -255640.02421875\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -296098.312500\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -229347.812500\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -228593.468750\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -172680.406250\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -227956.390625\n",
      "    epoch          : 459\n",
      "    loss           : -254791.550625\n",
      "    val_loss       : -255679.81484375\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -231406.671875\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -313591.281250\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -317798.906250\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -228894.718750\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -195807.125000\n",
      "    epoch          : 460\n",
      "    loss           : -254927.92953125\n",
      "    val_loss       : -255631.9875\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -332042.625000\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -231719.765625\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -224831.812500\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -198392.531250\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -302298.875000\n",
      "    epoch          : 461\n",
      "    loss           : -254956.5565625\n",
      "    val_loss       : -256680.346484375\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -235873.718750\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -235755.703125\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -229480.468750\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -164903.015625\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -230348.218750\n",
      "    epoch          : 462\n",
      "    loss           : -255118.61140625\n",
      "    val_loss       : -255609.071875\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -231982.312500\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -233826.734375\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -330253.937500\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -319226.218750\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -298272.687500\n",
      "    epoch          : 463\n",
      "    loss           : -255264.4571875\n",
      "    val_loss       : -255428.121875\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -173672.671875\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -318362.843750\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -235578.187500\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -317035.437500\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -320719.000000\n",
      "    epoch          : 464\n",
      "    loss           : -255392.38578125\n",
      "    val_loss       : -256261.192578125\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -232593.718750\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -237745.875000\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -332919.156250\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -296731.375000\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -196654.343750\n",
      "    epoch          : 465\n",
      "    loss           : -255423.4521875\n",
      "    val_loss       : -256439.740234375\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -160763.906250\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -163642.703125\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -163187.015625\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -178559.718750\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -297260.031250\n",
      "    epoch          : 466\n",
      "    loss           : -255558.05671875\n",
      "    val_loss       : -256538.464453125\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -230543.921875\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -299123.000000\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -233200.281250\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -320649.375000\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -318690.000000\n",
      "    epoch          : 467\n",
      "    loss           : -255669.56375\n",
      "    val_loss       : -256918.434375\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -229468.000000\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -317091.687500\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -318339.750000\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -231280.640625\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -331419.125000\n",
      "    epoch          : 468\n",
      "    loss           : -255883.68125\n",
      "    val_loss       : -257456.073828125\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -227331.265625\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -313536.781250\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -319789.343750\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -297472.343750\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -298601.593750\n",
      "    epoch          : 469\n",
      "    loss           : -255799.91484375\n",
      "    val_loss       : -256825.074609375\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -230050.187500\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -226102.937500\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -240081.500000\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -229226.765625\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -298687.093750\n",
      "    epoch          : 470\n",
      "    loss           : -255972.1853125\n",
      "    val_loss       : -257016.970703125\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -300937.937500\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -230215.062500\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -332212.187500\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -233116.828125\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -292681.750000\n",
      "    epoch          : 471\n",
      "    loss           : -256017.23515625\n",
      "    val_loss       : -256204.224609375\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -235236.593750\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -319793.062500\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -235816.593750\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -228394.328125\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -207502.312500\n",
      "    epoch          : 472\n",
      "    loss           : -256139.30109375\n",
      "    val_loss       : -257245.981640625\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -319343.562500\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -318827.375000\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -168449.062500\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -206346.468750\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -230247.312500\n",
      "    epoch          : 473\n",
      "    loss           : -256189.20984375\n",
      "    val_loss       : -257587.56640625\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -316209.312500\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -233297.625000\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -224946.250000\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -172070.312500\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -195870.640625\n",
      "    epoch          : 474\n",
      "    loss           : -256330.33828125\n",
      "    val_loss       : -257174.83203125\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -318169.062500\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -232989.468750\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -315087.468750\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -320569.750000\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -178763.515625\n",
      "    epoch          : 475\n",
      "    loss           : -256516.11046875\n",
      "    val_loss       : -257639.4265625\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -229086.218750\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -202972.015625\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -200997.265625\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -198915.031250\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -232900.078125\n",
      "    epoch          : 476\n",
      "    loss           : -256653.9128125\n",
      "    val_loss       : -258150.158984375\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -229596.906250\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -166186.828125\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -228887.593750\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -229437.031250\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -298535.750000\n",
      "    epoch          : 477\n",
      "    loss           : -256652.8534375\n",
      "    val_loss       : -257698.7828125\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -316309.687500\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -321141.625000\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -231885.468750\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -198543.062500\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -319789.281250\n",
      "    epoch          : 478\n",
      "    loss           : -256759.08859375\n",
      "    val_loss       : -258164.63203125\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -315326.062500\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -335891.656250\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -314809.875000\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -298449.937500\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -234281.093750\n",
      "    epoch          : 479\n",
      "    loss           : -256828.4815625\n",
      "    val_loss       : -257712.415625\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -318577.875000\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -230537.437500\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -321963.875000\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -237663.750000\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -232401.468750\n",
      "    epoch          : 480\n",
      "    loss           : -256878.02421875\n",
      "    val_loss       : -258240.92890625\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -238735.093750\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -319509.750000\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -298961.281250\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -229570.187500\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -315002.687500\n",
      "    epoch          : 481\n",
      "    loss           : -257070.026875\n",
      "    val_loss       : -258244.140234375\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -301697.375000\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -316912.000000\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -316350.562500\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -299272.375000\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -227306.531250\n",
      "    epoch          : 482\n",
      "    loss           : -257323.918125\n",
      "    val_loss       : -257992.160546875\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -227664.281250\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -239219.546875\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -228722.140625\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -166383.500000\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -227134.171875\n",
      "    epoch          : 483\n",
      "    loss           : -257297.98359375\n",
      "    val_loss       : -258539.799609375\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -335005.218750\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -333891.906250\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -232550.312500\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -299219.625000\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -226638.609375\n",
      "    epoch          : 484\n",
      "    loss           : -257387.37296875\n",
      "    val_loss       : -257851.344921875\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -320511.375000\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -320706.750000\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -321667.437500\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -232054.796875\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -299872.000000\n",
      "    epoch          : 485\n",
      "    loss           : -257397.63671875\n",
      "    val_loss       : -258402.5015625\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -169718.718750\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -230128.640625\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -322198.531250\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -333930.000000\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -233410.000000\n",
      "    epoch          : 486\n",
      "    loss           : -257595.90578125\n",
      "    val_loss       : -258057.946484375\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -235324.750000\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -174318.468750\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -193917.593750\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -232911.187500\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -336678.000000\n",
      "    epoch          : 487\n",
      "    loss           : -257725.18640625\n",
      "    val_loss       : -258635.130859375\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -318488.375000\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -233617.078125\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -317796.343750\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -321367.062500\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -201126.281250\n",
      "    epoch          : 488\n",
      "    loss           : -257763.32765625\n",
      "    val_loss       : -259110.941796875\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -204169.093750\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -318204.843750\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -203921.015625\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -171662.656250\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -205431.437500\n",
      "    epoch          : 489\n",
      "    loss           : -257887.8465625\n",
      "    val_loss       : -259317.789453125\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -334563.812500\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -227652.593750\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -239217.296875\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -233203.296875\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -321747.437500\n",
      "    epoch          : 490\n",
      "    loss           : -257975.99546875\n",
      "    val_loss       : -258749.415234375\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -317423.625000\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -301033.937500\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -336149.125000\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -224274.000000\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -322579.000000\n",
      "    epoch          : 491\n",
      "    loss           : -258069.69125\n",
      "    val_loss       : -259066.062890625\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -319682.812500\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -233167.765625\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -202466.218750\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -324269.437500\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -228103.312500\n",
      "    epoch          : 492\n",
      "    loss           : -258161.5740625\n",
      "    val_loss       : -258856.401171875\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -319638.218750\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -235682.703125\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -316613.125000\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -334653.687500\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -321480.062500\n",
      "    epoch          : 493\n",
      "    loss           : -258339.42921875\n",
      "    val_loss       : -258829.5625\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -169250.218750\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -319728.125000\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -207237.000000\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -235259.906250\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -321912.031250\n",
      "    epoch          : 494\n",
      "    loss           : -258435.50625\n",
      "    val_loss       : -259927.786328125\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -237114.875000\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -239345.812500\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -321784.875000\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -231181.484375\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -207317.109375\n",
      "    epoch          : 495\n",
      "    loss           : -258620.79046875\n",
      "    val_loss       : -260276.415625\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -232824.031250\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -237359.093750\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -201380.218750\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -205863.781250\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -232318.843750\n",
      "    epoch          : 496\n",
      "    loss           : -258562.74078125\n",
      "    val_loss       : -259599.8515625\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -233559.406250\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -240717.656250\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -236099.937500\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -172729.687500\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -232615.468750\n",
      "    epoch          : 497\n",
      "    loss           : -258738.61\n",
      "    val_loss       : -259410.16796875\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -168653.937500\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -161520.812500\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -335339.218750\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -205412.390625\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -304093.875000\n",
      "    epoch          : 498\n",
      "    loss           : -258732.2153125\n",
      "    val_loss       : -259584.128125\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -301770.250000\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -232375.203125\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -203803.656250\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -337719.750000\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -199118.625000\n",
      "    epoch          : 499\n",
      "    loss           : -258953.80296875\n",
      "    val_loss       : -260120.10390625\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -229970.281250\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -170165.484375\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -336093.687500\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -319145.437500\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -203547.984375\n",
      "    epoch          : 500\n",
      "    loss           : -259063.76609375\n",
      "    val_loss       : -259874.3\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch500.pth ...\n",
      "Train Epoch: 501 [512/54000 (1%)] Loss: -242053.203125\n",
      "Train Epoch: 501 [11776/54000 (22%)] Loss: -335921.625000\n",
      "Train Epoch: 501 [23040/54000 (43%)] Loss: -228868.531250\n",
      "Train Epoch: 501 [34304/54000 (64%)] Loss: -234034.437500\n",
      "Train Epoch: 501 [45568/54000 (84%)] Loss: -225220.984375\n",
      "    epoch          : 501\n",
      "    loss           : -259163.1334375\n",
      "    val_loss       : -259696.403515625\n",
      "Train Epoch: 502 [512/54000 (1%)] Loss: -318618.000000\n",
      "Train Epoch: 502 [11776/54000 (22%)] Loss: -318975.625000\n",
      "Train Epoch: 502 [23040/54000 (43%)] Loss: -172962.750000\n",
      "Train Epoch: 502 [34304/54000 (64%)] Loss: -231415.093750\n",
      "Train Epoch: 502 [45568/54000 (84%)] Loss: -228964.062500\n",
      "    epoch          : 502\n",
      "    loss           : -259140.35703125\n",
      "    val_loss       : -259975.5765625\n",
      "Train Epoch: 503 [512/54000 (1%)] Loss: -236813.875000\n",
      "Train Epoch: 503 [11776/54000 (22%)] Loss: -236085.265625\n",
      "Train Epoch: 503 [23040/54000 (43%)] Loss: -205606.375000\n",
      "Train Epoch: 503 [34304/54000 (64%)] Loss: -231457.437500\n",
      "Train Epoch: 503 [45568/54000 (84%)] Loss: -303135.375000\n",
      "    epoch          : 503\n",
      "    loss           : -259160.945625\n",
      "    val_loss       : -260324.937109375\n",
      "Train Epoch: 504 [512/54000 (1%)] Loss: -303561.843750\n",
      "Train Epoch: 504 [11776/54000 (22%)] Loss: -168967.468750\n",
      "Train Epoch: 504 [23040/54000 (43%)] Loss: -304161.718750\n",
      "Train Epoch: 504 [34304/54000 (64%)] Loss: -233066.125000\n",
      "Train Epoch: 504 [45568/54000 (84%)] Loss: -325553.625000\n",
      "    epoch          : 504\n",
      "    loss           : -259505.9296875\n",
      "    val_loss       : -260430.30234375\n",
      "Train Epoch: 505 [512/54000 (1%)] Loss: -226595.562500\n",
      "Train Epoch: 505 [11776/54000 (22%)] Loss: -325650.437500\n",
      "Train Epoch: 505 [23040/54000 (43%)] Loss: -324144.000000\n",
      "Train Epoch: 505 [34304/54000 (64%)] Loss: -324779.500000\n",
      "Train Epoch: 505 [45568/54000 (84%)] Loss: -323708.750000\n",
      "    epoch          : 505\n",
      "    loss           : -259430.71265625\n",
      "    val_loss       : -260074.583203125\n",
      "Train Epoch: 506 [512/54000 (1%)] Loss: -230338.484375\n",
      "Train Epoch: 506 [11776/54000 (22%)] Loss: -323081.625000\n",
      "Train Epoch: 506 [23040/54000 (43%)] Loss: -323615.718750\n",
      "Train Epoch: 506 [34304/54000 (64%)] Loss: -325457.000000\n",
      "Train Epoch: 506 [45568/54000 (84%)] Loss: -297961.906250\n",
      "    epoch          : 506\n",
      "    loss           : -259634.12234375\n",
      "    val_loss       : -260577.11328125\n",
      "Train Epoch: 507 [512/54000 (1%)] Loss: -324889.937500\n",
      "Train Epoch: 507 [11776/54000 (22%)] Loss: -233162.609375\n",
      "Train Epoch: 507 [23040/54000 (43%)] Loss: -333889.312500\n",
      "Train Epoch: 507 [34304/54000 (64%)] Loss: -325734.062500\n",
      "Train Epoch: 507 [45568/54000 (84%)] Loss: -326771.156250\n",
      "    epoch          : 507\n",
      "    loss           : -259697.87953125\n",
      "    val_loss       : -260954.85\n",
      "Train Epoch: 508 [512/54000 (1%)] Loss: -203029.593750\n",
      "Train Epoch: 508 [11776/54000 (22%)] Loss: -237524.031250\n",
      "Train Epoch: 508 [23040/54000 (43%)] Loss: -234864.234375\n",
      "Train Epoch: 508 [34304/54000 (64%)] Loss: -319619.000000\n",
      "Train Epoch: 508 [45568/54000 (84%)] Loss: -324030.187500\n",
      "    epoch          : 508\n",
      "    loss           : -259732.95828125\n",
      "    val_loss       : -260446.8890625\n",
      "Train Epoch: 509 [512/54000 (1%)] Loss: -320759.625000\n",
      "Train Epoch: 509 [11776/54000 (22%)] Loss: -321859.750000\n",
      "Train Epoch: 509 [23040/54000 (43%)] Loss: -174024.218750\n",
      "Train Epoch: 509 [34304/54000 (64%)] Loss: -324685.875000\n",
      "Train Epoch: 509 [45568/54000 (84%)] Loss: -304536.656250\n",
      "    epoch          : 509\n",
      "    loss           : -259947.96390625\n",
      "    val_loss       : -261027.30703125\n",
      "Train Epoch: 510 [512/54000 (1%)] Loss: -321907.093750\n",
      "Train Epoch: 510 [11776/54000 (22%)] Loss: -319420.250000\n",
      "Train Epoch: 510 [23040/54000 (43%)] Loss: -231898.093750\n",
      "Train Epoch: 510 [34304/54000 (64%)] Loss: -324005.750000\n",
      "Train Epoch: 510 [45568/54000 (84%)] Loss: -304346.312500\n",
      "    epoch          : 510\n",
      "    loss           : -259868.55265625\n",
      "    val_loss       : -261544.28046875\n",
      "Train Epoch: 511 [512/54000 (1%)] Loss: -239902.437500\n",
      "Train Epoch: 511 [11776/54000 (22%)] Loss: -320850.312500\n",
      "Train Epoch: 511 [23040/54000 (43%)] Loss: -233836.812500\n",
      "Train Epoch: 511 [34304/54000 (64%)] Loss: -325178.187500\n",
      "Train Epoch: 511 [45568/54000 (84%)] Loss: -198662.406250\n",
      "    epoch          : 511\n",
      "    loss           : -259929.45046875\n",
      "    val_loss       : -261220.648828125\n",
      "Train Epoch: 512 [512/54000 (1%)] Loss: -300299.500000\n",
      "Train Epoch: 512 [11776/54000 (22%)] Loss: -240127.937500\n",
      "Train Epoch: 512 [23040/54000 (43%)] Loss: -231996.843750\n",
      "Train Epoch: 512 [34304/54000 (64%)] Loss: -233058.828125\n",
      "Train Epoch: 512 [45568/54000 (84%)] Loss: -305732.750000\n",
      "    epoch          : 512\n",
      "    loss           : -260071.8378125\n",
      "    val_loss       : -260945.95390625\n",
      "Train Epoch: 513 [512/54000 (1%)] Loss: -199928.218750\n",
      "Train Epoch: 513 [11776/54000 (22%)] Loss: -202433.781250\n",
      "Train Epoch: 513 [23040/54000 (43%)] Loss: -229759.781250\n",
      "Train Epoch: 513 [34304/54000 (64%)] Loss: -201522.140625\n",
      "Train Epoch: 513 [45568/54000 (84%)] Loss: -305891.250000\n",
      "    epoch          : 513\n",
      "    loss           : -260247.1846875\n",
      "    val_loss       : -261526.3953125\n",
      "Train Epoch: 514 [512/54000 (1%)] Loss: -170150.609375\n",
      "Train Epoch: 514 [11776/54000 (22%)] Loss: -323717.687500\n",
      "Train Epoch: 514 [23040/54000 (43%)] Loss: -239702.437500\n",
      "Train Epoch: 514 [34304/54000 (64%)] Loss: -326895.937500\n",
      "Train Epoch: 514 [45568/54000 (84%)] Loss: -303598.156250\n",
      "    epoch          : 514\n",
      "    loss           : -260498.9625\n",
      "    val_loss       : -261497.194140625\n",
      "Train Epoch: 515 [512/54000 (1%)] Loss: -174973.578125\n",
      "Train Epoch: 515 [11776/54000 (22%)] Loss: -240824.250000\n",
      "Train Epoch: 515 [23040/54000 (43%)] Loss: -199649.640625\n",
      "Train Epoch: 515 [34304/54000 (64%)] Loss: -337078.750000\n",
      "Train Epoch: 515 [45568/54000 (84%)] Loss: -231072.078125\n",
      "    epoch          : 515\n",
      "    loss           : -260405.430625\n",
      "    val_loss       : -261409.776953125\n",
      "Train Epoch: 516 [512/54000 (1%)] Loss: -230945.562500\n",
      "Train Epoch: 516 [11776/54000 (22%)] Loss: -336805.750000\n",
      "Train Epoch: 516 [23040/54000 (43%)] Loss: -232314.968750\n",
      "Train Epoch: 516 [34304/54000 (64%)] Loss: -204626.484375\n",
      "Train Epoch: 516 [45568/54000 (84%)] Loss: -204053.250000\n",
      "    epoch          : 516\n",
      "    loss           : -260519.47609375\n",
      "    val_loss       : -262192.43671875\n",
      "Train Epoch: 517 [512/54000 (1%)] Loss: -304681.500000\n",
      "Train Epoch: 517 [11776/54000 (22%)] Loss: -324209.250000\n",
      "Train Epoch: 517 [23040/54000 (43%)] Loss: -232398.375000\n",
      "Train Epoch: 517 [34304/54000 (64%)] Loss: -226869.281250\n",
      "Train Epoch: 517 [45568/54000 (84%)] Loss: -235585.312500\n",
      "    epoch          : 517\n",
      "    loss           : -260596.82296875\n",
      "    val_loss       : -261746.979296875\n",
      "Train Epoch: 518 [512/54000 (1%)] Loss: -335430.250000\n",
      "Train Epoch: 518 [11776/54000 (22%)] Loss: -303894.375000\n",
      "Train Epoch: 518 [23040/54000 (43%)] Loss: -235929.125000\n",
      "Train Epoch: 518 [34304/54000 (64%)] Loss: -339013.406250\n",
      "Train Epoch: 518 [45568/54000 (84%)] Loss: -168446.875000\n",
      "    epoch          : 518\n",
      "    loss           : -260735.72828125\n",
      "    val_loss       : -261154.17734375\n",
      "Train Epoch: 519 [512/54000 (1%)] Loss: -203129.921875\n",
      "Train Epoch: 519 [11776/54000 (22%)] Loss: -234355.078125\n",
      "Train Epoch: 519 [23040/54000 (43%)] Loss: -204619.531250\n",
      "Train Epoch: 519 [34304/54000 (64%)] Loss: -338776.343750\n",
      "Train Epoch: 519 [45568/54000 (84%)] Loss: -303902.875000\n",
      "    epoch          : 519\n",
      "    loss           : -260789.9984375\n",
      "    val_loss       : -261717.373828125\n",
      "Train Epoch: 520 [512/54000 (1%)] Loss: -171104.031250\n",
      "Train Epoch: 520 [11776/54000 (22%)] Loss: -240597.250000\n",
      "Train Epoch: 520 [23040/54000 (43%)] Loss: -235994.593750\n",
      "Train Epoch: 520 [34304/54000 (64%)] Loss: -202390.406250\n",
      "Train Epoch: 520 [45568/54000 (84%)] Loss: -327212.718750\n",
      "    epoch          : 520\n",
      "    loss           : -260845.88109375\n",
      "    val_loss       : -262675.671875\n",
      "Train Epoch: 521 [512/54000 (1%)] Loss: -229803.187500\n",
      "Train Epoch: 521 [11776/54000 (22%)] Loss: -336417.218750\n",
      "Train Epoch: 521 [23040/54000 (43%)] Loss: -237320.718750\n",
      "Train Epoch: 521 [34304/54000 (64%)] Loss: -323742.406250\n",
      "Train Epoch: 521 [45568/54000 (84%)] Loss: -324786.625000\n",
      "    epoch          : 521\n",
      "    loss           : -261023.94984375\n",
      "    val_loss       : -261953.3296875\n",
      "Train Epoch: 522 [512/54000 (1%)] Loss: -170913.468750\n",
      "Train Epoch: 522 [11776/54000 (22%)] Loss: -233857.703125\n",
      "Train Epoch: 522 [23040/54000 (43%)] Loss: -241851.687500\n",
      "Train Epoch: 522 [34304/54000 (64%)] Loss: -203462.281250\n",
      "Train Epoch: 522 [45568/54000 (84%)] Loss: -233689.859375\n",
      "    epoch          : 522\n",
      "    loss           : -261226.34390625\n",
      "    val_loss       : -262366.346875\n",
      "Train Epoch: 523 [512/54000 (1%)] Loss: -325909.437500\n",
      "Train Epoch: 523 [11776/54000 (22%)] Loss: -338927.687500\n",
      "Train Epoch: 523 [23040/54000 (43%)] Loss: -305446.812500\n",
      "Train Epoch: 523 [34304/54000 (64%)] Loss: -321454.812500\n",
      "Train Epoch: 523 [45568/54000 (84%)] Loss: -228898.859375\n",
      "    epoch          : 523\n",
      "    loss           : -261087.08578125\n",
      "    val_loss       : -262358.816015625\n",
      "Train Epoch: 524 [512/54000 (1%)] Loss: -230408.031250\n",
      "Train Epoch: 524 [11776/54000 (22%)] Loss: -173015.593750\n",
      "Train Epoch: 524 [23040/54000 (43%)] Loss: -326452.468750\n",
      "Train Epoch: 524 [34304/54000 (64%)] Loss: -237861.281250\n",
      "Train Epoch: 524 [45568/54000 (84%)] Loss: -199980.812500\n",
      "    epoch          : 524\n",
      "    loss           : -261304.26875\n",
      "    val_loss       : -262760.98046875\n",
      "Train Epoch: 525 [512/54000 (1%)] Loss: -236557.156250\n",
      "Train Epoch: 525 [11776/54000 (22%)] Loss: -168949.906250\n",
      "Train Epoch: 525 [23040/54000 (43%)] Loss: -238054.843750\n",
      "Train Epoch: 525 [34304/54000 (64%)] Loss: -302821.968750\n",
      "Train Epoch: 525 [45568/54000 (84%)] Loss: -229112.703125\n",
      "    epoch          : 525\n",
      "    loss           : -261405.17046875\n",
      "    val_loss       : -261966.55390625\n",
      "Train Epoch: 526 [512/54000 (1%)] Loss: -171367.093750\n",
      "Train Epoch: 526 [11776/54000 (22%)] Loss: -238436.687500\n",
      "Train Epoch: 526 [23040/54000 (43%)] Loss: -235499.687500\n",
      "Train Epoch: 526 [34304/54000 (64%)] Loss: -304108.625000\n",
      "Train Epoch: 526 [45568/54000 (84%)] Loss: -326999.031250\n",
      "    epoch          : 526\n",
      "    loss           : -261431.96015625\n",
      "    val_loss       : -262007.50703125\n",
      "Train Epoch: 527 [512/54000 (1%)] Loss: -205033.671875\n",
      "Train Epoch: 527 [11776/54000 (22%)] Loss: -322064.625000\n",
      "Train Epoch: 527 [23040/54000 (43%)] Loss: -327690.812500\n",
      "Train Epoch: 527 [34304/54000 (64%)] Loss: -324006.000000\n",
      "Train Epoch: 527 [45568/54000 (84%)] Loss: -230038.781250\n",
      "    epoch          : 527\n",
      "    loss           : -261538.83125\n",
      "    val_loss       : -262385.586328125\n",
      "Train Epoch: 528 [512/54000 (1%)] Loss: -341222.468750\n",
      "Train Epoch: 528 [11776/54000 (22%)] Loss: -323984.125000\n",
      "Train Epoch: 528 [23040/54000 (43%)] Loss: -328503.812500\n",
      "Train Epoch: 528 [34304/54000 (64%)] Loss: -304376.156250\n",
      "Train Epoch: 528 [45568/54000 (84%)] Loss: -231413.078125\n",
      "    epoch          : 528\n",
      "    loss           : -261636.1409375\n",
      "    val_loss       : -263014.87890625\n",
      "Train Epoch: 529 [512/54000 (1%)] Loss: -304747.218750\n",
      "Train Epoch: 529 [11776/54000 (22%)] Loss: -339155.875000\n",
      "Train Epoch: 529 [23040/54000 (43%)] Loss: -338312.875000\n",
      "Train Epoch: 529 [34304/54000 (64%)] Loss: -239503.984375\n",
      "Train Epoch: 529 [45568/54000 (84%)] Loss: -307899.125000\n",
      "    epoch          : 529\n",
      "    loss           : -261700.305625\n",
      "    val_loss       : -262233.26328125\n",
      "Train Epoch: 530 [512/54000 (1%)] Loss: -173322.953125\n",
      "Train Epoch: 530 [11776/54000 (22%)] Loss: -232941.171875\n",
      "Train Epoch: 530 [23040/54000 (43%)] Loss: -307825.812500\n",
      "Train Epoch: 530 [34304/54000 (64%)] Loss: -238684.015625\n",
      "Train Epoch: 530 [45568/54000 (84%)] Loss: -338791.156250\n",
      "    epoch          : 530\n",
      "    loss           : -261678.1840625\n",
      "    val_loss       : -262592.416796875\n",
      "Train Epoch: 531 [512/54000 (1%)] Loss: -339889.625000\n",
      "Train Epoch: 531 [11776/54000 (22%)] Loss: -307777.937500\n",
      "Train Epoch: 531 [23040/54000 (43%)] Loss: -241606.437500\n",
      "Train Epoch: 531 [34304/54000 (64%)] Loss: -233492.375000\n",
      "Train Epoch: 531 [45568/54000 (84%)] Loss: -165493.734375\n",
      "    epoch          : 531\n",
      "    loss           : -262053.13953125\n",
      "    val_loss       : -262757.501171875\n",
      "Train Epoch: 532 [512/54000 (1%)] Loss: -243264.031250\n",
      "Train Epoch: 532 [11776/54000 (22%)] Loss: -178550.171875\n",
      "Train Epoch: 532 [23040/54000 (43%)] Loss: -301926.656250\n",
      "Train Epoch: 532 [34304/54000 (64%)] Loss: -236440.968750\n",
      "Train Epoch: 532 [45568/54000 (84%)] Loss: -237002.906250\n",
      "    epoch          : 532\n",
      "    loss           : -262091.8196875\n",
      "    val_loss       : -262707.638671875\n",
      "Train Epoch: 533 [512/54000 (1%)] Loss: -326602.687500\n",
      "Train Epoch: 533 [11776/54000 (22%)] Loss: -241824.437500\n",
      "Train Epoch: 533 [23040/54000 (43%)] Loss: -324239.437500\n",
      "Train Epoch: 533 [34304/54000 (64%)] Loss: -341539.437500\n",
      "Train Epoch: 533 [45568/54000 (84%)] Loss: -340287.187500\n",
      "    epoch          : 533\n",
      "    loss           : -261981.32984375\n",
      "    val_loss       : -263041.570703125\n",
      "Train Epoch: 534 [512/54000 (1%)] Loss: -234603.031250\n",
      "Train Epoch: 534 [11776/54000 (22%)] Loss: -323363.812500\n",
      "Train Epoch: 534 [23040/54000 (43%)] Loss: -237539.562500\n",
      "Train Epoch: 534 [34304/54000 (64%)] Loss: -240199.281250\n",
      "Train Epoch: 534 [45568/54000 (84%)] Loss: -329447.125000\n",
      "    epoch          : 534\n",
      "    loss           : -262094.9759375\n",
      "    val_loss       : -262918.2515625\n",
      "Train Epoch: 535 [512/54000 (1%)] Loss: -232910.046875\n",
      "Train Epoch: 535 [11776/54000 (22%)] Loss: -303258.625000\n",
      "Train Epoch: 535 [23040/54000 (43%)] Loss: -339230.343750\n",
      "Train Epoch: 535 [34304/54000 (64%)] Loss: -234262.750000\n",
      "Train Epoch: 535 [45568/54000 (84%)] Loss: -231857.453125\n",
      "    epoch          : 535\n",
      "    loss           : -262245.03546875\n",
      "    val_loss       : -263477.523046875\n",
      "Train Epoch: 536 [512/54000 (1%)] Loss: -173145.000000\n",
      "Train Epoch: 536 [11776/54000 (22%)] Loss: -236379.953125\n",
      "Train Epoch: 536 [23040/54000 (43%)] Loss: -236627.937500\n",
      "Train Epoch: 536 [34304/54000 (64%)] Loss: -205320.328125\n",
      "Train Epoch: 536 [45568/54000 (84%)] Loss: -230416.000000\n",
      "    epoch          : 536\n",
      "    loss           : -262317.99859375\n",
      "    val_loss       : -262871.089453125\n",
      "Train Epoch: 537 [512/54000 (1%)] Loss: -305379.125000\n",
      "Train Epoch: 537 [11776/54000 (22%)] Loss: -206619.343750\n",
      "Train Epoch: 537 [23040/54000 (43%)] Loss: -243005.531250\n",
      "Train Epoch: 537 [34304/54000 (64%)] Loss: -239116.656250\n",
      "Train Epoch: 537 [45568/54000 (84%)] Loss: -327560.750000\n",
      "    epoch          : 537\n",
      "    loss           : -262486.26625\n",
      "    val_loss       : -263512.37734375\n",
      "Train Epoch: 538 [512/54000 (1%)] Loss: -327301.437500\n",
      "Train Epoch: 538 [11776/54000 (22%)] Loss: -240090.000000\n",
      "Train Epoch: 538 [23040/54000 (43%)] Loss: -328202.625000\n",
      "Train Epoch: 538 [34304/54000 (64%)] Loss: -327543.187500\n",
      "Train Epoch: 538 [45568/54000 (84%)] Loss: -230871.281250\n",
      "    epoch          : 538\n",
      "    loss           : -262553.19078125\n",
      "    val_loss       : -263076.89921875\n",
      "Train Epoch: 539 [512/54000 (1%)] Loss: -322176.437500\n",
      "Train Epoch: 539 [11776/54000 (22%)] Loss: -233896.890625\n",
      "Train Epoch: 539 [23040/54000 (43%)] Loss: -322968.937500\n",
      "Train Epoch: 539 [34304/54000 (64%)] Loss: -207294.406250\n",
      "Train Epoch: 539 [45568/54000 (84%)] Loss: -208793.312500\n",
      "    epoch          : 539\n",
      "    loss           : -262525.23890625\n",
      "    val_loss       : -263148.87578125\n",
      "Train Epoch: 540 [512/54000 (1%)] Loss: -236130.328125\n",
      "Train Epoch: 540 [11776/54000 (22%)] Loss: -234768.859375\n",
      "Train Epoch: 540 [23040/54000 (43%)] Loss: -342133.375000\n",
      "Train Epoch: 540 [34304/54000 (64%)] Loss: -231168.531250\n",
      "Train Epoch: 540 [45568/54000 (84%)] Loss: -329373.437500\n",
      "    epoch          : 540\n",
      "    loss           : -262719.4778125\n",
      "    val_loss       : -263417.684765625\n",
      "Train Epoch: 541 [512/54000 (1%)] Loss: -234928.312500\n",
      "Train Epoch: 541 [11776/54000 (22%)] Loss: -304339.218750\n",
      "Train Epoch: 541 [23040/54000 (43%)] Loss: -233243.500000\n",
      "Train Epoch: 541 [34304/54000 (64%)] Loss: -342173.000000\n",
      "Train Epoch: 541 [45568/54000 (84%)] Loss: -235501.625000\n",
      "    epoch          : 541\n",
      "    loss           : -262795.7715625\n",
      "    val_loss       : -263928.437890625\n",
      "Train Epoch: 542 [512/54000 (1%)] Loss: -325504.812500\n",
      "Train Epoch: 542 [11776/54000 (22%)] Loss: -325498.000000\n",
      "Train Epoch: 542 [23040/54000 (43%)] Loss: -325722.562500\n",
      "Train Epoch: 542 [34304/54000 (64%)] Loss: -209328.187500\n",
      "Train Epoch: 542 [45568/54000 (84%)] Loss: -243107.500000\n",
      "    epoch          : 542\n",
      "    loss           : -262791.10890625\n",
      "    val_loss       : -263998.240625\n",
      "Train Epoch: 543 [512/54000 (1%)] Loss: -229660.734375\n",
      "Train Epoch: 543 [11776/54000 (22%)] Loss: -178861.750000\n",
      "Train Epoch: 543 [23040/54000 (43%)] Loss: -167574.750000\n",
      "Train Epoch: 543 [34304/54000 (64%)] Loss: -323963.156250\n",
      "Train Epoch: 543 [45568/54000 (84%)] Loss: -205041.031250\n",
      "    epoch          : 543\n",
      "    loss           : -262973.82234375\n",
      "    val_loss       : -264074.251953125\n",
      "Train Epoch: 544 [512/54000 (1%)] Loss: -237661.156250\n",
      "Train Epoch: 544 [11776/54000 (22%)] Loss: -228455.812500\n",
      "Train Epoch: 544 [23040/54000 (43%)] Loss: -232785.875000\n",
      "Train Epoch: 544 [34304/54000 (64%)] Loss: -327810.406250\n",
      "Train Epoch: 544 [45568/54000 (84%)] Loss: -331363.656250\n",
      "    epoch          : 544\n",
      "    loss           : -263108.0453125\n",
      "    val_loss       : -263476.75078125\n",
      "Train Epoch: 545 [512/54000 (1%)] Loss: -170227.656250\n",
      "Train Epoch: 545 [11776/54000 (22%)] Loss: -203726.656250\n",
      "Train Epoch: 545 [23040/54000 (43%)] Loss: -240025.937500\n",
      "Train Epoch: 545 [34304/54000 (64%)] Loss: -236397.468750\n",
      "Train Epoch: 545 [45568/54000 (84%)] Loss: -332312.000000\n",
      "    epoch          : 545\n",
      "    loss           : -263273.7253125\n",
      "    val_loss       : -263806.79375\n",
      "Train Epoch: 546 [512/54000 (1%)] Loss: -329926.250000\n",
      "Train Epoch: 546 [11776/54000 (22%)] Loss: -238862.187500\n",
      "Train Epoch: 546 [23040/54000 (43%)] Loss: -326361.781250\n",
      "Train Epoch: 546 [34304/54000 (64%)] Loss: -239004.171875\n",
      "Train Epoch: 546 [45568/54000 (84%)] Loss: -307310.750000\n",
      "    epoch          : 546\n",
      "    loss           : -263382.48078125\n",
      "    val_loss       : -263695.04375\n",
      "Train Epoch: 547 [512/54000 (1%)] Loss: -326192.343750\n",
      "Train Epoch: 547 [11776/54000 (22%)] Loss: -325208.093750\n",
      "Train Epoch: 547 [23040/54000 (43%)] Loss: -343565.187500\n",
      "Train Epoch: 547 [34304/54000 (64%)] Loss: -344632.375000\n",
      "Train Epoch: 547 [45568/54000 (84%)] Loss: -330212.062500\n",
      "    epoch          : 547\n",
      "    loss           : -263317.3346875\n",
      "    val_loss       : -264715.434375\n",
      "Train Epoch: 548 [512/54000 (1%)] Loss: -233076.000000\n",
      "Train Epoch: 548 [11776/54000 (22%)] Loss: -240927.265625\n",
      "Train Epoch: 548 [23040/54000 (43%)] Loss: -173230.921875\n",
      "Train Epoch: 548 [34304/54000 (64%)] Loss: -327792.906250\n",
      "Train Epoch: 548 [45568/54000 (84%)] Loss: -241319.500000\n",
      "    epoch          : 548\n",
      "    loss           : -263219.91984375\n",
      "    val_loss       : -264596.740625\n",
      "Train Epoch: 549 [512/54000 (1%)] Loss: -210994.859375\n",
      "Train Epoch: 549 [11776/54000 (22%)] Loss: -232319.218750\n",
      "Train Epoch: 549 [23040/54000 (43%)] Loss: -235233.812500\n",
      "Train Epoch: 549 [34304/54000 (64%)] Loss: -231822.593750\n",
      "Train Epoch: 549 [45568/54000 (84%)] Loss: -200111.390625\n",
      "    epoch          : 549\n",
      "    loss           : -263547.16640625\n",
      "    val_loss       : -265209.073828125\n",
      "Train Epoch: 550 [512/54000 (1%)] Loss: -170707.359375\n",
      "Train Epoch: 550 [11776/54000 (22%)] Loss: -237967.656250\n",
      "Train Epoch: 550 [23040/54000 (43%)] Loss: -304966.125000\n",
      "Train Epoch: 550 [34304/54000 (64%)] Loss: -326812.281250\n",
      "Train Epoch: 550 [45568/54000 (84%)] Loss: -240418.625000\n",
      "    epoch          : 550\n",
      "    loss           : -263667.7390625\n",
      "    val_loss       : -265456.57109375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch550.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 551 [512/54000 (1%)] Loss: -324819.750000\n",
      "Train Epoch: 551 [11776/54000 (22%)] Loss: -309784.187500\n",
      "Train Epoch: 551 [23040/54000 (43%)] Loss: -308321.312500\n",
      "Train Epoch: 551 [34304/54000 (64%)] Loss: -204046.875000\n",
      "Train Epoch: 551 [45568/54000 (84%)] Loss: -306299.906250\n",
      "    epoch          : 551\n",
      "    loss           : -263642.14203125\n",
      "    val_loss       : -264842.740625\n",
      "Train Epoch: 552 [512/54000 (1%)] Loss: -238077.687500\n",
      "Train Epoch: 552 [11776/54000 (22%)] Loss: -329610.781250\n",
      "Train Epoch: 552 [23040/54000 (43%)] Loss: -342121.562500\n",
      "Train Epoch: 552 [34304/54000 (64%)] Loss: -234630.062500\n",
      "Train Epoch: 552 [45568/54000 (84%)] Loss: -312186.156250\n",
      "    epoch          : 552\n",
      "    loss           : -263782.8678125\n",
      "    val_loss       : -264564.444921875\n",
      "Train Epoch: 553 [512/54000 (1%)] Loss: -343045.656250\n",
      "Train Epoch: 553 [11776/54000 (22%)] Loss: -232433.218750\n",
      "Train Epoch: 553 [23040/54000 (43%)] Loss: -236590.937500\n",
      "Train Epoch: 553 [34304/54000 (64%)] Loss: -309659.687500\n",
      "Train Epoch: 553 [45568/54000 (84%)] Loss: -331395.968750\n",
      "    epoch          : 553\n",
      "    loss           : -263882.5628125\n",
      "    val_loss       : -264923.405859375\n",
      "Train Epoch: 554 [512/54000 (1%)] Loss: -242594.750000\n",
      "Train Epoch: 554 [11776/54000 (22%)] Loss: -241047.156250\n",
      "Train Epoch: 554 [23040/54000 (43%)] Loss: -237981.250000\n",
      "Train Epoch: 554 [34304/54000 (64%)] Loss: -241592.593750\n",
      "Train Epoch: 554 [45568/54000 (84%)] Loss: -204429.656250\n",
      "    epoch          : 554\n",
      "    loss           : -263922.61703125\n",
      "    val_loss       : -264557.015625\n",
      "Train Epoch: 555 [512/54000 (1%)] Loss: -328438.593750\n",
      "Train Epoch: 555 [11776/54000 (22%)] Loss: -238283.593750\n",
      "Train Epoch: 555 [23040/54000 (43%)] Loss: -199650.906250\n",
      "Train Epoch: 555 [34304/54000 (64%)] Loss: -245755.937500\n",
      "Train Epoch: 555 [45568/54000 (84%)] Loss: -240345.812500\n",
      "    epoch          : 555\n",
      "    loss           : -264050.366875\n",
      "    val_loss       : -264880.191796875\n",
      "Train Epoch: 556 [512/54000 (1%)] Loss: -233397.015625\n",
      "Train Epoch: 556 [11776/54000 (22%)] Loss: -239128.500000\n",
      "Train Epoch: 556 [23040/54000 (43%)] Loss: -343910.281250\n",
      "Train Epoch: 556 [34304/54000 (64%)] Loss: -236819.031250\n",
      "Train Epoch: 556 [45568/54000 (84%)] Loss: -330446.312500\n",
      "    epoch          : 556\n",
      "    loss           : -264061.59515625\n",
      "    val_loss       : -264852.558984375\n",
      "Train Epoch: 557 [512/54000 (1%)] Loss: -232784.015625\n",
      "Train Epoch: 557 [11776/54000 (22%)] Loss: -332043.500000\n",
      "Train Epoch: 557 [23040/54000 (43%)] Loss: -328666.906250\n",
      "Train Epoch: 557 [34304/54000 (64%)] Loss: -239283.750000\n",
      "Train Epoch: 557 [45568/54000 (84%)] Loss: -309384.218750\n",
      "    epoch          : 557\n",
      "    loss           : -264203.7665625\n",
      "    val_loss       : -264826.015625\n",
      "Train Epoch: 558 [512/54000 (1%)] Loss: -325847.937500\n",
      "Train Epoch: 558 [11776/54000 (22%)] Loss: -208232.687500\n",
      "Train Epoch: 558 [23040/54000 (43%)] Loss: -234549.375000\n",
      "Train Epoch: 558 [34304/54000 (64%)] Loss: -240422.312500\n",
      "Train Epoch: 558 [45568/54000 (84%)] Loss: -244560.890625\n",
      "    epoch          : 558\n",
      "    loss           : -264376.1575\n",
      "    val_loss       : -265468.04453125\n",
      "Train Epoch: 559 [512/54000 (1%)] Loss: -244197.312500\n",
      "Train Epoch: 559 [11776/54000 (22%)] Loss: -241606.968750\n",
      "Train Epoch: 559 [23040/54000 (43%)] Loss: -241322.953125\n",
      "Train Epoch: 559 [34304/54000 (64%)] Loss: -170092.093750\n",
      "Train Epoch: 559 [45568/54000 (84%)] Loss: -232165.562500\n",
      "    epoch          : 559\n",
      "    loss           : -264323.2596875\n",
      "    val_loss       : -265445.86015625\n",
      "Train Epoch: 560 [512/54000 (1%)] Loss: -311425.593750\n",
      "Train Epoch: 560 [11776/54000 (22%)] Loss: -328827.218750\n",
      "Train Epoch: 560 [23040/54000 (43%)] Loss: -209554.546875\n",
      "Train Epoch: 560 [34304/54000 (64%)] Loss: -345088.718750\n",
      "Train Epoch: 560 [45568/54000 (84%)] Loss: -210752.250000\n",
      "    epoch          : 560\n",
      "    loss           : -264455.2721875\n",
      "    val_loss       : -264924.1390625\n",
      "Train Epoch: 561 [512/54000 (1%)] Loss: -172679.843750\n",
      "Train Epoch: 561 [11776/54000 (22%)] Loss: -331824.468750\n",
      "Train Epoch: 561 [23040/54000 (43%)] Loss: -244979.406250\n",
      "Train Epoch: 561 [34304/54000 (64%)] Loss: -238994.578125\n",
      "Train Epoch: 561 [45568/54000 (84%)] Loss: -313317.812500\n",
      "    epoch          : 561\n",
      "    loss           : -264534.08859375\n",
      "    val_loss       : -265566.456640625\n",
      "Train Epoch: 562 [512/54000 (1%)] Loss: -206981.296875\n",
      "Train Epoch: 562 [11776/54000 (22%)] Loss: -202182.625000\n",
      "Train Epoch: 562 [23040/54000 (43%)] Loss: -345575.000000\n",
      "Train Epoch: 562 [34304/54000 (64%)] Loss: -329663.812500\n",
      "Train Epoch: 562 [45568/54000 (84%)] Loss: -306735.625000\n",
      "    epoch          : 562\n",
      "    loss           : -264478.7465625\n",
      "    val_loss       : -266003.13984375\n",
      "Train Epoch: 563 [512/54000 (1%)] Loss: -238331.843750\n",
      "Train Epoch: 563 [11776/54000 (22%)] Loss: -174060.781250\n",
      "Train Epoch: 563 [23040/54000 (43%)] Loss: -311342.000000\n",
      "Train Epoch: 563 [34304/54000 (64%)] Loss: -243404.109375\n",
      "Train Epoch: 563 [45568/54000 (84%)] Loss: -309660.218750\n",
      "    epoch          : 563\n",
      "    loss           : -264680.02921875\n",
      "    val_loss       : -265654.71953125\n",
      "Train Epoch: 564 [512/54000 (1%)] Loss: -345804.531250\n",
      "Train Epoch: 564 [11776/54000 (22%)] Loss: -308214.000000\n",
      "Train Epoch: 564 [23040/54000 (43%)] Loss: -243497.343750\n",
      "Train Epoch: 564 [34304/54000 (64%)] Loss: -309033.500000\n",
      "Train Epoch: 564 [45568/54000 (84%)] Loss: -234240.562500\n",
      "    epoch          : 564\n",
      "    loss           : -264852.339375\n",
      "    val_loss       : -266048.56015625\n",
      "Train Epoch: 565 [512/54000 (1%)] Loss: -327139.437500\n",
      "Train Epoch: 565 [11776/54000 (22%)] Loss: -326936.562500\n",
      "Train Epoch: 565 [23040/54000 (43%)] Loss: -242307.500000\n",
      "Train Epoch: 565 [34304/54000 (64%)] Loss: -308440.281250\n",
      "Train Epoch: 565 [45568/54000 (84%)] Loss: -247361.875000\n",
      "    epoch          : 565\n",
      "    loss           : -264871.52125\n",
      "    val_loss       : -265857.65078125\n",
      "Train Epoch: 566 [512/54000 (1%)] Loss: -175495.390625\n",
      "Train Epoch: 566 [11776/54000 (22%)] Loss: -234321.843750\n",
      "Train Epoch: 566 [23040/54000 (43%)] Loss: -306951.500000\n",
      "Train Epoch: 566 [34304/54000 (64%)] Loss: -240465.531250\n",
      "Train Epoch: 566 [45568/54000 (84%)] Loss: -330915.000000\n",
      "    epoch          : 566\n",
      "    loss           : -264916.57671875\n",
      "    val_loss       : -265804.584765625\n",
      "Train Epoch: 567 [512/54000 (1%)] Loss: -168991.843750\n",
      "Train Epoch: 567 [11776/54000 (22%)] Loss: -236777.843750\n",
      "Train Epoch: 567 [23040/54000 (43%)] Loss: -237940.093750\n",
      "Train Epoch: 567 [34304/54000 (64%)] Loss: -345599.656250\n",
      "Train Epoch: 567 [45568/54000 (84%)] Loss: -327824.312500\n",
      "    epoch          : 567\n",
      "    loss           : -265031.70890625\n",
      "    val_loss       : -266369.4609375\n",
      "Train Epoch: 568 [512/54000 (1%)] Loss: -328306.781250\n",
      "Train Epoch: 568 [11776/54000 (22%)] Loss: -243207.578125\n",
      "Train Epoch: 568 [23040/54000 (43%)] Loss: -241108.765625\n",
      "Train Epoch: 568 [34304/54000 (64%)] Loss: -242273.343750\n",
      "Train Epoch: 568 [45568/54000 (84%)] Loss: -333098.687500\n",
      "    epoch          : 568\n",
      "    loss           : -264979.69125\n",
      "    val_loss       : -266442.901953125\n",
      "Train Epoch: 569 [512/54000 (1%)] Loss: -309840.375000\n",
      "Train Epoch: 569 [11776/54000 (22%)] Loss: -346047.437500\n",
      "Train Epoch: 569 [23040/54000 (43%)] Loss: -236913.734375\n",
      "Train Epoch: 569 [34304/54000 (64%)] Loss: -328433.718750\n",
      "Train Epoch: 569 [45568/54000 (84%)] Loss: -310747.562500\n",
      "    epoch          : 569\n",
      "    loss           : -265152.4296875\n",
      "    val_loss       : -266172.366796875\n",
      "Train Epoch: 570 [512/54000 (1%)] Loss: -176464.718750\n",
      "Train Epoch: 570 [11776/54000 (22%)] Loss: -346866.000000\n",
      "Train Epoch: 570 [23040/54000 (43%)] Loss: -329044.000000\n",
      "Train Epoch: 570 [34304/54000 (64%)] Loss: -240719.109375\n",
      "Train Epoch: 570 [45568/54000 (84%)] Loss: -236561.828125\n",
      "    epoch          : 570\n",
      "    loss           : -265275.165625\n",
      "    val_loss       : -266214.899609375\n",
      "Train Epoch: 571 [512/54000 (1%)] Loss: -242392.812500\n",
      "Train Epoch: 571 [11776/54000 (22%)] Loss: -204951.281250\n",
      "Train Epoch: 571 [23040/54000 (43%)] Loss: -328859.500000\n",
      "Train Epoch: 571 [34304/54000 (64%)] Loss: -243883.843750\n",
      "Train Epoch: 571 [45568/54000 (84%)] Loss: -205941.906250\n",
      "    epoch          : 571\n",
      "    loss           : -265357.9596875\n",
      "    val_loss       : -266554.09765625\n",
      "Train Epoch: 572 [512/54000 (1%)] Loss: -234278.625000\n",
      "Train Epoch: 572 [11776/54000 (22%)] Loss: -234539.343750\n",
      "Train Epoch: 572 [23040/54000 (43%)] Loss: -242654.562500\n",
      "Train Epoch: 572 [34304/54000 (64%)] Loss: -332259.187500\n",
      "Train Epoch: 572 [45568/54000 (84%)] Loss: -210538.562500\n",
      "    epoch          : 572\n",
      "    loss           : -265398.85\n",
      "    val_loss       : -266306.898046875\n",
      "Train Epoch: 573 [512/54000 (1%)] Loss: -241909.171875\n",
      "Train Epoch: 573 [11776/54000 (22%)] Loss: -235538.109375\n",
      "Train Epoch: 573 [23040/54000 (43%)] Loss: -334163.937500\n",
      "Train Epoch: 573 [34304/54000 (64%)] Loss: -241300.406250\n",
      "Train Epoch: 573 [45568/54000 (84%)] Loss: -307033.875000\n",
      "    epoch          : 573\n",
      "    loss           : -265508.68484375\n",
      "    val_loss       : -266005.5515625\n",
      "Train Epoch: 574 [512/54000 (1%)] Loss: -310294.375000\n",
      "Train Epoch: 574 [11776/54000 (22%)] Loss: -314245.187500\n",
      "Train Epoch: 574 [23040/54000 (43%)] Loss: -344810.437500\n",
      "Train Epoch: 574 [34304/54000 (64%)] Loss: -333156.343750\n",
      "Train Epoch: 574 [45568/54000 (84%)] Loss: -209530.390625\n",
      "    epoch          : 574\n",
      "    loss           : -265727.13125\n",
      "    val_loss       : -266448.406640625\n",
      "Train Epoch: 575 [512/54000 (1%)] Loss: -180590.640625\n",
      "Train Epoch: 575 [11776/54000 (22%)] Loss: -232844.140625\n",
      "Train Epoch: 575 [23040/54000 (43%)] Loss: -310828.812500\n",
      "Train Epoch: 575 [34304/54000 (64%)] Loss: -310086.687500\n",
      "Train Epoch: 575 [45568/54000 (84%)] Loss: -237443.375000\n",
      "    epoch          : 575\n",
      "    loss           : -265654.61484375\n",
      "    val_loss       : -266885.802734375\n",
      "Train Epoch: 576 [512/54000 (1%)] Loss: -174703.234375\n",
      "Train Epoch: 576 [11776/54000 (22%)] Loss: -241154.453125\n",
      "Train Epoch: 576 [23040/54000 (43%)] Loss: -242114.968750\n",
      "Train Epoch: 576 [34304/54000 (64%)] Loss: -237935.437500\n",
      "Train Epoch: 576 [45568/54000 (84%)] Loss: -235636.531250\n",
      "    epoch          : 576\n",
      "    loss           : -265777.154375\n",
      "    val_loss       : -266188.5640625\n",
      "Train Epoch: 577 [512/54000 (1%)] Loss: -332334.218750\n",
      "Train Epoch: 577 [11776/54000 (22%)] Loss: -344407.375000\n",
      "Train Epoch: 577 [23040/54000 (43%)] Loss: -346964.062500\n",
      "Train Epoch: 577 [34304/54000 (64%)] Loss: -332224.156250\n",
      "Train Epoch: 577 [45568/54000 (84%)] Loss: -200298.593750\n",
      "    epoch          : 577\n",
      "    loss           : -265801.37515625\n",
      "    val_loss       : -267153.529296875\n",
      "Train Epoch: 578 [512/54000 (1%)] Loss: -332958.000000\n",
      "Train Epoch: 578 [11776/54000 (22%)] Loss: -169419.718750\n",
      "Train Epoch: 578 [23040/54000 (43%)] Loss: -345681.812500\n",
      "Train Epoch: 578 [34304/54000 (64%)] Loss: -171694.140625\n",
      "Train Epoch: 578 [45568/54000 (84%)] Loss: -209838.031250\n",
      "    epoch          : 578\n",
      "    loss           : -265809.306875\n",
      "    val_loss       : -266956.673828125\n",
      "Train Epoch: 579 [512/54000 (1%)] Loss: -344041.687500\n",
      "Train Epoch: 579 [11776/54000 (22%)] Loss: -178107.203125\n",
      "Train Epoch: 579 [23040/54000 (43%)] Loss: -312327.375000\n",
      "Train Epoch: 579 [34304/54000 (64%)] Loss: -332812.125000\n",
      "Train Epoch: 579 [45568/54000 (84%)] Loss: -333297.187500\n",
      "    epoch          : 579\n",
      "    loss           : -266025.65703125\n",
      "    val_loss       : -267102.055859375\n",
      "Train Epoch: 580 [512/54000 (1%)] Loss: -312968.437500\n",
      "Train Epoch: 580 [11776/54000 (22%)] Loss: -309549.500000\n",
      "Train Epoch: 580 [23040/54000 (43%)] Loss: -331965.875000\n",
      "Train Epoch: 580 [34304/54000 (64%)] Loss: -173975.500000\n",
      "Train Epoch: 580 [45568/54000 (84%)] Loss: -240385.109375\n",
      "    epoch          : 580\n",
      "    loss           : -265956.4009375\n",
      "    val_loss       : -266992.043359375\n",
      "Train Epoch: 581 [512/54000 (1%)] Loss: -348277.187500\n",
      "Train Epoch: 581 [11776/54000 (22%)] Loss: -334514.125000\n",
      "Train Epoch: 581 [23040/54000 (43%)] Loss: -328822.812500\n",
      "Train Epoch: 581 [34304/54000 (64%)] Loss: -234856.296875\n",
      "Train Epoch: 581 [45568/54000 (84%)] Loss: -240402.078125\n",
      "    epoch          : 581\n",
      "    loss           : -266098.01796875\n",
      "    val_loss       : -267073.9734375\n",
      "Train Epoch: 582 [512/54000 (1%)] Loss: -245200.171875\n",
      "Train Epoch: 582 [11776/54000 (22%)] Loss: -238635.593750\n",
      "Train Epoch: 582 [23040/54000 (43%)] Loss: -310153.937500\n",
      "Train Epoch: 582 [34304/54000 (64%)] Loss: -347259.375000\n",
      "Train Epoch: 582 [45568/54000 (84%)] Loss: -311766.000000\n",
      "    epoch          : 582\n",
      "    loss           : -266267.0875\n",
      "    val_loss       : -267625.116796875\n",
      "Train Epoch: 583 [512/54000 (1%)] Loss: -329084.875000\n",
      "Train Epoch: 583 [11776/54000 (22%)] Loss: -329754.687500\n",
      "Train Epoch: 583 [23040/54000 (43%)] Loss: -170394.500000\n",
      "Train Epoch: 583 [34304/54000 (64%)] Loss: -180804.218750\n",
      "Train Epoch: 583 [45568/54000 (84%)] Loss: -231937.500000\n",
      "    epoch          : 583\n",
      "    loss           : -266423.2521875\n",
      "    val_loss       : -267477.156640625\n",
      "Train Epoch: 584 [512/54000 (1%)] Loss: -235853.703125\n",
      "Train Epoch: 584 [11776/54000 (22%)] Loss: -312232.031250\n",
      "Train Epoch: 584 [23040/54000 (43%)] Loss: -238333.046875\n",
      "Train Epoch: 584 [34304/54000 (64%)] Loss: -246965.781250\n",
      "Train Epoch: 584 [45568/54000 (84%)] Loss: -201546.843750\n",
      "    epoch          : 584\n",
      "    loss           : -266392.1359375\n",
      "    val_loss       : -267738.787109375\n",
      "Train Epoch: 585 [512/54000 (1%)] Loss: -242586.468750\n",
      "Train Epoch: 585 [11776/54000 (22%)] Loss: -206328.765625\n",
      "Train Epoch: 585 [23040/54000 (43%)] Loss: -310660.718750\n",
      "Train Epoch: 585 [34304/54000 (64%)] Loss: -239919.890625\n",
      "Train Epoch: 585 [45568/54000 (84%)] Loss: -235155.000000\n",
      "    epoch          : 585\n",
      "    loss           : -266487.02921875\n",
      "    val_loss       : -267542.760546875\n",
      "Train Epoch: 586 [512/54000 (1%)] Loss: -205941.281250\n",
      "Train Epoch: 586 [11776/54000 (22%)] Loss: -331962.500000\n",
      "Train Epoch: 586 [23040/54000 (43%)] Loss: -345657.687500\n",
      "Train Epoch: 586 [34304/54000 (64%)] Loss: -239623.328125\n",
      "Train Epoch: 586 [45568/54000 (84%)] Loss: -233773.593750\n",
      "    epoch          : 586\n",
      "    loss           : -266532.96625\n",
      "    val_loss       : -267148.9734375\n",
      "Train Epoch: 587 [512/54000 (1%)] Loss: -175069.656250\n",
      "Train Epoch: 587 [11776/54000 (22%)] Loss: -331091.500000\n",
      "Train Epoch: 587 [23040/54000 (43%)] Loss: -331778.875000\n",
      "Train Epoch: 587 [34304/54000 (64%)] Loss: -208208.562500\n",
      "Train Epoch: 587 [45568/54000 (84%)] Loss: -237693.484375\n",
      "    epoch          : 587\n",
      "    loss           : -266612.39125\n",
      "    val_loss       : -267184.68984375\n",
      "Train Epoch: 588 [512/54000 (1%)] Loss: -178951.921875\n",
      "Train Epoch: 588 [11776/54000 (22%)] Loss: -240920.265625\n",
      "Train Epoch: 588 [23040/54000 (43%)] Loss: -238279.562500\n",
      "Train Epoch: 588 [34304/54000 (64%)] Loss: -204335.875000\n",
      "Train Epoch: 588 [45568/54000 (84%)] Loss: -238339.546875\n",
      "    epoch          : 588\n",
      "    loss           : -266602.73828125\n",
      "    val_loss       : -268083.712890625\n",
      "Train Epoch: 589 [512/54000 (1%)] Loss: -235811.578125\n",
      "Train Epoch: 589 [11776/54000 (22%)] Loss: -246658.656250\n",
      "Train Epoch: 589 [23040/54000 (43%)] Loss: -331635.125000\n",
      "Train Epoch: 589 [34304/54000 (64%)] Loss: -242101.906250\n",
      "Train Epoch: 589 [45568/54000 (84%)] Loss: -332066.812500\n",
      "    epoch          : 589\n",
      "    loss           : -266726.62421875\n",
      "    val_loss       : -267496.069140625\n",
      "Train Epoch: 590 [512/54000 (1%)] Loss: -247217.328125\n",
      "Train Epoch: 590 [11776/54000 (22%)] Loss: -245769.750000\n",
      "Train Epoch: 590 [23040/54000 (43%)] Loss: -202147.968750\n",
      "Train Epoch: 590 [34304/54000 (64%)] Loss: -335945.125000\n",
      "Train Epoch: 590 [45568/54000 (84%)] Loss: -333708.750000\n",
      "    epoch          : 590\n",
      "    loss           : -266795.30546875\n",
      "    val_loss       : -267588.9484375\n",
      "Train Epoch: 591 [512/54000 (1%)] Loss: -243601.812500\n",
      "Train Epoch: 591 [11776/54000 (22%)] Loss: -236326.687500\n",
      "Train Epoch: 591 [23040/54000 (43%)] Loss: -204684.265625\n",
      "Train Epoch: 591 [34304/54000 (64%)] Loss: -335160.750000\n",
      "Train Epoch: 591 [45568/54000 (84%)] Loss: -317371.562500\n",
      "    epoch          : 591\n",
      "    loss           : -266914.6734375\n",
      "    val_loss       : -268151.680078125\n",
      "Train Epoch: 592 [512/54000 (1%)] Loss: -246065.609375\n",
      "Train Epoch: 592 [11776/54000 (22%)] Loss: -244997.640625\n",
      "Train Epoch: 592 [23040/54000 (43%)] Loss: -348166.812500\n",
      "Train Epoch: 592 [34304/54000 (64%)] Loss: -242133.031250\n",
      "Train Epoch: 592 [45568/54000 (84%)] Loss: -206295.625000\n",
      "    epoch          : 592\n",
      "    loss           : -266952.55328125\n",
      "    val_loss       : -268034.246875\n",
      "Train Epoch: 593 [512/54000 (1%)] Loss: -313343.187500\n",
      "Train Epoch: 593 [11776/54000 (22%)] Loss: -247876.718750\n",
      "Train Epoch: 593 [23040/54000 (43%)] Loss: -330232.062500\n",
      "Train Epoch: 593 [34304/54000 (64%)] Loss: -173598.031250\n",
      "Train Epoch: 593 [45568/54000 (84%)] Loss: -206907.718750\n",
      "    epoch          : 593\n",
      "    loss           : -267069.78703125\n",
      "    val_loss       : -268447.685546875\n",
      "Train Epoch: 594 [512/54000 (1%)] Loss: -210960.125000\n",
      "Train Epoch: 594 [11776/54000 (22%)] Loss: -236265.156250\n",
      "Train Epoch: 594 [23040/54000 (43%)] Loss: -335777.593750\n",
      "Train Epoch: 594 [34304/54000 (64%)] Loss: -169002.484375\n",
      "Train Epoch: 594 [45568/54000 (84%)] Loss: -206142.312500\n",
      "    epoch          : 594\n",
      "    loss           : -267213.7025\n",
      "    val_loss       : -268046.299609375\n",
      "Train Epoch: 595 [512/54000 (1%)] Loss: -311984.468750\n",
      "Train Epoch: 595 [11776/54000 (22%)] Loss: -241476.796875\n",
      "Train Epoch: 595 [23040/54000 (43%)] Loss: -319705.937500\n",
      "Train Epoch: 595 [34304/54000 (64%)] Loss: -314122.593750\n",
      "Train Epoch: 595 [45568/54000 (84%)] Loss: -211421.984375\n",
      "    epoch          : 595\n",
      "    loss           : -267165.691875\n",
      "    val_loss       : -267783.110546875\n",
      "Train Epoch: 596 [512/54000 (1%)] Loss: -329690.187500\n",
      "Train Epoch: 596 [11776/54000 (22%)] Loss: -208344.703125\n",
      "Train Epoch: 596 [23040/54000 (43%)] Loss: -238064.125000\n",
      "Train Epoch: 596 [34304/54000 (64%)] Loss: -213554.437500\n",
      "Train Epoch: 596 [45568/54000 (84%)] Loss: -234787.437500\n",
      "    epoch          : 596\n",
      "    loss           : -267297.6615625\n",
      "    val_loss       : -269000.562109375\n",
      "Train Epoch: 597 [512/54000 (1%)] Loss: -330376.437500\n",
      "Train Epoch: 597 [11776/54000 (22%)] Loss: -331552.562500\n",
      "Train Epoch: 597 [23040/54000 (43%)] Loss: -330271.468750\n",
      "Train Epoch: 597 [34304/54000 (64%)] Loss: -241904.171875\n",
      "Train Epoch: 597 [45568/54000 (84%)] Loss: -310456.812500\n",
      "    epoch          : 597\n",
      "    loss           : -267289.36234375\n",
      "    val_loss       : -269093.71875\n",
      "Train Epoch: 598 [512/54000 (1%)] Loss: -174138.750000\n",
      "Train Epoch: 598 [11776/54000 (22%)] Loss: -238726.593750\n",
      "Train Epoch: 598 [23040/54000 (43%)] Loss: -234316.031250\n",
      "Train Epoch: 598 [34304/54000 (64%)] Loss: -213280.375000\n",
      "Train Epoch: 598 [45568/54000 (84%)] Loss: -311474.187500\n",
      "    epoch          : 598\n",
      "    loss           : -267540.39109375\n",
      "    val_loss       : -268819.25625\n",
      "Train Epoch: 599 [512/54000 (1%)] Loss: -244610.218750\n",
      "Train Epoch: 599 [11776/54000 (22%)] Loss: -336663.781250\n",
      "Train Epoch: 599 [23040/54000 (43%)] Loss: -231715.843750\n",
      "Train Epoch: 599 [34304/54000 (64%)] Loss: -169890.796875\n",
      "Train Epoch: 599 [45568/54000 (84%)] Loss: -315897.000000\n",
      "    epoch          : 599\n",
      "    loss           : -267436.145625\n",
      "    val_loss       : -268628.986328125\n",
      "Train Epoch: 600 [512/54000 (1%)] Loss: -173684.406250\n",
      "Train Epoch: 600 [11776/54000 (22%)] Loss: -242704.687500\n",
      "Train Epoch: 600 [23040/54000 (43%)] Loss: -335319.031250\n",
      "Train Epoch: 600 [34304/54000 (64%)] Loss: -314129.250000\n",
      "Train Epoch: 600 [45568/54000 (84%)] Loss: -245166.781250\n",
      "    epoch          : 600\n",
      "    loss           : -267669.468125\n",
      "    val_loss       : -268351.869921875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch600.pth ...\n",
      "Train Epoch: 601 [512/54000 (1%)] Loss: -331174.687500\n",
      "Train Epoch: 601 [11776/54000 (22%)] Loss: -208256.000000\n",
      "Train Epoch: 601 [23040/54000 (43%)] Loss: -209230.875000\n",
      "Train Epoch: 601 [34304/54000 (64%)] Loss: -234019.390625\n",
      "Train Epoch: 601 [45568/54000 (84%)] Loss: -205467.312500\n",
      "    epoch          : 601\n",
      "    loss           : -267615.48984375\n",
      "    val_loss       : -268828.4921875\n",
      "Train Epoch: 602 [512/54000 (1%)] Loss: -248004.640625\n",
      "Train Epoch: 602 [11776/54000 (22%)] Loss: -240177.234375\n",
      "Train Epoch: 602 [23040/54000 (43%)] Loss: -207948.437500\n",
      "Train Epoch: 602 [34304/54000 (64%)] Loss: -331623.875000\n",
      "Train Epoch: 602 [45568/54000 (84%)] Loss: -313421.437500\n",
      "    epoch          : 602\n",
      "    loss           : -267823.353125\n",
      "    val_loss       : -269130.27421875\n",
      "Train Epoch: 603 [512/54000 (1%)] Loss: -333311.500000\n",
      "Train Epoch: 603 [11776/54000 (22%)] Loss: -331247.000000\n",
      "Train Epoch: 603 [23040/54000 (43%)] Loss: -331416.250000\n",
      "Train Epoch: 603 [34304/54000 (64%)] Loss: -239814.640625\n",
      "Train Epoch: 603 [45568/54000 (84%)] Loss: -208089.062500\n",
      "    epoch          : 603\n",
      "    loss           : -267902.3934375\n",
      "    val_loss       : -268268.103125\n",
      "Train Epoch: 604 [512/54000 (1%)] Loss: -243799.937500\n",
      "Train Epoch: 604 [11776/54000 (22%)] Loss: -243515.218750\n",
      "Train Epoch: 604 [23040/54000 (43%)] Loss: -240739.640625\n",
      "Train Epoch: 604 [34304/54000 (64%)] Loss: -238687.765625\n",
      "Train Epoch: 604 [45568/54000 (84%)] Loss: -249799.156250\n",
      "    epoch          : 604\n",
      "    loss           : -267977.59796875\n",
      "    val_loss       : -269283.37265625\n",
      "Train Epoch: 605 [512/54000 (1%)] Loss: -243054.046875\n",
      "Train Epoch: 605 [11776/54000 (22%)] Loss: -239781.796875\n",
      "Train Epoch: 605 [23040/54000 (43%)] Loss: -242845.765625\n",
      "Train Epoch: 605 [34304/54000 (64%)] Loss: -349843.843750\n",
      "Train Epoch: 605 [45568/54000 (84%)] Loss: -207130.437500\n",
      "    epoch          : 605\n",
      "    loss           : -268037.5334375\n",
      "    val_loss       : -269051.84765625\n",
      "Train Epoch: 606 [512/54000 (1%)] Loss: -348863.000000\n",
      "Train Epoch: 606 [11776/54000 (22%)] Loss: -313108.062500\n",
      "Train Epoch: 606 [23040/54000 (43%)] Loss: -206020.968750\n",
      "Train Epoch: 606 [34304/54000 (64%)] Loss: -331277.468750\n",
      "Train Epoch: 606 [45568/54000 (84%)] Loss: -212675.718750\n",
      "    epoch          : 606\n",
      "    loss           : -268053.53046875\n",
      "    val_loss       : -269292.989453125\n",
      "Train Epoch: 607 [512/54000 (1%)] Loss: -235712.937500\n",
      "Train Epoch: 607 [11776/54000 (22%)] Loss: -245757.062500\n",
      "Train Epoch: 607 [23040/54000 (43%)] Loss: -249076.968750\n",
      "Train Epoch: 607 [34304/54000 (64%)] Loss: -333907.500000\n",
      "Train Epoch: 607 [45568/54000 (84%)] Loss: -208320.468750\n",
      "    epoch          : 607\n",
      "    loss           : -268143.3534375\n",
      "    val_loss       : -269589.973828125\n",
      "Train Epoch: 608 [512/54000 (1%)] Loss: -170784.937500\n",
      "Train Epoch: 608 [11776/54000 (22%)] Loss: -245227.890625\n",
      "Train Epoch: 608 [23040/54000 (43%)] Loss: -331604.375000\n",
      "Train Epoch: 608 [34304/54000 (64%)] Loss: -336249.125000\n",
      "Train Epoch: 608 [45568/54000 (84%)] Loss: -335634.250000\n",
      "    epoch          : 608\n",
      "    loss           : -268254.581875\n",
      "    val_loss       : -269612.161328125\n",
      "Train Epoch: 609 [512/54000 (1%)] Loss: -351125.500000\n",
      "Train Epoch: 609 [11776/54000 (22%)] Loss: -245993.953125\n",
      "Train Epoch: 609 [23040/54000 (43%)] Loss: -240713.656250\n",
      "Train Epoch: 609 [34304/54000 (64%)] Loss: -247099.312500\n",
      "Train Epoch: 609 [45568/54000 (84%)] Loss: -335506.187500\n",
      "    epoch          : 609\n",
      "    loss           : -268312.5075\n",
      "    val_loss       : -268629.191015625\n",
      "Train Epoch: 610 [512/54000 (1%)] Loss: -313538.125000\n",
      "Train Epoch: 610 [11776/54000 (22%)] Loss: -335860.843750\n",
      "Train Epoch: 610 [23040/54000 (43%)] Loss: -177144.031250\n",
      "Train Epoch: 610 [34304/54000 (64%)] Loss: -243807.968750\n",
      "Train Epoch: 610 [45568/54000 (84%)] Loss: -242554.156250\n",
      "    epoch          : 610\n",
      "    loss           : -268414.04671875\n",
      "    val_loss       : -269293.54453125\n",
      "Train Epoch: 611 [512/54000 (1%)] Loss: -248814.562500\n",
      "Train Epoch: 611 [11776/54000 (22%)] Loss: -208271.218750\n",
      "Train Epoch: 611 [23040/54000 (43%)] Loss: -241628.406250\n",
      "Train Epoch: 611 [34304/54000 (64%)] Loss: -331890.250000\n",
      "Train Epoch: 611 [45568/54000 (84%)] Loss: -338004.687500\n",
      "    epoch          : 611\n",
      "    loss           : -268466.3196875\n",
      "    val_loss       : -269290.16796875\n",
      "Train Epoch: 612 [512/54000 (1%)] Loss: -314627.281250\n",
      "Train Epoch: 612 [11776/54000 (22%)] Loss: -238036.875000\n",
      "Train Epoch: 612 [23040/54000 (43%)] Loss: -209797.250000\n",
      "Train Epoch: 612 [34304/54000 (64%)] Loss: -251220.156250\n",
      "Train Epoch: 612 [45568/54000 (84%)] Loss: -210715.296875\n",
      "    epoch          : 612\n",
      "    loss           : -268497.16609375\n",
      "    val_loss       : -269582.172265625\n",
      "Train Epoch: 613 [512/54000 (1%)] Loss: -312971.062500\n",
      "Train Epoch: 613 [11776/54000 (22%)] Loss: -174921.281250\n",
      "Train Epoch: 613 [23040/54000 (43%)] Loss: -315688.843750\n",
      "Train Epoch: 613 [34304/54000 (64%)] Loss: -248027.750000\n",
      "Train Epoch: 613 [45568/54000 (84%)] Loss: -313744.343750\n",
      "    epoch          : 613\n",
      "    loss           : -268644.725\n",
      "    val_loss       : -269574.021875\n",
      "Train Epoch: 614 [512/54000 (1%)] Loss: -349765.000000\n",
      "Train Epoch: 614 [11776/54000 (22%)] Loss: -248483.937500\n",
      "Train Epoch: 614 [23040/54000 (43%)] Loss: -315285.562500\n",
      "Train Epoch: 614 [34304/54000 (64%)] Loss: -336346.937500\n",
      "Train Epoch: 614 [45568/54000 (84%)] Loss: -317269.906250\n",
      "    epoch          : 614\n",
      "    loss           : -268631.99640625\n",
      "    val_loss       : -269455.57109375\n",
      "Train Epoch: 615 [512/54000 (1%)] Loss: -353176.000000\n",
      "Train Epoch: 615 [11776/54000 (22%)] Loss: -354052.250000\n",
      "Train Epoch: 615 [23040/54000 (43%)] Loss: -236227.296875\n",
      "Train Epoch: 615 [34304/54000 (64%)] Loss: -317569.750000\n",
      "Train Epoch: 615 [45568/54000 (84%)] Loss: -317550.000000\n",
      "    epoch          : 615\n",
      "    loss           : -268821.39265625\n",
      "    val_loss       : -269214.3421875\n",
      "Train Epoch: 616 [512/54000 (1%)] Loss: -333374.562500\n",
      "Train Epoch: 616 [11776/54000 (22%)] Loss: -336847.625000\n",
      "Train Epoch: 616 [23040/54000 (43%)] Loss: -242862.687500\n",
      "Train Epoch: 616 [34304/54000 (64%)] Loss: -313429.125000\n",
      "Train Epoch: 616 [45568/54000 (84%)] Loss: -166958.046875\n",
      "    epoch          : 616\n",
      "    loss           : -268753.89953125\n",
      "    val_loss       : -269780.903515625\n",
      "Train Epoch: 617 [512/54000 (1%)] Loss: -209402.921875\n",
      "Train Epoch: 617 [11776/54000 (22%)] Loss: -314520.562500\n",
      "Train Epoch: 617 [23040/54000 (43%)] Loss: -180917.468750\n",
      "Train Epoch: 617 [34304/54000 (64%)] Loss: -238554.812500\n",
      "Train Epoch: 617 [45568/54000 (84%)] Loss: -203010.781250\n",
      "    epoch          : 617\n",
      "    loss           : -269001.26734375\n",
      "    val_loss       : -269160.070703125\n",
      "Train Epoch: 618 [512/54000 (1%)] Loss: -242795.125000\n",
      "Train Epoch: 618 [11776/54000 (22%)] Loss: -241929.843750\n",
      "Train Epoch: 618 [23040/54000 (43%)] Loss: -331661.500000\n",
      "Train Epoch: 618 [34304/54000 (64%)] Loss: -335300.250000\n",
      "Train Epoch: 618 [45568/54000 (84%)] Loss: -233802.968750\n",
      "    epoch          : 618\n",
      "    loss           : -269042.6515625\n",
      "    val_loss       : -270015.98359375\n",
      "Train Epoch: 619 [512/54000 (1%)] Loss: -178016.968750\n",
      "Train Epoch: 619 [11776/54000 (22%)] Loss: -242748.500000\n",
      "Train Epoch: 619 [23040/54000 (43%)] Loss: -204888.734375\n",
      "Train Epoch: 619 [34304/54000 (64%)] Loss: -213776.437500\n",
      "Train Epoch: 619 [45568/54000 (84%)] Loss: -241053.968750\n",
      "    epoch          : 619\n",
      "    loss           : -269128.7903125\n",
      "    val_loss       : -270068.557421875\n",
      "Train Epoch: 620 [512/54000 (1%)] Loss: -243431.875000\n",
      "Train Epoch: 620 [11776/54000 (22%)] Loss: -240119.906250\n",
      "Train Epoch: 620 [23040/54000 (43%)] Loss: -240086.781250\n",
      "Train Epoch: 620 [34304/54000 (64%)] Loss: -240697.078125\n",
      "Train Epoch: 620 [45568/54000 (84%)] Loss: -316448.718750\n",
      "    epoch          : 620\n",
      "    loss           : -269084.34140625\n",
      "    val_loss       : -269738.66171875\n",
      "Train Epoch: 621 [512/54000 (1%)] Loss: -315202.375000\n",
      "Train Epoch: 621 [11776/54000 (22%)] Loss: -240552.937500\n",
      "Train Epoch: 621 [23040/54000 (43%)] Loss: -176485.031250\n",
      "Train Epoch: 621 [34304/54000 (64%)] Loss: -237458.390625\n",
      "Train Epoch: 621 [45568/54000 (84%)] Loss: -210244.875000\n",
      "    epoch          : 621\n",
      "    loss           : -269113.01421875\n",
      "    val_loss       : -270202.48671875\n",
      "Train Epoch: 622 [512/54000 (1%)] Loss: -170146.656250\n",
      "Train Epoch: 622 [11776/54000 (22%)] Loss: -248297.500000\n",
      "Train Epoch: 622 [23040/54000 (43%)] Loss: -338994.312500\n",
      "Train Epoch: 622 [34304/54000 (64%)] Loss: -207260.281250\n",
      "Train Epoch: 622 [45568/54000 (84%)] Loss: -335328.187500\n",
      "    epoch          : 622\n",
      "    loss           : -269228.17515625\n",
      "    val_loss       : -270390.067578125\n",
      "Train Epoch: 623 [512/54000 (1%)] Loss: -336820.031250\n",
      "Train Epoch: 623 [11776/54000 (22%)] Loss: -336183.687500\n",
      "Train Epoch: 623 [23040/54000 (43%)] Loss: -244253.640625\n",
      "Train Epoch: 623 [34304/54000 (64%)] Loss: -351177.875000\n",
      "Train Epoch: 623 [45568/54000 (84%)] Loss: -216377.859375\n",
      "    epoch          : 623\n",
      "    loss           : -269290.17484375\n",
      "    val_loss       : -270448.108203125\n",
      "Train Epoch: 624 [512/54000 (1%)] Loss: -337624.281250\n",
      "Train Epoch: 624 [11776/54000 (22%)] Loss: -320070.156250\n",
      "Train Epoch: 624 [23040/54000 (43%)] Loss: -334075.187500\n",
      "Train Epoch: 624 [34304/54000 (64%)] Loss: -176337.890625\n",
      "Train Epoch: 624 [45568/54000 (84%)] Loss: -239778.328125\n",
      "    epoch          : 624\n",
      "    loss           : -269394.96703125\n",
      "    val_loss       : -269983.06171875\n",
      "Train Epoch: 625 [512/54000 (1%)] Loss: -337692.687500\n",
      "Train Epoch: 625 [11776/54000 (22%)] Loss: -333837.625000\n",
      "Train Epoch: 625 [23040/54000 (43%)] Loss: -332776.218750\n",
      "Train Epoch: 625 [34304/54000 (64%)] Loss: -235432.218750\n",
      "Train Epoch: 625 [45568/54000 (84%)] Loss: -234124.281250\n",
      "    epoch          : 625\n",
      "    loss           : -269608.1190625\n",
      "    val_loss       : -269950.7453125\n",
      "Train Epoch: 626 [512/54000 (1%)] Loss: -240816.812500\n",
      "Train Epoch: 626 [11776/54000 (22%)] Loss: -334198.500000\n",
      "Train Epoch: 626 [23040/54000 (43%)] Loss: -351037.812500\n",
      "Train Epoch: 626 [34304/54000 (64%)] Loss: -244530.593750\n",
      "Train Epoch: 626 [45568/54000 (84%)] Loss: -237769.968750\n",
      "    epoch          : 626\n",
      "    loss           : -269625.06859375\n",
      "    val_loss       : -269852.056640625\n",
      "Train Epoch: 627 [512/54000 (1%)] Loss: -240222.250000\n",
      "Train Epoch: 627 [11776/54000 (22%)] Loss: -334186.625000\n",
      "Train Epoch: 627 [23040/54000 (43%)] Loss: -241314.156250\n",
      "Train Epoch: 627 [34304/54000 (64%)] Loss: -243108.546875\n",
      "Train Epoch: 627 [45568/54000 (84%)] Loss: -338557.406250\n",
      "    epoch          : 627\n",
      "    loss           : -269613.28546875\n",
      "    val_loss       : -271749.4\n",
      "Train Epoch: 628 [512/54000 (1%)] Loss: -239094.187500\n",
      "Train Epoch: 628 [11776/54000 (22%)] Loss: -338440.187500\n",
      "Train Epoch: 628 [23040/54000 (43%)] Loss: -355502.125000\n",
      "Train Epoch: 628 [34304/54000 (64%)] Loss: -337408.937500\n",
      "Train Epoch: 628 [45568/54000 (84%)] Loss: -203763.343750\n",
      "    epoch          : 628\n",
      "    loss           : -269694.99734375\n",
      "    val_loss       : -270829.222265625\n",
      "Train Epoch: 629 [512/54000 (1%)] Loss: -239263.500000\n",
      "Train Epoch: 629 [11776/54000 (22%)] Loss: -243688.625000\n",
      "Train Epoch: 629 [23040/54000 (43%)] Loss: -162488.281250\n",
      "Train Epoch: 629 [34304/54000 (64%)] Loss: -341907.906250\n",
      "Train Epoch: 629 [45568/54000 (84%)] Loss: -315406.281250\n",
      "    epoch          : 629\n",
      "    loss           : -269774.52328125\n",
      "    val_loss       : -271133.002734375\n",
      "Train Epoch: 630 [512/54000 (1%)] Loss: -241482.656250\n",
      "Train Epoch: 630 [11776/54000 (22%)] Loss: -335219.718750\n",
      "Train Epoch: 630 [23040/54000 (43%)] Loss: -318146.000000\n",
      "Train Epoch: 630 [34304/54000 (64%)] Loss: -335992.937500\n",
      "Train Epoch: 630 [45568/54000 (84%)] Loss: -246385.843750\n",
      "    epoch          : 630\n",
      "    loss           : -269885.4859375\n",
      "    val_loss       : -271203.697265625\n",
      "Train Epoch: 631 [512/54000 (1%)] Loss: -318280.000000\n",
      "Train Epoch: 631 [11776/54000 (22%)] Loss: -175594.484375\n",
      "Train Epoch: 631 [23040/54000 (43%)] Loss: -177252.515625\n",
      "Train Epoch: 631 [34304/54000 (64%)] Loss: -340912.062500\n",
      "Train Epoch: 631 [45568/54000 (84%)] Loss: -317236.000000\n",
      "    epoch          : 631\n",
      "    loss           : -269989.76234375\n",
      "    val_loss       : -271157.95859375\n",
      "Train Epoch: 632 [512/54000 (1%)] Loss: -354009.843750\n",
      "Train Epoch: 632 [11776/54000 (22%)] Loss: -354665.125000\n",
      "Train Epoch: 632 [23040/54000 (43%)] Loss: -172648.484375\n",
      "Train Epoch: 632 [34304/54000 (64%)] Loss: -249143.843750\n",
      "Train Epoch: 632 [45568/54000 (84%)] Loss: -245771.375000\n",
      "    epoch          : 632\n",
      "    loss           : -269979.06703125\n",
      "    val_loss       : -270994.641796875\n",
      "Train Epoch: 633 [512/54000 (1%)] Loss: -240090.156250\n",
      "Train Epoch: 633 [11776/54000 (22%)] Loss: -352170.843750\n",
      "Train Epoch: 633 [23040/54000 (43%)] Loss: -336037.937500\n",
      "Train Epoch: 633 [34304/54000 (64%)] Loss: -243378.109375\n",
      "Train Epoch: 633 [45568/54000 (84%)] Loss: -209810.562500\n",
      "    epoch          : 633\n",
      "    loss           : -270145.93359375\n",
      "    val_loss       : -271273.669921875\n",
      "Train Epoch: 634 [512/54000 (1%)] Loss: -338217.781250\n",
      "Train Epoch: 634 [11776/54000 (22%)] Loss: -239993.281250\n",
      "Train Epoch: 634 [23040/54000 (43%)] Loss: -335194.437500\n",
      "Train Epoch: 634 [34304/54000 (64%)] Loss: -247244.750000\n",
      "Train Epoch: 634 [45568/54000 (84%)] Loss: -340317.562500\n",
      "    epoch          : 634\n",
      "    loss           : -270261.8584375\n",
      "    val_loss       : -271389.429296875\n",
      "Train Epoch: 635 [512/54000 (1%)] Loss: -208335.093750\n",
      "Train Epoch: 635 [11776/54000 (22%)] Loss: -354697.218750\n",
      "Train Epoch: 635 [23040/54000 (43%)] Loss: -246933.328125\n",
      "Train Epoch: 635 [34304/54000 (64%)] Loss: -234115.968750\n",
      "Train Epoch: 635 [45568/54000 (84%)] Loss: -238198.562500\n",
      "    epoch          : 635\n",
      "    loss           : -270079.6765625\n",
      "    val_loss       : -271926.226171875\n",
      "Train Epoch: 636 [512/54000 (1%)] Loss: -179231.500000\n",
      "Train Epoch: 636 [11776/54000 (22%)] Loss: -340016.656250\n",
      "Train Epoch: 636 [23040/54000 (43%)] Loss: -318398.750000\n",
      "Train Epoch: 636 [34304/54000 (64%)] Loss: -167814.781250\n",
      "Train Epoch: 636 [45568/54000 (84%)] Loss: -235251.843750\n",
      "    epoch          : 636\n",
      "    loss           : -270337.3125\n",
      "    val_loss       : -271233.825390625\n",
      "Train Epoch: 637 [512/54000 (1%)] Loss: -165989.843750\n",
      "Train Epoch: 637 [11776/54000 (22%)] Loss: -240569.234375\n",
      "Train Epoch: 637 [23040/54000 (43%)] Loss: -334914.312500\n",
      "Train Epoch: 637 [34304/54000 (64%)] Loss: -216287.031250\n",
      "Train Epoch: 637 [45568/54000 (84%)] Loss: -319652.843750\n",
      "    epoch          : 637\n",
      "    loss           : -270201.26015625\n",
      "    val_loss       : -270809.53125\n",
      "Train Epoch: 638 [512/54000 (1%)] Loss: -246148.500000\n",
      "Train Epoch: 638 [11776/54000 (22%)] Loss: -335062.156250\n",
      "Train Epoch: 638 [23040/54000 (43%)] Loss: -319855.062500\n",
      "Train Epoch: 638 [34304/54000 (64%)] Loss: -243142.953125\n",
      "Train Epoch: 638 [45568/54000 (84%)] Loss: -236789.031250\n",
      "    epoch          : 638\n",
      "    loss           : -270332.24390625\n",
      "    val_loss       : -271553.44296875\n",
      "Train Epoch: 639 [512/54000 (1%)] Loss: -251924.375000\n",
      "Train Epoch: 639 [11776/54000 (22%)] Loss: -321183.031250\n",
      "Train Epoch: 639 [23040/54000 (43%)] Loss: -341520.437500\n",
      "Train Epoch: 639 [34304/54000 (64%)] Loss: -174110.359375\n",
      "Train Epoch: 639 [45568/54000 (84%)] Loss: -213753.421875\n",
      "    epoch          : 639\n",
      "    loss           : -270523.4678125\n",
      "    val_loss       : -271202.693359375\n",
      "Train Epoch: 640 [512/54000 (1%)] Loss: -169768.171875\n",
      "Train Epoch: 640 [11776/54000 (22%)] Loss: -246245.578125\n",
      "Train Epoch: 640 [23040/54000 (43%)] Loss: -316548.312500\n",
      "Train Epoch: 640 [34304/54000 (64%)] Loss: -209471.234375\n",
      "Train Epoch: 640 [45568/54000 (84%)] Loss: -248149.109375\n",
      "    epoch          : 640\n",
      "    loss           : -270547.59875\n",
      "    val_loss       : -271724.493359375\n",
      "Train Epoch: 641 [512/54000 (1%)] Loss: -207331.234375\n",
      "Train Epoch: 641 [11776/54000 (22%)] Loss: -353707.312500\n",
      "Train Epoch: 641 [23040/54000 (43%)] Loss: -334938.937500\n",
      "Train Epoch: 641 [34304/54000 (64%)] Loss: -175553.843750\n",
      "Train Epoch: 641 [45568/54000 (84%)] Loss: -341059.156250\n",
      "    epoch          : 641\n",
      "    loss           : -270645.11484375\n",
      "    val_loss       : -271905.803515625\n",
      "Train Epoch: 642 [512/54000 (1%)] Loss: -218940.531250\n",
      "Train Epoch: 642 [11776/54000 (22%)] Loss: -245029.078125\n",
      "Train Epoch: 642 [23040/54000 (43%)] Loss: -242932.156250\n",
      "Train Epoch: 642 [34304/54000 (64%)] Loss: -355162.281250\n",
      "Train Epoch: 642 [45568/54000 (84%)] Loss: -339397.000000\n",
      "    epoch          : 642\n",
      "    loss           : -270688.313125\n",
      "    val_loss       : -272438.26875\n",
      "Train Epoch: 643 [512/54000 (1%)] Loss: -320393.375000\n",
      "Train Epoch: 643 [11776/54000 (22%)] Loss: -243064.734375\n",
      "Train Epoch: 643 [23040/54000 (43%)] Loss: -237050.734375\n",
      "Train Epoch: 643 [34304/54000 (64%)] Loss: -237143.843750\n",
      "Train Epoch: 643 [45568/54000 (84%)] Loss: -209648.656250\n",
      "    epoch          : 643\n",
      "    loss           : -270763.73125\n",
      "    val_loss       : -271933.796875\n",
      "Train Epoch: 644 [512/54000 (1%)] Loss: -206403.187500\n",
      "Train Epoch: 644 [11776/54000 (22%)] Loss: -247029.640625\n",
      "Train Epoch: 644 [23040/54000 (43%)] Loss: -180405.156250\n",
      "Train Epoch: 644 [34304/54000 (64%)] Loss: -355234.062500\n",
      "Train Epoch: 644 [45568/54000 (84%)] Loss: -240495.156250\n",
      "    epoch          : 644\n",
      "    loss           : -270950.675625\n",
      "    val_loss       : -271654.775\n",
      "Train Epoch: 645 [512/54000 (1%)] Loss: -249551.156250\n",
      "Train Epoch: 645 [11776/54000 (22%)] Loss: -243365.875000\n",
      "Train Epoch: 645 [23040/54000 (43%)] Loss: -354150.750000\n",
      "Train Epoch: 645 [34304/54000 (64%)] Loss: -317706.312500\n",
      "Train Epoch: 645 [45568/54000 (84%)] Loss: -340977.437500\n",
      "    epoch          : 645\n",
      "    loss           : -270948.83328125\n",
      "    val_loss       : -271584.467578125\n",
      "Train Epoch: 646 [512/54000 (1%)] Loss: -340265.750000\n",
      "Train Epoch: 646 [11776/54000 (22%)] Loss: -171167.171875\n",
      "Train Epoch: 646 [23040/54000 (43%)] Loss: -251989.312500\n",
      "Train Epoch: 646 [34304/54000 (64%)] Loss: -248271.625000\n",
      "Train Epoch: 646 [45568/54000 (84%)] Loss: -317276.187500\n",
      "    epoch          : 646\n",
      "    loss           : -270890.319375\n",
      "    val_loss       : -272408.7796875\n",
      "Train Epoch: 647 [512/54000 (1%)] Loss: -341328.875000\n",
      "Train Epoch: 647 [11776/54000 (22%)] Loss: -208013.343750\n",
      "Train Epoch: 647 [23040/54000 (43%)] Loss: -340822.125000\n",
      "Train Epoch: 647 [34304/54000 (64%)] Loss: -211663.843750\n",
      "Train Epoch: 647 [45568/54000 (84%)] Loss: -320723.062500\n",
      "    epoch          : 647\n",
      "    loss           : -271120.66078125\n",
      "    val_loss       : -271326.251953125\n",
      "Train Epoch: 648 [512/54000 (1%)] Loss: -239687.906250\n",
      "Train Epoch: 648 [11776/54000 (22%)] Loss: -242103.031250\n",
      "Train Epoch: 648 [23040/54000 (43%)] Loss: -245184.234375\n",
      "Train Epoch: 648 [34304/54000 (64%)] Loss: -317960.875000\n",
      "Train Epoch: 648 [45568/54000 (84%)] Loss: -216115.562500\n",
      "    epoch          : 648\n",
      "    loss           : -270993.2975\n",
      "    val_loss       : -272795.06796875\n",
      "Train Epoch: 649 [512/54000 (1%)] Loss: -176289.953125\n",
      "Train Epoch: 649 [11776/54000 (22%)] Loss: -213734.640625\n",
      "Train Epoch: 649 [23040/54000 (43%)] Loss: -335446.593750\n",
      "Train Epoch: 649 [34304/54000 (64%)] Loss: -339313.687500\n",
      "Train Epoch: 649 [45568/54000 (84%)] Loss: -240197.062500\n",
      "    epoch          : 649\n",
      "    loss           : -271161.6784375\n",
      "    val_loss       : -272054.41171875\n",
      "Train Epoch: 650 [512/54000 (1%)] Loss: -336205.312500\n",
      "Train Epoch: 650 [11776/54000 (22%)] Loss: -250541.671875\n",
      "Train Epoch: 650 [23040/54000 (43%)] Loss: -174819.156250\n",
      "Train Epoch: 650 [34304/54000 (64%)] Loss: -240893.484375\n",
      "Train Epoch: 650 [45568/54000 (84%)] Loss: -250845.078125\n",
      "    epoch          : 650\n",
      "    loss           : -271213.89015625\n",
      "    val_loss       : -272100.215234375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch650.pth ...\n",
      "Train Epoch: 651 [512/54000 (1%)] Loss: -321675.687500\n",
      "Train Epoch: 651 [11776/54000 (22%)] Loss: -339440.375000\n",
      "Train Epoch: 651 [23040/54000 (43%)] Loss: -337934.937500\n",
      "Train Epoch: 651 [34304/54000 (64%)] Loss: -215326.687500\n",
      "Train Epoch: 651 [45568/54000 (84%)] Loss: -319103.468750\n",
      "    epoch          : 651\n",
      "    loss           : -271392.72375\n",
      "    val_loss       : -271791.751171875\n",
      "Train Epoch: 652 [512/54000 (1%)] Loss: -321768.000000\n",
      "Train Epoch: 652 [11776/54000 (22%)] Loss: -241995.187500\n",
      "Train Epoch: 652 [23040/54000 (43%)] Loss: -205081.437500\n",
      "Train Epoch: 652 [34304/54000 (64%)] Loss: -341753.312500\n",
      "Train Epoch: 652 [45568/54000 (84%)] Loss: -352722.875000\n",
      "    epoch          : 652\n",
      "    loss           : -271462.29859375\n",
      "    val_loss       : -272306.260546875\n",
      "Train Epoch: 653 [512/54000 (1%)] Loss: -238329.609375\n",
      "Train Epoch: 653 [11776/54000 (22%)] Loss: -321215.281250\n",
      "Train Epoch: 653 [23040/54000 (43%)] Loss: -341268.062500\n",
      "Train Epoch: 653 [34304/54000 (64%)] Loss: -336116.343750\n",
      "Train Epoch: 653 [45568/54000 (84%)] Loss: -354420.687500\n",
      "    epoch          : 653\n",
      "    loss           : -271603.1390625\n",
      "    val_loss       : -272249.173046875\n",
      "Train Epoch: 654 [512/54000 (1%)] Loss: -338250.937500\n",
      "Train Epoch: 654 [11776/54000 (22%)] Loss: -341588.812500\n",
      "Train Epoch: 654 [23040/54000 (43%)] Loss: -246368.703125\n",
      "Train Epoch: 654 [34304/54000 (64%)] Loss: -336727.500000\n",
      "Train Epoch: 654 [45568/54000 (84%)] Loss: -204585.500000\n",
      "    epoch          : 654\n",
      "    loss           : -271580.7671875\n",
      "    val_loss       : -272676.787890625\n",
      "Train Epoch: 655 [512/54000 (1%)] Loss: -320927.062500\n",
      "Train Epoch: 655 [11776/54000 (22%)] Loss: -243599.625000\n",
      "Train Epoch: 655 [23040/54000 (43%)] Loss: -355436.062500\n",
      "Train Epoch: 655 [34304/54000 (64%)] Loss: -241451.718750\n",
      "Train Epoch: 655 [45568/54000 (84%)] Loss: -317059.875000\n",
      "    epoch          : 655\n",
      "    loss           : -271652.658125\n",
      "    val_loss       : -272845.53984375\n",
      "Train Epoch: 656 [512/54000 (1%)] Loss: -245068.218750\n",
      "Train Epoch: 656 [11776/54000 (22%)] Loss: -170016.171875\n",
      "Train Epoch: 656 [23040/54000 (43%)] Loss: -243992.031250\n",
      "Train Epoch: 656 [34304/54000 (64%)] Loss: -209144.359375\n",
      "Train Epoch: 656 [45568/54000 (84%)] Loss: -242538.234375\n",
      "    epoch          : 656\n",
      "    loss           : -271755.82875\n",
      "    val_loss       : -273180.273828125\n",
      "Train Epoch: 657 [512/54000 (1%)] Loss: -355421.875000\n",
      "Train Epoch: 657 [11776/54000 (22%)] Loss: -356021.906250\n",
      "Train Epoch: 657 [23040/54000 (43%)] Loss: -250106.000000\n",
      "Train Epoch: 657 [34304/54000 (64%)] Loss: -212127.687500\n",
      "Train Epoch: 657 [45568/54000 (84%)] Loss: -241169.890625\n",
      "    epoch          : 657\n",
      "    loss           : -271706.29421875\n",
      "    val_loss       : -273212.45\n",
      "Train Epoch: 658 [512/54000 (1%)] Loss: -319986.000000\n",
      "Train Epoch: 658 [11776/54000 (22%)] Loss: -356988.187500\n",
      "Train Epoch: 658 [23040/54000 (43%)] Loss: -341744.562500\n",
      "Train Epoch: 658 [34304/54000 (64%)] Loss: -207966.062500\n",
      "Train Epoch: 658 [45568/54000 (84%)] Loss: -343100.312500\n",
      "    epoch          : 658\n",
      "    loss           : -271787.14359375\n",
      "    val_loss       : -272965.033203125\n",
      "Train Epoch: 659 [512/54000 (1%)] Loss: -318766.375000\n",
      "Train Epoch: 659 [11776/54000 (22%)] Loss: -339297.562500\n",
      "Train Epoch: 659 [23040/54000 (43%)] Loss: -318839.281250\n",
      "Train Epoch: 659 [34304/54000 (64%)] Loss: -213986.296875\n",
      "Train Epoch: 659 [45568/54000 (84%)] Loss: -322842.125000\n",
      "    epoch          : 659\n",
      "    loss           : -271919.19703125\n",
      "    val_loss       : -272707.3515625\n",
      "Train Epoch: 660 [512/54000 (1%)] Loss: -354381.250000\n",
      "Train Epoch: 660 [11776/54000 (22%)] Loss: -216399.843750\n",
      "Train Epoch: 660 [23040/54000 (43%)] Loss: -242780.093750\n",
      "Train Epoch: 660 [34304/54000 (64%)] Loss: -240874.062500\n",
      "Train Epoch: 660 [45568/54000 (84%)] Loss: -238055.687500\n",
      "    epoch          : 660\n",
      "    loss           : -271940.1140625\n",
      "    val_loss       : -273478.305078125\n",
      "Train Epoch: 661 [512/54000 (1%)] Loss: -249939.843750\n",
      "Train Epoch: 661 [11776/54000 (22%)] Loss: -357917.093750\n",
      "Train Epoch: 661 [23040/54000 (43%)] Loss: -325171.250000\n",
      "Train Epoch: 661 [34304/54000 (64%)] Loss: -317211.187500\n",
      "Train Epoch: 661 [45568/54000 (84%)] Loss: -247860.187500\n",
      "    epoch          : 661\n",
      "    loss           : -272005.8903125\n",
      "    val_loss       : -273069.97578125\n",
      "Train Epoch: 662 [512/54000 (1%)] Loss: -251754.890625\n",
      "Train Epoch: 662 [11776/54000 (22%)] Loss: -246148.125000\n",
      "Train Epoch: 662 [23040/54000 (43%)] Loss: -174550.593750\n",
      "Train Epoch: 662 [34304/54000 (64%)] Loss: -209310.406250\n",
      "Train Epoch: 662 [45568/54000 (84%)] Loss: -242655.109375\n",
      "    epoch          : 662\n",
      "    loss           : -272179.75828125\n",
      "    val_loss       : -273011.145703125\n",
      "Train Epoch: 663 [512/54000 (1%)] Loss: -243634.687500\n",
      "Train Epoch: 663 [11776/54000 (22%)] Loss: -319780.812500\n",
      "Train Epoch: 663 [23040/54000 (43%)] Loss: -165364.843750\n",
      "Train Epoch: 663 [34304/54000 (64%)] Loss: -176029.812500\n",
      "Train Epoch: 663 [45568/54000 (84%)] Loss: -245629.062500\n",
      "    epoch          : 663\n",
      "    loss           : -272120.81421875\n",
      "    val_loss       : -273108.307421875\n",
      "Train Epoch: 664 [512/54000 (1%)] Loss: -175339.593750\n",
      "Train Epoch: 664 [11776/54000 (22%)] Loss: -256378.421875\n",
      "Train Epoch: 664 [23040/54000 (43%)] Loss: -187757.468750\n",
      "Train Epoch: 664 [34304/54000 (64%)] Loss: -244515.468750\n",
      "Train Epoch: 664 [45568/54000 (84%)] Loss: -315937.562500\n",
      "    epoch          : 664\n",
      "    loss           : -272238.215\n",
      "    val_loss       : -272554.585546875\n",
      "Train Epoch: 665 [512/54000 (1%)] Loss: -245791.734375\n",
      "Train Epoch: 665 [11776/54000 (22%)] Loss: -247395.437500\n",
      "Train Epoch: 665 [23040/54000 (43%)] Loss: -246898.156250\n",
      "Train Epoch: 665 [34304/54000 (64%)] Loss: -342944.375000\n",
      "Train Epoch: 665 [45568/54000 (84%)] Loss: -240169.468750\n",
      "    epoch          : 665\n",
      "    loss           : -272264.68859375\n",
      "    val_loss       : -272502.503125\n",
      "Train Epoch: 666 [512/54000 (1%)] Loss: -342147.531250\n",
      "Train Epoch: 666 [11776/54000 (22%)] Loss: -354626.218750\n",
      "Train Epoch: 666 [23040/54000 (43%)] Loss: -339240.312500\n",
      "Train Epoch: 666 [34304/54000 (64%)] Loss: -355039.187500\n",
      "Train Epoch: 666 [45568/54000 (84%)] Loss: -240186.718750\n",
      "    epoch          : 666\n",
      "    loss           : -272404.64421875\n",
      "    val_loss       : -273657.440625\n",
      "Train Epoch: 667 [512/54000 (1%)] Loss: -252584.750000\n",
      "Train Epoch: 667 [11776/54000 (22%)] Loss: -246855.890625\n",
      "Train Epoch: 667 [23040/54000 (43%)] Loss: -337543.968750\n",
      "Train Epoch: 667 [34304/54000 (64%)] Loss: -246523.781250\n",
      "Train Epoch: 667 [45568/54000 (84%)] Loss: -239454.125000\n",
      "    epoch          : 667\n",
      "    loss           : -272502.29171875\n",
      "    val_loss       : -273966.63125\n",
      "Train Epoch: 668 [512/54000 (1%)] Loss: -245360.671875\n",
      "Train Epoch: 668 [11776/54000 (22%)] Loss: -338425.062500\n",
      "Train Epoch: 668 [23040/54000 (43%)] Loss: -339152.312500\n",
      "Train Epoch: 668 [34304/54000 (64%)] Loss: -338729.875000\n",
      "Train Epoch: 668 [45568/54000 (84%)] Loss: -213260.250000\n",
      "    epoch          : 668\n",
      "    loss           : -272505.44359375\n",
      "    val_loss       : -273769.49765625\n",
      "Train Epoch: 669 [512/54000 (1%)] Loss: -245962.765625\n",
      "Train Epoch: 669 [11776/54000 (22%)] Loss: -343549.375000\n",
      "Train Epoch: 669 [23040/54000 (43%)] Loss: -338190.000000\n",
      "Train Epoch: 669 [34304/54000 (64%)] Loss: -242430.625000\n",
      "Train Epoch: 669 [45568/54000 (84%)] Loss: -320527.625000\n",
      "    epoch          : 669\n",
      "    loss           : -272575.71140625\n",
      "    val_loss       : -273481.595703125\n",
      "Train Epoch: 670 [512/54000 (1%)] Loss: -337401.562500\n",
      "Train Epoch: 670 [11776/54000 (22%)] Loss: -174236.250000\n",
      "Train Epoch: 670 [23040/54000 (43%)] Loss: -339302.437500\n",
      "Train Epoch: 670 [34304/54000 (64%)] Loss: -237584.593750\n",
      "Train Epoch: 670 [45568/54000 (84%)] Loss: -318667.406250\n",
      "    epoch          : 670\n",
      "    loss           : -272617.4478125\n",
      "    val_loss       : -274228.44375\n",
      "Train Epoch: 671 [512/54000 (1%)] Loss: -248693.718750\n",
      "Train Epoch: 671 [11776/54000 (22%)] Loss: -213751.562500\n",
      "Train Epoch: 671 [23040/54000 (43%)] Loss: -212265.156250\n",
      "Train Epoch: 671 [34304/54000 (64%)] Loss: -237263.031250\n",
      "Train Epoch: 671 [45568/54000 (84%)] Loss: -215680.875000\n",
      "    epoch          : 671\n",
      "    loss           : -272685.58265625\n",
      "    val_loss       : -274050.361328125\n",
      "Train Epoch: 672 [512/54000 (1%)] Loss: -250087.781250\n",
      "Train Epoch: 672 [11776/54000 (22%)] Loss: -242748.812500\n",
      "Train Epoch: 672 [23040/54000 (43%)] Loss: -338908.687500\n",
      "Train Epoch: 672 [34304/54000 (64%)] Loss: -357060.406250\n",
      "Train Epoch: 672 [45568/54000 (84%)] Loss: -250282.906250\n",
      "    epoch          : 672\n",
      "    loss           : -272800.3671875\n",
      "    val_loss       : -273705.11484375\n",
      "Train Epoch: 673 [512/54000 (1%)] Loss: -337147.031250\n",
      "Train Epoch: 673 [11776/54000 (22%)] Loss: -246390.843750\n",
      "Train Epoch: 673 [23040/54000 (43%)] Loss: -243644.812500\n",
      "Train Epoch: 673 [34304/54000 (64%)] Loss: -176377.250000\n",
      "Train Epoch: 673 [45568/54000 (84%)] Loss: -241468.562500\n",
      "    epoch          : 673\n",
      "    loss           : -272806.40125\n",
      "    val_loss       : -274600.961328125\n",
      "Train Epoch: 674 [512/54000 (1%)] Loss: -239720.906250\n",
      "Train Epoch: 674 [11776/54000 (22%)] Loss: -182662.718750\n",
      "Train Epoch: 674 [23040/54000 (43%)] Loss: -169680.921875\n",
      "Train Epoch: 674 [34304/54000 (64%)] Loss: -251027.500000\n",
      "Train Epoch: 674 [45568/54000 (84%)] Loss: -212963.843750\n",
      "    epoch          : 674\n",
      "    loss           : -272989.67421875\n",
      "    val_loss       : -274208.648828125\n",
      "Train Epoch: 675 [512/54000 (1%)] Loss: -341060.687500\n",
      "Train Epoch: 675 [11776/54000 (22%)] Loss: -243429.921875\n",
      "Train Epoch: 675 [23040/54000 (43%)] Loss: -250449.343750\n",
      "Train Epoch: 675 [34304/54000 (64%)] Loss: -338236.812500\n",
      "Train Epoch: 675 [45568/54000 (84%)] Loss: -321826.281250\n",
      "    epoch          : 675\n",
      "    loss           : -273013.13703125\n",
      "    val_loss       : -274380.50625\n",
      "Train Epoch: 676 [512/54000 (1%)] Loss: -243425.281250\n",
      "Train Epoch: 676 [11776/54000 (22%)] Loss: -252336.937500\n",
      "Train Epoch: 676 [23040/54000 (43%)] Loss: -338383.343750\n",
      "Train Epoch: 676 [34304/54000 (64%)] Loss: -343748.187500\n",
      "Train Epoch: 676 [45568/54000 (84%)] Loss: -203176.531250\n",
      "    epoch          : 676\n",
      "    loss           : -273007.43078125\n",
      "    val_loss       : -274626.8765625\n",
      "Train Epoch: 677 [512/54000 (1%)] Loss: -244532.484375\n",
      "Train Epoch: 677 [11776/54000 (22%)] Loss: -251314.109375\n",
      "Train Epoch: 677 [23040/54000 (43%)] Loss: -321385.937500\n",
      "Train Epoch: 677 [34304/54000 (64%)] Loss: -210018.140625\n",
      "Train Epoch: 677 [45568/54000 (84%)] Loss: -318693.125000\n",
      "    epoch          : 677\n",
      "    loss           : -273061.30734375\n",
      "    val_loss       : -274428.305859375\n",
      "Train Epoch: 678 [512/54000 (1%)] Loss: -357387.406250\n",
      "Train Epoch: 678 [11776/54000 (22%)] Loss: -247836.468750\n",
      "Train Epoch: 678 [23040/54000 (43%)] Loss: -359002.937500\n",
      "Train Epoch: 678 [34304/54000 (64%)] Loss: -249006.906250\n",
      "Train Epoch: 678 [45568/54000 (84%)] Loss: -214533.343750\n",
      "    epoch          : 678\n",
      "    loss           : -273064.97796875\n",
      "    val_loss       : -274096.0765625\n",
      "Train Epoch: 679 [512/54000 (1%)] Loss: -240829.656250\n",
      "Train Epoch: 679 [11776/54000 (22%)] Loss: -253696.937500\n",
      "Train Epoch: 679 [23040/54000 (43%)] Loss: -251138.125000\n",
      "Train Epoch: 679 [34304/54000 (64%)] Loss: -251358.109375\n",
      "Train Epoch: 679 [45568/54000 (84%)] Loss: -343263.625000\n",
      "    epoch          : 679\n",
      "    loss           : -273170.5578125\n",
      "    val_loss       : -274405.99375\n",
      "Train Epoch: 680 [512/54000 (1%)] Loss: -339927.375000\n",
      "Train Epoch: 680 [11776/54000 (22%)] Loss: -338288.906250\n",
      "Train Epoch: 680 [23040/54000 (43%)] Loss: -356408.843750\n",
      "Train Epoch: 680 [34304/54000 (64%)] Loss: -321115.843750\n",
      "Train Epoch: 680 [45568/54000 (84%)] Loss: -211667.687500\n",
      "    epoch          : 680\n",
      "    loss           : -273189.43875\n",
      "    val_loss       : -274396.11171875\n",
      "Train Epoch: 681 [512/54000 (1%)] Loss: -239649.859375\n",
      "Train Epoch: 681 [11776/54000 (22%)] Loss: -355922.687500\n",
      "Train Epoch: 681 [23040/54000 (43%)] Loss: -242543.906250\n",
      "Train Epoch: 681 [34304/54000 (64%)] Loss: -323909.875000\n",
      "Train Epoch: 681 [45568/54000 (84%)] Loss: -211410.421875\n",
      "    epoch          : 681\n",
      "    loss           : -273361.563125\n",
      "    val_loss       : -275187.26171875\n",
      "Train Epoch: 682 [512/54000 (1%)] Loss: -356782.125000\n",
      "Train Epoch: 682 [11776/54000 (22%)] Loss: -246889.328125\n",
      "Train Epoch: 682 [23040/54000 (43%)] Loss: -323632.937500\n",
      "Train Epoch: 682 [34304/54000 (64%)] Loss: -242516.375000\n",
      "Train Epoch: 682 [45568/54000 (84%)] Loss: -212740.250000\n",
      "    epoch          : 682\n",
      "    loss           : -273375.07390625\n",
      "    val_loss       : -274208.89375\n",
      "Train Epoch: 683 [512/54000 (1%)] Loss: -177794.312500\n",
      "Train Epoch: 683 [11776/54000 (22%)] Loss: -252676.875000\n",
      "Train Epoch: 683 [23040/54000 (43%)] Loss: -252965.984375\n",
      "Train Epoch: 683 [34304/54000 (64%)] Loss: -177271.046875\n",
      "Train Epoch: 683 [45568/54000 (84%)] Loss: -212940.937500\n",
      "    epoch          : 683\n",
      "    loss           : -273560.9665625\n",
      "    val_loss       : -274220.284765625\n",
      "Train Epoch: 684 [512/54000 (1%)] Loss: -358129.125000\n",
      "Train Epoch: 684 [11776/54000 (22%)] Loss: -341821.125000\n",
      "Train Epoch: 684 [23040/54000 (43%)] Loss: -249697.984375\n",
      "Train Epoch: 684 [34304/54000 (64%)] Loss: -210017.750000\n",
      "Train Epoch: 684 [45568/54000 (84%)] Loss: -213479.015625\n",
      "    epoch          : 684\n",
      "    loss           : -273477.56328125\n",
      "    val_loss       : -274503.030078125\n",
      "Train Epoch: 685 [512/54000 (1%)] Loss: -180898.171875\n",
      "Train Epoch: 685 [11776/54000 (22%)] Loss: -247197.156250\n",
      "Train Epoch: 685 [23040/54000 (43%)] Loss: -250720.343750\n",
      "Train Epoch: 685 [34304/54000 (64%)] Loss: -244997.218750\n",
      "Train Epoch: 685 [45568/54000 (84%)] Loss: -243705.031250\n",
      "    epoch          : 685\n",
      "    loss           : -273662.82640625\n",
      "    val_loss       : -274223.541796875\n",
      "Train Epoch: 686 [512/54000 (1%)] Loss: -170039.281250\n",
      "Train Epoch: 686 [11776/54000 (22%)] Loss: -201948.453125\n",
      "Train Epoch: 686 [23040/54000 (43%)] Loss: -249898.156250\n",
      "Train Epoch: 686 [34304/54000 (64%)] Loss: -217627.734375\n",
      "Train Epoch: 686 [45568/54000 (84%)] Loss: -328916.156250\n",
      "    epoch          : 686\n",
      "    loss           : -273644.113125\n",
      "    val_loss       : -274534.801171875\n",
      "Train Epoch: 687 [512/54000 (1%)] Loss: -338714.406250\n",
      "Train Epoch: 687 [11776/54000 (22%)] Loss: -248940.250000\n",
      "Train Epoch: 687 [23040/54000 (43%)] Loss: -242695.609375\n",
      "Train Epoch: 687 [34304/54000 (64%)] Loss: -345659.906250\n",
      "Train Epoch: 687 [45568/54000 (84%)] Loss: -219614.437500\n",
      "    epoch          : 687\n",
      "    loss           : -273808.0315625\n",
      "    val_loss       : -275021.583984375\n",
      "Train Epoch: 688 [512/54000 (1%)] Loss: -338934.187500\n",
      "Train Epoch: 688 [11776/54000 (22%)] Loss: -180469.640625\n",
      "Train Epoch: 688 [23040/54000 (43%)] Loss: -248469.781250\n",
      "Train Epoch: 688 [34304/54000 (64%)] Loss: -179777.171875\n",
      "Train Epoch: 688 [45568/54000 (84%)] Loss: -323265.562500\n",
      "    epoch          : 688\n",
      "    loss           : -273777.6165625\n",
      "    val_loss       : -275049.96953125\n",
      "Train Epoch: 689 [512/54000 (1%)] Loss: -244164.218750\n",
      "Train Epoch: 689 [11776/54000 (22%)] Loss: -248470.812500\n",
      "Train Epoch: 689 [23040/54000 (43%)] Loss: -251367.546875\n",
      "Train Epoch: 689 [34304/54000 (64%)] Loss: -241478.781250\n",
      "Train Epoch: 689 [45568/54000 (84%)] Loss: -242045.593750\n",
      "    epoch          : 689\n",
      "    loss           : -273867.9709375\n",
      "    val_loss       : -275252.133984375\n",
      "Train Epoch: 690 [512/54000 (1%)] Loss: -217012.656250\n",
      "Train Epoch: 690 [11776/54000 (22%)] Loss: -326577.312500\n",
      "Train Epoch: 690 [23040/54000 (43%)] Loss: -241438.437500\n",
      "Train Epoch: 690 [34304/54000 (64%)] Loss: -339369.125000\n",
      "Train Epoch: 690 [45568/54000 (84%)] Loss: -250093.718750\n",
      "    epoch          : 690\n",
      "    loss           : -273926.82578125\n",
      "    val_loss       : -274987.14921875\n",
      "Train Epoch: 691 [512/54000 (1%)] Loss: -249216.531250\n",
      "Train Epoch: 691 [11776/54000 (22%)] Loss: -338941.625000\n",
      "Train Epoch: 691 [23040/54000 (43%)] Loss: -248870.218750\n",
      "Train Epoch: 691 [34304/54000 (64%)] Loss: -323816.281250\n",
      "Train Epoch: 691 [45568/54000 (84%)] Loss: -250023.593750\n",
      "    epoch          : 691\n",
      "    loss           : -273903.5771875\n",
      "    val_loss       : -274907.722265625\n",
      "Train Epoch: 692 [512/54000 (1%)] Loss: -249398.375000\n",
      "Train Epoch: 692 [11776/54000 (22%)] Loss: -243613.859375\n",
      "Train Epoch: 692 [23040/54000 (43%)] Loss: -245856.265625\n",
      "Train Epoch: 692 [34304/54000 (64%)] Loss: -245915.343750\n",
      "Train Epoch: 692 [45568/54000 (84%)] Loss: -344127.031250\n",
      "    epoch          : 692\n",
      "    loss           : -274094.55421875\n",
      "    val_loss       : -275547.132421875\n",
      "Train Epoch: 693 [512/54000 (1%)] Loss: -346870.406250\n",
      "Train Epoch: 693 [11776/54000 (22%)] Loss: -243327.500000\n",
      "Train Epoch: 693 [23040/54000 (43%)] Loss: -324517.687500\n",
      "Train Epoch: 693 [34304/54000 (64%)] Loss: -248184.546875\n",
      "Train Epoch: 693 [45568/54000 (84%)] Loss: -212616.218750\n",
      "    epoch          : 693\n",
      "    loss           : -274190.01390625\n",
      "    val_loss       : -275305.082421875\n",
      "Train Epoch: 694 [512/54000 (1%)] Loss: -339218.093750\n",
      "Train Epoch: 694 [11776/54000 (22%)] Loss: -176020.750000\n",
      "Train Epoch: 694 [23040/54000 (43%)] Loss: -346322.906250\n",
      "Train Epoch: 694 [34304/54000 (64%)] Loss: -319245.437500\n",
      "Train Epoch: 694 [45568/54000 (84%)] Loss: -246849.281250\n",
      "    epoch          : 694\n",
      "    loss           : -274223.8996875\n",
      "    val_loss       : -275603.954296875\n",
      "Train Epoch: 695 [512/54000 (1%)] Loss: -342536.312500\n",
      "Train Epoch: 695 [11776/54000 (22%)] Loss: -174138.984375\n",
      "Train Epoch: 695 [23040/54000 (43%)] Loss: -244211.250000\n",
      "Train Epoch: 695 [34304/54000 (64%)] Loss: -322094.812500\n",
      "Train Epoch: 695 [45568/54000 (84%)] Loss: -344139.937500\n",
      "    epoch          : 695\n",
      "    loss           : -274201.43609375\n",
      "    val_loss       : -275375.234765625\n",
      "Train Epoch: 696 [512/54000 (1%)] Loss: -344749.343750\n",
      "Train Epoch: 696 [11776/54000 (22%)] Loss: -341314.687500\n",
      "Train Epoch: 696 [23040/54000 (43%)] Loss: -345416.187500\n",
      "Train Epoch: 696 [34304/54000 (64%)] Loss: -171913.781250\n",
      "Train Epoch: 696 [45568/54000 (84%)] Loss: -344968.593750\n",
      "    epoch          : 696\n",
      "    loss           : -274314.24828125\n",
      "    val_loss       : -275199.995703125\n",
      "Train Epoch: 697 [512/54000 (1%)] Loss: -210893.218750\n",
      "Train Epoch: 697 [11776/54000 (22%)] Loss: -341777.875000\n",
      "Train Epoch: 697 [23040/54000 (43%)] Loss: -176448.593750\n",
      "Train Epoch: 697 [34304/54000 (64%)] Loss: -250665.718750\n",
      "Train Epoch: 697 [45568/54000 (84%)] Loss: -345756.593750\n",
      "    epoch          : 697\n",
      "    loss           : -274439.01109375\n",
      "    val_loss       : -274804.726171875\n",
      "Train Epoch: 698 [512/54000 (1%)] Loss: -340147.875000\n",
      "Train Epoch: 698 [11776/54000 (22%)] Loss: -250715.562500\n",
      "Train Epoch: 698 [23040/54000 (43%)] Loss: -171168.625000\n",
      "Train Epoch: 698 [34304/54000 (64%)] Loss: -251952.562500\n",
      "Train Epoch: 698 [45568/54000 (84%)] Loss: -212817.250000\n",
      "    epoch          : 698\n",
      "    loss           : -274415.40015625\n",
      "    val_loss       : -275370.03828125\n",
      "Train Epoch: 699 [512/54000 (1%)] Loss: -172014.312500\n",
      "Train Epoch: 699 [11776/54000 (22%)] Loss: -355976.781250\n",
      "Train Epoch: 699 [23040/54000 (43%)] Loss: -178135.781250\n",
      "Train Epoch: 699 [34304/54000 (64%)] Loss: -252468.000000\n",
      "Train Epoch: 699 [45568/54000 (84%)] Loss: -248424.125000\n",
      "    epoch          : 699\n",
      "    loss           : -274573.90625\n",
      "    val_loss       : -275401.4828125\n",
      "Train Epoch: 700 [512/54000 (1%)] Loss: -342107.156250\n",
      "Train Epoch: 700 [11776/54000 (22%)] Loss: -340092.375000\n",
      "Train Epoch: 700 [23040/54000 (43%)] Loss: -215742.203125\n",
      "Train Epoch: 700 [34304/54000 (64%)] Loss: -346294.843750\n",
      "Train Epoch: 700 [45568/54000 (84%)] Loss: -326684.406250\n",
      "    epoch          : 700\n",
      "    loss           : -274519.34453125\n",
      "    val_loss       : -275393.445703125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch700.pth ...\n",
      "Train Epoch: 701 [512/54000 (1%)] Loss: -339821.718750\n",
      "Train Epoch: 701 [11776/54000 (22%)] Loss: -248987.250000\n",
      "Train Epoch: 701 [23040/54000 (43%)] Loss: -320186.312500\n",
      "Train Epoch: 701 [34304/54000 (64%)] Loss: -247385.281250\n",
      "Train Epoch: 701 [45568/54000 (84%)] Loss: -253509.093750\n",
      "    epoch          : 701\n",
      "    loss           : -274532.385\n",
      "    val_loss       : -275625.346875\n",
      "Train Epoch: 702 [512/54000 (1%)] Loss: -360803.093750\n",
      "Train Epoch: 702 [11776/54000 (22%)] Loss: -211547.765625\n",
      "Train Epoch: 702 [23040/54000 (43%)] Loss: -238839.437500\n",
      "Train Epoch: 702 [34304/54000 (64%)] Loss: -207312.656250\n",
      "Train Epoch: 702 [45568/54000 (84%)] Loss: -326448.968750\n",
      "    epoch          : 702\n",
      "    loss           : -274563.2684375\n",
      "    val_loss       : -275705.837890625\n",
      "Train Epoch: 703 [512/54000 (1%)] Loss: -243178.843750\n",
      "Train Epoch: 703 [11776/54000 (22%)] Loss: -344764.625000\n",
      "Train Epoch: 703 [23040/54000 (43%)] Loss: -253176.781250\n",
      "Train Epoch: 703 [34304/54000 (64%)] Loss: -251100.093750\n",
      "Train Epoch: 703 [45568/54000 (84%)] Loss: -216981.171875\n",
      "    epoch          : 703\n",
      "    loss           : -274660.460625\n",
      "    val_loss       : -274767.142578125\n",
      "Train Epoch: 704 [512/54000 (1%)] Loss: -212530.250000\n",
      "Train Epoch: 704 [11776/54000 (22%)] Loss: -323099.625000\n",
      "Train Epoch: 704 [23040/54000 (43%)] Loss: -245899.656250\n",
      "Train Epoch: 704 [34304/54000 (64%)] Loss: -248469.593750\n",
      "Train Epoch: 704 [45568/54000 (84%)] Loss: -342271.875000\n",
      "    epoch          : 704\n",
      "    loss           : -274893.8953125\n",
      "    val_loss       : -276349.97109375\n",
      "Train Epoch: 705 [512/54000 (1%)] Loss: -245402.187500\n",
      "Train Epoch: 705 [11776/54000 (22%)] Loss: -239600.125000\n",
      "Train Epoch: 705 [23040/54000 (43%)] Loss: -358352.156250\n",
      "Train Epoch: 705 [34304/54000 (64%)] Loss: -236982.093750\n",
      "Train Epoch: 705 [45568/54000 (84%)] Loss: -326818.250000\n",
      "    epoch          : 705\n",
      "    loss           : -274860.92765625\n",
      "    val_loss       : -275754.860546875\n",
      "Train Epoch: 706 [512/54000 (1%)] Loss: -244666.843750\n",
      "Train Epoch: 706 [11776/54000 (22%)] Loss: -172800.031250\n",
      "Train Epoch: 706 [23040/54000 (43%)] Loss: -180279.468750\n",
      "Train Epoch: 706 [34304/54000 (64%)] Loss: -217973.031250\n",
      "Train Epoch: 706 [45568/54000 (84%)] Loss: -341513.687500\n",
      "    epoch          : 706\n",
      "    loss           : -274914.71109375\n",
      "    val_loss       : -275729.408984375\n",
      "Train Epoch: 707 [512/54000 (1%)] Loss: -346013.375000\n",
      "Train Epoch: 707 [11776/54000 (22%)] Loss: -253521.968750\n",
      "Train Epoch: 707 [23040/54000 (43%)] Loss: -340869.625000\n",
      "Train Epoch: 707 [34304/54000 (64%)] Loss: -342710.718750\n",
      "Train Epoch: 707 [45568/54000 (84%)] Loss: -250284.781250\n",
      "    epoch          : 707\n",
      "    loss           : -274950.67328125\n",
      "    val_loss       : -276136.328515625\n",
      "Train Epoch: 708 [512/54000 (1%)] Loss: -241813.375000\n",
      "Train Epoch: 708 [11776/54000 (22%)] Loss: -360480.093750\n",
      "Train Epoch: 708 [23040/54000 (43%)] Loss: -167068.593750\n",
      "Train Epoch: 708 [34304/54000 (64%)] Loss: -207230.187500\n",
      "Train Epoch: 708 [45568/54000 (84%)] Loss: -207588.296875\n",
      "    epoch          : 708\n",
      "    loss           : -274937.1353125\n",
      "    val_loss       : -276085.89453125\n",
      "Train Epoch: 709 [512/54000 (1%)] Loss: -360331.625000\n",
      "Train Epoch: 709 [11776/54000 (22%)] Loss: -324161.218750\n",
      "Train Epoch: 709 [23040/54000 (43%)] Loss: -344105.125000\n",
      "Train Epoch: 709 [34304/54000 (64%)] Loss: -255364.484375\n",
      "Train Epoch: 709 [45568/54000 (84%)] Loss: -321890.937500\n",
      "    epoch          : 709\n",
      "    loss           : -275111.476875\n",
      "    val_loss       : -276784.078515625\n",
      "Train Epoch: 710 [512/54000 (1%)] Loss: -250043.421875\n",
      "Train Epoch: 710 [11776/54000 (22%)] Loss: -323381.000000\n",
      "Train Epoch: 710 [23040/54000 (43%)] Loss: -325563.687500\n",
      "Train Epoch: 710 [34304/54000 (64%)] Loss: -245329.953125\n",
      "Train Epoch: 710 [45568/54000 (84%)] Loss: -323487.062500\n",
      "    epoch          : 710\n",
      "    loss           : -275296.566875\n",
      "    val_loss       : -276145.85625\n",
      "Train Epoch: 711 [512/54000 (1%)] Loss: -361368.375000\n",
      "Train Epoch: 711 [11776/54000 (22%)] Loss: -178035.031250\n",
      "Train Epoch: 711 [23040/54000 (43%)] Loss: -345558.375000\n",
      "Train Epoch: 711 [34304/54000 (64%)] Loss: -344044.843750\n",
      "Train Epoch: 711 [45568/54000 (84%)] Loss: -220888.375000\n",
      "    epoch          : 711\n",
      "    loss           : -275322.28390625\n",
      "    val_loss       : -275809.32734375\n",
      "Train Epoch: 712 [512/54000 (1%)] Loss: -242930.312500\n",
      "Train Epoch: 712 [11776/54000 (22%)] Loss: -350364.875000\n",
      "Train Epoch: 712 [23040/54000 (43%)] Loss: -247608.906250\n",
      "Train Epoch: 712 [34304/54000 (64%)] Loss: -241519.171875\n",
      "Train Epoch: 712 [45568/54000 (84%)] Loss: -346810.750000\n",
      "    epoch          : 712\n",
      "    loss           : -275219.355\n",
      "    val_loss       : -276328.271484375\n",
      "Train Epoch: 713 [512/54000 (1%)] Loss: -360334.968750\n",
      "Train Epoch: 713 [11776/54000 (22%)] Loss: -251588.343750\n",
      "Train Epoch: 713 [23040/54000 (43%)] Loss: -244613.312500\n",
      "Train Epoch: 713 [34304/54000 (64%)] Loss: -255005.843750\n",
      "Train Epoch: 713 [45568/54000 (84%)] Loss: -249562.359375\n",
      "    epoch          : 713\n",
      "    loss           : -275436.4875\n",
      "    val_loss       : -276085.546484375\n",
      "Train Epoch: 714 [512/54000 (1%)] Loss: -215151.250000\n",
      "Train Epoch: 714 [11776/54000 (22%)] Loss: -249345.953125\n",
      "Train Epoch: 714 [23040/54000 (43%)] Loss: -324497.343750\n",
      "Train Epoch: 714 [34304/54000 (64%)] Loss: -324406.437500\n",
      "Train Epoch: 714 [45568/54000 (84%)] Loss: -345910.625000\n",
      "    epoch          : 714\n",
      "    loss           : -275305.3640625\n",
      "    val_loss       : -276650.071484375\n",
      "Train Epoch: 715 [512/54000 (1%)] Loss: -342946.406250\n",
      "Train Epoch: 715 [11776/54000 (22%)] Loss: -325718.031250\n",
      "Train Epoch: 715 [23040/54000 (43%)] Loss: -214247.234375\n",
      "Train Epoch: 715 [34304/54000 (64%)] Loss: -254369.203125\n",
      "Train Epoch: 715 [45568/54000 (84%)] Loss: -347662.000000\n",
      "    epoch          : 715\n",
      "    loss           : -275403.53015625\n",
      "    val_loss       : -276667.161328125\n",
      "Train Epoch: 716 [512/54000 (1%)] Loss: -249932.734375\n",
      "Train Epoch: 716 [11776/54000 (22%)] Loss: -172238.703125\n",
      "Train Epoch: 716 [23040/54000 (43%)] Loss: -248736.937500\n",
      "Train Epoch: 716 [34304/54000 (64%)] Loss: -320800.250000\n",
      "Train Epoch: 716 [45568/54000 (84%)] Loss: -214098.625000\n",
      "    epoch          : 716\n",
      "    loss           : -275662.309375\n",
      "    val_loss       : -276722.95625\n",
      "Train Epoch: 717 [512/54000 (1%)] Loss: -251788.031250\n",
      "Train Epoch: 717 [11776/54000 (22%)] Loss: -327508.187500\n",
      "Train Epoch: 717 [23040/54000 (43%)] Loss: -238965.281250\n",
      "Train Epoch: 717 [34304/54000 (64%)] Loss: -248333.468750\n",
      "Train Epoch: 717 [45568/54000 (84%)] Loss: -244961.796875\n",
      "    epoch          : 717\n",
      "    loss           : -275590.18625\n",
      "    val_loss       : -276277.46171875\n",
      "Train Epoch: 718 [512/54000 (1%)] Loss: -175748.375000\n",
      "Train Epoch: 718 [11776/54000 (22%)] Loss: -250772.468750\n",
      "Train Epoch: 718 [23040/54000 (43%)] Loss: -346562.812500\n",
      "Train Epoch: 718 [34304/54000 (64%)] Loss: -246020.234375\n",
      "Train Epoch: 718 [45568/54000 (84%)] Loss: -215327.250000\n",
      "    epoch          : 718\n",
      "    loss           : -275624.8990625\n",
      "    val_loss       : -276580.586328125\n",
      "Train Epoch: 719 [512/54000 (1%)] Loss: -246981.781250\n",
      "Train Epoch: 719 [11776/54000 (22%)] Loss: -246436.156250\n",
      "Train Epoch: 719 [23040/54000 (43%)] Loss: -249077.093750\n",
      "Train Epoch: 719 [34304/54000 (64%)] Loss: -240679.953125\n",
      "Train Epoch: 719 [45568/54000 (84%)] Loss: -346246.812500\n",
      "    epoch          : 719\n",
      "    loss           : -275750.24265625\n",
      "    val_loss       : -276465.47109375\n",
      "Train Epoch: 720 [512/54000 (1%)] Loss: -248270.421875\n",
      "Train Epoch: 720 [11776/54000 (22%)] Loss: -252541.187500\n",
      "Train Epoch: 720 [23040/54000 (43%)] Loss: -256324.406250\n",
      "Train Epoch: 720 [34304/54000 (64%)] Loss: -212945.718750\n",
      "Train Epoch: 720 [45568/54000 (84%)] Loss: -244159.343750\n",
      "    epoch          : 720\n",
      "    loss           : -275765.6290625\n",
      "    val_loss       : -277003.499609375\n",
      "Train Epoch: 721 [512/54000 (1%)] Loss: -327666.281250\n",
      "Train Epoch: 721 [11776/54000 (22%)] Loss: -325354.500000\n",
      "Train Epoch: 721 [23040/54000 (43%)] Loss: -348484.750000\n",
      "Train Epoch: 721 [34304/54000 (64%)] Loss: -237667.437500\n",
      "Train Epoch: 721 [45568/54000 (84%)] Loss: -242387.281250\n",
      "    epoch          : 721\n",
      "    loss           : -275808.59140625\n",
      "    val_loss       : -276713.6609375\n",
      "Train Epoch: 722 [512/54000 (1%)] Loss: -322398.500000\n",
      "Train Epoch: 722 [11776/54000 (22%)] Loss: -241805.703125\n",
      "Train Epoch: 722 [23040/54000 (43%)] Loss: -324877.656250\n",
      "Train Epoch: 722 [34304/54000 (64%)] Loss: -327121.968750\n",
      "Train Epoch: 722 [45568/54000 (84%)] Loss: -324460.000000\n",
      "    epoch          : 722\n",
      "    loss           : -275871.118125\n",
      "    val_loss       : -277347.950390625\n",
      "Train Epoch: 723 [512/54000 (1%)] Loss: -362460.218750\n",
      "Train Epoch: 723 [11776/54000 (22%)] Loss: -342388.125000\n",
      "Train Epoch: 723 [23040/54000 (43%)] Loss: -348804.375000\n",
      "Train Epoch: 723 [34304/54000 (64%)] Loss: -342504.750000\n",
      "Train Epoch: 723 [45568/54000 (84%)] Loss: -323652.562500\n",
      "    epoch          : 723\n",
      "    loss           : -275936.6040625\n",
      "    val_loss       : -276730.94921875\n",
      "Train Epoch: 724 [512/54000 (1%)] Loss: -346046.718750\n",
      "Train Epoch: 724 [11776/54000 (22%)] Loss: -170807.890625\n",
      "Train Epoch: 724 [23040/54000 (43%)] Loss: -217741.171875\n",
      "Train Epoch: 724 [34304/54000 (64%)] Loss: -257748.687500\n",
      "Train Epoch: 724 [45568/54000 (84%)] Loss: -211535.468750\n",
      "    epoch          : 724\n",
      "    loss           : -276160.74328125\n",
      "    val_loss       : -277526.5671875\n",
      "Train Epoch: 725 [512/54000 (1%)] Loss: -243602.562500\n",
      "Train Epoch: 725 [11776/54000 (22%)] Loss: -252523.562500\n",
      "Train Epoch: 725 [23040/54000 (43%)] Loss: -248668.437500\n",
      "Train Epoch: 725 [34304/54000 (64%)] Loss: -359666.500000\n",
      "Train Epoch: 725 [45568/54000 (84%)] Loss: -222366.968750\n",
      "    epoch          : 725\n",
      "    loss           : -276133.156875\n",
      "    val_loss       : -277119.52421875\n",
      "Train Epoch: 726 [512/54000 (1%)] Loss: -324723.125000\n",
      "Train Epoch: 726 [11776/54000 (22%)] Loss: -215962.343750\n",
      "Train Epoch: 726 [23040/54000 (43%)] Loss: -252785.375000\n",
      "Train Epoch: 726 [34304/54000 (64%)] Loss: -242548.250000\n",
      "Train Epoch: 726 [45568/54000 (84%)] Loss: -212057.281250\n",
      "    epoch          : 726\n",
      "    loss           : -276278.956875\n",
      "    val_loss       : -277228.09765625\n",
      "Train Epoch: 727 [512/54000 (1%)] Loss: -324036.937500\n",
      "Train Epoch: 727 [11776/54000 (22%)] Loss: -327632.250000\n",
      "Train Epoch: 727 [23040/54000 (43%)] Loss: -330308.343750\n",
      "Train Epoch: 727 [34304/54000 (64%)] Loss: -341354.562500\n",
      "Train Epoch: 727 [45568/54000 (84%)] Loss: -210910.718750\n",
      "    epoch          : 727\n",
      "    loss           : -276277.3565625\n",
      "    val_loss       : -277256.187109375\n",
      "Train Epoch: 728 [512/54000 (1%)] Loss: -343889.437500\n",
      "Train Epoch: 728 [11776/54000 (22%)] Loss: -346819.812500\n",
      "Train Epoch: 728 [23040/54000 (43%)] Loss: -347851.500000\n",
      "Train Epoch: 728 [34304/54000 (64%)] Loss: -217917.250000\n",
      "Train Epoch: 728 [45568/54000 (84%)] Loss: -348250.937500\n",
      "    epoch          : 728\n",
      "    loss           : -276261.66859375\n",
      "    val_loss       : -276548.22265625\n",
      "Train Epoch: 729 [512/54000 (1%)] Loss: -166430.906250\n",
      "Train Epoch: 729 [11776/54000 (22%)] Loss: -349337.687500\n",
      "Train Epoch: 729 [23040/54000 (43%)] Loss: -244401.312500\n",
      "Train Epoch: 729 [34304/54000 (64%)] Loss: -347877.625000\n",
      "Train Epoch: 729 [45568/54000 (84%)] Loss: -348205.656250\n",
      "    epoch          : 729\n",
      "    loss           : -276317.353125\n",
      "    val_loss       : -277411.639453125\n",
      "Train Epoch: 730 [512/54000 (1%)] Loss: -348438.812500\n",
      "Train Epoch: 730 [11776/54000 (22%)] Loss: -241454.062500\n",
      "Train Epoch: 730 [23040/54000 (43%)] Loss: -245117.718750\n",
      "Train Epoch: 730 [34304/54000 (64%)] Loss: -246244.750000\n",
      "Train Epoch: 730 [45568/54000 (84%)] Loss: -324759.468750\n",
      "    epoch          : 730\n",
      "    loss           : -276373.04625\n",
      "    val_loss       : -277080.02578125\n",
      "Train Epoch: 731 [512/54000 (1%)] Loss: -249243.812500\n",
      "Train Epoch: 731 [11776/54000 (22%)] Loss: -255381.203125\n",
      "Train Epoch: 731 [23040/54000 (43%)] Loss: -348626.031250\n",
      "Train Epoch: 731 [34304/54000 (64%)] Loss: -341895.218750\n",
      "Train Epoch: 731 [45568/54000 (84%)] Loss: -326676.281250\n",
      "    epoch          : 731\n",
      "    loss           : -276465.44546875\n",
      "    val_loss       : -277169.775\n",
      "Train Epoch: 732 [512/54000 (1%)] Loss: -252149.734375\n",
      "Train Epoch: 732 [11776/54000 (22%)] Loss: -241393.265625\n",
      "Train Epoch: 732 [23040/54000 (43%)] Loss: -246210.781250\n",
      "Train Epoch: 732 [34304/54000 (64%)] Loss: -358503.187500\n",
      "Train Epoch: 732 [45568/54000 (84%)] Loss: -326984.593750\n",
      "    epoch          : 732\n",
      "    loss           : -276552.62171875\n",
      "    val_loss       : -277869.83984375\n",
      "Train Epoch: 733 [512/54000 (1%)] Loss: -362109.406250\n",
      "Train Epoch: 733 [11776/54000 (22%)] Loss: -360260.343750\n",
      "Train Epoch: 733 [23040/54000 (43%)] Loss: -345291.312500\n",
      "Train Epoch: 733 [34304/54000 (64%)] Loss: -325943.187500\n",
      "Train Epoch: 733 [45568/54000 (84%)] Loss: -327736.625000\n",
      "    epoch          : 733\n",
      "    loss           : -276583.77671875\n",
      "    val_loss       : -277564.3046875\n",
      "Train Epoch: 734 [512/54000 (1%)] Loss: -251008.312500\n",
      "Train Epoch: 734 [11776/54000 (22%)] Loss: -171572.406250\n",
      "Train Epoch: 734 [23040/54000 (43%)] Loss: -244803.796875\n",
      "Train Epoch: 734 [34304/54000 (64%)] Loss: -216056.937500\n",
      "Train Epoch: 734 [45568/54000 (84%)] Loss: -323569.156250\n",
      "    epoch          : 734\n",
      "    loss           : -276558.49859375\n",
      "    val_loss       : -277278.176953125\n",
      "Train Epoch: 735 [512/54000 (1%)] Loss: -343707.187500\n",
      "Train Epoch: 735 [11776/54000 (22%)] Loss: -364952.156250\n",
      "Train Epoch: 735 [23040/54000 (43%)] Loss: -350545.906250\n",
      "Train Epoch: 735 [34304/54000 (64%)] Loss: -248817.625000\n",
      "Train Epoch: 735 [45568/54000 (84%)] Loss: -351356.375000\n",
      "    epoch          : 735\n",
      "    loss           : -276603.59203125\n",
      "    val_loss       : -278077.96796875\n",
      "Train Epoch: 736 [512/54000 (1%)] Loss: -255497.500000\n",
      "Train Epoch: 736 [11776/54000 (22%)] Loss: -344358.062500\n",
      "Train Epoch: 736 [23040/54000 (43%)] Loss: -250193.593750\n",
      "Train Epoch: 736 [34304/54000 (64%)] Loss: -253521.718750\n",
      "Train Epoch: 736 [45568/54000 (84%)] Loss: -212170.437500\n",
      "    epoch          : 736\n",
      "    loss           : -276519.92671875\n",
      "    val_loss       : -277865.7703125\n",
      "Train Epoch: 737 [512/54000 (1%)] Loss: -239077.750000\n",
      "Train Epoch: 737 [11776/54000 (22%)] Loss: -326806.875000\n",
      "Train Epoch: 737 [23040/54000 (43%)] Loss: -363017.312500\n",
      "Train Epoch: 737 [34304/54000 (64%)] Loss: -351416.812500\n",
      "Train Epoch: 737 [45568/54000 (84%)] Loss: -212014.156250\n",
      "    epoch          : 737\n",
      "    loss           : -276909.22765625\n",
      "    val_loss       : -278218.396484375\n",
      "Train Epoch: 738 [512/54000 (1%)] Loss: -247031.500000\n",
      "Train Epoch: 738 [11776/54000 (22%)] Loss: -342610.312500\n",
      "Train Epoch: 738 [23040/54000 (43%)] Loss: -213358.281250\n",
      "Train Epoch: 738 [34304/54000 (64%)] Loss: -344183.562500\n",
      "Train Epoch: 738 [45568/54000 (84%)] Loss: -241616.437500\n",
      "    epoch          : 738\n",
      "    loss           : -276834.72375\n",
      "    val_loss       : -278048.326953125\n",
      "Train Epoch: 739 [512/54000 (1%)] Loss: -252902.156250\n",
      "Train Epoch: 739 [11776/54000 (22%)] Loss: -349294.500000\n",
      "Train Epoch: 739 [23040/54000 (43%)] Loss: -325819.718750\n",
      "Train Epoch: 739 [34304/54000 (64%)] Loss: -251916.875000\n",
      "Train Epoch: 739 [45568/54000 (84%)] Loss: -348185.625000\n",
      "    epoch          : 739\n",
      "    loss           : -277048.9946875\n",
      "    val_loss       : -277944.675390625\n",
      "Train Epoch: 740 [512/54000 (1%)] Loss: -326203.812500\n",
      "Train Epoch: 740 [11776/54000 (22%)] Loss: -325825.656250\n",
      "Train Epoch: 740 [23040/54000 (43%)] Loss: -364005.656250\n",
      "Train Epoch: 740 [34304/54000 (64%)] Loss: -252424.906250\n",
      "Train Epoch: 740 [45568/54000 (84%)] Loss: -237586.125000\n",
      "    epoch          : 740\n",
      "    loss           : -276903.40359375\n",
      "    val_loss       : -278556.731640625\n",
      "Train Epoch: 741 [512/54000 (1%)] Loss: -180971.171875\n",
      "Train Epoch: 741 [11776/54000 (22%)] Loss: -250957.375000\n",
      "Train Epoch: 741 [23040/54000 (43%)] Loss: -184492.125000\n",
      "Train Epoch: 741 [34304/54000 (64%)] Loss: -351420.125000\n",
      "Train Epoch: 741 [45568/54000 (84%)] Loss: -327436.812500\n",
      "    epoch          : 741\n",
      "    loss           : -277087.70203125\n",
      "    val_loss       : -278639.53125\n",
      "Train Epoch: 742 [512/54000 (1%)] Loss: -322983.656250\n",
      "Train Epoch: 742 [11776/54000 (22%)] Loss: -341734.093750\n",
      "Train Epoch: 742 [23040/54000 (43%)] Loss: -240551.062500\n",
      "Train Epoch: 742 [34304/54000 (64%)] Loss: -362732.718750\n",
      "Train Epoch: 742 [45568/54000 (84%)] Loss: -241836.875000\n",
      "    epoch          : 742\n",
      "    loss           : -277068.70609375\n",
      "    val_loss       : -278139.35234375\n",
      "Train Epoch: 743 [512/54000 (1%)] Loss: -343441.093750\n",
      "Train Epoch: 743 [11776/54000 (22%)] Loss: -361759.593750\n",
      "Train Epoch: 743 [23040/54000 (43%)] Loss: -178869.062500\n",
      "Train Epoch: 743 [34304/54000 (64%)] Loss: -327640.281250\n",
      "Train Epoch: 743 [45568/54000 (84%)] Loss: -209634.406250\n",
      "    epoch          : 743\n",
      "    loss           : -277186.27515625\n",
      "    val_loss       : -278976.018359375\n",
      "Train Epoch: 744 [512/54000 (1%)] Loss: -251001.468750\n",
      "Train Epoch: 744 [11776/54000 (22%)] Loss: -219078.375000\n",
      "Train Epoch: 744 [23040/54000 (43%)] Loss: -242688.312500\n",
      "Train Epoch: 744 [34304/54000 (64%)] Loss: -362318.781250\n",
      "Train Epoch: 744 [45568/54000 (84%)] Loss: -347585.812500\n",
      "    epoch          : 744\n",
      "    loss           : -277141.64375\n",
      "    val_loss       : -278134.2703125\n",
      "Train Epoch: 745 [512/54000 (1%)] Loss: -255259.875000\n",
      "Train Epoch: 745 [11776/54000 (22%)] Loss: -349609.843750\n",
      "Train Epoch: 745 [23040/54000 (43%)] Loss: -252133.156250\n",
      "Train Epoch: 745 [34304/54000 (64%)] Loss: -257298.953125\n",
      "Train Epoch: 745 [45568/54000 (84%)] Loss: -360808.375000\n",
      "    epoch          : 745\n",
      "    loss           : -277169.26609375\n",
      "    val_loss       : -278817.069921875\n",
      "Train Epoch: 746 [512/54000 (1%)] Loss: -251500.281250\n",
      "Train Epoch: 746 [11776/54000 (22%)] Loss: -182142.125000\n",
      "Train Epoch: 746 [23040/54000 (43%)] Loss: -218366.609375\n",
      "Train Epoch: 746 [34304/54000 (64%)] Loss: -243501.218750\n",
      "Train Epoch: 746 [45568/54000 (84%)] Loss: -327197.750000\n",
      "    epoch          : 746\n",
      "    loss           : -277283.425625\n",
      "    val_loss       : -278448.371875\n",
      "Train Epoch: 747 [512/54000 (1%)] Loss: -242755.187500\n",
      "Train Epoch: 747 [11776/54000 (22%)] Loss: -178193.531250\n",
      "Train Epoch: 747 [23040/54000 (43%)] Loss: -253291.578125\n",
      "Train Epoch: 747 [34304/54000 (64%)] Loss: -326499.375000\n",
      "Train Epoch: 747 [45568/54000 (84%)] Loss: -244526.406250\n",
      "    epoch          : 747\n",
      "    loss           : -277353.08015625\n",
      "    val_loss       : -278361.2890625\n",
      "Train Epoch: 748 [512/54000 (1%)] Loss: -214439.796875\n",
      "Train Epoch: 748 [11776/54000 (22%)] Loss: -360683.656250\n",
      "Train Epoch: 748 [23040/54000 (43%)] Loss: -218279.796875\n",
      "Train Epoch: 748 [34304/54000 (64%)] Loss: -254157.593750\n",
      "Train Epoch: 748 [45568/54000 (84%)] Loss: -252002.343750\n",
      "    epoch          : 748\n",
      "    loss           : -277372.9278125\n",
      "    val_loss       : -278422.354296875\n",
      "Train Epoch: 749 [512/54000 (1%)] Loss: -257112.281250\n",
      "Train Epoch: 749 [11776/54000 (22%)] Loss: -246555.531250\n",
      "Train Epoch: 749 [23040/54000 (43%)] Loss: -254145.046875\n",
      "Train Epoch: 749 [34304/54000 (64%)] Loss: -248985.125000\n",
      "Train Epoch: 749 [45568/54000 (84%)] Loss: -329301.718750\n",
      "    epoch          : 749\n",
      "    loss           : -277510.33984375\n",
      "    val_loss       : -278401.321875\n",
      "Train Epoch: 750 [512/54000 (1%)] Loss: -216880.750000\n",
      "Train Epoch: 750 [11776/54000 (22%)] Loss: -169876.843750\n",
      "Train Epoch: 750 [23040/54000 (43%)] Loss: -254259.031250\n",
      "Train Epoch: 750 [34304/54000 (64%)] Loss: -175790.468750\n",
      "Train Epoch: 750 [45568/54000 (84%)] Loss: -189531.828125\n",
      "    epoch          : 750\n",
      "    loss           : -277637.92796875\n",
      "    val_loss       : -279225.521875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch750.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 751 [512/54000 (1%)] Loss: -183138.171875\n",
      "Train Epoch: 751 [11776/54000 (22%)] Loss: -178871.125000\n",
      "Train Epoch: 751 [23040/54000 (43%)] Loss: -242929.406250\n",
      "Train Epoch: 751 [34304/54000 (64%)] Loss: -216043.718750\n",
      "Train Epoch: 751 [45568/54000 (84%)] Loss: -353741.531250\n",
      "    epoch          : 751\n",
      "    loss           : -277560.2909375\n",
      "    val_loss       : -278847.948828125\n",
      "Train Epoch: 752 [512/54000 (1%)] Loss: -349736.750000\n",
      "Train Epoch: 752 [11776/54000 (22%)] Loss: -173948.171875\n",
      "Train Epoch: 752 [23040/54000 (43%)] Loss: -255283.015625\n",
      "Train Epoch: 752 [34304/54000 (64%)] Loss: -344930.031250\n",
      "Train Epoch: 752 [45568/54000 (84%)] Loss: -214581.671875\n",
      "    epoch          : 752\n",
      "    loss           : -277618.9109375\n",
      "    val_loss       : -279028.5421875\n",
      "Train Epoch: 753 [512/54000 (1%)] Loss: -345334.156250\n",
      "Train Epoch: 753 [11776/54000 (22%)] Loss: -351413.875000\n",
      "Train Epoch: 753 [23040/54000 (43%)] Loss: -250260.156250\n",
      "Train Epoch: 753 [34304/54000 (64%)] Loss: -363713.125000\n",
      "Train Epoch: 753 [45568/54000 (84%)] Loss: -216030.640625\n",
      "    epoch          : 753\n",
      "    loss           : -277754.03859375\n",
      "    val_loss       : -278367.16796875\n",
      "Train Epoch: 754 [512/54000 (1%)] Loss: -250180.812500\n",
      "Train Epoch: 754 [11776/54000 (22%)] Loss: -256748.187500\n",
      "Train Epoch: 754 [23040/54000 (43%)] Loss: -241500.453125\n",
      "Train Epoch: 754 [34304/54000 (64%)] Loss: -330285.187500\n",
      "Train Epoch: 754 [45568/54000 (84%)] Loss: -328106.437500\n",
      "    epoch          : 754\n",
      "    loss           : -277780.1321875\n",
      "    val_loss       : -278801.471484375\n",
      "Train Epoch: 755 [512/54000 (1%)] Loss: -245188.593750\n",
      "Train Epoch: 755 [11776/54000 (22%)] Loss: -362281.750000\n",
      "Train Epoch: 755 [23040/54000 (43%)] Loss: -179652.640625\n",
      "Train Epoch: 755 [34304/54000 (64%)] Loss: -256000.953125\n",
      "Train Epoch: 755 [45568/54000 (84%)] Loss: -236992.687500\n",
      "    epoch          : 755\n",
      "    loss           : -277756.8434375\n",
      "    val_loss       : -278930.985546875\n",
      "Train Epoch: 756 [512/54000 (1%)] Loss: -249442.750000\n",
      "Train Epoch: 756 [11776/54000 (22%)] Loss: -251749.062500\n",
      "Train Epoch: 756 [23040/54000 (43%)] Loss: -253641.625000\n",
      "Train Epoch: 756 [34304/54000 (64%)] Loss: -261095.703125\n",
      "Train Epoch: 756 [45568/54000 (84%)] Loss: -249787.031250\n",
      "    epoch          : 756\n",
      "    loss           : -277898.633125\n",
      "    val_loss       : -278854.614453125\n",
      "Train Epoch: 757 [512/54000 (1%)] Loss: -351188.812500\n",
      "Train Epoch: 757 [11776/54000 (22%)] Loss: -254402.906250\n",
      "Train Epoch: 757 [23040/54000 (43%)] Loss: -344886.875000\n",
      "Train Epoch: 757 [34304/54000 (64%)] Loss: -345853.875000\n",
      "Train Epoch: 757 [45568/54000 (84%)] Loss: -246746.031250\n",
      "    epoch          : 757\n",
      "    loss           : -277732.36390625\n",
      "    val_loss       : -279013.1125\n",
      "Train Epoch: 758 [512/54000 (1%)] Loss: -330048.250000\n",
      "Train Epoch: 758 [11776/54000 (22%)] Loss: -345325.687500\n",
      "Train Epoch: 758 [23040/54000 (43%)] Loss: -345746.000000\n",
      "Train Epoch: 758 [34304/54000 (64%)] Loss: -247121.593750\n",
      "Train Epoch: 758 [45568/54000 (84%)] Loss: -242355.718750\n",
      "    epoch          : 758\n",
      "    loss           : -277864.4184375\n",
      "    val_loss       : -279029.6078125\n",
      "Train Epoch: 759 [512/54000 (1%)] Loss: -247307.078125\n",
      "Train Epoch: 759 [11776/54000 (22%)] Loss: -169731.921875\n",
      "Train Epoch: 759 [23040/54000 (43%)] Loss: -326235.625000\n",
      "Train Epoch: 759 [34304/54000 (64%)] Loss: -258345.390625\n",
      "Train Epoch: 759 [45568/54000 (84%)] Loss: -351449.187500\n",
      "    epoch          : 759\n",
      "    loss           : -278152.203125\n",
      "    val_loss       : -279081.69296875\n",
      "Train Epoch: 760 [512/54000 (1%)] Loss: -256326.718750\n",
      "Train Epoch: 760 [11776/54000 (22%)] Loss: -346092.468750\n",
      "Train Epoch: 760 [23040/54000 (43%)] Loss: -171008.343750\n",
      "Train Epoch: 760 [34304/54000 (64%)] Loss: -223648.125000\n",
      "Train Epoch: 760 [45568/54000 (84%)] Loss: -331653.437500\n",
      "    epoch          : 760\n",
      "    loss           : -278103.38234375\n",
      "    val_loss       : -279263.47265625\n",
      "Train Epoch: 761 [512/54000 (1%)] Loss: -364898.437500\n",
      "Train Epoch: 761 [11776/54000 (22%)] Loss: -328768.656250\n",
      "Train Epoch: 761 [23040/54000 (43%)] Loss: -251732.281250\n",
      "Train Epoch: 761 [34304/54000 (64%)] Loss: -212916.390625\n",
      "Train Epoch: 761 [45568/54000 (84%)] Loss: -212959.562500\n",
      "    epoch          : 761\n",
      "    loss           : -278243.568125\n",
      "    val_loss       : -278988.097265625\n",
      "Train Epoch: 762 [512/54000 (1%)] Loss: -249707.265625\n",
      "Train Epoch: 762 [11776/54000 (22%)] Loss: -254293.406250\n",
      "Train Epoch: 762 [23040/54000 (43%)] Loss: -331482.406250\n",
      "Train Epoch: 762 [34304/54000 (64%)] Loss: -253155.187500\n",
      "Train Epoch: 762 [45568/54000 (84%)] Loss: -350310.281250\n",
      "    epoch          : 762\n",
      "    loss           : -278327.26078125\n",
      "    val_loss       : -279839.758203125\n",
      "Train Epoch: 763 [512/54000 (1%)] Loss: -256194.937500\n",
      "Train Epoch: 763 [11776/54000 (22%)] Loss: -351993.250000\n",
      "Train Epoch: 763 [23040/54000 (43%)] Loss: -349855.937500\n",
      "Train Epoch: 763 [34304/54000 (64%)] Loss: -215110.687500\n",
      "Train Epoch: 763 [45568/54000 (84%)] Loss: -240151.640625\n",
      "    epoch          : 763\n",
      "    loss           : -278316.334375\n",
      "    val_loss       : -279288.76875\n",
      "Train Epoch: 764 [512/54000 (1%)] Loss: -241713.546875\n",
      "Train Epoch: 764 [11776/54000 (22%)] Loss: -253272.250000\n",
      "Train Epoch: 764 [23040/54000 (43%)] Loss: -347338.562500\n",
      "Train Epoch: 764 [34304/54000 (64%)] Loss: -212199.671875\n",
      "Train Epoch: 764 [45568/54000 (84%)] Loss: -347276.562500\n",
      "    epoch          : 764\n",
      "    loss           : -278370.73359375\n",
      "    val_loss       : -279546.150390625\n",
      "Train Epoch: 765 [512/54000 (1%)] Loss: -242243.031250\n",
      "Train Epoch: 765 [11776/54000 (22%)] Loss: -255355.375000\n",
      "Train Epoch: 765 [23040/54000 (43%)] Loss: -249156.375000\n",
      "Train Epoch: 765 [34304/54000 (64%)] Loss: -244080.625000\n",
      "Train Epoch: 765 [45568/54000 (84%)] Loss: -362066.906250\n",
      "    epoch          : 765\n",
      "    loss           : -278400.7078125\n",
      "    val_loss       : -279008.083984375\n",
      "Train Epoch: 766 [512/54000 (1%)] Loss: -254419.500000\n",
      "Train Epoch: 766 [11776/54000 (22%)] Loss: -345945.843750\n",
      "Train Epoch: 766 [23040/54000 (43%)] Loss: -367653.562500\n",
      "Train Epoch: 766 [34304/54000 (64%)] Loss: -245794.843750\n",
      "Train Epoch: 766 [45568/54000 (84%)] Loss: -241346.406250\n",
      "    epoch          : 766\n",
      "    loss           : -278480.270625\n",
      "    val_loss       : -279194.247265625\n",
      "Train Epoch: 767 [512/54000 (1%)] Loss: -367180.625000\n",
      "Train Epoch: 767 [11776/54000 (22%)] Loss: -351208.906250\n",
      "Train Epoch: 767 [23040/54000 (43%)] Loss: -351438.437500\n",
      "Train Epoch: 767 [34304/54000 (64%)] Loss: -351659.687500\n",
      "Train Epoch: 767 [45568/54000 (84%)] Loss: -352679.500000\n",
      "    epoch          : 767\n",
      "    loss           : -278563.740625\n",
      "    val_loss       : -279331.06328125\n",
      "Train Epoch: 768 [512/54000 (1%)] Loss: -344708.312500\n",
      "Train Epoch: 768 [11776/54000 (22%)] Loss: -366198.000000\n",
      "Train Epoch: 768 [23040/54000 (43%)] Loss: -181468.750000\n",
      "Train Epoch: 768 [34304/54000 (64%)] Loss: -248555.375000\n",
      "Train Epoch: 768 [45568/54000 (84%)] Loss: -331821.875000\n",
      "    epoch          : 768\n",
      "    loss           : -278585.136875\n",
      "    val_loss       : -279642.828125\n",
      "Train Epoch: 769 [512/54000 (1%)] Loss: -220799.359375\n",
      "Train Epoch: 769 [11776/54000 (22%)] Loss: -211961.031250\n",
      "Train Epoch: 769 [23040/54000 (43%)] Loss: -250518.109375\n",
      "Train Epoch: 769 [34304/54000 (64%)] Loss: -352913.875000\n",
      "Train Epoch: 769 [45568/54000 (84%)] Loss: -349973.375000\n",
      "    epoch          : 769\n",
      "    loss           : -278667.6778125\n",
      "    val_loss       : -279285.030078125\n",
      "Train Epoch: 770 [512/54000 (1%)] Loss: -256545.578125\n",
      "Train Epoch: 770 [11776/54000 (22%)] Loss: -242569.000000\n",
      "Train Epoch: 770 [23040/54000 (43%)] Loss: -180989.343750\n",
      "Train Epoch: 770 [34304/54000 (64%)] Loss: -245082.812500\n",
      "Train Epoch: 770 [45568/54000 (84%)] Loss: -244427.968750\n",
      "    epoch          : 770\n",
      "    loss           : -278677.5840625\n",
      "    val_loss       : -279923.699609375\n",
      "Train Epoch: 771 [512/54000 (1%)] Loss: -332583.406250\n",
      "Train Epoch: 771 [11776/54000 (22%)] Loss: -256528.828125\n",
      "Train Epoch: 771 [23040/54000 (43%)] Loss: -243656.359375\n",
      "Train Epoch: 771 [34304/54000 (64%)] Loss: -348652.875000\n",
      "Train Epoch: 771 [45568/54000 (84%)] Loss: -350085.812500\n",
      "    epoch          : 771\n",
      "    loss           : -278771.50875\n",
      "    val_loss       : -279832.555078125\n",
      "Train Epoch: 772 [512/54000 (1%)] Loss: -244010.187500\n",
      "Train Epoch: 772 [11776/54000 (22%)] Loss: -245728.359375\n",
      "Train Epoch: 772 [23040/54000 (43%)] Loss: -247379.562500\n",
      "Train Epoch: 772 [34304/54000 (64%)] Loss: -351202.687500\n",
      "Train Epoch: 772 [45568/54000 (84%)] Loss: -325926.812500\n",
      "    epoch          : 772\n",
      "    loss           : -278793.8028125\n",
      "    val_loss       : -279884.6484375\n",
      "Train Epoch: 773 [512/54000 (1%)] Loss: -329940.781250\n",
      "Train Epoch: 773 [11776/54000 (22%)] Loss: -250378.546875\n",
      "Train Epoch: 773 [23040/54000 (43%)] Loss: -212257.640625\n",
      "Train Epoch: 773 [34304/54000 (64%)] Loss: -180263.375000\n",
      "Train Epoch: 773 [45568/54000 (84%)] Loss: -214913.078125\n",
      "    epoch          : 773\n",
      "    loss           : -278831.0875\n",
      "    val_loss       : -280104.517578125\n",
      "Train Epoch: 774 [512/54000 (1%)] Loss: -251948.437500\n",
      "Train Epoch: 774 [11776/54000 (22%)] Loss: -367188.531250\n",
      "Train Epoch: 774 [23040/54000 (43%)] Loss: -333346.062500\n",
      "Train Epoch: 774 [34304/54000 (64%)] Loss: -351272.375000\n",
      "Train Epoch: 774 [45568/54000 (84%)] Loss: -246277.421875\n",
      "    epoch          : 774\n",
      "    loss           : -278913.73078125\n",
      "    val_loss       : -280035.694921875\n",
      "Train Epoch: 775 [512/54000 (1%)] Loss: -250718.250000\n",
      "Train Epoch: 775 [11776/54000 (22%)] Loss: -217748.031250\n",
      "Train Epoch: 775 [23040/54000 (43%)] Loss: -252878.093750\n",
      "Train Epoch: 775 [34304/54000 (64%)] Loss: -244859.031250\n",
      "Train Epoch: 775 [45568/54000 (84%)] Loss: -329406.500000\n",
      "    epoch          : 775\n",
      "    loss           : -278917.5203125\n",
      "    val_loss       : -279866.52890625\n",
      "Train Epoch: 776 [512/54000 (1%)] Loss: -245285.500000\n",
      "Train Epoch: 776 [11776/54000 (22%)] Loss: -253361.625000\n",
      "Train Epoch: 776 [23040/54000 (43%)] Loss: -246493.375000\n",
      "Train Epoch: 776 [34304/54000 (64%)] Loss: -368377.406250\n",
      "Train Epoch: 776 [45568/54000 (84%)] Loss: -245079.093750\n",
      "    epoch          : 776\n",
      "    loss           : -278876.41296875\n",
      "    val_loss       : -280277.3125\n",
      "Train Epoch: 777 [512/54000 (1%)] Loss: -179422.984375\n",
      "Train Epoch: 777 [11776/54000 (22%)] Loss: -222825.812500\n",
      "Train Epoch: 777 [23040/54000 (43%)] Loss: -247217.687500\n",
      "Train Epoch: 777 [34304/54000 (64%)] Loss: -326873.000000\n",
      "Train Epoch: 777 [45568/54000 (84%)] Loss: -242420.687500\n",
      "    epoch          : 777\n",
      "    loss           : -279104.8828125\n",
      "    val_loss       : -280186.31953125\n",
      "Train Epoch: 778 [512/54000 (1%)] Loss: -249100.781250\n",
      "Train Epoch: 778 [11776/54000 (22%)] Loss: -352470.000000\n",
      "Train Epoch: 778 [23040/54000 (43%)] Loss: -328022.406250\n",
      "Train Epoch: 778 [34304/54000 (64%)] Loss: -250080.359375\n",
      "Train Epoch: 778 [45568/54000 (84%)] Loss: -351961.375000\n",
      "    epoch          : 778\n",
      "    loss           : -279205.16890625\n",
      "    val_loss       : -279477.834765625\n",
      "Train Epoch: 779 [512/54000 (1%)] Loss: -178978.937500\n",
      "Train Epoch: 779 [11776/54000 (22%)] Loss: -327727.125000\n",
      "Train Epoch: 779 [23040/54000 (43%)] Loss: -246057.953125\n",
      "Train Epoch: 779 [34304/54000 (64%)] Loss: -258291.593750\n",
      "Train Epoch: 779 [45568/54000 (84%)] Loss: -216543.171875\n",
      "    epoch          : 779\n",
      "    loss           : -279139.6871875\n",
      "    val_loss       : -280602.61875\n",
      "Train Epoch: 780 [512/54000 (1%)] Loss: -183607.859375\n",
      "Train Epoch: 780 [11776/54000 (22%)] Loss: -363227.875000\n",
      "Train Epoch: 780 [23040/54000 (43%)] Loss: -247443.234375\n",
      "Train Epoch: 780 [34304/54000 (64%)] Loss: -215546.671875\n",
      "Train Epoch: 780 [45568/54000 (84%)] Loss: -350041.406250\n",
      "    epoch          : 780\n",
      "    loss           : -279183.62484375\n",
      "    val_loss       : -280576.8875\n",
      "Train Epoch: 781 [512/54000 (1%)] Loss: -348638.218750\n",
      "Train Epoch: 781 [11776/54000 (22%)] Loss: -256342.437500\n",
      "Train Epoch: 781 [23040/54000 (43%)] Loss: -353681.687500\n",
      "Train Epoch: 781 [34304/54000 (64%)] Loss: -220544.750000\n",
      "Train Epoch: 781 [45568/54000 (84%)] Loss: -350548.937500\n",
      "    epoch          : 781\n",
      "    loss           : -279346.41859375\n",
      "    val_loss       : -280482.33046875\n",
      "Train Epoch: 782 [512/54000 (1%)] Loss: -348055.687500\n",
      "Train Epoch: 782 [11776/54000 (22%)] Loss: -366998.218750\n",
      "Train Epoch: 782 [23040/54000 (43%)] Loss: -172458.515625\n",
      "Train Epoch: 782 [34304/54000 (64%)] Loss: -245520.046875\n",
      "Train Epoch: 782 [45568/54000 (84%)] Loss: -330720.062500\n",
      "    epoch          : 782\n",
      "    loss           : -279255.94015625\n",
      "    val_loss       : -280353.596875\n",
      "Train Epoch: 783 [512/54000 (1%)] Loss: -256767.484375\n",
      "Train Epoch: 783 [11776/54000 (22%)] Loss: -353052.968750\n",
      "Train Epoch: 783 [23040/54000 (43%)] Loss: -251066.093750\n",
      "Train Epoch: 783 [34304/54000 (64%)] Loss: -181829.796875\n",
      "Train Epoch: 783 [45568/54000 (84%)] Loss: -241662.843750\n",
      "    epoch          : 783\n",
      "    loss           : -279436.59796875\n",
      "    val_loss       : -280218.5015625\n",
      "Train Epoch: 784 [512/54000 (1%)] Loss: -366213.500000\n",
      "Train Epoch: 784 [11776/54000 (22%)] Loss: -178880.265625\n",
      "Train Epoch: 784 [23040/54000 (43%)] Loss: -368586.812500\n",
      "Train Epoch: 784 [34304/54000 (64%)] Loss: -328691.843750\n",
      "Train Epoch: 784 [45568/54000 (84%)] Loss: -354891.468750\n",
      "    epoch          : 784\n",
      "    loss           : -279340.20796875\n",
      "    val_loss       : -280446.02578125\n",
      "Train Epoch: 785 [512/54000 (1%)] Loss: -345057.687500\n",
      "Train Epoch: 785 [11776/54000 (22%)] Loss: -328337.156250\n",
      "Train Epoch: 785 [23040/54000 (43%)] Loss: -367878.062500\n",
      "Train Epoch: 785 [34304/54000 (64%)] Loss: -245174.578125\n",
      "Train Epoch: 785 [45568/54000 (84%)] Loss: -257743.281250\n",
      "    epoch          : 785\n",
      "    loss           : -279454.1090625\n",
      "    val_loss       : -280909.425390625\n",
      "Train Epoch: 786 [512/54000 (1%)] Loss: -253323.265625\n",
      "Train Epoch: 786 [11776/54000 (22%)] Loss: -218026.781250\n",
      "Train Epoch: 786 [23040/54000 (43%)] Loss: -223546.812500\n",
      "Train Epoch: 786 [34304/54000 (64%)] Loss: -367325.312500\n",
      "Train Epoch: 786 [45568/54000 (84%)] Loss: -259352.843750\n",
      "    epoch          : 786\n",
      "    loss           : -279666.82140625\n",
      "    val_loss       : -280805.430078125\n",
      "Train Epoch: 787 [512/54000 (1%)] Loss: -242785.031250\n",
      "Train Epoch: 787 [11776/54000 (22%)] Loss: -255156.031250\n",
      "Train Epoch: 787 [23040/54000 (43%)] Loss: -257987.750000\n",
      "Train Epoch: 787 [34304/54000 (64%)] Loss: -256408.109375\n",
      "Train Epoch: 787 [45568/54000 (84%)] Loss: -219019.609375\n",
      "    epoch          : 787\n",
      "    loss           : -279666.76375\n",
      "    val_loss       : -281141.995703125\n",
      "Train Epoch: 788 [512/54000 (1%)] Loss: -249552.156250\n",
      "Train Epoch: 788 [11776/54000 (22%)] Loss: -365925.781250\n",
      "Train Epoch: 788 [23040/54000 (43%)] Loss: -366067.718750\n",
      "Train Epoch: 788 [34304/54000 (64%)] Loss: -329938.500000\n",
      "Train Epoch: 788 [45568/54000 (84%)] Loss: -328952.000000\n",
      "    epoch          : 788\n",
      "    loss           : -279587.67484375\n",
      "    val_loss       : -280489.14921875\n",
      "Train Epoch: 789 [512/54000 (1%)] Loss: -251985.390625\n",
      "Train Epoch: 789 [11776/54000 (22%)] Loss: -365972.343750\n",
      "Train Epoch: 789 [23040/54000 (43%)] Loss: -368955.000000\n",
      "Train Epoch: 789 [34304/54000 (64%)] Loss: -348227.593750\n",
      "Train Epoch: 789 [45568/54000 (84%)] Loss: -219126.125000\n",
      "    epoch          : 789\n",
      "    loss           : -279739.15296875\n",
      "    val_loss       : -280722.058203125\n",
      "Train Epoch: 790 [512/54000 (1%)] Loss: -256430.812500\n",
      "Train Epoch: 790 [11776/54000 (22%)] Loss: -330528.656250\n",
      "Train Epoch: 790 [23040/54000 (43%)] Loss: -249550.828125\n",
      "Train Epoch: 790 [34304/54000 (64%)] Loss: -246726.906250\n",
      "Train Epoch: 790 [45568/54000 (84%)] Loss: -239137.359375\n",
      "    epoch          : 790\n",
      "    loss           : -279681.1753125\n",
      "    val_loss       : -280856.677734375\n",
      "Train Epoch: 791 [512/54000 (1%)] Loss: -368885.343750\n",
      "Train Epoch: 791 [11776/54000 (22%)] Loss: -172614.500000\n",
      "Train Epoch: 791 [23040/54000 (43%)] Loss: -260736.109375\n",
      "Train Epoch: 791 [34304/54000 (64%)] Loss: -255434.140625\n",
      "Train Epoch: 791 [45568/54000 (84%)] Loss: -212097.312500\n",
      "    epoch          : 791\n",
      "    loss           : -279813.84859375\n",
      "    val_loss       : -280653.378125\n",
      "Train Epoch: 792 [512/54000 (1%)] Loss: -351408.968750\n",
      "Train Epoch: 792 [11776/54000 (22%)] Loss: -241331.296875\n",
      "Train Epoch: 792 [23040/54000 (43%)] Loss: -255420.468750\n",
      "Train Epoch: 792 [34304/54000 (64%)] Loss: -365477.750000\n",
      "Train Epoch: 792 [45568/54000 (84%)] Loss: -251929.578125\n",
      "    epoch          : 792\n",
      "    loss           : -279972.81859375\n",
      "    val_loss       : -280689.126953125\n",
      "Train Epoch: 793 [512/54000 (1%)] Loss: -255575.703125\n",
      "Train Epoch: 793 [11776/54000 (22%)] Loss: -248502.171875\n",
      "Train Epoch: 793 [23040/54000 (43%)] Loss: -364088.593750\n",
      "Train Epoch: 793 [34304/54000 (64%)] Loss: -346942.593750\n",
      "Train Epoch: 793 [45568/54000 (84%)] Loss: -330506.093750\n",
      "    epoch          : 793\n",
      "    loss           : -279940.66640625\n",
      "    val_loss       : -280949.829296875\n",
      "Train Epoch: 794 [512/54000 (1%)] Loss: -263952.468750\n",
      "Train Epoch: 794 [11776/54000 (22%)] Loss: -244502.906250\n",
      "Train Epoch: 794 [23040/54000 (43%)] Loss: -259842.296875\n",
      "Train Epoch: 794 [34304/54000 (64%)] Loss: -257329.281250\n",
      "Train Epoch: 794 [45568/54000 (84%)] Loss: -221181.718750\n",
      "    epoch          : 794\n",
      "    loss           : -280030.5853125\n",
      "    val_loss       : -281128.356640625\n",
      "Train Epoch: 795 [512/54000 (1%)] Loss: -176127.921875\n",
      "Train Epoch: 795 [11776/54000 (22%)] Loss: -245503.656250\n",
      "Train Epoch: 795 [23040/54000 (43%)] Loss: -256387.640625\n",
      "Train Epoch: 795 [34304/54000 (64%)] Loss: -245660.312500\n",
      "Train Epoch: 795 [45568/54000 (84%)] Loss: -332284.687500\n",
      "    epoch          : 795\n",
      "    loss           : -280087.47171875\n",
      "    val_loss       : -281202.068359375\n",
      "Train Epoch: 796 [512/54000 (1%)] Loss: -347184.750000\n",
      "Train Epoch: 796 [11776/54000 (22%)] Loss: -367901.781250\n",
      "Train Epoch: 796 [23040/54000 (43%)] Loss: -213188.531250\n",
      "Train Epoch: 796 [34304/54000 (64%)] Loss: -222031.218750\n",
      "Train Epoch: 796 [45568/54000 (84%)] Loss: -366783.875000\n",
      "    epoch          : 796\n",
      "    loss           : -280077.1059375\n",
      "    val_loss       : -281296.377734375\n",
      "Train Epoch: 797 [512/54000 (1%)] Loss: -253955.859375\n",
      "Train Epoch: 797 [11776/54000 (22%)] Loss: -252938.562500\n",
      "Train Epoch: 797 [23040/54000 (43%)] Loss: -177000.968750\n",
      "Train Epoch: 797 [34304/54000 (64%)] Loss: -256032.843750\n",
      "Train Epoch: 797 [45568/54000 (84%)] Loss: -330848.156250\n",
      "    epoch          : 797\n",
      "    loss           : -280058.378125\n",
      "    val_loss       : -280898.961328125\n",
      "Train Epoch: 798 [512/54000 (1%)] Loss: -349371.031250\n",
      "Train Epoch: 798 [11776/54000 (22%)] Loss: -352412.000000\n",
      "Train Epoch: 798 [23040/54000 (43%)] Loss: -172213.671875\n",
      "Train Epoch: 798 [34304/54000 (64%)] Loss: -177181.765625\n",
      "Train Epoch: 798 [45568/54000 (84%)] Loss: -333178.843750\n",
      "    epoch          : 798\n",
      "    loss           : -280152.49015625\n",
      "    val_loss       : -281738.9375\n",
      "Train Epoch: 799 [512/54000 (1%)] Loss: -328606.312500\n",
      "Train Epoch: 799 [11776/54000 (22%)] Loss: -355463.718750\n",
      "Train Epoch: 799 [23040/54000 (43%)] Loss: -174643.000000\n",
      "Train Epoch: 799 [34304/54000 (64%)] Loss: -351484.781250\n",
      "Train Epoch: 799 [45568/54000 (84%)] Loss: -330766.031250\n",
      "    epoch          : 799\n",
      "    loss           : -280273.79140625\n",
      "    val_loss       : -282053.3796875\n",
      "Train Epoch: 800 [512/54000 (1%)] Loss: -245523.000000\n",
      "Train Epoch: 800 [11776/54000 (22%)] Loss: -366000.375000\n",
      "Train Epoch: 800 [23040/54000 (43%)] Loss: -224939.406250\n",
      "Train Epoch: 800 [34304/54000 (64%)] Loss: -222220.062500\n",
      "Train Epoch: 800 [45568/54000 (84%)] Loss: -332592.187500\n",
      "    epoch          : 800\n",
      "    loss           : -280412.20078125\n",
      "    val_loss       : -281486.26328125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch800.pth ...\n",
      "Train Epoch: 801 [512/54000 (1%)] Loss: -253282.500000\n",
      "Train Epoch: 801 [11776/54000 (22%)] Loss: -368735.000000\n",
      "Train Epoch: 801 [23040/54000 (43%)] Loss: -256720.906250\n",
      "Train Epoch: 801 [34304/54000 (64%)] Loss: -257728.125000\n",
      "Train Epoch: 801 [45568/54000 (84%)] Loss: -346827.531250\n",
      "    epoch          : 801\n",
      "    loss           : -280428.3228125\n",
      "    val_loss       : -281779.252734375\n",
      "Train Epoch: 802 [512/54000 (1%)] Loss: -357755.906250\n",
      "Train Epoch: 802 [11776/54000 (22%)] Loss: -353821.843750\n",
      "Train Epoch: 802 [23040/54000 (43%)] Loss: -245459.562500\n",
      "Train Epoch: 802 [34304/54000 (64%)] Loss: -257656.156250\n",
      "Train Epoch: 802 [45568/54000 (84%)] Loss: -353383.125000\n",
      "    epoch          : 802\n",
      "    loss           : -280539.71796875\n",
      "    val_loss       : -281470.707421875\n",
      "Train Epoch: 803 [512/54000 (1%)] Loss: -330896.093750\n",
      "Train Epoch: 803 [11776/54000 (22%)] Loss: -351965.250000\n",
      "Train Epoch: 803 [23040/54000 (43%)] Loss: -258907.312500\n",
      "Train Epoch: 803 [34304/54000 (64%)] Loss: -254211.250000\n",
      "Train Epoch: 803 [45568/54000 (84%)] Loss: -215779.500000\n",
      "    epoch          : 803\n",
      "    loss           : -280445.07453125\n",
      "    val_loss       : -281601.05234375\n",
      "Train Epoch: 804 [512/54000 (1%)] Loss: -247396.359375\n",
      "Train Epoch: 804 [11776/54000 (22%)] Loss: -257903.109375\n",
      "Train Epoch: 804 [23040/54000 (43%)] Loss: -167112.250000\n",
      "Train Epoch: 804 [34304/54000 (64%)] Loss: -218230.031250\n",
      "Train Epoch: 804 [45568/54000 (84%)] Loss: -216570.937500\n",
      "    epoch          : 804\n",
      "    loss           : -280416.5878125\n",
      "    val_loss       : -281374.116015625\n",
      "Train Epoch: 805 [512/54000 (1%)] Loss: -257527.437500\n",
      "Train Epoch: 805 [11776/54000 (22%)] Loss: -350354.812500\n",
      "Train Epoch: 805 [23040/54000 (43%)] Loss: -221380.468750\n",
      "Train Epoch: 805 [34304/54000 (64%)] Loss: -350050.468750\n",
      "Train Epoch: 805 [45568/54000 (84%)] Loss: -211811.234375\n",
      "    epoch          : 805\n",
      "    loss           : -280449.30453125\n",
      "    val_loss       : -281211.3078125\n",
      "Train Epoch: 806 [512/54000 (1%)] Loss: -348084.250000\n",
      "Train Epoch: 806 [11776/54000 (22%)] Loss: -242444.281250\n",
      "Train Epoch: 806 [23040/54000 (43%)] Loss: -332308.937500\n",
      "Train Epoch: 806 [34304/54000 (64%)] Loss: -219996.281250\n",
      "Train Epoch: 806 [45568/54000 (84%)] Loss: -331316.468750\n",
      "    epoch          : 806\n",
      "    loss           : -280603.84375\n",
      "    val_loss       : -282390.43359375\n",
      "Train Epoch: 807 [512/54000 (1%)] Loss: -181288.484375\n",
      "Train Epoch: 807 [11776/54000 (22%)] Loss: -171983.421875\n",
      "Train Epoch: 807 [23040/54000 (43%)] Loss: -242990.593750\n",
      "Train Epoch: 807 [34304/54000 (64%)] Loss: -217705.250000\n",
      "Train Epoch: 807 [45568/54000 (84%)] Loss: -370385.968750\n",
      "    epoch          : 807\n",
      "    loss           : -280658.93953125\n",
      "    val_loss       : -282089.630078125\n",
      "Train Epoch: 808 [512/54000 (1%)] Loss: -333001.937500\n",
      "Train Epoch: 808 [11776/54000 (22%)] Loss: -256388.546875\n",
      "Train Epoch: 808 [23040/54000 (43%)] Loss: -370470.812500\n",
      "Train Epoch: 808 [34304/54000 (64%)] Loss: -257268.718750\n",
      "Train Epoch: 808 [45568/54000 (84%)] Loss: -253124.703125\n",
      "    epoch          : 808\n",
      "    loss           : -280609.26359375\n",
      "    val_loss       : -281091.709375\n",
      "Train Epoch: 809 [512/54000 (1%)] Loss: -251368.828125\n",
      "Train Epoch: 809 [11776/54000 (22%)] Loss: -329033.375000\n",
      "Train Epoch: 809 [23040/54000 (43%)] Loss: -247378.093750\n",
      "Train Epoch: 809 [34304/54000 (64%)] Loss: -332234.437500\n",
      "Train Epoch: 809 [45568/54000 (84%)] Loss: -253261.437500\n",
      "    epoch          : 809\n",
      "    loss           : -280655.16640625\n",
      "    val_loss       : -281658.492578125\n",
      "Train Epoch: 810 [512/54000 (1%)] Loss: -250874.812500\n",
      "Train Epoch: 810 [11776/54000 (22%)] Loss: -349689.375000\n",
      "Train Epoch: 810 [23040/54000 (43%)] Loss: -172963.437500\n",
      "Train Epoch: 810 [34304/54000 (64%)] Loss: -348079.375000\n",
      "Train Epoch: 810 [45568/54000 (84%)] Loss: -333838.593750\n",
      "    epoch          : 810\n",
      "    loss           : -280773.2203125\n",
      "    val_loss       : -281914.975\n",
      "Train Epoch: 811 [512/54000 (1%)] Loss: -176982.390625\n",
      "Train Epoch: 811 [11776/54000 (22%)] Loss: -329688.843750\n",
      "Train Epoch: 811 [23040/54000 (43%)] Loss: -251143.890625\n",
      "Train Epoch: 811 [34304/54000 (64%)] Loss: -359442.687500\n",
      "Train Epoch: 811 [45568/54000 (84%)] Loss: -213505.828125\n",
      "    epoch          : 811\n",
      "    loss           : -280836.30828125\n",
      "    val_loss       : -281783.225390625\n",
      "Train Epoch: 812 [512/54000 (1%)] Loss: -257175.703125\n",
      "Train Epoch: 812 [11776/54000 (22%)] Loss: -259784.625000\n",
      "Train Epoch: 812 [23040/54000 (43%)] Loss: -355732.625000\n",
      "Train Epoch: 812 [34304/54000 (64%)] Loss: -368030.312500\n",
      "Train Epoch: 812 [45568/54000 (84%)] Loss: -333988.656250\n",
      "    epoch          : 812\n",
      "    loss           : -281090.80265625\n",
      "    val_loss       : -282219.784765625\n",
      "Train Epoch: 813 [512/54000 (1%)] Loss: -256569.687500\n",
      "Train Epoch: 813 [11776/54000 (22%)] Loss: -246044.125000\n",
      "Train Epoch: 813 [23040/54000 (43%)] Loss: -329934.562500\n",
      "Train Epoch: 813 [34304/54000 (64%)] Loss: -329377.187500\n",
      "Train Epoch: 813 [45568/54000 (84%)] Loss: -244025.000000\n",
      "    epoch          : 813\n",
      "    loss           : -280954.51734375\n",
      "    val_loss       : -282676.92734375\n",
      "Train Epoch: 814 [512/54000 (1%)] Loss: -177641.359375\n",
      "Train Epoch: 814 [11776/54000 (22%)] Loss: -367968.375000\n",
      "Train Epoch: 814 [23040/54000 (43%)] Loss: -328023.375000\n",
      "Train Epoch: 814 [34304/54000 (64%)] Loss: -249060.750000\n",
      "Train Epoch: 814 [45568/54000 (84%)] Loss: -242937.015625\n",
      "    epoch          : 814\n",
      "    loss           : -281011.765625\n",
      "    val_loss       : -282262.36953125\n",
      "Train Epoch: 815 [512/54000 (1%)] Loss: -258344.812500\n",
      "Train Epoch: 815 [11776/54000 (22%)] Loss: -159752.921875\n",
      "Train Epoch: 815 [23040/54000 (43%)] Loss: -217424.765625\n",
      "Train Epoch: 815 [34304/54000 (64%)] Loss: -352830.656250\n",
      "Train Epoch: 815 [45568/54000 (84%)] Loss: -370405.437500\n",
      "    epoch          : 815\n",
      "    loss           : -281134.30875\n",
      "    val_loss       : -282025.248046875\n",
      "Train Epoch: 816 [512/54000 (1%)] Loss: -246745.281250\n",
      "Train Epoch: 816 [11776/54000 (22%)] Loss: -245186.281250\n",
      "Train Epoch: 816 [23040/54000 (43%)] Loss: -354740.093750\n",
      "Train Epoch: 816 [34304/54000 (64%)] Loss: -355704.625000\n",
      "Train Epoch: 816 [45568/54000 (84%)] Loss: -332874.531250\n",
      "    epoch          : 816\n",
      "    loss           : -281069.8859375\n",
      "    val_loss       : -281655.891796875\n",
      "Train Epoch: 817 [512/54000 (1%)] Loss: -176163.187500\n",
      "Train Epoch: 817 [11776/54000 (22%)] Loss: -349385.437500\n",
      "Train Epoch: 817 [23040/54000 (43%)] Loss: -348717.750000\n",
      "Train Epoch: 817 [34304/54000 (64%)] Loss: -181919.593750\n",
      "Train Epoch: 817 [45568/54000 (84%)] Loss: -328498.000000\n",
      "    epoch          : 817\n",
      "    loss           : -281274.80109375\n",
      "    val_loss       : -281570.901953125\n",
      "Train Epoch: 818 [512/54000 (1%)] Loss: -348918.812500\n",
      "Train Epoch: 818 [11776/54000 (22%)] Loss: -166703.343750\n",
      "Train Epoch: 818 [23040/54000 (43%)] Loss: -332280.500000\n",
      "Train Epoch: 818 [34304/54000 (64%)] Loss: -368060.500000\n",
      "Train Epoch: 818 [45568/54000 (84%)] Loss: -218800.656250\n",
      "    epoch          : 818\n",
      "    loss           : -281000.13828125\n",
      "    val_loss       : -282537.96015625\n",
      "Train Epoch: 819 [512/54000 (1%)] Loss: -164789.718750\n",
      "Train Epoch: 819 [11776/54000 (22%)] Loss: -249488.125000\n",
      "Train Epoch: 819 [23040/54000 (43%)] Loss: -333342.187500\n",
      "Train Epoch: 819 [34304/54000 (64%)] Loss: -256149.468750\n",
      "Train Epoch: 819 [45568/54000 (84%)] Loss: -357169.843750\n",
      "    epoch          : 819\n",
      "    loss           : -281148.14046875\n",
      "    val_loss       : -282279.936328125\n",
      "Train Epoch: 820 [512/54000 (1%)] Loss: -259294.984375\n",
      "Train Epoch: 820 [11776/54000 (22%)] Loss: -349154.125000\n",
      "Train Epoch: 820 [23040/54000 (43%)] Loss: -214073.062500\n",
      "Train Epoch: 820 [34304/54000 (64%)] Loss: -258230.687500\n",
      "Train Epoch: 820 [45568/54000 (84%)] Loss: -331592.625000\n",
      "    epoch          : 820\n",
      "    loss           : -281470.8625\n",
      "    val_loss       : -282306.483203125\n",
      "Train Epoch: 821 [512/54000 (1%)] Loss: -253994.890625\n",
      "Train Epoch: 821 [11776/54000 (22%)] Loss: -333733.625000\n",
      "Train Epoch: 821 [23040/54000 (43%)] Loss: -259465.718750\n",
      "Train Epoch: 821 [34304/54000 (64%)] Loss: -369840.562500\n",
      "Train Epoch: 821 [45568/54000 (84%)] Loss: -355601.250000\n",
      "    epoch          : 821\n",
      "    loss           : -281270.795\n",
      "    val_loss       : -282720.154296875\n",
      "Train Epoch: 822 [512/54000 (1%)] Loss: -221459.062500\n",
      "Train Epoch: 822 [11776/54000 (22%)] Loss: -255273.468750\n",
      "Train Epoch: 822 [23040/54000 (43%)] Loss: -176467.406250\n",
      "Train Epoch: 822 [34304/54000 (64%)] Loss: -169181.062500\n",
      "Train Epoch: 822 [45568/54000 (84%)] Loss: -367359.531250\n",
      "    epoch          : 822\n",
      "    loss           : -281528.87484375\n",
      "    val_loss       : -282273.455078125\n",
      "Train Epoch: 823 [512/54000 (1%)] Loss: -348759.062500\n",
      "Train Epoch: 823 [11776/54000 (22%)] Loss: -252502.625000\n",
      "Train Epoch: 823 [23040/54000 (43%)] Loss: -244231.562500\n",
      "Train Epoch: 823 [34304/54000 (64%)] Loss: -244030.218750\n",
      "Train Epoch: 823 [45568/54000 (84%)] Loss: -371877.531250\n",
      "    epoch          : 823\n",
      "    loss           : -281446.32640625\n",
      "    val_loss       : -282883.25\n",
      "Train Epoch: 824 [512/54000 (1%)] Loss: -252220.750000\n",
      "Train Epoch: 824 [11776/54000 (22%)] Loss: -243918.156250\n",
      "Train Epoch: 824 [23040/54000 (43%)] Loss: -258500.828125\n",
      "Train Epoch: 824 [34304/54000 (64%)] Loss: -262341.593750\n",
      "Train Epoch: 824 [45568/54000 (84%)] Loss: -370319.312500\n",
      "    epoch          : 824\n",
      "    loss           : -281471.6253125\n",
      "    val_loss       : -282519.0078125\n",
      "Train Epoch: 825 [512/54000 (1%)] Loss: -219086.718750\n",
      "Train Epoch: 825 [11776/54000 (22%)] Loss: -352656.593750\n",
      "Train Epoch: 825 [23040/54000 (43%)] Loss: -259247.937500\n",
      "Train Epoch: 825 [34304/54000 (64%)] Loss: -332893.875000\n",
      "Train Epoch: 825 [45568/54000 (84%)] Loss: -253162.515625\n",
      "    epoch          : 825\n",
      "    loss           : -281575.61890625\n",
      "    val_loss       : -283247.80078125\n",
      "Train Epoch: 826 [512/54000 (1%)] Loss: -245065.968750\n",
      "Train Epoch: 826 [11776/54000 (22%)] Loss: -221245.750000\n",
      "Train Epoch: 826 [23040/54000 (43%)] Loss: -357373.937500\n",
      "Train Epoch: 826 [34304/54000 (64%)] Loss: -372480.812500\n",
      "Train Epoch: 826 [45568/54000 (84%)] Loss: -222256.562500\n",
      "    epoch          : 826\n",
      "    loss           : -281687.98046875\n",
      "    val_loss       : -283462.4015625\n",
      "Train Epoch: 827 [512/54000 (1%)] Loss: -332584.500000\n",
      "Train Epoch: 827 [11776/54000 (22%)] Loss: -351221.906250\n",
      "Train Epoch: 827 [23040/54000 (43%)] Loss: -330087.937500\n",
      "Train Epoch: 827 [34304/54000 (64%)] Loss: -182237.718750\n",
      "Train Epoch: 827 [45568/54000 (84%)] Loss: -267158.250000\n",
      "    epoch          : 827\n",
      "    loss           : -281678.330625\n",
      "    val_loss       : -282714.933203125\n",
      "Train Epoch: 828 [512/54000 (1%)] Loss: -368692.187500\n",
      "Train Epoch: 828 [11776/54000 (22%)] Loss: -173747.890625\n",
      "Train Epoch: 828 [23040/54000 (43%)] Loss: -249159.156250\n",
      "Train Epoch: 828 [34304/54000 (64%)] Loss: -348628.343750\n",
      "Train Epoch: 828 [45568/54000 (84%)] Loss: -183851.687500\n",
      "    epoch          : 828\n",
      "    loss           : -281736.5784375\n",
      "    val_loss       : -282849.361328125\n",
      "Train Epoch: 829 [512/54000 (1%)] Loss: -369742.250000\n",
      "Train Epoch: 829 [11776/54000 (22%)] Loss: -252023.765625\n",
      "Train Epoch: 829 [23040/54000 (43%)] Loss: -359109.187500\n",
      "Train Epoch: 829 [34304/54000 (64%)] Loss: -241507.828125\n",
      "Train Epoch: 829 [45568/54000 (84%)] Loss: -355074.750000\n",
      "    epoch          : 829\n",
      "    loss           : -281841.614375\n",
      "    val_loss       : -282400.64140625\n",
      "Train Epoch: 830 [512/54000 (1%)] Loss: -246366.250000\n",
      "Train Epoch: 830 [11776/54000 (22%)] Loss: -351583.468750\n",
      "Train Epoch: 830 [23040/54000 (43%)] Loss: -239741.625000\n",
      "Train Epoch: 830 [34304/54000 (64%)] Loss: -217859.968750\n",
      "Train Epoch: 830 [45568/54000 (84%)] Loss: -357229.656250\n",
      "    epoch          : 830\n",
      "    loss           : -281882.85375\n",
      "    val_loss       : -282872.295703125\n",
      "Train Epoch: 831 [512/54000 (1%)] Loss: -256430.406250\n",
      "Train Epoch: 831 [11776/54000 (22%)] Loss: -220104.453125\n",
      "Train Epoch: 831 [23040/54000 (43%)] Loss: -250027.187500\n",
      "Train Epoch: 831 [34304/54000 (64%)] Loss: -354438.250000\n",
      "Train Epoch: 831 [45568/54000 (84%)] Loss: -333029.500000\n",
      "    epoch          : 831\n",
      "    loss           : -281887.64078125\n",
      "    val_loss       : -283402.261328125\n",
      "Train Epoch: 832 [512/54000 (1%)] Loss: -351051.750000\n",
      "Train Epoch: 832 [11776/54000 (22%)] Loss: -241749.703125\n",
      "Train Epoch: 832 [23040/54000 (43%)] Loss: -252187.312500\n",
      "Train Epoch: 832 [34304/54000 (64%)] Loss: -328723.812500\n",
      "Train Epoch: 832 [45568/54000 (84%)] Loss: -372083.750000\n",
      "    epoch          : 832\n",
      "    loss           : -281970.59828125\n",
      "    val_loss       : -283243.083203125\n",
      "Train Epoch: 833 [512/54000 (1%)] Loss: -176162.062500\n",
      "Train Epoch: 833 [11776/54000 (22%)] Loss: -256872.656250\n",
      "Train Epoch: 833 [23040/54000 (43%)] Loss: -242110.062500\n",
      "Train Epoch: 833 [34304/54000 (64%)] Loss: -255462.437500\n",
      "Train Epoch: 833 [45568/54000 (84%)] Loss: -370665.875000\n",
      "    epoch          : 833\n",
      "    loss           : -282039.9225\n",
      "    val_loss       : -283419.851953125\n",
      "Train Epoch: 834 [512/54000 (1%)] Loss: -350409.062500\n",
      "Train Epoch: 834 [11776/54000 (22%)] Loss: -246811.546875\n",
      "Train Epoch: 834 [23040/54000 (43%)] Loss: -181692.156250\n",
      "Train Epoch: 834 [34304/54000 (64%)] Loss: -259845.375000\n",
      "Train Epoch: 834 [45568/54000 (84%)] Loss: -220536.812500\n",
      "    epoch          : 834\n",
      "    loss           : -281999.93296875\n",
      "    val_loss       : -283010.3203125\n",
      "Train Epoch: 835 [512/54000 (1%)] Loss: -357375.250000\n",
      "Train Epoch: 835 [11776/54000 (22%)] Loss: -355412.218750\n",
      "Train Epoch: 835 [23040/54000 (43%)] Loss: -249372.468750\n",
      "Train Epoch: 835 [34304/54000 (64%)] Loss: -220899.078125\n",
      "Train Epoch: 835 [45568/54000 (84%)] Loss: -255548.343750\n",
      "    epoch          : 835\n",
      "    loss           : -282010.76828125\n",
      "    val_loss       : -283225.020703125\n",
      "Train Epoch: 836 [512/54000 (1%)] Loss: -259939.750000\n",
      "Train Epoch: 836 [11776/54000 (22%)] Loss: -253859.234375\n",
      "Train Epoch: 836 [23040/54000 (43%)] Loss: -215857.000000\n",
      "Train Epoch: 836 [34304/54000 (64%)] Loss: -354945.187500\n",
      "Train Epoch: 836 [45568/54000 (84%)] Loss: -357821.437500\n",
      "    epoch          : 836\n",
      "    loss           : -281991.77546875\n",
      "    val_loss       : -283456.85703125\n",
      "Train Epoch: 837 [512/54000 (1%)] Loss: -370417.000000\n",
      "Train Epoch: 837 [11776/54000 (22%)] Loss: -252817.718750\n",
      "Train Epoch: 837 [23040/54000 (43%)] Loss: -255145.328125\n",
      "Train Epoch: 837 [34304/54000 (64%)] Loss: -356840.906250\n",
      "Train Epoch: 837 [45568/54000 (84%)] Loss: -220411.812500\n",
      "    epoch          : 837\n",
      "    loss           : -282290.06\n",
      "    val_loss       : -283605.221484375\n",
      "Train Epoch: 838 [512/54000 (1%)] Loss: -249037.000000\n",
      "Train Epoch: 838 [11776/54000 (22%)] Loss: -246290.890625\n",
      "Train Epoch: 838 [23040/54000 (43%)] Loss: -258414.625000\n",
      "Train Epoch: 838 [34304/54000 (64%)] Loss: -367681.875000\n",
      "Train Epoch: 838 [45568/54000 (84%)] Loss: -251304.781250\n",
      "    epoch          : 838\n",
      "    loss           : -282254.31625\n",
      "    val_loss       : -283331.36484375\n",
      "Train Epoch: 839 [512/54000 (1%)] Loss: -255664.937500\n",
      "Train Epoch: 839 [11776/54000 (22%)] Loss: -256337.000000\n",
      "Train Epoch: 839 [23040/54000 (43%)] Loss: -357927.000000\n",
      "Train Epoch: 839 [34304/54000 (64%)] Loss: -219793.359375\n",
      "Train Epoch: 839 [45568/54000 (84%)] Loss: -222185.906250\n",
      "    epoch          : 839\n",
      "    loss           : -282217.16390625\n",
      "    val_loss       : -283079.031640625\n",
      "Train Epoch: 840 [512/54000 (1%)] Loss: -243354.875000\n",
      "Train Epoch: 840 [11776/54000 (22%)] Loss: -216776.890625\n",
      "Train Epoch: 840 [23040/54000 (43%)] Loss: -351502.125000\n",
      "Train Epoch: 840 [34304/54000 (64%)] Loss: -254767.093750\n",
      "Train Epoch: 840 [45568/54000 (84%)] Loss: -244483.156250\n",
      "    epoch          : 840\n",
      "    loss           : -282205.64140625\n",
      "    val_loss       : -283552.224609375\n",
      "Train Epoch: 841 [512/54000 (1%)] Loss: -213290.187500\n",
      "Train Epoch: 841 [11776/54000 (22%)] Loss: -255131.140625\n",
      "Train Epoch: 841 [23040/54000 (43%)] Loss: -350595.375000\n",
      "Train Epoch: 841 [34304/54000 (64%)] Loss: -217236.812500\n",
      "Train Epoch: 841 [45568/54000 (84%)] Loss: -175812.562500\n",
      "    epoch          : 841\n",
      "    loss           : -282208.47515625\n",
      "    val_loss       : -283716.873046875\n",
      "Train Epoch: 842 [512/54000 (1%)] Loss: -350257.687500\n",
      "Train Epoch: 842 [11776/54000 (22%)] Loss: -215774.718750\n",
      "Train Epoch: 842 [23040/54000 (43%)] Loss: -332055.187500\n",
      "Train Epoch: 842 [34304/54000 (64%)] Loss: -179265.890625\n",
      "Train Epoch: 842 [45568/54000 (84%)] Loss: -356371.843750\n",
      "    epoch          : 842\n",
      "    loss           : -282554.58671875\n",
      "    val_loss       : -284313.678515625\n",
      "Train Epoch: 843 [512/54000 (1%)] Loss: -260826.843750\n",
      "Train Epoch: 843 [11776/54000 (22%)] Loss: -249127.593750\n",
      "Train Epoch: 843 [23040/54000 (43%)] Loss: -262290.937500\n",
      "Train Epoch: 843 [34304/54000 (64%)] Loss: -252702.359375\n",
      "Train Epoch: 843 [45568/54000 (84%)] Loss: -249688.593750\n",
      "    epoch          : 843\n",
      "    loss           : -282446.954375\n",
      "    val_loss       : -283351.623828125\n",
      "Train Epoch: 844 [512/54000 (1%)] Loss: -252924.468750\n",
      "Train Epoch: 844 [11776/54000 (22%)] Loss: -355172.187500\n",
      "Train Epoch: 844 [23040/54000 (43%)] Loss: -356940.437500\n",
      "Train Epoch: 844 [34304/54000 (64%)] Loss: -351228.437500\n",
      "Train Epoch: 844 [45568/54000 (84%)] Loss: -219078.000000\n",
      "    epoch          : 844\n",
      "    loss           : -282493.99984375\n",
      "    val_loss       : -283300.537109375\n",
      "Train Epoch: 845 [512/54000 (1%)] Loss: -175371.500000\n",
      "Train Epoch: 845 [11776/54000 (22%)] Loss: -369823.281250\n",
      "Train Epoch: 845 [23040/54000 (43%)] Loss: -332347.406250\n",
      "Train Epoch: 845 [34304/54000 (64%)] Loss: -249764.671875\n",
      "Train Epoch: 845 [45568/54000 (84%)] Loss: -336368.812500\n",
      "    epoch          : 845\n",
      "    loss           : -282608.26265625\n",
      "    val_loss       : -283794.646875\n",
      "Train Epoch: 846 [512/54000 (1%)] Loss: -350299.062500\n",
      "Train Epoch: 846 [11776/54000 (22%)] Loss: -254121.312500\n",
      "Train Epoch: 846 [23040/54000 (43%)] Loss: -257096.859375\n",
      "Train Epoch: 846 [34304/54000 (64%)] Loss: -360046.125000\n",
      "Train Epoch: 846 [45568/54000 (84%)] Loss: -245282.703125\n",
      "    epoch          : 846\n",
      "    loss           : -282702.805\n",
      "    val_loss       : -283135.142578125\n",
      "Train Epoch: 847 [512/54000 (1%)] Loss: -261537.328125\n",
      "Train Epoch: 847 [11776/54000 (22%)] Loss: -219918.812500\n",
      "Train Epoch: 847 [23040/54000 (43%)] Loss: -246050.562500\n",
      "Train Epoch: 847 [34304/54000 (64%)] Loss: -333022.437500\n",
      "Train Epoch: 847 [45568/54000 (84%)] Loss: -340784.812500\n",
      "    epoch          : 847\n",
      "    loss           : -282587.2178125\n",
      "    val_loss       : -283138.459765625\n",
      "Train Epoch: 848 [512/54000 (1%)] Loss: -356504.593750\n",
      "Train Epoch: 848 [11776/54000 (22%)] Loss: -259089.484375\n",
      "Train Epoch: 848 [23040/54000 (43%)] Loss: -256622.203125\n",
      "Train Epoch: 848 [34304/54000 (64%)] Loss: -371492.375000\n",
      "Train Epoch: 848 [45568/54000 (84%)] Loss: -354738.875000\n",
      "    epoch          : 848\n",
      "    loss           : -282649.46765625\n",
      "    val_loss       : -284065.441015625\n",
      "Train Epoch: 849 [512/54000 (1%)] Loss: -263912.343750\n",
      "Train Epoch: 849 [11776/54000 (22%)] Loss: -327392.031250\n",
      "Train Epoch: 849 [23040/54000 (43%)] Loss: -183470.281250\n",
      "Train Epoch: 849 [34304/54000 (64%)] Loss: -255928.000000\n",
      "Train Epoch: 849 [45568/54000 (84%)] Loss: -244704.906250\n",
      "    epoch          : 849\n",
      "    loss           : -282783.11515625\n",
      "    val_loss       : -284132.546484375\n",
      "Train Epoch: 850 [512/54000 (1%)] Loss: -257532.218750\n",
      "Train Epoch: 850 [11776/54000 (22%)] Loss: -259806.812500\n",
      "Train Epoch: 850 [23040/54000 (43%)] Loss: -350870.250000\n",
      "Train Epoch: 850 [34304/54000 (64%)] Loss: -182126.171875\n",
      "Train Epoch: 850 [45568/54000 (84%)] Loss: -329777.625000\n",
      "    epoch          : 850\n",
      "    loss           : -282889.4471875\n",
      "    val_loss       : -283859.901953125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch850.pth ...\n",
      "Train Epoch: 851 [512/54000 (1%)] Loss: -260360.265625\n",
      "Train Epoch: 851 [11776/54000 (22%)] Loss: -260131.375000\n",
      "Train Epoch: 851 [23040/54000 (43%)] Loss: -216962.312500\n",
      "Train Epoch: 851 [34304/54000 (64%)] Loss: -254340.218750\n",
      "Train Epoch: 851 [45568/54000 (84%)] Loss: -248641.968750\n",
      "    epoch          : 851\n",
      "    loss           : -282864.025\n",
      "    val_loss       : -284065.6484375\n",
      "Train Epoch: 852 [512/54000 (1%)] Loss: -170039.890625\n",
      "Train Epoch: 852 [11776/54000 (22%)] Loss: -357597.375000\n",
      "Train Epoch: 852 [23040/54000 (43%)] Loss: -360606.312500\n",
      "Train Epoch: 852 [34304/54000 (64%)] Loss: -248209.203125\n",
      "Train Epoch: 852 [45568/54000 (84%)] Loss: -254431.843750\n",
      "    epoch          : 852\n",
      "    loss           : -282854.11453125\n",
      "    val_loss       : -284349.8859375\n",
      "Train Epoch: 853 [512/54000 (1%)] Loss: -243616.171875\n",
      "Train Epoch: 853 [11776/54000 (22%)] Loss: -253294.000000\n",
      "Train Epoch: 853 [23040/54000 (43%)] Loss: -257875.312500\n",
      "Train Epoch: 853 [34304/54000 (64%)] Loss: -371431.000000\n",
      "Train Epoch: 853 [45568/54000 (84%)] Loss: -357459.562500\n",
      "    epoch          : 853\n",
      "    loss           : -283030.38984375\n",
      "    val_loss       : -284172.069921875\n",
      "Train Epoch: 854 [512/54000 (1%)] Loss: -251747.718750\n",
      "Train Epoch: 854 [11776/54000 (22%)] Loss: -218375.000000\n",
      "Train Epoch: 854 [23040/54000 (43%)] Loss: -265808.125000\n",
      "Train Epoch: 854 [34304/54000 (64%)] Loss: -221241.281250\n",
      "Train Epoch: 854 [45568/54000 (84%)] Loss: -221376.562500\n",
      "    epoch          : 854\n",
      "    loss           : -282994.184375\n",
      "    val_loss       : -283841.918359375\n",
      "Train Epoch: 855 [512/54000 (1%)] Loss: -251826.796875\n",
      "Train Epoch: 855 [11776/54000 (22%)] Loss: -375866.625000\n",
      "Train Epoch: 855 [23040/54000 (43%)] Loss: -261114.937500\n",
      "Train Epoch: 855 [34304/54000 (64%)] Loss: -369206.000000\n",
      "Train Epoch: 855 [45568/54000 (84%)] Loss: -250469.859375\n",
      "    epoch          : 855\n",
      "    loss           : -283202.92109375\n",
      "    val_loss       : -284134.7671875\n",
      "Train Epoch: 856 [512/54000 (1%)] Loss: -250188.281250\n",
      "Train Epoch: 856 [11776/54000 (22%)] Loss: -254834.343750\n",
      "Train Epoch: 856 [23040/54000 (43%)] Loss: -174374.906250\n",
      "Train Epoch: 856 [34304/54000 (64%)] Loss: -258829.218750\n",
      "Train Epoch: 856 [45568/54000 (84%)] Loss: -222390.109375\n",
      "    epoch          : 856\n",
      "    loss           : -283066.63625\n",
      "    val_loss       : -283925.13671875\n",
      "Train Epoch: 857 [512/54000 (1%)] Loss: -250481.484375\n",
      "Train Epoch: 857 [11776/54000 (22%)] Loss: -353219.312500\n",
      "Train Epoch: 857 [23040/54000 (43%)] Loss: -185127.593750\n",
      "Train Epoch: 857 [34304/54000 (64%)] Loss: -354127.062500\n",
      "Train Epoch: 857 [45568/54000 (84%)] Loss: -358045.187500\n",
      "    epoch          : 857\n",
      "    loss           : -283238.936875\n",
      "    val_loss       : -284233.741015625\n",
      "Train Epoch: 858 [512/54000 (1%)] Loss: -176775.593750\n",
      "Train Epoch: 858 [11776/54000 (22%)] Loss: -369320.062500\n",
      "Train Epoch: 858 [23040/54000 (43%)] Loss: -259345.203125\n",
      "Train Epoch: 858 [34304/54000 (64%)] Loss: -371619.125000\n",
      "Train Epoch: 858 [45568/54000 (84%)] Loss: -330727.562500\n",
      "    epoch          : 858\n",
      "    loss           : -283225.91859375\n",
      "    val_loss       : -284928.294140625\n",
      "Train Epoch: 859 [512/54000 (1%)] Loss: -170754.109375\n",
      "Train Epoch: 859 [11776/54000 (22%)] Loss: -261437.265625\n",
      "Train Epoch: 859 [23040/54000 (43%)] Loss: -261491.312500\n",
      "Train Epoch: 859 [34304/54000 (64%)] Loss: -254315.875000\n",
      "Train Epoch: 859 [45568/54000 (84%)] Loss: -330859.062500\n",
      "    epoch          : 859\n",
      "    loss           : -283227.37546875\n",
      "    val_loss       : -284644.244921875\n",
      "Train Epoch: 860 [512/54000 (1%)] Loss: -370465.031250\n",
      "Train Epoch: 860 [11776/54000 (22%)] Loss: -253861.000000\n",
      "Train Epoch: 860 [23040/54000 (43%)] Loss: -252663.187500\n",
      "Train Epoch: 860 [34304/54000 (64%)] Loss: -352655.500000\n",
      "Train Epoch: 860 [45568/54000 (84%)] Loss: -358281.656250\n",
      "    epoch          : 860\n",
      "    loss           : -283246.61703125\n",
      "    val_loss       : -284104.528515625\n",
      "Train Epoch: 861 [512/54000 (1%)] Loss: -374168.937500\n",
      "Train Epoch: 861 [11776/54000 (22%)] Loss: -373391.062500\n",
      "Train Epoch: 861 [23040/54000 (43%)] Loss: -215465.093750\n",
      "Train Epoch: 861 [34304/54000 (64%)] Loss: -352734.000000\n",
      "Train Epoch: 861 [45568/54000 (84%)] Loss: -350805.875000\n",
      "    epoch          : 861\n",
      "    loss           : -283377.53484375\n",
      "    val_loss       : -284100.97109375\n",
      "Train Epoch: 862 [512/54000 (1%)] Loss: -336133.250000\n",
      "Train Epoch: 862 [11776/54000 (22%)] Loss: -332018.250000\n",
      "Train Epoch: 862 [23040/54000 (43%)] Loss: -256188.781250\n",
      "Train Epoch: 862 [34304/54000 (64%)] Loss: -369992.125000\n",
      "Train Epoch: 862 [45568/54000 (84%)] Loss: -250792.578125\n",
      "    epoch          : 862\n",
      "    loss           : -283459.86625\n",
      "    val_loss       : -284618.06796875\n",
      "Train Epoch: 863 [512/54000 (1%)] Loss: -255185.906250\n",
      "Train Epoch: 863 [11776/54000 (22%)] Loss: -260998.984375\n",
      "Train Epoch: 863 [23040/54000 (43%)] Loss: -263805.125000\n",
      "Train Epoch: 863 [34304/54000 (64%)] Loss: -358474.343750\n",
      "Train Epoch: 863 [45568/54000 (84%)] Loss: -359678.500000\n",
      "    epoch          : 863\n",
      "    loss           : -283380.924375\n",
      "    val_loss       : -284102.672265625\n",
      "Train Epoch: 864 [512/54000 (1%)] Loss: -253507.015625\n",
      "Train Epoch: 864 [11776/54000 (22%)] Loss: -351847.281250\n",
      "Train Epoch: 864 [23040/54000 (43%)] Loss: -353847.531250\n",
      "Train Epoch: 864 [34304/54000 (64%)] Loss: -249032.906250\n",
      "Train Epoch: 864 [45568/54000 (84%)] Loss: -333736.625000\n",
      "    epoch          : 864\n",
      "    loss           : -283622.1559375\n",
      "    val_loss       : -284200.058203125\n",
      "Train Epoch: 865 [512/54000 (1%)] Loss: -179002.640625\n",
      "Train Epoch: 865 [11776/54000 (22%)] Loss: -216611.843750\n",
      "Train Epoch: 865 [23040/54000 (43%)] Loss: -258165.718750\n",
      "Train Epoch: 865 [34304/54000 (64%)] Loss: -257564.500000\n",
      "Train Epoch: 865 [45568/54000 (84%)] Loss: -331868.500000\n",
      "    epoch          : 865\n",
      "    loss           : -283479.86359375\n",
      "    val_loss       : -285081.3765625\n",
      "Train Epoch: 866 [512/54000 (1%)] Loss: -261453.187500\n",
      "Train Epoch: 866 [11776/54000 (22%)] Loss: -352444.281250\n",
      "Train Epoch: 866 [23040/54000 (43%)] Loss: -255181.718750\n",
      "Train Epoch: 866 [34304/54000 (64%)] Loss: -371397.437500\n",
      "Train Epoch: 866 [45568/54000 (84%)] Loss: -332303.281250\n",
      "    epoch          : 866\n",
      "    loss           : -283655.831875\n",
      "    val_loss       : -284444.975390625\n",
      "Train Epoch: 867 [512/54000 (1%)] Loss: -259958.906250\n",
      "Train Epoch: 867 [11776/54000 (22%)] Loss: -253044.046875\n",
      "Train Epoch: 867 [23040/54000 (43%)] Loss: -253778.968750\n",
      "Train Epoch: 867 [34304/54000 (64%)] Loss: -250600.468750\n",
      "Train Epoch: 867 [45568/54000 (84%)] Loss: -359900.812500\n",
      "    epoch          : 867\n",
      "    loss           : -283605.5690625\n",
      "    val_loss       : -285050.808203125\n",
      "Train Epoch: 868 [512/54000 (1%)] Loss: -254691.343750\n",
      "Train Epoch: 868 [11776/54000 (22%)] Loss: -336564.125000\n",
      "Train Epoch: 868 [23040/54000 (43%)] Loss: -354170.875000\n",
      "Train Epoch: 868 [34304/54000 (64%)] Loss: -251998.531250\n",
      "Train Epoch: 868 [45568/54000 (84%)] Loss: -332888.031250\n",
      "    epoch          : 868\n",
      "    loss           : -283641.11890625\n",
      "    val_loss       : -285169.76640625\n",
      "Train Epoch: 869 [512/54000 (1%)] Loss: -172583.171875\n",
      "Train Epoch: 869 [11776/54000 (22%)] Loss: -354161.500000\n",
      "Train Epoch: 869 [23040/54000 (43%)] Loss: -171473.718750\n",
      "Train Epoch: 869 [34304/54000 (64%)] Loss: -329225.812500\n",
      "Train Epoch: 869 [45568/54000 (84%)] Loss: -360706.343750\n",
      "    epoch          : 869\n",
      "    loss           : -283872.38390625\n",
      "    val_loss       : -285036.95\n",
      "Train Epoch: 870 [512/54000 (1%)] Loss: -254638.421875\n",
      "Train Epoch: 870 [11776/54000 (22%)] Loss: -254562.781250\n",
      "Train Epoch: 870 [23040/54000 (43%)] Loss: -262126.515625\n",
      "Train Epoch: 870 [34304/54000 (64%)] Loss: -180811.750000\n",
      "Train Epoch: 870 [45568/54000 (84%)] Loss: -374016.937500\n",
      "    epoch          : 870\n",
      "    loss           : -283678.595625\n",
      "    val_loss       : -284438.692578125\n",
      "Train Epoch: 871 [512/54000 (1%)] Loss: -255824.734375\n",
      "Train Epoch: 871 [11776/54000 (22%)] Loss: -263433.656250\n",
      "Train Epoch: 871 [23040/54000 (43%)] Loss: -249868.718750\n",
      "Train Epoch: 871 [34304/54000 (64%)] Loss: -263598.625000\n",
      "Train Epoch: 871 [45568/54000 (84%)] Loss: -252226.906250\n",
      "    epoch          : 871\n",
      "    loss           : -283697.8490625\n",
      "    val_loss       : -284700.462109375\n",
      "Train Epoch: 872 [512/54000 (1%)] Loss: -255678.312500\n",
      "Train Epoch: 872 [11776/54000 (22%)] Loss: -360273.125000\n",
      "Train Epoch: 872 [23040/54000 (43%)] Loss: -173953.296875\n",
      "Train Epoch: 872 [34304/54000 (64%)] Loss: -332778.562500\n",
      "Train Epoch: 872 [45568/54000 (84%)] Loss: -177808.375000\n",
      "    epoch          : 872\n",
      "    loss           : -283834.861875\n",
      "    val_loss       : -284910.943359375\n",
      "Train Epoch: 873 [512/54000 (1%)] Loss: -168623.796875\n",
      "Train Epoch: 873 [11776/54000 (22%)] Loss: -183003.656250\n",
      "Train Epoch: 873 [23040/54000 (43%)] Loss: -253735.187500\n",
      "Train Epoch: 873 [34304/54000 (64%)] Loss: -182992.625000\n",
      "Train Epoch: 873 [45568/54000 (84%)] Loss: -251742.968750\n",
      "    epoch          : 873\n",
      "    loss           : -283819.819375\n",
      "    val_loss       : -285098.437109375\n",
      "Train Epoch: 874 [512/54000 (1%)] Loss: -333036.562500\n",
      "Train Epoch: 874 [11776/54000 (22%)] Loss: -243728.484375\n",
      "Train Epoch: 874 [23040/54000 (43%)] Loss: -338449.093750\n",
      "Train Epoch: 874 [34304/54000 (64%)] Loss: -372627.875000\n",
      "Train Epoch: 874 [45568/54000 (84%)] Loss: -178290.265625\n",
      "    epoch          : 874\n",
      "    loss           : -283937.93390625\n",
      "    val_loss       : -285365.604296875\n",
      "Train Epoch: 875 [512/54000 (1%)] Loss: -360227.750000\n",
      "Train Epoch: 875 [11776/54000 (22%)] Loss: -180474.625000\n",
      "Train Epoch: 875 [23040/54000 (43%)] Loss: -359196.062500\n",
      "Train Epoch: 875 [34304/54000 (64%)] Loss: -257575.593750\n",
      "Train Epoch: 875 [45568/54000 (84%)] Loss: -218545.093750\n",
      "    epoch          : 875\n",
      "    loss           : -284163.09546875\n",
      "    val_loss       : -285718.0078125\n",
      "Train Epoch: 876 [512/54000 (1%)] Loss: -359378.468750\n",
      "Train Epoch: 876 [11776/54000 (22%)] Loss: -259585.718750\n",
      "Train Epoch: 876 [23040/54000 (43%)] Loss: -258049.546875\n",
      "Train Epoch: 876 [34304/54000 (64%)] Loss: -247204.187500\n",
      "Train Epoch: 876 [45568/54000 (84%)] Loss: -358858.343750\n",
      "    epoch          : 876\n",
      "    loss           : -284150.71015625\n",
      "    val_loss       : -285179.256640625\n",
      "Train Epoch: 877 [512/54000 (1%)] Loss: -171959.515625\n",
      "Train Epoch: 877 [11776/54000 (22%)] Loss: -226457.609375\n",
      "Train Epoch: 877 [23040/54000 (43%)] Loss: -251215.875000\n",
      "Train Epoch: 877 [34304/54000 (64%)] Loss: -359974.312500\n",
      "Train Epoch: 877 [45568/54000 (84%)] Loss: -253172.375000\n",
      "    epoch          : 877\n",
      "    loss           : -284064.50421875\n",
      "    val_loss       : -285749.48984375\n",
      "Train Epoch: 878 [512/54000 (1%)] Loss: -263259.312500\n",
      "Train Epoch: 878 [11776/54000 (22%)] Loss: -371044.062500\n",
      "Train Epoch: 878 [23040/54000 (43%)] Loss: -260498.328125\n",
      "Train Epoch: 878 [34304/54000 (64%)] Loss: -258935.156250\n",
      "Train Epoch: 878 [45568/54000 (84%)] Loss: -226684.843750\n",
      "    epoch          : 878\n",
      "    loss           : -283992.73453125\n",
      "    val_loss       : -284865.821875\n",
      "Train Epoch: 879 [512/54000 (1%)] Loss: -352133.312500\n",
      "Train Epoch: 879 [11776/54000 (22%)] Loss: -352228.312500\n",
      "Train Epoch: 879 [23040/54000 (43%)] Loss: -374527.312500\n",
      "Train Epoch: 879 [34304/54000 (64%)] Loss: -247059.515625\n",
      "Train Epoch: 879 [45568/54000 (84%)] Loss: -224941.218750\n",
      "    epoch          : 879\n",
      "    loss           : -284148.86578125\n",
      "    val_loss       : -284348.5578125\n",
      "Train Epoch: 880 [512/54000 (1%)] Loss: -269265.500000\n",
      "Train Epoch: 880 [11776/54000 (22%)] Loss: -354305.375000\n",
      "Train Epoch: 880 [23040/54000 (43%)] Loss: -261616.562500\n",
      "Train Epoch: 880 [34304/54000 (64%)] Loss: -258111.312500\n",
      "Train Epoch: 880 [45568/54000 (84%)] Loss: -248144.843750\n",
      "    epoch          : 880\n",
      "    loss           : -284409.44203125\n",
      "    val_loss       : -285242.715625\n",
      "Train Epoch: 881 [512/54000 (1%)] Loss: -255178.484375\n",
      "Train Epoch: 881 [11776/54000 (22%)] Loss: -256242.531250\n",
      "Train Epoch: 881 [23040/54000 (43%)] Loss: -374538.656250\n",
      "Train Epoch: 881 [34304/54000 (64%)] Loss: -356151.687500\n",
      "Train Epoch: 881 [45568/54000 (84%)] Loss: -224900.968750\n",
      "    epoch          : 881\n",
      "    loss           : -284311.37734375\n",
      "    val_loss       : -285702.6109375\n",
      "Train Epoch: 882 [512/54000 (1%)] Loss: -374822.000000\n",
      "Train Epoch: 882 [11776/54000 (22%)] Loss: -372499.531250\n",
      "Train Epoch: 882 [23040/54000 (43%)] Loss: -332791.375000\n",
      "Train Epoch: 882 [34304/54000 (64%)] Loss: -249046.265625\n",
      "Train Epoch: 882 [45568/54000 (84%)] Loss: -358931.125000\n",
      "    epoch          : 882\n",
      "    loss           : -284317.99078125\n",
      "    val_loss       : -285589.3234375\n",
      "Train Epoch: 883 [512/54000 (1%)] Loss: -178910.750000\n",
      "Train Epoch: 883 [11776/54000 (22%)] Loss: -174986.937500\n",
      "Train Epoch: 883 [23040/54000 (43%)] Loss: -175966.984375\n",
      "Train Epoch: 883 [34304/54000 (64%)] Loss: -375471.687500\n",
      "Train Epoch: 883 [45568/54000 (84%)] Loss: -249781.906250\n",
      "    epoch          : 883\n",
      "    loss           : -284406.04203125\n",
      "    val_loss       : -285627.2625\n",
      "Train Epoch: 884 [512/54000 (1%)] Loss: -218049.906250\n",
      "Train Epoch: 884 [11776/54000 (22%)] Loss: -255971.187500\n",
      "Train Epoch: 884 [23040/54000 (43%)] Loss: -252420.656250\n",
      "Train Epoch: 884 [34304/54000 (64%)] Loss: -374491.468750\n",
      "Train Epoch: 884 [45568/54000 (84%)] Loss: -334190.625000\n",
      "    epoch          : 884\n",
      "    loss           : -284468.930625\n",
      "    val_loss       : -286021.93984375\n",
      "Train Epoch: 885 [512/54000 (1%)] Loss: -260194.640625\n",
      "Train Epoch: 885 [11776/54000 (22%)] Loss: -256662.937500\n",
      "Train Epoch: 885 [23040/54000 (43%)] Loss: -177412.421875\n",
      "Train Epoch: 885 [34304/54000 (64%)] Loss: -170220.281250\n",
      "Train Epoch: 885 [45568/54000 (84%)] Loss: -220416.265625\n",
      "    epoch          : 885\n",
      "    loss           : -284536.9409375\n",
      "    val_loss       : -285555.719921875\n",
      "Train Epoch: 886 [512/54000 (1%)] Loss: -360215.812500\n",
      "Train Epoch: 886 [11776/54000 (22%)] Loss: -259645.562500\n",
      "Train Epoch: 886 [23040/54000 (43%)] Loss: -360749.593750\n",
      "Train Epoch: 886 [34304/54000 (64%)] Loss: -253516.031250\n",
      "Train Epoch: 886 [45568/54000 (84%)] Loss: -242975.531250\n",
      "    epoch          : 886\n",
      "    loss           : -284556.3775\n",
      "    val_loss       : -286186.366796875\n",
      "Train Epoch: 887 [512/54000 (1%)] Loss: -372882.937500\n",
      "Train Epoch: 887 [11776/54000 (22%)] Loss: -375030.000000\n",
      "Train Epoch: 887 [23040/54000 (43%)] Loss: -265810.750000\n",
      "Train Epoch: 887 [34304/54000 (64%)] Loss: -367810.125000\n",
      "Train Epoch: 887 [45568/54000 (84%)] Loss: -217730.484375\n",
      "    epoch          : 887\n",
      "    loss           : -284595.2353125\n",
      "    val_loss       : -286386.110546875\n",
      "Train Epoch: 888 [512/54000 (1%)] Loss: -353529.437500\n",
      "Train Epoch: 888 [11776/54000 (22%)] Loss: -355544.375000\n",
      "Train Epoch: 888 [23040/54000 (43%)] Loss: -255645.406250\n",
      "Train Epoch: 888 [34304/54000 (64%)] Loss: -216833.500000\n",
      "Train Epoch: 888 [45568/54000 (84%)] Loss: -363436.218750\n",
      "    epoch          : 888\n",
      "    loss           : -284601.21453125\n",
      "    val_loss       : -285715.156640625\n",
      "Train Epoch: 889 [512/54000 (1%)] Loss: -256463.328125\n",
      "Train Epoch: 889 [11776/54000 (22%)] Loss: -353527.281250\n",
      "Train Epoch: 889 [23040/54000 (43%)] Loss: -376290.250000\n",
      "Train Epoch: 889 [34304/54000 (64%)] Loss: -249896.562500\n",
      "Train Epoch: 889 [45568/54000 (84%)] Loss: -184324.359375\n",
      "    epoch          : 889\n",
      "    loss           : -284524.615625\n",
      "    val_loss       : -285637.14453125\n",
      "Train Epoch: 890 [512/54000 (1%)] Loss: -261089.406250\n",
      "Train Epoch: 890 [11776/54000 (22%)] Loss: -254664.250000\n",
      "Train Epoch: 890 [23040/54000 (43%)] Loss: -374462.468750\n",
      "Train Epoch: 890 [34304/54000 (64%)] Loss: -175054.906250\n",
      "Train Epoch: 890 [45568/54000 (84%)] Loss: -359064.250000\n",
      "    epoch          : 890\n",
      "    loss           : -284696.25875\n",
      "    val_loss       : -286592.58515625\n",
      "Train Epoch: 891 [512/54000 (1%)] Loss: -354924.156250\n",
      "Train Epoch: 891 [11776/54000 (22%)] Loss: -247341.234375\n",
      "Train Epoch: 891 [23040/54000 (43%)] Loss: -374072.937500\n",
      "Train Epoch: 891 [34304/54000 (64%)] Loss: -353604.187500\n",
      "Train Epoch: 891 [45568/54000 (84%)] Loss: -333094.500000\n",
      "    epoch          : 891\n",
      "    loss           : -284775.3603125\n",
      "    val_loss       : -285652.740234375\n",
      "Train Epoch: 892 [512/54000 (1%)] Loss: -256941.968750\n",
      "Train Epoch: 892 [11776/54000 (22%)] Loss: -260566.812500\n",
      "Train Epoch: 892 [23040/54000 (43%)] Loss: -258998.328125\n",
      "Train Epoch: 892 [34304/54000 (64%)] Loss: -336209.625000\n",
      "Train Epoch: 892 [45568/54000 (84%)] Loss: -249879.078125\n",
      "    epoch          : 892\n",
      "    loss           : -284713.901875\n",
      "    val_loss       : -285062.319140625\n",
      "Train Epoch: 893 [512/54000 (1%)] Loss: -255733.375000\n",
      "Train Epoch: 893 [11776/54000 (22%)] Loss: -176744.625000\n",
      "Train Epoch: 893 [23040/54000 (43%)] Loss: -359877.750000\n",
      "Train Epoch: 893 [34304/54000 (64%)] Loss: -375372.687500\n",
      "Train Epoch: 893 [45568/54000 (84%)] Loss: -224922.843750\n",
      "    epoch          : 893\n",
      "    loss           : -284734.07484375\n",
      "    val_loss       : -285942.591796875\n",
      "Train Epoch: 894 [512/54000 (1%)] Loss: -218608.359375\n",
      "Train Epoch: 894 [11776/54000 (22%)] Loss: -224022.156250\n",
      "Train Epoch: 894 [23040/54000 (43%)] Loss: -338717.375000\n",
      "Train Epoch: 894 [34304/54000 (64%)] Loss: -258291.593750\n",
      "Train Epoch: 894 [45568/54000 (84%)] Loss: -358898.062500\n",
      "    epoch          : 894\n",
      "    loss           : -284868.66796875\n",
      "    val_loss       : -286591.12421875\n",
      "Train Epoch: 895 [512/54000 (1%)] Loss: -215947.687500\n",
      "Train Epoch: 895 [11776/54000 (22%)] Loss: -218157.250000\n",
      "Train Epoch: 895 [23040/54000 (43%)] Loss: -265016.750000\n",
      "Train Epoch: 895 [34304/54000 (64%)] Loss: -376850.843750\n",
      "Train Epoch: 895 [45568/54000 (84%)] Loss: -244352.593750\n",
      "    epoch          : 895\n",
      "    loss           : -285011.688125\n",
      "    val_loss       : -286292.329296875\n",
      "Train Epoch: 896 [512/54000 (1%)] Loss: -262164.687500\n",
      "Train Epoch: 896 [11776/54000 (22%)] Loss: -375820.875000\n",
      "Train Epoch: 896 [23040/54000 (43%)] Loss: -223103.906250\n",
      "Train Epoch: 896 [34304/54000 (64%)] Loss: -247146.500000\n",
      "Train Epoch: 896 [45568/54000 (84%)] Loss: -252585.218750\n",
      "    epoch          : 896\n",
      "    loss           : -284919.66359375\n",
      "    val_loss       : -286083.0890625\n",
      "Train Epoch: 897 [512/54000 (1%)] Loss: -252925.890625\n",
      "Train Epoch: 897 [11776/54000 (22%)] Loss: -254724.390625\n",
      "Train Epoch: 897 [23040/54000 (43%)] Loss: -260525.281250\n",
      "Train Epoch: 897 [34304/54000 (64%)] Loss: -247487.062500\n",
      "Train Epoch: 897 [45568/54000 (84%)] Loss: -179609.250000\n",
      "    epoch          : 897\n",
      "    loss           : -285037.6\n",
      "    val_loss       : -285660.56015625\n",
      "Train Epoch: 898 [512/54000 (1%)] Loss: -262313.250000\n",
      "Train Epoch: 898 [11776/54000 (22%)] Loss: -264716.500000\n",
      "Train Epoch: 898 [23040/54000 (43%)] Loss: -264017.406250\n",
      "Train Epoch: 898 [34304/54000 (64%)] Loss: -249646.046875\n",
      "Train Epoch: 898 [45568/54000 (84%)] Loss: -375124.843750\n",
      "    epoch          : 898\n",
      "    loss           : -285153.160625\n",
      "    val_loss       : -286058.632421875\n",
      "Train Epoch: 899 [512/54000 (1%)] Loss: -360165.375000\n",
      "Train Epoch: 899 [11776/54000 (22%)] Loss: -362763.312500\n",
      "Train Epoch: 899 [23040/54000 (43%)] Loss: -361493.218750\n",
      "Train Epoch: 899 [34304/54000 (64%)] Loss: -353961.156250\n",
      "Train Epoch: 899 [45568/54000 (84%)] Loss: -221928.000000\n",
      "    epoch          : 899\n",
      "    loss           : -285111.916875\n",
      "    val_loss       : -285969.565625\n",
      "Train Epoch: 900 [512/54000 (1%)] Loss: -362304.218750\n",
      "Train Epoch: 900 [11776/54000 (22%)] Loss: -253005.593750\n",
      "Train Epoch: 900 [23040/54000 (43%)] Loss: -228442.750000\n",
      "Train Epoch: 900 [34304/54000 (64%)] Loss: -250051.875000\n",
      "Train Epoch: 900 [45568/54000 (84%)] Loss: -337204.000000\n",
      "    epoch          : 900\n",
      "    loss           : -285097.7571875\n",
      "    val_loss       : -286793.6890625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch900.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 901 [512/54000 (1%)] Loss: -259200.078125\n",
      "Train Epoch: 901 [11776/54000 (22%)] Loss: -182112.140625\n",
      "Train Epoch: 901 [23040/54000 (43%)] Loss: -249623.093750\n",
      "Train Epoch: 901 [34304/54000 (64%)] Loss: -338074.437500\n",
      "Train Epoch: 901 [45568/54000 (84%)] Loss: -250246.718750\n",
      "    epoch          : 901\n",
      "    loss           : -285172.3103125\n",
      "    val_loss       : -285735.9890625\n",
      "Train Epoch: 902 [512/54000 (1%)] Loss: -259541.656250\n",
      "Train Epoch: 902 [11776/54000 (22%)] Loss: -355322.750000\n",
      "Train Epoch: 902 [23040/54000 (43%)] Loss: -175034.093750\n",
      "Train Epoch: 902 [34304/54000 (64%)] Loss: -255416.875000\n",
      "Train Epoch: 902 [45568/54000 (84%)] Loss: -334647.031250\n",
      "    epoch          : 902\n",
      "    loss           : -285306.23515625\n",
      "    val_loss       : -286556.86484375\n",
      "Train Epoch: 903 [512/54000 (1%)] Loss: -262294.812500\n",
      "Train Epoch: 903 [11776/54000 (22%)] Loss: -263626.937500\n",
      "Train Epoch: 903 [23040/54000 (43%)] Loss: -264602.625000\n",
      "Train Epoch: 903 [34304/54000 (64%)] Loss: -249474.937500\n",
      "Train Epoch: 903 [45568/54000 (84%)] Loss: -218477.984375\n",
      "    epoch          : 903\n",
      "    loss           : -285250.571875\n",
      "    val_loss       : -287121.834765625\n",
      "Train Epoch: 904 [512/54000 (1%)] Loss: -266877.312500\n",
      "Train Epoch: 904 [11776/54000 (22%)] Loss: -258403.812500\n",
      "Train Epoch: 904 [23040/54000 (43%)] Loss: -355578.250000\n",
      "Train Epoch: 904 [34304/54000 (64%)] Loss: -333146.468750\n",
      "Train Epoch: 904 [45568/54000 (84%)] Loss: -375467.718750\n",
      "    epoch          : 904\n",
      "    loss           : -285330.0178125\n",
      "    val_loss       : -286139.9359375\n",
      "Train Epoch: 905 [512/54000 (1%)] Loss: -253592.781250\n",
      "Train Epoch: 905 [11776/54000 (22%)] Loss: -374506.812500\n",
      "Train Epoch: 905 [23040/54000 (43%)] Loss: -258261.625000\n",
      "Train Epoch: 905 [34304/54000 (64%)] Loss: -252933.250000\n",
      "Train Epoch: 905 [45568/54000 (84%)] Loss: -222740.921875\n",
      "    epoch          : 905\n",
      "    loss           : -285362.87578125\n",
      "    val_loss       : -286229.63671875\n",
      "Train Epoch: 906 [512/54000 (1%)] Loss: -172953.703125\n",
      "Train Epoch: 906 [11776/54000 (22%)] Loss: -218711.906250\n",
      "Train Epoch: 906 [23040/54000 (43%)] Loss: -250578.859375\n",
      "Train Epoch: 906 [34304/54000 (64%)] Loss: -256832.015625\n",
      "Train Epoch: 906 [45568/54000 (84%)] Loss: -362264.843750\n",
      "    epoch          : 906\n",
      "    loss           : -285363.3503125\n",
      "    val_loss       : -287318.077734375\n",
      "Train Epoch: 907 [512/54000 (1%)] Loss: -261199.609375\n",
      "Train Epoch: 907 [11776/54000 (22%)] Loss: -260190.343750\n",
      "Train Epoch: 907 [23040/54000 (43%)] Loss: -254861.093750\n",
      "Train Epoch: 907 [34304/54000 (64%)] Loss: -359205.812500\n",
      "Train Epoch: 907 [45568/54000 (84%)] Loss: -263765.218750\n",
      "    epoch          : 907\n",
      "    loss           : -285457.531875\n",
      "    val_loss       : -287299.145703125\n",
      "Train Epoch: 908 [512/54000 (1%)] Loss: -376165.312500\n",
      "Train Epoch: 908 [11776/54000 (22%)] Loss: -167839.078125\n",
      "Train Epoch: 908 [23040/54000 (43%)] Loss: -261484.750000\n",
      "Train Epoch: 908 [34304/54000 (64%)] Loss: -260114.406250\n",
      "Train Epoch: 908 [45568/54000 (84%)] Loss: -334938.375000\n",
      "    epoch          : 908\n",
      "    loss           : -285547.1465625\n",
      "    val_loss       : -286633.33125\n",
      "Train Epoch: 909 [512/54000 (1%)] Loss: -265520.562500\n",
      "Train Epoch: 909 [11776/54000 (22%)] Loss: -258181.859375\n",
      "Train Epoch: 909 [23040/54000 (43%)] Loss: -245428.921875\n",
      "Train Epoch: 909 [34304/54000 (64%)] Loss: -354067.968750\n",
      "Train Epoch: 909 [45568/54000 (84%)] Loss: -254539.750000\n",
      "    epoch          : 909\n",
      "    loss           : -285627.145\n",
      "    val_loss       : -286316.799609375\n",
      "Train Epoch: 910 [512/54000 (1%)] Loss: -222439.265625\n",
      "Train Epoch: 910 [11776/54000 (22%)] Loss: -261737.218750\n",
      "Train Epoch: 910 [23040/54000 (43%)] Loss: -249317.984375\n",
      "Train Epoch: 910 [34304/54000 (64%)] Loss: -252458.015625\n",
      "Train Epoch: 910 [45568/54000 (84%)] Loss: -360218.281250\n",
      "    epoch          : 910\n",
      "    loss           : -285521.018125\n",
      "    val_loss       : -286955.18125\n",
      "Train Epoch: 911 [512/54000 (1%)] Loss: -259806.859375\n",
      "Train Epoch: 911 [11776/54000 (22%)] Loss: -356185.875000\n",
      "Train Epoch: 911 [23040/54000 (43%)] Loss: -339071.062500\n",
      "Train Epoch: 911 [34304/54000 (64%)] Loss: -229848.546875\n",
      "Train Epoch: 911 [45568/54000 (84%)] Loss: -340372.875000\n",
      "    epoch          : 911\n",
      "    loss           : -285652.5559375\n",
      "    val_loss       : -287638.462109375\n",
      "Train Epoch: 912 [512/54000 (1%)] Loss: -377374.406250\n",
      "Train Epoch: 912 [11776/54000 (22%)] Loss: -255785.859375\n",
      "Train Epoch: 912 [23040/54000 (43%)] Loss: -222321.562500\n",
      "Train Epoch: 912 [34304/54000 (64%)] Loss: -246621.406250\n",
      "Train Epoch: 912 [45568/54000 (84%)] Loss: -249124.390625\n",
      "    epoch          : 912\n",
      "    loss           : -285842.073125\n",
      "    val_loss       : -287054.63203125\n",
      "Train Epoch: 913 [512/54000 (1%)] Loss: -162184.828125\n",
      "Train Epoch: 913 [11776/54000 (22%)] Loss: -248358.625000\n",
      "Train Epoch: 913 [23040/54000 (43%)] Loss: -253097.906250\n",
      "Train Epoch: 913 [34304/54000 (64%)] Loss: -261043.890625\n",
      "Train Epoch: 913 [45568/54000 (84%)] Loss: -246361.125000\n",
      "    epoch          : 913\n",
      "    loss           : -285568.2496875\n",
      "    val_loss       : -286954.979296875\n",
      "Train Epoch: 914 [512/54000 (1%)] Loss: -227773.156250\n",
      "Train Epoch: 914 [11776/54000 (22%)] Loss: -257597.968750\n",
      "Train Epoch: 914 [23040/54000 (43%)] Loss: -182060.000000\n",
      "Train Epoch: 914 [34304/54000 (64%)] Loss: -376115.125000\n",
      "Train Epoch: 914 [45568/54000 (84%)] Loss: -337921.250000\n",
      "    epoch          : 914\n",
      "    loss           : -285771.221875\n",
      "    val_loss       : -287272.70859375\n",
      "Train Epoch: 915 [512/54000 (1%)] Loss: -256319.093750\n",
      "Train Epoch: 915 [11776/54000 (22%)] Loss: -336617.750000\n",
      "Train Epoch: 915 [23040/54000 (43%)] Loss: -378202.562500\n",
      "Train Epoch: 915 [34304/54000 (64%)] Loss: -248357.906250\n",
      "Train Epoch: 915 [45568/54000 (84%)] Loss: -375826.375000\n",
      "    epoch          : 915\n",
      "    loss           : -285801.92390625\n",
      "    val_loss       : -286264.628515625\n",
      "Train Epoch: 916 [512/54000 (1%)] Loss: -355526.375000\n",
      "Train Epoch: 916 [11776/54000 (22%)] Loss: -354838.062500\n",
      "Train Epoch: 916 [23040/54000 (43%)] Loss: -258419.468750\n",
      "Train Epoch: 916 [34304/54000 (64%)] Loss: -251781.531250\n",
      "Train Epoch: 916 [45568/54000 (84%)] Loss: -252717.968750\n",
      "    epoch          : 916\n",
      "    loss           : -286041.6246875\n",
      "    val_loss       : -287634.81640625\n",
      "Train Epoch: 917 [512/54000 (1%)] Loss: -247630.625000\n",
      "Train Epoch: 917 [11776/54000 (22%)] Loss: -174231.812500\n",
      "Train Epoch: 917 [23040/54000 (43%)] Loss: -177367.734375\n",
      "Train Epoch: 917 [34304/54000 (64%)] Loss: -220136.125000\n",
      "Train Epoch: 917 [45568/54000 (84%)] Loss: -339589.312500\n",
      "    epoch          : 917\n",
      "    loss           : -285875.19640625\n",
      "    val_loss       : -286628.010546875\n",
      "Train Epoch: 918 [512/54000 (1%)] Loss: -256682.890625\n",
      "Train Epoch: 918 [11776/54000 (22%)] Loss: -251343.437500\n",
      "Train Epoch: 918 [23040/54000 (43%)] Loss: -356552.687500\n",
      "Train Epoch: 918 [34304/54000 (64%)] Loss: -181726.265625\n",
      "Train Epoch: 918 [45568/54000 (84%)] Loss: -261790.406250\n",
      "    epoch          : 918\n",
      "    loss           : -285906.2115625\n",
      "    val_loss       : -287465.237109375\n",
      "Train Epoch: 919 [512/54000 (1%)] Loss: -376678.937500\n",
      "Train Epoch: 919 [11776/54000 (22%)] Loss: -164559.625000\n",
      "Train Epoch: 919 [23040/54000 (43%)] Loss: -223969.750000\n",
      "Train Epoch: 919 [34304/54000 (64%)] Loss: -245016.671875\n",
      "Train Epoch: 919 [45568/54000 (84%)] Loss: -359722.625000\n",
      "    epoch          : 919\n",
      "    loss           : -286212.3690625\n",
      "    val_loss       : -287646.1140625\n",
      "Train Epoch: 920 [512/54000 (1%)] Loss: -227148.312500\n",
      "Train Epoch: 920 [11776/54000 (22%)] Loss: -263593.406250\n",
      "Train Epoch: 920 [23040/54000 (43%)] Loss: -336752.500000\n",
      "Train Epoch: 920 [34304/54000 (64%)] Loss: -244080.937500\n",
      "Train Epoch: 920 [45568/54000 (84%)] Loss: -252563.687500\n",
      "    epoch          : 920\n",
      "    loss           : -286091.029375\n",
      "    val_loss       : -286529.02421875\n",
      "Train Epoch: 921 [512/54000 (1%)] Loss: -186404.750000\n",
      "Train Epoch: 921 [11776/54000 (22%)] Loss: -375257.562500\n",
      "Train Epoch: 921 [23040/54000 (43%)] Loss: -361004.125000\n",
      "Train Epoch: 921 [34304/54000 (64%)] Loss: -226439.906250\n",
      "Train Epoch: 921 [45568/54000 (84%)] Loss: -338710.062500\n",
      "    epoch          : 921\n",
      "    loss           : -286084.4071875\n",
      "    val_loss       : -286943.6265625\n",
      "Train Epoch: 922 [512/54000 (1%)] Loss: -259733.468750\n",
      "Train Epoch: 922 [11776/54000 (22%)] Loss: -376022.875000\n",
      "Train Epoch: 922 [23040/54000 (43%)] Loss: -262806.406250\n",
      "Train Epoch: 922 [34304/54000 (64%)] Loss: -335817.312500\n",
      "Train Epoch: 922 [45568/54000 (84%)] Loss: -336261.437500\n",
      "    epoch          : 922\n",
      "    loss           : -286114.563125\n",
      "    val_loss       : -287400.26640625\n",
      "Train Epoch: 923 [512/54000 (1%)] Loss: -340271.625000\n",
      "Train Epoch: 923 [11776/54000 (22%)] Loss: -261033.625000\n",
      "Train Epoch: 923 [23040/54000 (43%)] Loss: -253026.296875\n",
      "Train Epoch: 923 [34304/54000 (64%)] Loss: -334398.312500\n",
      "Train Epoch: 923 [45568/54000 (84%)] Loss: -339576.062500\n",
      "    epoch          : 923\n",
      "    loss           : -286177.2234375\n",
      "    val_loss       : -288015.823828125\n",
      "Train Epoch: 924 [512/54000 (1%)] Loss: -259851.562500\n",
      "Train Epoch: 924 [11776/54000 (22%)] Loss: -211192.968750\n",
      "Train Epoch: 924 [23040/54000 (43%)] Loss: -362214.531250\n",
      "Train Epoch: 924 [34304/54000 (64%)] Loss: -362771.593750\n",
      "Train Epoch: 924 [45568/54000 (84%)] Loss: -355549.562500\n",
      "    epoch          : 924\n",
      "    loss           : -286132.1975\n",
      "    val_loss       : -288162.133203125\n",
      "Train Epoch: 925 [512/54000 (1%)] Loss: -184670.937500\n",
      "Train Epoch: 925 [11776/54000 (22%)] Loss: -362623.062500\n",
      "Train Epoch: 925 [23040/54000 (43%)] Loss: -257563.468750\n",
      "Train Epoch: 925 [34304/54000 (64%)] Loss: -247020.000000\n",
      "Train Epoch: 925 [45568/54000 (84%)] Loss: -376193.218750\n",
      "    epoch          : 925\n",
      "    loss           : -286157.4759375\n",
      "    val_loss       : -287828.105078125\n",
      "Train Epoch: 926 [512/54000 (1%)] Loss: -336401.781250\n",
      "Train Epoch: 926 [11776/54000 (22%)] Loss: -259690.375000\n",
      "Train Epoch: 926 [23040/54000 (43%)] Loss: -261125.125000\n",
      "Train Epoch: 926 [34304/54000 (64%)] Loss: -224010.968750\n",
      "Train Epoch: 926 [45568/54000 (84%)] Loss: -225412.656250\n",
      "    epoch          : 926\n",
      "    loss           : -286456.98390625\n",
      "    val_loss       : -287571.56015625\n",
      "Train Epoch: 927 [512/54000 (1%)] Loss: -357788.937500\n",
      "Train Epoch: 927 [11776/54000 (22%)] Loss: -259385.375000\n",
      "Train Epoch: 927 [23040/54000 (43%)] Loss: -264047.562500\n",
      "Train Epoch: 927 [34304/54000 (64%)] Loss: -221606.250000\n",
      "Train Epoch: 927 [45568/54000 (84%)] Loss: -364806.781250\n",
      "    epoch          : 927\n",
      "    loss           : -286230.350625\n",
      "    val_loss       : -288176.11875\n",
      "Train Epoch: 928 [512/54000 (1%)] Loss: -378937.187500\n",
      "Train Epoch: 928 [11776/54000 (22%)] Loss: -161356.843750\n",
      "Train Epoch: 928 [23040/54000 (43%)] Loss: -190167.125000\n",
      "Train Epoch: 928 [34304/54000 (64%)] Loss: -182936.125000\n",
      "Train Epoch: 928 [45568/54000 (84%)] Loss: -172790.640625\n",
      "    epoch          : 928\n",
      "    loss           : -286483.21328125\n",
      "    val_loss       : -288100.1234375\n",
      "Train Epoch: 929 [512/54000 (1%)] Loss: -177595.531250\n",
      "Train Epoch: 929 [11776/54000 (22%)] Loss: -357067.218750\n",
      "Train Epoch: 929 [23040/54000 (43%)] Loss: -355879.125000\n",
      "Train Epoch: 929 [34304/54000 (64%)] Loss: -170815.765625\n",
      "Train Epoch: 929 [45568/54000 (84%)] Loss: -360725.312500\n",
      "    epoch          : 929\n",
      "    loss           : -286489.64875\n",
      "    val_loss       : -287819.001953125\n",
      "Train Epoch: 930 [512/54000 (1%)] Loss: -375182.125000\n",
      "Train Epoch: 930 [11776/54000 (22%)] Loss: -253653.218750\n",
      "Train Epoch: 930 [23040/54000 (43%)] Loss: -162802.359375\n",
      "Train Epoch: 930 [34304/54000 (64%)] Loss: -174984.359375\n",
      "Train Epoch: 930 [45568/54000 (84%)] Loss: -247827.671875\n",
      "    epoch          : 930\n",
      "    loss           : -286594.8175\n",
      "    val_loss       : -287843.847265625\n",
      "Train Epoch: 931 [512/54000 (1%)] Loss: -258928.250000\n",
      "Train Epoch: 931 [11776/54000 (22%)] Loss: -338908.250000\n",
      "Train Epoch: 931 [23040/54000 (43%)] Loss: -261892.093750\n",
      "Train Epoch: 931 [34304/54000 (64%)] Loss: -376351.156250\n",
      "Train Epoch: 931 [45568/54000 (84%)] Loss: -266279.000000\n",
      "    epoch          : 931\n",
      "    loss           : -286517.47875\n",
      "    val_loss       : -287373.799609375\n",
      "Train Epoch: 932 [512/54000 (1%)] Loss: -258646.562500\n",
      "Train Epoch: 932 [11776/54000 (22%)] Loss: -256766.906250\n",
      "Train Epoch: 932 [23040/54000 (43%)] Loss: -168783.296875\n",
      "Train Epoch: 932 [34304/54000 (64%)] Loss: -343656.187500\n",
      "Train Epoch: 932 [45568/54000 (84%)] Loss: -265996.000000\n",
      "    epoch          : 932\n",
      "    loss           : -286652.3453125\n",
      "    val_loss       : -286572.2109375\n",
      "Train Epoch: 933 [512/54000 (1%)] Loss: -375767.656250\n",
      "Train Epoch: 933 [11776/54000 (22%)] Loss: -226230.281250\n",
      "Train Epoch: 933 [23040/54000 (43%)] Loss: -338062.968750\n",
      "Train Epoch: 933 [34304/54000 (64%)] Loss: -178811.453125\n",
      "Train Epoch: 933 [45568/54000 (84%)] Loss: -338323.843750\n",
      "    epoch          : 933\n",
      "    loss           : -286682.135\n",
      "    val_loss       : -287393.2515625\n",
      "Train Epoch: 934 [512/54000 (1%)] Loss: -260132.718750\n",
      "Train Epoch: 934 [11776/54000 (22%)] Loss: -260661.531250\n",
      "Train Epoch: 934 [23040/54000 (43%)] Loss: -266840.937500\n",
      "Train Epoch: 934 [34304/54000 (64%)] Loss: -263718.375000\n",
      "Train Epoch: 934 [45568/54000 (84%)] Loss: -360202.312500\n",
      "    epoch          : 934\n",
      "    loss           : -286617.71296875\n",
      "    val_loss       : -288275.13984375\n",
      "Train Epoch: 935 [512/54000 (1%)] Loss: -258517.859375\n",
      "Train Epoch: 935 [11776/54000 (22%)] Loss: -266208.875000\n",
      "Train Epoch: 935 [23040/54000 (43%)] Loss: -362394.843750\n",
      "Train Epoch: 935 [34304/54000 (64%)] Loss: -341797.625000\n",
      "Train Epoch: 935 [45568/54000 (84%)] Loss: -256568.031250\n",
      "    epoch          : 935\n",
      "    loss           : -286701.978125\n",
      "    val_loss       : -288209.14765625\n",
      "Train Epoch: 936 [512/54000 (1%)] Loss: -223460.296875\n",
      "Train Epoch: 936 [11776/54000 (22%)] Loss: -262946.343750\n",
      "Train Epoch: 936 [23040/54000 (43%)] Loss: -250983.343750\n",
      "Train Epoch: 936 [34304/54000 (64%)] Loss: -255924.171875\n",
      "Train Epoch: 936 [45568/54000 (84%)] Loss: -364569.750000\n",
      "    epoch          : 936\n",
      "    loss           : -286744.86671875\n",
      "    val_loss       : -288711.069140625\n",
      "Train Epoch: 937 [512/54000 (1%)] Loss: -247338.343750\n",
      "Train Epoch: 937 [11776/54000 (22%)] Loss: -357498.375000\n",
      "Train Epoch: 937 [23040/54000 (43%)] Loss: -267616.062500\n",
      "Train Epoch: 937 [34304/54000 (64%)] Loss: -220298.406250\n",
      "Train Epoch: 937 [45568/54000 (84%)] Loss: -337154.468750\n",
      "    epoch          : 937\n",
      "    loss           : -286775.39625\n",
      "    val_loss       : -288113.34765625\n",
      "Train Epoch: 938 [512/54000 (1%)] Loss: -257133.218750\n",
      "Train Epoch: 938 [11776/54000 (22%)] Loss: -357596.062500\n",
      "Train Epoch: 938 [23040/54000 (43%)] Loss: -376677.656250\n",
      "Train Epoch: 938 [34304/54000 (64%)] Loss: -341999.437500\n",
      "Train Epoch: 938 [45568/54000 (84%)] Loss: -251472.156250\n",
      "    epoch          : 938\n",
      "    loss           : -286944.14109375\n",
      "    val_loss       : -287959.701171875\n",
      "Train Epoch: 939 [512/54000 (1%)] Loss: -337530.437500\n",
      "Train Epoch: 939 [11776/54000 (22%)] Loss: -359257.562500\n",
      "Train Epoch: 939 [23040/54000 (43%)] Loss: -366738.843750\n",
      "Train Epoch: 939 [34304/54000 (64%)] Loss: -227705.062500\n",
      "Train Epoch: 939 [45568/54000 (84%)] Loss: -246036.125000\n",
      "    epoch          : 939\n",
      "    loss           : -286963.41953125\n",
      "    val_loss       : -288453.334765625\n",
      "Train Epoch: 940 [512/54000 (1%)] Loss: -264756.968750\n",
      "Train Epoch: 940 [11776/54000 (22%)] Loss: -246739.343750\n",
      "Train Epoch: 940 [23040/54000 (43%)] Loss: -251908.000000\n",
      "Train Epoch: 940 [34304/54000 (64%)] Loss: -265103.437500\n",
      "Train Epoch: 940 [45568/54000 (84%)] Loss: -225445.328125\n",
      "    epoch          : 940\n",
      "    loss           : -286921.09765625\n",
      "    val_loss       : -288204.8640625\n",
      "Train Epoch: 941 [512/54000 (1%)] Loss: -378071.062500\n",
      "Train Epoch: 941 [11776/54000 (22%)] Loss: -244598.218750\n",
      "Train Epoch: 941 [23040/54000 (43%)] Loss: -358906.562500\n",
      "Train Epoch: 941 [34304/54000 (64%)] Loss: -261336.656250\n",
      "Train Epoch: 941 [45568/54000 (84%)] Loss: -221626.093750\n",
      "    epoch          : 941\n",
      "    loss           : -287058.7775\n",
      "    val_loss       : -287813.77421875\n",
      "Train Epoch: 942 [512/54000 (1%)] Loss: -172143.156250\n",
      "Train Epoch: 942 [11776/54000 (22%)] Loss: -340473.156250\n",
      "Train Epoch: 942 [23040/54000 (43%)] Loss: -225788.718750\n",
      "Train Epoch: 942 [34304/54000 (64%)] Loss: -356668.343750\n",
      "Train Epoch: 942 [45568/54000 (84%)] Loss: -363593.687500\n",
      "    epoch          : 942\n",
      "    loss           : -287129.16171875\n",
      "    val_loss       : -288423.291015625\n",
      "Train Epoch: 943 [512/54000 (1%)] Loss: -256894.546875\n",
      "Train Epoch: 943 [11776/54000 (22%)] Loss: -358247.718750\n",
      "Train Epoch: 943 [23040/54000 (43%)] Loss: -375292.593750\n",
      "Train Epoch: 943 [34304/54000 (64%)] Loss: -249793.718750\n",
      "Train Epoch: 943 [45568/54000 (84%)] Loss: -335874.406250\n",
      "    epoch          : 943\n",
      "    loss           : -287143.96953125\n",
      "    val_loss       : -287843.70078125\n",
      "Train Epoch: 944 [512/54000 (1%)] Loss: -338380.625000\n",
      "Train Epoch: 944 [11776/54000 (22%)] Loss: -358385.750000\n",
      "Train Epoch: 944 [23040/54000 (43%)] Loss: -357249.437500\n",
      "Train Epoch: 944 [34304/54000 (64%)] Loss: -336562.406250\n",
      "Train Epoch: 944 [45568/54000 (84%)] Loss: -342727.687500\n",
      "    epoch          : 944\n",
      "    loss           : -287100.72359375\n",
      "    val_loss       : -288886.62734375\n",
      "Train Epoch: 945 [512/54000 (1%)] Loss: -171175.609375\n",
      "Train Epoch: 945 [11776/54000 (22%)] Loss: -257079.281250\n",
      "Train Epoch: 945 [23040/54000 (43%)] Loss: -377588.375000\n",
      "Train Epoch: 945 [34304/54000 (64%)] Loss: -337493.000000\n",
      "Train Epoch: 945 [45568/54000 (84%)] Loss: -363559.062500\n",
      "    epoch          : 945\n",
      "    loss           : -287041.075625\n",
      "    val_loss       : -288451.1015625\n",
      "Train Epoch: 946 [512/54000 (1%)] Loss: -255410.468750\n",
      "Train Epoch: 946 [11776/54000 (22%)] Loss: -170605.765625\n",
      "Train Epoch: 946 [23040/54000 (43%)] Loss: -260100.687500\n",
      "Train Epoch: 946 [34304/54000 (64%)] Loss: -221112.156250\n",
      "Train Epoch: 946 [45568/54000 (84%)] Loss: -251651.531250\n",
      "    epoch          : 946\n",
      "    loss           : -287103.85234375\n",
      "    val_loss       : -288058.80390625\n",
      "Train Epoch: 947 [512/54000 (1%)] Loss: -263502.187500\n",
      "Train Epoch: 947 [11776/54000 (22%)] Loss: -358568.437500\n",
      "Train Epoch: 947 [23040/54000 (43%)] Loss: -260200.734375\n",
      "Train Epoch: 947 [34304/54000 (64%)] Loss: -251730.375000\n",
      "Train Epoch: 947 [45568/54000 (84%)] Loss: -220113.312500\n",
      "    epoch          : 947\n",
      "    loss           : -287264.03953125\n",
      "    val_loss       : -288277.2859375\n",
      "Train Epoch: 948 [512/54000 (1%)] Loss: -188602.265625\n",
      "Train Epoch: 948 [11776/54000 (22%)] Loss: -218583.406250\n",
      "Train Epoch: 948 [23040/54000 (43%)] Loss: -181551.078125\n",
      "Train Epoch: 948 [34304/54000 (64%)] Loss: -260988.843750\n",
      "Train Epoch: 948 [45568/54000 (84%)] Loss: -338774.750000\n",
      "    epoch          : 948\n",
      "    loss           : -287238.9965625\n",
      "    val_loss       : -287861.79375\n",
      "Train Epoch: 949 [512/54000 (1%)] Loss: -356345.687500\n",
      "Train Epoch: 949 [11776/54000 (22%)] Loss: -254790.656250\n",
      "Train Epoch: 949 [23040/54000 (43%)] Loss: -271916.812500\n",
      "Train Epoch: 949 [34304/54000 (64%)] Loss: -243778.046875\n",
      "Train Epoch: 949 [45568/54000 (84%)] Loss: -224201.937500\n",
      "    epoch          : 949\n",
      "    loss           : -287335.8434375\n",
      "    val_loss       : -288918.6734375\n",
      "Train Epoch: 950 [512/54000 (1%)] Loss: -248921.281250\n",
      "Train Epoch: 950 [11776/54000 (22%)] Loss: -361553.000000\n",
      "Train Epoch: 950 [23040/54000 (43%)] Loss: -267885.343750\n",
      "Train Epoch: 950 [34304/54000 (64%)] Loss: -257361.156250\n",
      "Train Epoch: 950 [45568/54000 (84%)] Loss: -336880.031250\n",
      "    epoch          : 950\n",
      "    loss           : -287603.51640625\n",
      "    val_loss       : -288313.589453125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch950.pth ...\n",
      "Train Epoch: 951 [512/54000 (1%)] Loss: -340106.687500\n",
      "Train Epoch: 951 [11776/54000 (22%)] Loss: -249076.812500\n",
      "Train Epoch: 951 [23040/54000 (43%)] Loss: -378683.187500\n",
      "Train Epoch: 951 [34304/54000 (64%)] Loss: -223696.062500\n",
      "Train Epoch: 951 [45568/54000 (84%)] Loss: -356463.812500\n",
      "    epoch          : 951\n",
      "    loss           : -287523.9646875\n",
      "    val_loss       : -288941.038671875\n",
      "Train Epoch: 952 [512/54000 (1%)] Loss: -182168.187500\n",
      "Train Epoch: 952 [11776/54000 (22%)] Loss: -375618.000000\n",
      "Train Epoch: 952 [23040/54000 (43%)] Loss: -357494.312500\n",
      "Train Epoch: 952 [34304/54000 (64%)] Loss: -362981.312500\n",
      "Train Epoch: 952 [45568/54000 (84%)] Loss: -364152.562500\n",
      "    epoch          : 952\n",
      "    loss           : -287534.77546875\n",
      "    val_loss       : -287914.849609375\n",
      "Train Epoch: 953 [512/54000 (1%)] Loss: -267591.875000\n",
      "Train Epoch: 953 [11776/54000 (22%)] Loss: -267113.281250\n",
      "Train Epoch: 953 [23040/54000 (43%)] Loss: -380413.031250\n",
      "Train Epoch: 953 [34304/54000 (64%)] Loss: -364741.281250\n",
      "Train Epoch: 953 [45568/54000 (84%)] Loss: -222628.625000\n",
      "    epoch          : 953\n",
      "    loss           : -287533.490625\n",
      "    val_loss       : -288310.61328125\n",
      "Train Epoch: 954 [512/54000 (1%)] Loss: -259996.281250\n",
      "Train Epoch: 954 [11776/54000 (22%)] Loss: -257847.000000\n",
      "Train Epoch: 954 [23040/54000 (43%)] Loss: -380531.250000\n",
      "Train Epoch: 954 [34304/54000 (64%)] Loss: -341588.250000\n",
      "Train Epoch: 954 [45568/54000 (84%)] Loss: -337450.093750\n",
      "    epoch          : 954\n",
      "    loss           : -287441.03640625\n",
      "    val_loss       : -288754.3109375\n",
      "Train Epoch: 955 [512/54000 (1%)] Loss: -266425.500000\n",
      "Train Epoch: 955 [11776/54000 (22%)] Loss: -250573.875000\n",
      "Train Epoch: 955 [23040/54000 (43%)] Loss: -342692.281250\n",
      "Train Epoch: 955 [34304/54000 (64%)] Loss: -365879.000000\n",
      "Train Epoch: 955 [45568/54000 (84%)] Loss: -253668.843750\n",
      "    epoch          : 955\n",
      "    loss           : -287627.310625\n",
      "    val_loss       : -288394.65390625\n",
      "Train Epoch: 956 [512/54000 (1%)] Loss: -222937.531250\n",
      "Train Epoch: 956 [11776/54000 (22%)] Loss: -378600.343750\n",
      "Train Epoch: 956 [23040/54000 (43%)] Loss: -258575.078125\n",
      "Train Epoch: 956 [34304/54000 (64%)] Loss: -253342.515625\n",
      "Train Epoch: 956 [45568/54000 (84%)] Loss: -264928.218750\n",
      "    epoch          : 956\n",
      "    loss           : -287507.821875\n",
      "    val_loss       : -288465.4578125\n",
      "Train Epoch: 957 [512/54000 (1%)] Loss: -363473.562500\n",
      "Train Epoch: 957 [11776/54000 (22%)] Loss: -366641.062500\n",
      "Train Epoch: 957 [23040/54000 (43%)] Loss: -379173.562500\n",
      "Train Epoch: 957 [34304/54000 (64%)] Loss: -364228.375000\n",
      "Train Epoch: 957 [45568/54000 (84%)] Loss: -338086.468750\n",
      "    epoch          : 957\n",
      "    loss           : -287680.05328125\n",
      "    val_loss       : -289562.3953125\n",
      "Train Epoch: 958 [512/54000 (1%)] Loss: -357157.718750\n",
      "Train Epoch: 958 [11776/54000 (22%)] Loss: -357124.375000\n",
      "Train Epoch: 958 [23040/54000 (43%)] Loss: -258987.968750\n",
      "Train Epoch: 958 [34304/54000 (64%)] Loss: -257114.625000\n",
      "Train Epoch: 958 [45568/54000 (84%)] Loss: -246129.156250\n",
      "    epoch          : 958\n",
      "    loss           : -287801.3190625\n",
      "    val_loss       : -288001.634375\n",
      "Train Epoch: 959 [512/54000 (1%)] Loss: -168547.921875\n",
      "Train Epoch: 959 [11776/54000 (22%)] Loss: -178000.984375\n",
      "Train Epoch: 959 [23040/54000 (43%)] Loss: -262237.062500\n",
      "Train Epoch: 959 [34304/54000 (64%)] Loss: -359073.406250\n",
      "Train Epoch: 959 [45568/54000 (84%)] Loss: -364384.875000\n",
      "    epoch          : 959\n",
      "    loss           : -287701.17328125\n",
      "    val_loss       : -289129.998046875\n",
      "Train Epoch: 960 [512/54000 (1%)] Loss: -253799.546875\n",
      "Train Epoch: 960 [11776/54000 (22%)] Loss: -261266.046875\n",
      "Train Epoch: 960 [23040/54000 (43%)] Loss: -269114.812500\n",
      "Train Epoch: 960 [34304/54000 (64%)] Loss: -266286.750000\n",
      "Train Epoch: 960 [45568/54000 (84%)] Loss: -247214.500000\n",
      "    epoch          : 960\n",
      "    loss           : -287912.9721875\n",
      "    val_loss       : -288674.10390625\n",
      "Train Epoch: 961 [512/54000 (1%)] Loss: -342266.875000\n",
      "Train Epoch: 961 [11776/54000 (22%)] Loss: -262876.562500\n",
      "Train Epoch: 961 [23040/54000 (43%)] Loss: -380220.281250\n",
      "Train Epoch: 961 [34304/54000 (64%)] Loss: -378578.562500\n",
      "Train Epoch: 961 [45568/54000 (84%)] Loss: -226819.265625\n",
      "    epoch          : 961\n",
      "    loss           : -287839.99546875\n",
      "    val_loss       : -288634.28203125\n",
      "Train Epoch: 962 [512/54000 (1%)] Loss: -223039.156250\n",
      "Train Epoch: 962 [11776/54000 (22%)] Loss: -263610.000000\n",
      "Train Epoch: 962 [23040/54000 (43%)] Loss: -380818.343750\n",
      "Train Epoch: 962 [34304/54000 (64%)] Loss: -251599.875000\n",
      "Train Epoch: 962 [45568/54000 (84%)] Loss: -366209.437500\n",
      "    epoch          : 962\n",
      "    loss           : -287869.2609375\n",
      "    val_loss       : -288795.089453125\n",
      "Train Epoch: 963 [512/54000 (1%)] Loss: -173279.562500\n",
      "Train Epoch: 963 [11776/54000 (22%)] Loss: -264092.687500\n",
      "Train Epoch: 963 [23040/54000 (43%)] Loss: -375573.875000\n",
      "Train Epoch: 963 [34304/54000 (64%)] Loss: -257622.781250\n",
      "Train Epoch: 963 [45568/54000 (84%)] Loss: -246160.625000\n",
      "    epoch          : 963\n",
      "    loss           : -287858.13421875\n",
      "    val_loss       : -288947.97734375\n",
      "Train Epoch: 964 [512/54000 (1%)] Loss: -362706.625000\n",
      "Train Epoch: 964 [11776/54000 (22%)] Loss: -259242.125000\n",
      "Train Epoch: 964 [23040/54000 (43%)] Loss: -222948.531250\n",
      "Train Epoch: 964 [34304/54000 (64%)] Loss: -363333.031250\n",
      "Train Epoch: 964 [45568/54000 (84%)] Loss: -369008.062500\n",
      "    epoch          : 964\n",
      "    loss           : -287929.2490625\n",
      "    val_loss       : -289157.969140625\n",
      "Train Epoch: 965 [512/54000 (1%)] Loss: -358269.937500\n",
      "Train Epoch: 965 [11776/54000 (22%)] Loss: -339389.718750\n",
      "Train Epoch: 965 [23040/54000 (43%)] Loss: -265497.593750\n",
      "Train Epoch: 965 [34304/54000 (64%)] Loss: -221551.453125\n",
      "Train Epoch: 965 [45568/54000 (84%)] Loss: -249582.375000\n",
      "    epoch          : 965\n",
      "    loss           : -288010.3628125\n",
      "    val_loss       : -289046.72734375\n",
      "Train Epoch: 966 [512/54000 (1%)] Loss: -359727.375000\n",
      "Train Epoch: 966 [11776/54000 (22%)] Loss: -359223.656250\n",
      "Train Epoch: 966 [23040/54000 (43%)] Loss: -368989.187500\n",
      "Train Epoch: 966 [34304/54000 (64%)] Loss: -338992.687500\n",
      "Train Epoch: 966 [45568/54000 (84%)] Loss: -242842.593750\n",
      "    epoch          : 966\n",
      "    loss           : -288074.4428125\n",
      "    val_loss       : -289382.66171875\n",
      "Train Epoch: 967 [512/54000 (1%)] Loss: -263598.750000\n",
      "Train Epoch: 967 [11776/54000 (22%)] Loss: -248927.796875\n",
      "Train Epoch: 967 [23040/54000 (43%)] Loss: -256772.515625\n",
      "Train Epoch: 967 [34304/54000 (64%)] Loss: -222376.937500\n",
      "Train Epoch: 967 [45568/54000 (84%)] Loss: -220788.140625\n",
      "    epoch          : 967\n",
      "    loss           : -288100.58359375\n",
      "    val_loss       : -288562.61328125\n",
      "Train Epoch: 968 [512/54000 (1%)] Loss: -366804.281250\n",
      "Train Epoch: 968 [11776/54000 (22%)] Loss: -182015.906250\n",
      "Train Epoch: 968 [23040/54000 (43%)] Loss: -261941.875000\n",
      "Train Epoch: 968 [34304/54000 (64%)] Loss: -177990.375000\n",
      "Train Epoch: 968 [45568/54000 (84%)] Loss: -224737.718750\n",
      "    epoch          : 968\n",
      "    loss           : -288177.554375\n",
      "    val_loss       : -288949.171875\n",
      "Train Epoch: 969 [512/54000 (1%)] Loss: -229599.328125\n",
      "Train Epoch: 969 [11776/54000 (22%)] Loss: -254368.968750\n",
      "Train Epoch: 969 [23040/54000 (43%)] Loss: -253243.968750\n",
      "Train Epoch: 969 [34304/54000 (64%)] Loss: -262871.531250\n",
      "Train Epoch: 969 [45568/54000 (84%)] Loss: -365761.875000\n",
      "    epoch          : 969\n",
      "    loss           : -288025.97703125\n",
      "    val_loss       : -289430.36015625\n",
      "Train Epoch: 970 [512/54000 (1%)] Loss: -252480.078125\n",
      "Train Epoch: 970 [11776/54000 (22%)] Loss: -253569.531250\n",
      "Train Epoch: 970 [23040/54000 (43%)] Loss: -257249.562500\n",
      "Train Epoch: 970 [34304/54000 (64%)] Loss: -357658.500000\n",
      "Train Epoch: 970 [45568/54000 (84%)] Loss: -341249.250000\n",
      "    epoch          : 970\n",
      "    loss           : -288287.55296875\n",
      "    val_loss       : -288647.345703125\n",
      "Train Epoch: 971 [512/54000 (1%)] Loss: -376044.156250\n",
      "Train Epoch: 971 [11776/54000 (22%)] Loss: -380834.875000\n",
      "Train Epoch: 971 [23040/54000 (43%)] Loss: -188974.421875\n",
      "Train Epoch: 971 [34304/54000 (64%)] Loss: -267443.750000\n",
      "Train Epoch: 971 [45568/54000 (84%)] Loss: -247999.890625\n",
      "    epoch          : 971\n",
      "    loss           : -288294.6275\n",
      "    val_loss       : -289543.0859375\n",
      "Train Epoch: 972 [512/54000 (1%)] Loss: -264614.281250\n",
      "Train Epoch: 972 [11776/54000 (22%)] Loss: -267733.437500\n",
      "Train Epoch: 972 [23040/54000 (43%)] Loss: -342679.937500\n",
      "Train Epoch: 972 [34304/54000 (64%)] Loss: -365000.937500\n",
      "Train Epoch: 972 [45568/54000 (84%)] Loss: -252151.453125\n",
      "    epoch          : 972\n",
      "    loss           : -288282.61859375\n",
      "    val_loss       : -288957.73359375\n",
      "Train Epoch: 973 [512/54000 (1%)] Loss: -375816.937500\n",
      "Train Epoch: 973 [11776/54000 (22%)] Loss: -378622.562500\n",
      "Train Epoch: 973 [23040/54000 (43%)] Loss: -264775.281250\n",
      "Train Epoch: 973 [34304/54000 (64%)] Loss: -257259.312500\n",
      "Train Epoch: 973 [45568/54000 (84%)] Loss: -223974.125000\n",
      "    epoch          : 973\n",
      "    loss           : -288381.4434375\n",
      "    val_loss       : -290412.136328125\n",
      "Train Epoch: 974 [512/54000 (1%)] Loss: -364941.250000\n",
      "Train Epoch: 974 [11776/54000 (22%)] Loss: -255336.625000\n",
      "Train Epoch: 974 [23040/54000 (43%)] Loss: -358227.750000\n",
      "Train Epoch: 974 [34304/54000 (64%)] Loss: -381845.656250\n",
      "Train Epoch: 974 [45568/54000 (84%)] Loss: -341628.187500\n",
      "    epoch          : 974\n",
      "    loss           : -288471.47171875\n",
      "    val_loss       : -289553.86015625\n",
      "Train Epoch: 975 [512/54000 (1%)] Loss: -227531.687500\n",
      "Train Epoch: 975 [11776/54000 (22%)] Loss: -264623.625000\n",
      "Train Epoch: 975 [23040/54000 (43%)] Loss: -365039.062500\n",
      "Train Epoch: 975 [34304/54000 (64%)] Loss: -249907.578125\n",
      "Train Epoch: 975 [45568/54000 (84%)] Loss: -252394.046875\n",
      "    epoch          : 975\n",
      "    loss           : -288298.48046875\n",
      "    val_loss       : -290277.394140625\n",
      "Train Epoch: 976 [512/54000 (1%)] Loss: -268115.656250\n",
      "Train Epoch: 976 [11776/54000 (22%)] Loss: -248561.812500\n",
      "Train Epoch: 976 [23040/54000 (43%)] Loss: -366321.062500\n",
      "Train Epoch: 976 [34304/54000 (64%)] Loss: -365106.312500\n",
      "Train Epoch: 976 [45568/54000 (84%)] Loss: -341644.531250\n",
      "    epoch          : 976\n",
      "    loss           : -288414.6434375\n",
      "    val_loss       : -288799.301953125\n",
      "Train Epoch: 977 [512/54000 (1%)] Loss: -259498.125000\n",
      "Train Epoch: 977 [11776/54000 (22%)] Loss: -382102.531250\n",
      "Train Epoch: 977 [23040/54000 (43%)] Loss: -258997.625000\n",
      "Train Epoch: 977 [34304/54000 (64%)] Loss: -225903.968750\n",
      "Train Epoch: 977 [45568/54000 (84%)] Loss: -226097.859375\n",
      "    epoch          : 977\n",
      "    loss           : -288442.68109375\n",
      "    val_loss       : -289280.442578125\n",
      "Train Epoch: 978 [512/54000 (1%)] Loss: -344905.062500\n",
      "Train Epoch: 978 [11776/54000 (22%)] Loss: -341972.375000\n",
      "Train Epoch: 978 [23040/54000 (43%)] Loss: -267136.437500\n",
      "Train Epoch: 978 [34304/54000 (64%)] Loss: -246967.093750\n",
      "Train Epoch: 978 [45568/54000 (84%)] Loss: -223102.343750\n",
      "    epoch          : 978\n",
      "    loss           : -288520.2578125\n",
      "    val_loss       : -289987.01171875\n",
      "Train Epoch: 979 [512/54000 (1%)] Loss: -358920.812500\n",
      "Train Epoch: 979 [11776/54000 (22%)] Loss: -253266.203125\n",
      "Train Epoch: 979 [23040/54000 (43%)] Loss: -249156.125000\n",
      "Train Epoch: 979 [34304/54000 (64%)] Loss: -359094.000000\n",
      "Train Epoch: 979 [45568/54000 (84%)] Loss: -226432.015625\n",
      "    epoch          : 979\n",
      "    loss           : -288659.08203125\n",
      "    val_loss       : -289793.944140625\n",
      "Train Epoch: 980 [512/54000 (1%)] Loss: -248360.312500\n",
      "Train Epoch: 980 [11776/54000 (22%)] Loss: -359005.812500\n",
      "Train Epoch: 980 [23040/54000 (43%)] Loss: -253258.296875\n",
      "Train Epoch: 980 [34304/54000 (64%)] Loss: -381298.937500\n",
      "Train Epoch: 980 [45568/54000 (84%)] Loss: -229856.812500\n",
      "    epoch          : 980\n",
      "    loss           : -288776.26125\n",
      "    val_loss       : -289127.618359375\n",
      "Train Epoch: 981 [512/54000 (1%)] Loss: -336903.625000\n",
      "Train Epoch: 981 [11776/54000 (22%)] Loss: -258902.343750\n",
      "Train Epoch: 981 [23040/54000 (43%)] Loss: -364503.500000\n",
      "Train Epoch: 981 [34304/54000 (64%)] Loss: -340827.562500\n",
      "Train Epoch: 981 [45568/54000 (84%)] Loss: -254966.046875\n",
      "    epoch          : 981\n",
      "    loss           : -288593.21078125\n",
      "    val_loss       : -290231.3953125\n",
      "Train Epoch: 982 [512/54000 (1%)] Loss: -360619.062500\n",
      "Train Epoch: 982 [11776/54000 (22%)] Loss: -227409.703125\n",
      "Train Epoch: 982 [23040/54000 (43%)] Loss: -344090.625000\n",
      "Train Epoch: 982 [34304/54000 (64%)] Loss: -244586.312500\n",
      "Train Epoch: 982 [45568/54000 (84%)] Loss: -251467.421875\n",
      "    epoch          : 982\n",
      "    loss           : -288756.08\n",
      "    val_loss       : -289533.4546875\n",
      "Train Epoch: 983 [512/54000 (1%)] Loss: -384244.843750\n",
      "Train Epoch: 983 [11776/54000 (22%)] Loss: -358157.937500\n",
      "Train Epoch: 983 [23040/54000 (43%)] Loss: -254126.421875\n",
      "Train Epoch: 983 [34304/54000 (64%)] Loss: -249635.468750\n",
      "Train Epoch: 983 [45568/54000 (84%)] Loss: -343916.875000\n",
      "    epoch          : 983\n",
      "    loss           : -288670.7821875\n",
      "    val_loss       : -290339.5796875\n",
      "Train Epoch: 984 [512/54000 (1%)] Loss: -338032.781250\n",
      "Train Epoch: 984 [11776/54000 (22%)] Loss: -222184.156250\n",
      "Train Epoch: 984 [23040/54000 (43%)] Loss: -225938.421875\n",
      "Train Epoch: 984 [34304/54000 (64%)] Loss: -365036.625000\n",
      "Train Epoch: 984 [45568/54000 (84%)] Loss: -341269.312500\n",
      "    epoch          : 984\n",
      "    loss           : -288680.9965625\n",
      "    val_loss       : -290094.95234375\n",
      "Train Epoch: 985 [512/54000 (1%)] Loss: -264605.000000\n",
      "Train Epoch: 985 [11776/54000 (22%)] Loss: -342036.187500\n",
      "Train Epoch: 985 [23040/54000 (43%)] Loss: -263071.093750\n",
      "Train Epoch: 985 [34304/54000 (64%)] Loss: -365441.687500\n",
      "Train Epoch: 985 [45568/54000 (84%)] Loss: -363777.375000\n",
      "    epoch          : 985\n",
      "    loss           : -288754.0953125\n",
      "    val_loss       : -289613.0046875\n",
      "Train Epoch: 986 [512/54000 (1%)] Loss: -361002.531250\n",
      "Train Epoch: 986 [11776/54000 (22%)] Loss: -221663.000000\n",
      "Train Epoch: 986 [23040/54000 (43%)] Loss: -261495.843750\n",
      "Train Epoch: 986 [34304/54000 (64%)] Loss: -227715.187500\n",
      "Train Epoch: 986 [45568/54000 (84%)] Loss: -249187.000000\n",
      "    epoch          : 986\n",
      "    loss           : -288861.94609375\n",
      "    val_loss       : -290650.569921875\n",
      "Train Epoch: 987 [512/54000 (1%)] Loss: -176504.421875\n",
      "Train Epoch: 987 [11776/54000 (22%)] Loss: -356833.812500\n",
      "Train Epoch: 987 [23040/54000 (43%)] Loss: -269062.218750\n",
      "Train Epoch: 987 [34304/54000 (64%)] Loss: -359422.812500\n",
      "Train Epoch: 987 [45568/54000 (84%)] Loss: -252628.953125\n",
      "    epoch          : 987\n",
      "    loss           : -288777.0921875\n",
      "    val_loss       : -290522.314453125\n",
      "Train Epoch: 988 [512/54000 (1%)] Loss: -258189.343750\n",
      "Train Epoch: 988 [11776/54000 (22%)] Loss: -359945.437500\n",
      "Train Epoch: 988 [23040/54000 (43%)] Loss: -261732.953125\n",
      "Train Epoch: 988 [34304/54000 (64%)] Loss: -376741.062500\n",
      "Train Epoch: 988 [45568/54000 (84%)] Loss: -226479.593750\n",
      "    epoch          : 988\n",
      "    loss           : -288757.19609375\n",
      "    val_loss       : -289662.124609375\n",
      "Train Epoch: 989 [512/54000 (1%)] Loss: -358595.062500\n",
      "Train Epoch: 989 [11776/54000 (22%)] Loss: -265290.593750\n",
      "Train Epoch: 989 [23040/54000 (43%)] Loss: -262374.406250\n",
      "Train Epoch: 989 [34304/54000 (64%)] Loss: -340658.937500\n",
      "Train Epoch: 989 [45568/54000 (84%)] Loss: -266502.281250\n",
      "    epoch          : 989\n",
      "    loss           : -289079.0484375\n",
      "    val_loss       : -290160.033203125\n",
      "Train Epoch: 990 [512/54000 (1%)] Loss: -364354.062500\n",
      "Train Epoch: 990 [11776/54000 (22%)] Loss: -361316.281250\n",
      "Train Epoch: 990 [23040/54000 (43%)] Loss: -265296.937500\n",
      "Train Epoch: 990 [34304/54000 (64%)] Loss: -259315.203125\n",
      "Train Epoch: 990 [45568/54000 (84%)] Loss: -180807.375000\n",
      "    epoch          : 990\n",
      "    loss           : -289172.23078125\n",
      "    val_loss       : -290530.83203125\n",
      "Train Epoch: 991 [512/54000 (1%)] Loss: -258660.453125\n",
      "Train Epoch: 991 [11776/54000 (22%)] Loss: -263132.937500\n",
      "Train Epoch: 991 [23040/54000 (43%)] Loss: -341675.625000\n",
      "Train Epoch: 991 [34304/54000 (64%)] Loss: -268123.500000\n",
      "Train Epoch: 991 [45568/54000 (84%)] Loss: -252764.218750\n",
      "    epoch          : 991\n",
      "    loss           : -289047.21234375\n",
      "    val_loss       : -290189.42421875\n",
      "Train Epoch: 992 [512/54000 (1%)] Loss: -367204.781250\n",
      "Train Epoch: 992 [11776/54000 (22%)] Loss: -361180.937500\n",
      "Train Epoch: 992 [23040/54000 (43%)] Loss: -259303.640625\n",
      "Train Epoch: 992 [34304/54000 (64%)] Loss: -257754.890625\n",
      "Train Epoch: 992 [45568/54000 (84%)] Loss: -338395.437500\n",
      "    epoch          : 992\n",
      "    loss           : -289198.931875\n",
      "    val_loss       : -290299.9421875\n",
      "Train Epoch: 993 [512/54000 (1%)] Loss: -252568.406250\n",
      "Train Epoch: 993 [11776/54000 (22%)] Loss: -263255.968750\n",
      "Train Epoch: 993 [23040/54000 (43%)] Loss: -367382.125000\n",
      "Train Epoch: 993 [34304/54000 (64%)] Loss: -367405.250000\n",
      "Train Epoch: 993 [45568/54000 (84%)] Loss: -368831.937500\n",
      "    epoch          : 993\n",
      "    loss           : -289178.9996875\n",
      "    val_loss       : -290638.996484375\n",
      "Train Epoch: 994 [512/54000 (1%)] Loss: -270692.937500\n",
      "Train Epoch: 994 [11776/54000 (22%)] Loss: -264304.187500\n",
      "Train Epoch: 994 [23040/54000 (43%)] Loss: -358643.187500\n",
      "Train Epoch: 994 [34304/54000 (64%)] Loss: -256530.468750\n",
      "Train Epoch: 994 [45568/54000 (84%)] Loss: -338311.531250\n",
      "    epoch          : 994\n",
      "    loss           : -289220.5696875\n",
      "    val_loss       : -290384.25859375\n",
      "Train Epoch: 995 [512/54000 (1%)] Loss: -381578.375000\n",
      "Train Epoch: 995 [11776/54000 (22%)] Loss: -226342.625000\n",
      "Train Epoch: 995 [23040/54000 (43%)] Loss: -260752.343750\n",
      "Train Epoch: 995 [34304/54000 (64%)] Loss: -359215.781250\n",
      "Train Epoch: 995 [45568/54000 (84%)] Loss: -366413.406250\n",
      "    epoch          : 995\n",
      "    loss           : -289226.5209375\n",
      "    val_loss       : -289270.984765625\n",
      "Train Epoch: 996 [512/54000 (1%)] Loss: -365448.906250\n",
      "Train Epoch: 996 [11776/54000 (22%)] Loss: -381593.125000\n",
      "Train Epoch: 996 [23040/54000 (43%)] Loss: -363481.250000\n",
      "Train Epoch: 996 [34304/54000 (64%)] Loss: -262709.906250\n",
      "Train Epoch: 996 [45568/54000 (84%)] Loss: -360531.500000\n",
      "    epoch          : 996\n",
      "    loss           : -289121.835625\n",
      "    val_loss       : -290024.60078125\n",
      "Train Epoch: 997 [512/54000 (1%)] Loss: -256964.671875\n",
      "Train Epoch: 997 [11776/54000 (22%)] Loss: -174784.078125\n",
      "Train Epoch: 997 [23040/54000 (43%)] Loss: -258677.406250\n",
      "Train Epoch: 997 [34304/54000 (64%)] Loss: -258137.265625\n",
      "Train Epoch: 997 [45568/54000 (84%)] Loss: -269870.781250\n",
      "    epoch          : 997\n",
      "    loss           : -289249.69546875\n",
      "    val_loss       : -290370.958203125\n",
      "Train Epoch: 998 [512/54000 (1%)] Loss: -263362.687500\n",
      "Train Epoch: 998 [11776/54000 (22%)] Loss: -358479.031250\n",
      "Train Epoch: 998 [23040/54000 (43%)] Loss: -338639.562500\n",
      "Train Epoch: 998 [34304/54000 (64%)] Loss: -366130.437500\n",
      "Train Epoch: 998 [45568/54000 (84%)] Loss: -340339.218750\n",
      "    epoch          : 998\n",
      "    loss           : -289275.188125\n",
      "    val_loss       : -290243.554296875\n",
      "Train Epoch: 999 [512/54000 (1%)] Loss: -366586.250000\n",
      "Train Epoch: 999 [11776/54000 (22%)] Loss: -266215.656250\n",
      "Train Epoch: 999 [23040/54000 (43%)] Loss: -269700.968750\n",
      "Train Epoch: 999 [34304/54000 (64%)] Loss: -234258.031250\n",
      "Train Epoch: 999 [45568/54000 (84%)] Loss: -225169.156250\n",
      "    epoch          : 999\n",
      "    loss           : -289145.92765625\n",
      "    val_loss       : -291000.45078125\n",
      "Train Epoch: 1000 [512/54000 (1%)] Loss: -260770.734375\n",
      "Train Epoch: 1000 [11776/54000 (22%)] Loss: -259640.906250\n",
      "Train Epoch: 1000 [23040/54000 (43%)] Loss: -345707.250000\n",
      "Train Epoch: 1000 [34304/54000 (64%)] Loss: -360736.281250\n",
      "Train Epoch: 1000 [45568/54000 (84%)] Loss: -339607.812500\n",
      "    epoch          : 1000\n",
      "    loss           : -289446.1925\n",
      "    val_loss       : -290315.357421875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0714_235821/checkpoint-epoch1000.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAECategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=136, bias=True)\n",
       "        (1): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=136, out_features=136, bias=True)\n",
       "        (4): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=136, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=136, bias=True)\n",
       "        (1): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=136, out_features=136, bias=True)\n",
       "        (4): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=136, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=264, bias=True)\n",
       "        (1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=264, out_features=264, bias=True)\n",
       "        (4): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=264, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=264, bias=True)\n",
       "        (1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=264, out_features=264, bias=True)\n",
       "        (4): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=264, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=400, bias=True)\n",
       "        (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (4): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=400, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_5_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "        (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (4): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=400, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=144, bias=True)\n",
       "        (1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=144, out_features=144, bias=True)\n",
       "        (4): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=144, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=144, bias=True)\n",
       "        (1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=144, out_features=144, bias=True)\n",
       "        (4): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=144, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=272, bias=True)\n",
       "        (1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=272, out_features=272, bias=True)\n",
       "        (4): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=272, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=272, bias=True)\n",
       "        (1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=272, out_features=272, bias=True)\n",
       "        (4): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=272, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_10): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=408, bias=True)\n",
       "        (1): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=408, out_features=408, bias=True)\n",
       "        (4): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=408, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_10_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=408, bias=True)\n",
       "        (1): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=408, out_features=408, bias=True)\n",
       "        (4): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=408, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_11): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_11_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_12): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_12_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_13): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_13_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_14): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_14_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_15): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_15_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_16): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_16_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_17): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_17_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_18): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_18_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_19): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_19_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_20): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_20_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "    (global_element_6): StandardNormal()\n",
       "  )\n",
       "  (guide_embedding): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (guide_confidences): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_distances): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=28, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
