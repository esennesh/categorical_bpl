{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 41793.312500\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -291610.437500\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -196678.796875\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -241876.046875\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -291129.531250\n",
      "    epoch          : 1\n",
      "    loss           : -257322.98510742188\n",
      "    val_loss       : -368796.734765625\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -487543.062500\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -449956.906250\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -557099.125000\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -577167.812500\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -346062.406250\n",
      "    epoch          : 2\n",
      "    loss           : -407873.31265625\n",
      "    val_loss       : -444460.4223388672\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -558575.437500\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -631338.812500\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -398025.906250\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -416127.406250\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -393821.437500\n",
      "    epoch          : 3\n",
      "    loss           : -438667.44875\n",
      "    val_loss       : -454145.37526855466\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -560162.500000\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -551695.750000\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -347904.937500\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -446116.000000\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -604308.375000\n",
      "    epoch          : 4\n",
      "    loss           : -460443.519375\n",
      "    val_loss       : -469022.0328613281\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -677080.312500\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -642556.625000\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -462071.312500\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -649536.250000\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -402433.437500\n",
      "    epoch          : 5\n",
      "    loss           : -477276.76875\n",
      "    val_loss       : -497221.2759277344\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -391466.031250\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -315665.406250\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -707485.625000\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -602305.562500\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -356279.625000\n",
      "    epoch          : 6\n",
      "    loss           : -501231.296875\n",
      "    val_loss       : -510795.06755371095\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -400670.375000\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -430393.375000\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -439047.875000\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -518651.125000\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -628796.187500\n",
      "    epoch          : 7\n",
      "    loss           : -509276.553125\n",
      "    val_loss       : -523254.1518798828\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -407632.718750\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -449169.218750\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -686250.625000\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -625737.000000\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -447970.750000\n",
      "    epoch          : 8\n",
      "    loss           : -519966.1359375\n",
      "    val_loss       : -530928.5789794922\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -639190.625000\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -691422.875000\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -373143.187500\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -401482.593750\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -546177.625000\n",
      "    epoch          : 9\n",
      "    loss           : -533389.6034375\n",
      "    val_loss       : -539457.0836181641\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -657768.937500\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -656670.625000\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -593861.500000\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -615870.062500\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -373695.062500\n",
      "    epoch          : 10\n",
      "    loss           : -529834.4478125\n",
      "    val_loss       : -527933.6436279297\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -401372.625000\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -609606.625000\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -374513.156250\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -601679.375000\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -666732.062500\n",
      "    epoch          : 11\n",
      "    loss           : -522652.0165625\n",
      "    val_loss       : -545303.8484375\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -666771.687500\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -726391.000000\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -580120.750000\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -668590.937500\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -646496.000000\n",
      "    epoch          : 12\n",
      "    loss           : -547106.45875\n",
      "    val_loss       : -546069.3131347656\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -618036.250000\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -409038.531250\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -736652.375000\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -486957.437500\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -581335.750000\n",
      "    epoch          : 13\n",
      "    loss           : -552604.5140625\n",
      "    val_loss       : -563885.9035888672\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -634564.875000\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -491463.343750\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -424916.000000\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -481681.156250\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -617664.312500\n",
      "    epoch          : 14\n",
      "    loss           : -556144.3090625\n",
      "    val_loss       : -549440.2256835938\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -468192.906250\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -482656.031250\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -682043.125000\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -493888.812500\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -424710.625000\n",
      "    epoch          : 15\n",
      "    loss           : -556217.8271875\n",
      "    val_loss       : -565947.6473144531\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -437744.968750\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -438994.406250\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -462508.218750\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -488706.687500\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -673290.125000\n",
      "    epoch          : 16\n",
      "    loss           : -553228.7059375\n",
      "    val_loss       : -554729.7821044922\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -732199.625000\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -601974.625000\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -477326.500000\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -588755.375000\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -498583.875000\n",
      "    epoch          : 17\n",
      "    loss           : -557707.065\n",
      "    val_loss       : -570274.2236083985\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -641762.000000\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -746477.562500\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -424910.625000\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -418311.750000\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -583509.187500\n",
      "    epoch          : 18\n",
      "    loss           : -557603.276875\n",
      "    val_loss       : -567742.0180664062\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -675431.125000\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -443374.437500\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -641769.437500\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -584322.250000\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -591700.312500\n",
      "    epoch          : 19\n",
      "    loss           : -562957.1765625\n",
      "    val_loss       : -563155.2409912109\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -433696.000000\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -745969.500000\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -491513.656250\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -684896.875000\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -440254.406250\n",
      "    epoch          : 20\n",
      "    loss           : -565994.1015625\n",
      "    val_loss       : -574036.633227539\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -739968.625000\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -674271.875000\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -442242.531250\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -678297.187500\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -551685.250000\n",
      "    epoch          : 21\n",
      "    loss           : -563478.558125\n",
      "    val_loss       : -568685.3139892578\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -723893.375000\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -446117.031250\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -433721.406250\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -439456.718750\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -588326.375000\n",
      "    epoch          : 22\n",
      "    loss           : -567321.2259375\n",
      "    val_loss       : -573380.1105957031\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -701711.625000\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -691956.375000\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -495052.125000\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -504073.281250\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -488368.781250\n",
      "    epoch          : 23\n",
      "    loss           : -572374.6715625\n",
      "    val_loss       : -565092.4470947266\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -430562.718750\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -485498.375000\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -646425.000000\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -503768.250000\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -605775.625000\n",
      "    epoch          : 24\n",
      "    loss           : -572492.2059375\n",
      "    val_loss       : -573711.0577392578\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -596686.625000\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -455843.093750\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -415338.062500\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -691334.250000\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -496777.750000\n",
      "    epoch          : 25\n",
      "    loss           : -567134.079375\n",
      "    val_loss       : -580691.8572509766\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -730991.250000\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -462883.312500\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -749709.062500\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -695943.875000\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -688711.750000\n",
      "    epoch          : 26\n",
      "    loss           : -582449.1746875\n",
      "    val_loss       : -584750.5999023437\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -466686.875000\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -492357.625000\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -435540.218750\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -485335.281250\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -428458.000000\n",
      "    epoch          : 27\n",
      "    loss           : -573226.3865625\n",
      "    val_loss       : -557941.2252197266\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -406389.343750\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -497968.625000\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -452818.312500\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -458541.812500\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -455129.812500\n",
      "    epoch          : 28\n",
      "    loss           : -579120.0821875\n",
      "    val_loss       : -587654.091772461\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -752545.250000\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -752754.250000\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -463616.000000\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -516025.250000\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -593830.062500\n",
      "    epoch          : 29\n",
      "    loss           : -582108.7328125\n",
      "    val_loss       : -567067.1576660157\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -703527.312500\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -483837.625000\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -745833.000000\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -438650.343750\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -588178.500000\n",
      "    epoch          : 30\n",
      "    loss           : -567214.5415625\n",
      "    val_loss       : -582454.8525878906\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -454831.281250\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -748279.437500\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -463052.437500\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -658287.562500\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -700594.375000\n",
      "    epoch          : 31\n",
      "    loss           : -583668.6925\n",
      "    val_loss       : -586920.2161132812\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -649958.312500\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -460765.125000\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -630248.500000\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -511769.718750\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -734484.062500\n",
      "    epoch          : 32\n",
      "    loss           : -577844.455\n",
      "    val_loss       : -590682.8279052734\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -461833.062500\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -463836.812500\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -736348.875000\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -463075.093750\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -502637.062500\n",
      "    epoch          : 33\n",
      "    loss           : -578924.8871875\n",
      "    val_loss       : -573544.0201171875\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -689652.375000\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -451501.437500\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -660909.125000\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -653014.000000\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -593264.687500\n",
      "    epoch          : 34\n",
      "    loss           : -581006.874375\n",
      "    val_loss       : -588419.0009521485\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -707275.250000\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -464584.375000\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -664195.875000\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -662056.375000\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -615848.812500\n",
      "    epoch          : 35\n",
      "    loss           : -589592.9296875\n",
      "    val_loss       : -590769.3041015625\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -754217.437500\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -729453.812500\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -499462.343750\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -746076.562500\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -458649.000000\n",
      "    epoch          : 36\n",
      "    loss           : -586769.611875\n",
      "    val_loss       : -594839.5317626953\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -521265.750000\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -705797.750000\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -644307.125000\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -419644.531250\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -497616.843750\n",
      "    epoch          : 37\n",
      "    loss           : -574442.29875\n",
      "    val_loss       : -582444.2036865235\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -727390.250000\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -693765.125000\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -748080.625000\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -525639.875000\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -619429.437500\n",
      "    epoch          : 38\n",
      "    loss           : -584007.75375\n",
      "    val_loss       : -593620.488671875\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -615789.750000\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -749611.937500\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -461605.031250\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -667104.000000\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -709661.812500\n",
      "    epoch          : 39\n",
      "    loss           : -582722.6665625\n",
      "    val_loss       : -592288.347265625\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -468539.406250\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -742516.000000\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -614780.250000\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -517685.812500\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -528380.250000\n",
      "    epoch          : 40\n",
      "    loss           : -594690.9759375\n",
      "    val_loss       : -598091.92265625\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -470948.281250\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -472211.187500\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -662568.875000\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -697846.812500\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -650237.437500\n",
      "    epoch          : 41\n",
      "    loss           : -589750.43125\n",
      "    val_loss       : -593975.4929443359\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -476692.750000\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -523956.500000\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -668675.937500\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -522304.125000\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -521147.500000\n",
      "    epoch          : 42\n",
      "    loss           : -595980.95875\n",
      "    val_loss       : -591821.8442138672\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -515483.750000\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -663732.500000\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -608682.125000\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -461880.093750\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -471594.500000\n",
      "    epoch          : 43\n",
      "    loss           : -594652.874375\n",
      "    val_loss       : -589469.9594482422\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -465648.687500\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -452596.750000\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -758507.625000\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -615953.875000\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -708684.250000\n",
      "    epoch          : 44\n",
      "    loss           : -590875.55875\n",
      "    val_loss       : -593812.9796386719\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -516556.031250\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -753887.500000\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -472282.156250\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -480012.625000\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -526895.250000\n",
      "    epoch          : 45\n",
      "    loss           : -593899.1496875\n",
      "    val_loss       : -598721.9227050781\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -672562.187500\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -521539.375000\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -475269.812500\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -668865.875000\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -461879.968750\n",
      "    epoch          : 46\n",
      "    loss           : -596318.820625\n",
      "    val_loss       : -599099.8230224609\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -759248.750000\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -753264.125000\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -516846.000000\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -518492.968750\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -525965.750000\n",
      "    epoch          : 47\n",
      "    loss           : -594137.7159375\n",
      "    val_loss       : -601791.5032470704\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -674323.687500\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -713132.750000\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -753097.437500\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -458502.500000\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -463778.250000\n",
      "    epoch          : 48\n",
      "    loss           : -589245.2659375\n",
      "    val_loss       : -590114.5558349609\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -753357.125000\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -709633.937500\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -519807.687500\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -764861.125000\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -678834.750000\n",
      "    epoch          : 49\n",
      "    loss           : -598817.281875\n",
      "    val_loss       : -602343.3256591797\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -763984.375000\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -758017.000000\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -704965.250000\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -720587.187500\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -671617.375000\n",
      "    epoch          : 50\n",
      "    loss           : -595296.654375\n",
      "    val_loss       : -602298.9693359375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0805_171053/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -748977.312500\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -756368.375000\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -522222.593750\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -523332.031250\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -621528.687500\n",
      "    epoch          : 51\n",
      "    loss           : -600734.1575\n",
      "    val_loss       : -599730.9053955078\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -471941.812500\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -759974.125000\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -474877.937500\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -522185.875000\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -529706.125000\n",
      "    epoch          : 52\n",
      "    loss           : -600756.4590625\n",
      "    val_loss       : -600902.8110351562\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -755983.000000\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -475509.375000\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -680468.562500\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -672339.687500\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -765554.187500\n",
      "    epoch          : 53\n",
      "    loss           : -600291.174375\n",
      "    val_loss       : -601927.8639404296\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -476272.000000\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -618003.000000\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -498670.312500\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -457748.687500\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -670729.750000\n",
      "    epoch          : 54\n",
      "    loss           : -596417.8434375\n",
      "    val_loss       : -603278.232006836\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -528311.750000\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -758510.500000\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -622896.500000\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -719117.000000\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -721517.375000\n",
      "    epoch          : 55\n",
      "    loss           : -601874.8225\n",
      "    val_loss       : -606468.6758300781\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -537600.500000\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -763320.250000\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -684601.500000\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -686897.062500\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -465246.093750\n",
      "    epoch          : 56\n",
      "    loss           : -604387.2184375\n",
      "    val_loss       : -606215.6625976562\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -486666.437500\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -474744.125000\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -762183.250000\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -673011.125000\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -631845.250000\n",
      "    epoch          : 57\n",
      "    loss           : -596812.30625\n",
      "    val_loss       : -579897.1141357422\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -708876.125000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -744243.750000\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -618528.875000\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -461719.281250\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -622140.875000\n",
      "    epoch          : 58\n",
      "    loss           : -596455.2965625\n",
      "    val_loss       : -608523.6594482422\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -480584.187500\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -477824.593750\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -766889.250000\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -508314.562500\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -681306.125000\n",
      "    epoch          : 59\n",
      "    loss           : -603941.1096875\n",
      "    val_loss       : -605813.2104492188\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -527106.937500\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -478256.968750\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -622246.125000\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -477084.593750\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -728598.312500\n",
      "    epoch          : 60\n",
      "    loss           : -600951.413125\n",
      "    val_loss       : -597564.9150146485\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -618573.000000\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -471487.156250\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -529965.375000\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -764955.875000\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -677887.250000\n",
      "    epoch          : 61\n",
      "    loss           : -600463.3834375\n",
      "    val_loss       : -607813.3724365234\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -539682.125000\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -680744.875000\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -679521.687500\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -762609.250000\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -715161.562500\n",
      "    epoch          : 62\n",
      "    loss           : -601067.961875\n",
      "    val_loss       : -606441.6097412109\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -464672.187500\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -536964.062500\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -632662.625000\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -474511.281250\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -480585.843750\n",
      "    epoch          : 63\n",
      "    loss           : -607086.09375\n",
      "    val_loss       : -609829.104296875\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -764537.500000\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -532984.812500\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -528560.875000\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -755151.937500\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -434627.437500\n",
      "    epoch          : 64\n",
      "    loss           : -602867.4015625\n",
      "    val_loss       : -599629.8077636719\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -512774.968750\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -524748.750000\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -762808.500000\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -484715.062500\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -731853.750000\n",
      "    epoch          : 65\n",
      "    loss           : -601061.836875\n",
      "    val_loss       : -605243.4703125\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -483014.250000\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -474163.156250\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -756996.750000\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -513718.968750\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -480219.437500\n",
      "    epoch          : 66\n",
      "    loss           : -604773.85375\n",
      "    val_loss       : -610084.8246826172\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -763365.250000\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -617707.250000\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -767517.500000\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -761375.625000\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -471154.125000\n",
      "    epoch          : 67\n",
      "    loss           : -606523.170625\n",
      "    val_loss       : -610959.876538086\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -487726.281250\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -526380.375000\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -769060.437500\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -628560.312500\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -722454.062500\n",
      "    epoch          : 68\n",
      "    loss           : -606455.0309375\n",
      "    val_loss       : -597897.422290039\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -520706.750000\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -533444.250000\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -483912.187500\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -757000.500000\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -676528.375000\n",
      "    epoch          : 69\n",
      "    loss           : -598840.995625\n",
      "    val_loss       : -607973.0805175782\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -534498.625000\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -764027.187500\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -727421.312500\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -759910.812500\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -474357.000000\n",
      "    epoch          : 70\n",
      "    loss           : -609734.076875\n",
      "    val_loss       : -603275.5934326171\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -524283.531250\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -762907.375000\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -672375.125000\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -626841.562500\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -476201.812500\n",
      "    epoch          : 71\n",
      "    loss           : -606526.10125\n",
      "    val_loss       : -602393.8776855469\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -474523.093750\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -764477.750000\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -530820.562500\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -687761.062500\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -687170.937500\n",
      "    epoch          : 72\n",
      "    loss           : -605709.186875\n",
      "    val_loss       : -610659.223071289\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -537263.812500\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -541714.625000\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -534131.437500\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -680698.125000\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -455626.281250\n",
      "    epoch          : 73\n",
      "    loss           : -607900.1865625\n",
      "    val_loss       : -611256.6775878906\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -475081.937500\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -537014.000000\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -474501.250000\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -521616.843750\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -722243.875000\n",
      "    epoch          : 74\n",
      "    loss           : -603166.463125\n",
      "    val_loss       : -609577.7689208984\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -482699.843750\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -680928.312500\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -539243.812500\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -635248.250000\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -735445.125000\n",
      "    epoch          : 75\n",
      "    loss           : -610561.7334375\n",
      "    val_loss       : -613285.6020996093\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -534368.187500\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -768616.312500\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -632778.187500\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -770336.750000\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -759249.000000\n",
      "    epoch          : 76\n",
      "    loss           : -609726.6725\n",
      "    val_loss       : -612402.1594482422\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -691267.437500\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -489922.937500\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -528670.875000\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -731247.437500\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -522050.343750\n",
      "    epoch          : 77\n",
      "    loss           : -605421.121875\n",
      "    val_loss       : -610051.1187011718\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -686590.312500\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -535923.562500\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -478691.843750\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -628058.625000\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -756284.312500\n",
      "    epoch          : 78\n",
      "    loss           : -606941.6759375\n",
      "    val_loss       : -607962.1553955078\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -691661.500000\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -478378.781250\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -479634.625000\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -767481.125000\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -734093.500000\n",
      "    epoch          : 79\n",
      "    loss           : -606776.1790625\n",
      "    val_loss       : -606822.1755126953\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -470839.875000\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -533177.000000\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -488211.437500\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -622479.687500\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -718562.625000\n",
      "    epoch          : 80\n",
      "    loss           : -603112.889375\n",
      "    val_loss       : -605356.1335205078\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -685868.500000\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -768744.500000\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -689025.375000\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -637552.500000\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -727101.187500\n",
      "    epoch          : 81\n",
      "    loss           : -610016.044375\n",
      "    val_loss       : -611615.0831787109\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -764707.937500\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -688782.687500\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -478701.750000\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -692226.125000\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -475787.250000\n",
      "    epoch          : 82\n",
      "    loss           : -612904.095625\n",
      "    val_loss       : -615268.7051513672\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -531666.812500\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -530698.375000\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -635146.312500\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -503305.875000\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -688413.937500\n",
      "    epoch          : 83\n",
      "    loss           : -614135.994375\n",
      "    val_loss       : -617210.5723388672\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -773552.937500\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -498484.843750\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -683518.562500\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -488076.156250\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -520179.687500\n",
      "    epoch          : 84\n",
      "    loss           : -613090.6621875\n",
      "    val_loss       : -589675.5240478516\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -661645.125000\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -685746.625000\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -683261.437500\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -623987.062500\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -479279.000000\n",
      "    epoch          : 85\n",
      "    loss           : -602584.2384375\n",
      "    val_loss       : -615600.77109375\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -773223.250000\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -496148.437500\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -484490.125000\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -494232.562500\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -729563.375000\n",
      "    epoch          : 86\n",
      "    loss           : -615449.0265625\n",
      "    val_loss       : -616964.1680664063\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -770449.625000\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -637995.562500\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -488878.750000\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -492536.093750\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -478606.687500\n",
      "    epoch          : 87\n",
      "    loss           : -615519.4653125\n",
      "    val_loss       : -614908.3275390625\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -482092.687500\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -768897.875000\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -686745.625000\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -729988.375000\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -626605.625000\n",
      "    epoch          : 88\n",
      "    loss           : -610059.4875\n",
      "    val_loss       : -612015.5865234375\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -529357.125000\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -685353.250000\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -689258.750000\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -491942.718750\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -637187.625000\n",
      "    epoch          : 89\n",
      "    loss           : -614407.6471875\n",
      "    val_loss       : -618434.5510009766\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -492566.656250\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -689779.500000\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -547697.000000\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -771241.125000\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -690249.750000\n",
      "    epoch          : 90\n",
      "    loss           : -613603.1646875\n",
      "    val_loss       : -604658.1984619141\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -756367.062500\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -549315.875000\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -537586.937500\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -537569.875000\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -695661.437500\n",
      "    epoch          : 91\n",
      "    loss           : -614815.113125\n",
      "    val_loss       : -618345.7552978515\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -775744.437500\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -540688.500000\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -767932.687500\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -536114.187500\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -627829.187500\n",
      "    epoch          : 92\n",
      "    loss           : -613526.325625\n",
      "    val_loss       : -614227.0315185547\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -683210.750000\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -491655.218750\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -497166.687500\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -633270.562500\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -473321.906250\n",
      "    epoch          : 93\n",
      "    loss           : -612931.368125\n",
      "    val_loss       : -613231.0126708985\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -759310.375000\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -773361.187500\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -766145.187500\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -685088.375000\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -639335.875000\n",
      "    epoch          : 94\n",
      "    loss           : -610192.3503125\n",
      "    val_loss       : -614212.0577392578\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -639537.875000\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -485696.718750\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -492377.437500\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -695694.937500\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -642852.062500\n",
      "    epoch          : 95\n",
      "    loss           : -613290.3871875\n",
      "    val_loss       : -617826.3382568359\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -482677.500000\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -774586.500000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -771638.500000\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -496064.812500\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -489964.750000\n",
      "    epoch          : 96\n",
      "    loss           : -617631.6378125\n",
      "    val_loss       : -620230.3337646484\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -693062.250000\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -541505.687500\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -466671.687500\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -740015.750000\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -445844.750000\n",
      "    epoch          : 97\n",
      "    loss           : -607408.9584375\n",
      "    val_loss       : -606092.0190917968\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -543488.812500\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -541815.625000\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -485241.343750\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -763298.250000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -482508.406250\n",
      "    epoch          : 98\n",
      "    loss           : -612330.743125\n",
      "    val_loss       : -615934.9515136719\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -477353.875000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -694060.625000\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -690837.812500\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -482829.750000\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -481076.375000\n",
      "    epoch          : 99\n",
      "    loss           : -609093.155\n",
      "    val_loss       : -612755.2132324219\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -764827.062500\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -637731.125000\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -492958.718750\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -499871.375000\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -741253.500000\n",
      "    epoch          : 100\n",
      "    loss           : -617200.28125\n",
      "    val_loss       : -619604.4912109375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0805_171053/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -546754.875000\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -535941.250000\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -738328.125000\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -777447.687500\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -491669.156250\n",
      "    epoch          : 101\n",
      "    loss           : -619540.070625\n",
      "    val_loss       : -622333.1246337891\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -502048.312500\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -550285.000000\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -769723.187500\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -490272.062500\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -500389.406250\n",
      "    epoch          : 102\n",
      "    loss           : -611172.45375\n",
      "    val_loss       : -592865.1370849609\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -747013.687500\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -712451.000000\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -523303.312500\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -633667.125000\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -726229.125000\n",
      "    epoch          : 103\n",
      "    loss           : -604321.0990625\n",
      "    val_loss       : -616178.9554443359\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -491892.812500\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -549570.250000\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -690239.125000\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -548515.687500\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -484191.218750\n",
      "    epoch          : 104\n",
      "    loss           : -616435.0046875\n",
      "    val_loss       : -617573.1825927735\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -731996.562500\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -775356.437500\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -768819.687500\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -488911.500000\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -693223.187500\n",
      "    epoch          : 105\n",
      "    loss           : -619313.7653125\n",
      "    val_loss       : -621275.8869140625\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -538222.812500\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -776915.500000\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -466945.250000\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -487618.750000\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -736964.000000\n",
      "    epoch          : 106\n",
      "    loss           : -617046.2503125\n",
      "    val_loss       : -621696.7165771484\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -485052.281250\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -493902.312500\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -739659.500000\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -743351.875000\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -642142.187500\n",
      "    epoch          : 107\n",
      "    loss           : -617782.265625\n",
      "    val_loss       : -622492.4233398438\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -642443.250000\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -644918.750000\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -486920.250000\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -484188.750000\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -491395.500000\n",
      "    epoch          : 108\n",
      "    loss           : -617291.835\n",
      "    val_loss       : -612448.8788574219\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -534166.750000\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -489079.250000\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -497768.531250\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -689022.000000\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -534241.375000\n",
      "    epoch          : 109\n",
      "    loss           : -610937.2971875\n",
      "    val_loss       : -618624.2451660156\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -690502.687500\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -489215.312500\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -547714.500000\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -695389.625000\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -731346.000000\n",
      "    epoch          : 110\n",
      "    loss           : -618247.8796875\n",
      "    val_loss       : -620126.0549804687\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -542534.125000\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -633280.562500\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -543406.937500\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -775473.687500\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -770746.125000\n",
      "    epoch          : 111\n",
      "    loss           : -615516.89\n",
      "    val_loss       : -618908.1965332031\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -682821.937500\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -545748.437500\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -732888.250000\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -738928.500000\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -493811.156250\n",
      "    epoch          : 112\n",
      "    loss           : -616973.6534375\n",
      "    val_loss       : -619058.9914550781\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -487171.718750\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -772487.687500\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -489526.937500\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -688300.125000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -738358.812500\n",
      "    epoch          : 113\n",
      "    loss           : -618929.8959375\n",
      "    val_loss       : -622074.0396972656\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -769971.312500\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -777128.062500\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -502942.906250\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -491623.406250\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -491211.968750\n",
      "    epoch          : 114\n",
      "    loss           : -620418.590625\n",
      "    val_loss       : -621766.4237792969\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -547414.750000\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -741678.625000\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -649682.812500\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -631847.187500\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -470448.343750\n",
      "    epoch          : 115\n",
      "    loss           : -616347.3625\n",
      "    val_loss       : -613738.8479248047\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -482415.937500\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -679320.250000\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -492459.093750\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -481559.125000\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -497659.625000\n",
      "    epoch          : 116\n",
      "    loss           : -613261.4334375\n",
      "    val_loss       : -622690.3978271484\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -496681.281250\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -733115.375000\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -775492.312500\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -496312.375000\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -637198.562500\n",
      "    epoch          : 117\n",
      "    loss           : -618364.9596875\n",
      "    val_loss       : -621560.6879638672\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -692303.250000\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -495521.937500\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -773645.750000\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -773245.437500\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -484307.062500\n",
      "    epoch          : 118\n",
      "    loss           : -618838.3353125\n",
      "    val_loss       : -620542.9640136719\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -535727.437500\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -505532.437500\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -542909.875000\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -645345.500000\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -483824.812500\n",
      "    epoch          : 119\n",
      "    loss           : -619971.4703125\n",
      "    val_loss       : -613071.1600830078\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -482417.500000\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -492457.562500\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -771708.125000\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -535491.812500\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -738601.562500\n",
      "    epoch          : 120\n",
      "    loss           : -617755.2546875\n",
      "    val_loss       : -619662.7984375\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -763650.000000\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -778548.562500\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -644830.500000\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -643923.125000\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -653603.500000\n",
      "    epoch          : 121\n",
      "    loss           : -615680.2\n",
      "    val_loss       : -615290.6745605469\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -767136.625000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -767018.187500\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -768148.937500\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -750061.625000\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -734588.250000\n",
      "    epoch          : 122\n",
      "    loss           : -612920.44125\n",
      "    val_loss       : -618800.5845947266\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -766761.125000\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -775129.000000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -549157.187500\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -538313.250000\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -692419.250000\n",
      "    epoch          : 123\n",
      "    loss           : -620659.1084375\n",
      "    val_loss       : -622549.9383789062\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -648679.562500\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -773155.062500\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -776396.125000\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -503188.031250\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -635181.500000\n",
      "    epoch          : 124\n",
      "    loss           : -619586.949375\n",
      "    val_loss       : -611543.7237304688\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -776435.250000\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -480302.343750\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -500532.750000\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -542042.437500\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -482957.937500\n",
      "    epoch          : 125\n",
      "    loss           : -615205.2440625\n",
      "    val_loss       : -615673.8022705078\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -479520.937500\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -548029.250000\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -484287.781250\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -490421.125000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -496831.156250\n",
      "    epoch          : 126\n",
      "    loss           : -618337.88875\n",
      "    val_loss       : -621802.6700683594\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -546154.250000\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -698189.312500\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -743195.875000\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -739920.375000\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -702978.750000\n",
      "    epoch          : 127\n",
      "    loss           : -621944.223125\n",
      "    val_loss       : -624623.6022460938\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -492145.281250\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -512068.437500\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -685547.187500\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -536206.687500\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -484439.312500\n",
      "    epoch          : 128\n",
      "    loss           : -613930.000625\n",
      "    val_loss       : -621690.1728027344\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -542354.375000\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -540632.687500\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -492872.687500\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -501226.187500\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -488812.562500\n",
      "    epoch          : 129\n",
      "    loss           : -620744.6346875\n",
      "    val_loss       : -623209.4650634766\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -542289.750000\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -652571.500000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -643061.312500\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -489902.687500\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -687756.125000\n",
      "    epoch          : 130\n",
      "    loss           : -621896.868125\n",
      "    val_loss       : -624894.1299316406\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -697787.500000\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -509385.906250\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -500638.875000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -552777.250000\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -498313.062500\n",
      "    epoch          : 131\n",
      "    loss           : -623673.71625\n",
      "    val_loss       : -625552.2137695312\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -547918.125000\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -498561.031250\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -778945.000000\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -544353.437500\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -639159.187500\n",
      "    epoch          : 132\n",
      "    loss           : -617032.574375\n",
      "    val_loss       : -619786.0061767579\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -546971.375000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -642114.375000\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -547034.250000\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -496573.750000\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -488054.937500\n",
      "    epoch          : 133\n",
      "    loss           : -620623.475\n",
      "    val_loss       : -619059.2504882812\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -491253.593750\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -551383.687500\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -740006.875000\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -544643.437500\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -638188.250000\n",
      "    epoch          : 134\n",
      "    loss           : -613549.661875\n",
      "    val_loss       : -622258.4664306641\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -778234.125000\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -545781.125000\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -691988.125000\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -636318.312500\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -726541.375000\n",
      "    epoch          : 135\n",
      "    loss           : -613649.8078125\n",
      "    val_loss       : -621559.8489501954\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -507936.156250\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -548540.437500\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -493098.312500\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -491343.812500\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -776821.750000\n",
      "    epoch          : 136\n",
      "    loss           : -618864.085\n",
      "    val_loss       : -624310.519116211\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -749087.812500\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -495521.406250\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -550549.125000\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -641010.750000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -645329.812500\n",
      "    epoch          : 137\n",
      "    loss           : -623348.9665625\n",
      "    val_loss       : -624453.6337890625\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -499569.250000\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -742585.437500\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -551691.125000\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -501434.281250\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -785476.500000\n",
      "    epoch          : 138\n",
      "    loss           : -620280.8684375\n",
      "    val_loss       : -623091.1375488281\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -489442.000000\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -667873.125000\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -746975.500000\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -730624.687500\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -695565.437500\n",
      "    epoch          : 139\n",
      "    loss           : -614436.009375\n",
      "    val_loss       : -620645.5850585938\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -775721.687500\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -479532.656250\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -545734.187500\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -548435.875000\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -483400.718750\n",
      "    epoch          : 140\n",
      "    loss           : -620436.8165625\n",
      "    val_loss       : -620401.9247558594\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -539774.875000\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -556680.687500\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -532162.875000\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -736812.375000\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -486307.125000\n",
      "    epoch          : 141\n",
      "    loss           : -609476.6590625\n",
      "    val_loss       : -619777.8684326172\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -487432.906250\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -492882.968750\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -550215.937500\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -543527.625000\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -494753.062500\n",
      "    epoch          : 142\n",
      "    loss           : -622648.83\n",
      "    val_loss       : -626022.1360351562\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -776524.375000\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -541199.000000\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -699607.250000\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -741519.437500\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -654039.000000\n",
      "    epoch          : 143\n",
      "    loss           : -624569.1296875\n",
      "    val_loss       : -624550.0374755859\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -507479.500000\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -503957.312500\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -494995.937500\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -778767.812500\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -466061.687500\n",
      "    epoch          : 144\n",
      "    loss           : -621699.83875\n",
      "    val_loss       : -612127.4576660156\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -673107.812500\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -751563.687500\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -772457.812500\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -543511.687500\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -645147.437500\n",
      "    epoch          : 145\n",
      "    loss           : -620028.885625\n",
      "    val_loss       : -622373.4998046875\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -547141.375000\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -642449.750000\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -642740.125000\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -497751.750000\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -493080.125000\n",
      "    epoch          : 146\n",
      "    loss           : -622387.3834375\n",
      "    val_loss       : -626666.3466064453\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -778760.750000\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -552262.500000\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -702581.812500\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -695456.750000\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -551650.000000\n",
      "    epoch          : 147\n",
      "    loss           : -625598.1809375\n",
      "    val_loss       : -628437.2326171875\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -700205.750000\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -774989.937500\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -696855.125000\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -508467.406250\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -648515.250000\n",
      "    epoch          : 148\n",
      "    loss           : -625746.0846875\n",
      "    val_loss       : -627291.1159423828\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -506475.593750\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -502111.750000\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -546750.062500\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -500147.812500\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -545473.187500\n",
      "    epoch          : 149\n",
      "    loss           : -626001.159375\n",
      "    val_loss       : -627073.0010009765\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -700000.750000\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -750381.812500\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -749538.250000\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -491050.156250\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -744500.312500\n",
      "    epoch          : 150\n",
      "    loss           : -624200.6753125\n",
      "    val_loss       : -612263.6083496094\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0805_171053/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -515438.250000\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -495888.000000\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -633458.187500\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -497219.375000\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -779160.750000\n",
      "    epoch          : 151\n",
      "    loss           : -616383.5303125\n",
      "    val_loss       : -622477.9364257812\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -507995.875000\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -503505.812500\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -695617.875000\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -485086.718750\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -654375.812500\n",
      "    epoch          : 152\n",
      "    loss           : -624715.6715625\n",
      "    val_loss       : -627728.2470947265\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -509829.312500\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -743029.062500\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -740928.625000\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -498589.218750\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -739604.625000\n",
      "    epoch          : 153\n",
      "    loss           : -621061.42\n",
      "    val_loss       : -608637.5819091797\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -491366.437500\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -544719.750000\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -772032.625000\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -779778.812500\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -747067.375000\n",
      "    epoch          : 154\n",
      "    loss           : -621322.8859375\n",
      "    val_loss       : -626291.7313720703\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -643738.437500\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -734846.375000\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -753063.500000\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -501808.000000\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -705113.187500\n",
      "    epoch          : 155\n",
      "    loss           : -624804.074375\n",
      "    val_loss       : -625789.3314697265\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -497587.500000\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -492641.000000\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -649516.062500\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -638441.312500\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -496408.812500\n",
      "    epoch          : 156\n",
      "    loss           : -619032.1290625\n",
      "    val_loss       : -623663.1709960938\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -775267.375000\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -780509.562500\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -537598.875000\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -479171.906250\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -689269.437500\n",
      "    epoch          : 157\n",
      "    loss           : -619718.3459375\n",
      "    val_loss       : -617547.8725830078\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -489737.687500\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -697872.437500\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -782043.875000\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -782515.250000\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -548912.500000\n",
      "    epoch          : 158\n",
      "    loss           : -623416.10625\n",
      "    val_loss       : -622729.9176513671\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -751682.625000\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -500882.656250\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -508421.500000\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -642113.000000\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -651885.937500\n",
      "    epoch          : 159\n",
      "    loss           : -624198.891875\n",
      "    val_loss       : -626343.5488769531\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -649843.125000\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -498348.312500\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -498660.437500\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -643631.750000\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -494051.875000\n",
      "    epoch          : 160\n",
      "    loss           : -625915.556875\n",
      "    val_loss       : -626582.4838134765\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -540384.750000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -500068.062500\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -504936.593750\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -539124.375000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -741943.250000\n",
      "    epoch          : 161\n",
      "    loss           : -623975.85125\n",
      "    val_loss       : -624088.7053222656\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -553707.750000\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -545034.625000\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -501144.875000\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -736213.812500\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -741427.875000\n",
      "    epoch          : 162\n",
      "    loss           : -617786.396875\n",
      "    val_loss       : -610917.2860351562\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -774517.437500\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -696672.312500\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -492100.218750\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -490472.187500\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -499291.937500\n",
      "    epoch          : 163\n",
      "    loss           : -622079.4159375\n",
      "    val_loss       : -627939.975024414\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -500766.968750\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -503903.062500\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -741832.250000\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -694211.937500\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -491641.250000\n",
      "    epoch          : 164\n",
      "    loss           : -624921.5121875\n",
      "    val_loss       : -621495.6633544922\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -774058.187500\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -490268.625000\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -543133.125000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -498745.687500\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -744912.687500\n",
      "    epoch          : 165\n",
      "    loss           : -622302.4296875\n",
      "    val_loss       : -625054.9990234375\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -650517.187500\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -500421.000000\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -554001.500000\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -541564.937500\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -638416.750000\n",
      "    epoch          : 166\n",
      "    loss           : -619981.006875\n",
      "    val_loss       : -622098.8615234375\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -504963.812500\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -654671.000000\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -489016.812500\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -780955.937500\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -743636.625000\n",
      "    epoch          : 167\n",
      "    loss           : -626107.5071875\n",
      "    val_loss       : -628624.120703125\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -772691.562500\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -482244.500000\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -776948.312500\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -787320.437500\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -745806.500000\n",
      "    epoch          : 168\n",
      "    loss           : -626198.1240625\n",
      "    val_loss       : -626989.7948730469\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -780728.937500\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -506089.937500\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -778355.000000\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -556247.250000\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -698715.125000\n",
      "    epoch          : 169\n",
      "    loss           : -622847.29875\n",
      "    val_loss       : -623553.0370361328\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -652300.625000\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -547199.500000\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -514871.000000\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -652385.750000\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -647765.250000\n",
      "    epoch          : 170\n",
      "    loss           : -625008.5153125\n",
      "    val_loss       : -626771.2472412109\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -545644.500000\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -550969.375000\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -544406.000000\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -701140.875000\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -642936.625000\n",
      "    epoch          : 171\n",
      "    loss           : -623917.4659375\n",
      "    val_loss       : -624338.6260986328\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -540652.500000\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -652538.187500\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -777215.125000\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -777137.875000\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -491716.156250\n",
      "    epoch          : 172\n",
      "    loss           : -626079.833125\n",
      "    val_loss       : -628689.1252929687\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -694179.062500\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -697508.375000\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -778522.937500\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -644522.937500\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -703828.625000\n",
      "    epoch          : 173\n",
      "    loss           : -625250.935\n",
      "    val_loss       : -628833.1174804687\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -780925.750000\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -505939.656250\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -650381.375000\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -782704.625000\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -483885.281250\n",
      "    epoch          : 174\n",
      "    loss           : -626541.3871875\n",
      "    val_loss       : -621044.4912841797\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -753459.750000\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -769444.562500\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -534896.562500\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -487135.687500\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -541113.312500\n",
      "    epoch          : 175\n",
      "    loss           : -618964.65125\n",
      "    val_loss       : -626779.2381347656\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -774924.500000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -776206.625000\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -653433.437500\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -704193.250000\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -535587.062500\n",
      "    epoch          : 176\n",
      "    loss           : -623882.305\n",
      "    val_loss       : -617327.5504882813\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -746703.437500\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -778633.375000\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -777118.250000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -749036.625000\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -488989.625000\n",
      "    epoch          : 177\n",
      "    loss           : -623300.8678125\n",
      "    val_loss       : -626953.5521240234\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -748915.375000\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -543154.875000\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -483933.562500\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -782097.625000\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -652021.125000\n",
      "    epoch          : 178\n",
      "    loss           : -624873.53875\n",
      "    val_loss       : -625703.2081298828\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -513672.187500\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -779197.250000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -780883.375000\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -742559.812500\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -497246.968750\n",
      "    epoch          : 179\n",
      "    loss           : -625199.5521875\n",
      "    val_loss       : -626550.5906005859\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -550072.750000\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -500804.125000\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -779595.875000\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -753253.937500\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -743800.625000\n",
      "    epoch          : 180\n",
      "    loss           : -628151.60125\n",
      "    val_loss       : -630608.530834961\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -553697.125000\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -743831.125000\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -784013.687500\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -747872.062500\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -647251.750000\n",
      "    epoch          : 181\n",
      "    loss           : -628454.3221875\n",
      "    val_loss       : -629339.8117675781\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -751985.437500\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -781075.000000\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -499756.500000\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -685751.437500\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -538441.062500\n",
      "    epoch          : 182\n",
      "    loss           : -621367.94625\n",
      "    val_loss       : -626656.0129882812\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -786523.750000\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -644345.375000\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -651070.187500\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -553124.812500\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -789274.250000\n",
      "    epoch          : 183\n",
      "    loss           : -625180.3\n",
      "    val_loss       : -629359.1141113281\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -554630.875000\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -546296.000000\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -654506.250000\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -753989.062500\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -682532.375000\n",
      "    epoch          : 184\n",
      "    loss           : -625942.924375\n",
      "    val_loss       : -625755.2482910156\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -502399.500000\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -494716.312500\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -495491.593750\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -490455.531250\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -481921.281250\n",
      "    epoch          : 185\n",
      "    loss           : -617460.1375\n",
      "    val_loss       : -624972.6477294922\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -689962.375000\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -497039.000000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -706540.625000\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -707708.875000\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -551539.125000\n",
      "    epoch          : 186\n",
      "    loss           : -625225.4228125\n",
      "    val_loss       : -627733.1637695313\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -499714.906250\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -785319.500000\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -785330.750000\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -757580.750000\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -645928.187500\n",
      "    epoch          : 187\n",
      "    loss           : -627776.7565625\n",
      "    val_loss       : -628744.4500244141\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -778475.187500\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -550223.250000\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -553887.875000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -508032.656250\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -781824.250000\n",
      "    epoch          : 188\n",
      "    loss           : -627314.9128125\n",
      "    val_loss       : -631326.6247802734\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -781250.250000\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -551323.562500\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -553373.875000\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -752225.625000\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -781918.187500\n",
      "    epoch          : 189\n",
      "    loss           : -626735.420625\n",
      "    val_loss       : -612843.3538085937\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -776604.000000\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -759489.500000\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -746773.875000\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -694038.875000\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -493501.687500\n",
      "    epoch          : 190\n",
      "    loss           : -622632.705\n",
      "    val_loss       : -629556.0895019531\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -689672.625000\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -779846.437500\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -635490.312500\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -635963.375000\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -549157.562500\n",
      "    epoch          : 191\n",
      "    loss           : -619597.1609375\n",
      "    val_loss       : -628146.2413818359\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -503917.218750\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -648467.500000\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -504658.593750\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -492248.593750\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -497203.125000\n",
      "    epoch          : 192\n",
      "    loss           : -625605.7540625\n",
      "    val_loss       : -627220.0223632812\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -550620.875000\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -509507.000000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -747104.625000\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -553517.875000\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -697694.625000\n",
      "    epoch          : 193\n",
      "    loss           : -627240.576875\n",
      "    val_loss       : -625819.8353759765\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -492796.062500\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -492989.156250\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -542715.875000\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -494629.750000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -743349.375000\n",
      "    epoch          : 194\n",
      "    loss           : -625320.0209375\n",
      "    val_loss       : -626502.6002441406\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -776422.687500\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -773347.000000\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -741824.437500\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -497690.906250\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -645385.250000\n",
      "    epoch          : 195\n",
      "    loss           : -626032.4059375\n",
      "    val_loss       : -626425.6216308593\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -751007.750000\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -743194.125000\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -504679.937500\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -745604.875000\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -786113.375000\n",
      "    epoch          : 196\n",
      "    loss           : -627446.144375\n",
      "    val_loss       : -630207.1400634765\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -780028.625000\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -547051.500000\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -777477.000000\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -783055.062500\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -645000.562500\n",
      "    epoch          : 197\n",
      "    loss           : -627269.05375\n",
      "    val_loss       : -625526.4887939453\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -649251.500000\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -497296.562500\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -550342.875000\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -756844.562500\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -745048.437500\n",
      "    epoch          : 198\n",
      "    loss           : -619739.623125\n",
      "    val_loss       : -627258.39140625\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -777424.125000\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -787555.000000\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -500173.468750\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -779398.375000\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -785830.687500\n",
      "    epoch          : 199\n",
      "    loss           : -626802.6578125\n",
      "    val_loss       : -628963.5267333984\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -780699.812500\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -756926.250000\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -555406.625000\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -649447.812500\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -654992.750000\n",
      "    epoch          : 200\n",
      "    loss           : -626817.52125\n",
      "    val_loss       : -627094.1457519531\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0805_171053/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -549124.312500\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -782655.187500\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -705988.375000\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -784841.937500\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -493093.062500\n",
      "    epoch          : 201\n",
      "    loss           : -628543.7678125\n",
      "    val_loss       : -629269.7682373046\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -779601.312500\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -784076.000000\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -704130.375000\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -557923.312500\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -694685.812500\n",
      "    epoch          : 202\n",
      "    loss           : -627607.828125\n",
      "    val_loss       : -618230.3173583985\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -776073.500000\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -476616.437500\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -774380.750000\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -494169.625000\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -495896.375000\n",
      "    epoch          : 203\n",
      "    loss           : -621394.870625\n",
      "    val_loss       : -628717.3932373046\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -554772.500000\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -751366.250000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -701976.875000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -752916.187500\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -651919.187500\n",
      "    epoch          : 204\n",
      "    loss           : -629444.4675\n",
      "    val_loss       : -624324.1880859375\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -781713.875000\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -497014.031250\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -780700.875000\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -649824.187500\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -757125.687500\n",
      "    epoch          : 205\n",
      "    loss           : -625659.49\n",
      "    val_loss       : -629348.6739013672\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -554676.750000\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -651134.750000\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -701224.500000\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -779891.062500\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -503263.093750\n",
      "    epoch          : 206\n",
      "    loss           : -628227.0465625\n",
      "    val_loss       : -630468.9703857421\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -497303.406250\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -789085.250000\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -776028.187500\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -493789.281250\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -639425.312500\n",
      "    epoch          : 207\n",
      "    loss           : -625101.614375\n",
      "    val_loss       : -625264.1421386718\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -543700.062500\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -775283.125000\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -775942.812500\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -775476.187500\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -557855.750000\n",
      "    epoch          : 208\n",
      "    loss           : -627790.4325\n",
      "    val_loss       : -632356.2491210938\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -500608.031250\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -750101.750000\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -779343.250000\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -750557.125000\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -524304.250000\n",
      "    epoch          : 209\n",
      "    loss           : -625145.133125\n",
      "    val_loss       : -628145.1819580079\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -778200.250000\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -513456.750000\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -499915.125000\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -758487.687500\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -751330.000000\n",
      "    epoch          : 210\n",
      "    loss           : -626670.801875\n",
      "    val_loss       : -630884.6801757812\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -549456.437500\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -792147.125000\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -503942.437500\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -508939.468750\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -705187.875000\n",
      "    epoch          : 211\n",
      "    loss           : -629838.1084375\n",
      "    val_loss       : -632575.8643554688\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -700957.437500\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -553639.500000\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -650768.937500\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -710875.250000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -703131.375000\n",
      "    epoch          : 212\n",
      "    loss           : -630791.0715625\n",
      "    val_loss       : -631330.5965576172\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -700714.312500\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -508997.468750\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -649763.500000\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -708148.875000\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -552812.687500\n",
      "    epoch          : 213\n",
      "    loss           : -629891.6709375\n",
      "    val_loss       : -628427.1413085938\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -697465.437500\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -755467.562500\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -531641.375000\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -746094.250000\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -486574.125000\n",
      "    epoch          : 214\n",
      "    loss           : -623148.468125\n",
      "    val_loss       : -621703.6084228515\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -651639.375000\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -501985.750000\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -708343.875000\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -783970.250000\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -705698.687500\n",
      "    epoch          : 215\n",
      "    loss           : -628229.3415625\n",
      "    val_loss       : -631415.0197998047\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -518589.593750\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -780347.812500\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -782563.562500\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -787405.062500\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -758521.812500\n",
      "    epoch          : 216\n",
      "    loss           : -630473.923125\n",
      "    val_loss       : -632759.7236083985\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -704773.000000\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -649350.375000\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -641169.625000\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -772348.437500\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -775155.437500\n",
      "    epoch          : 217\n",
      "    loss           : -625945.0265625\n",
      "    val_loss       : -625961.594921875\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -704338.125000\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -698045.750000\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -696216.437500\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -553712.875000\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -706438.125000\n",
      "    epoch          : 218\n",
      "    loss           : -627873.8775\n",
      "    val_loss       : -631550.088671875\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -760823.687500\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -752843.937500\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -553067.187500\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -749581.687500\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -745380.750000\n",
      "    epoch          : 219\n",
      "    loss           : -626498.81625\n",
      "    val_loss       : -624343.0806884766\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -505607.125000\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -479192.281250\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -477446.406250\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -707938.687500\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -506965.906250\n",
      "    epoch          : 220\n",
      "    loss           : -621383.0046875\n",
      "    val_loss       : -631258.4164794922\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -510093.281250\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -778010.437500\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -562010.437500\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -552070.000000\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -700961.750000\n",
      "    epoch          : 221\n",
      "    loss           : -630657.54875\n",
      "    val_loss       : -632465.1659179687\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -780896.375000\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -784106.375000\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -663030.187500\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -659036.312500\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -498638.343750\n",
      "    epoch          : 222\n",
      "    loss           : -631026.2571875\n",
      "    val_loss       : -629793.3807861328\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -784634.625000\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -780694.687500\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -542818.187500\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -771432.125000\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -753661.187500\n",
      "    epoch          : 223\n",
      "    loss           : -619393.0734375\n",
      "    val_loss       : -627599.497631836\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -558420.500000\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -786657.250000\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -780448.125000\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -699343.625000\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -514001.437500\n",
      "    epoch          : 224\n",
      "    loss           : -628421.815625\n",
      "    val_loss       : -631645.8024902344\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -509718.593750\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -557786.875000\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -504407.187500\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -780754.250000\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -704763.125000\n",
      "    epoch          : 225\n",
      "    loss           : -631745.46125\n",
      "    val_loss       : -632235.8092285156\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -783686.125000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -506655.375000\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -703367.375000\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -554413.937500\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -513569.406250\n",
      "    epoch          : 226\n",
      "    loss           : -631843.2634375\n",
      "    val_loss       : -631866.3096435547\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -561703.250000\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -760549.437500\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -549678.000000\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -562406.437500\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -700594.812500\n",
      "    epoch          : 227\n",
      "    loss           : -631659.99125\n",
      "    val_loss       : -634309.0744140625\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -751049.000000\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -507656.937500\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -547002.562500\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -506045.718750\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -703302.250000\n",
      "    epoch          : 228\n",
      "    loss           : -631898.6671875\n",
      "    val_loss       : -632303.5272949219\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -553665.437500\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -552821.937500\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -549542.500000\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -492823.968750\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -503880.906250\n",
      "    epoch          : 229\n",
      "    loss           : -625791.2084375\n",
      "    val_loss       : -628695.060961914\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -555885.000000\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -748673.312500\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -552277.437500\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -516322.093750\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -660903.187500\n",
      "    epoch          : 230\n",
      "    loss           : -630209.7478125\n",
      "    val_loss       : -632852.2490478515\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -544503.625000\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -756718.625000\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -787430.437500\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -493486.875000\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -507804.437500\n",
      "    epoch          : 231\n",
      "    loss           : -626810.6003125\n",
      "    val_loss       : -628532.9469726563\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -790286.562500\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -559264.375000\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -694153.437500\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -702423.437500\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -705804.375000\n",
      "    epoch          : 232\n",
      "    loss           : -625809.80625\n",
      "    val_loss       : -631238.7189941406\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -485989.875000\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -498384.718750\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -508578.625000\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -500752.687500\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -691730.375000\n",
      "    epoch          : 233\n",
      "    loss           : -626358.84375\n",
      "    val_loss       : -629543.4697265625\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -543424.250000\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -777278.000000\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -776809.687500\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -655058.375000\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -712789.750000\n",
      "    epoch          : 234\n",
      "    loss           : -627370.0896875\n",
      "    val_loss       : -630491.0698730468\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -782146.875000\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -557217.875000\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -491108.656250\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -498095.687500\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -757857.562500\n",
      "    epoch          : 235\n",
      "    loss           : -628725.526875\n",
      "    val_loss       : -630658.7740478516\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -546636.812500\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -519531.750000\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -781138.687500\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -507615.500000\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -502663.562500\n",
      "    epoch          : 236\n",
      "    loss           : -629401.799375\n",
      "    val_loss       : -631471.599633789\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -775820.437500\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -786356.500000\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -508948.250000\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -506148.312500\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -757535.312500\n",
      "    epoch          : 237\n",
      "    loss           : -629471.7753125\n",
      "    val_loss       : -632241.2447998046\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -779381.875000\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -511421.062500\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -750559.625000\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -645974.125000\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -764638.000000\n",
      "    epoch          : 238\n",
      "    loss           : -626852.4921875\n",
      "    val_loss       : -626380.3721679688\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -510562.500000\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -499123.468750\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -503795.250000\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -781523.312500\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -653396.750000\n",
      "    epoch          : 239\n",
      "    loss           : -629296.168125\n",
      "    val_loss       : -627236.7590576172\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -499320.593750\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -783848.375000\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -547711.875000\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -515246.312500\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -652231.625000\n",
      "    epoch          : 240\n",
      "    loss           : -627137.255625\n",
      "    val_loss       : -628832.0576904297\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -654605.500000\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -552964.125000\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -778644.562500\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -506501.656250\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -783185.937500\n",
      "    epoch          : 241\n",
      "    loss           : -631310.471875\n",
      "    val_loss       : -633274.7205078125\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -507454.687500\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -553687.062500\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -781830.062500\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -506133.750000\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -510392.437500\n",
      "    epoch          : 242\n",
      "    loss           : -630180.2746875\n",
      "    val_loss       : -630674.0378173828\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -555371.312500\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -776892.312500\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -655650.062500\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -754148.875000\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -743233.812500\n",
      "    epoch          : 243\n",
      "    loss           : -630962.9609375\n",
      "    val_loss       : -627873.566821289\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -785461.750000\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -697359.750000\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -787076.000000\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -546392.625000\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -750264.000000\n",
      "    epoch          : 244\n",
      "    loss           : -628639.69875\n",
      "    val_loss       : -630678.9908203125\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -708590.750000\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -783643.875000\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -696310.375000\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -510750.406250\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -593374.437500\n",
      "    epoch          : 245\n",
      "    loss           : -624704.4378125\n",
      "    val_loss       : -612212.2548339844\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -543365.375000\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -632028.875000\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -505583.250000\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -555446.812500\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -711187.375000\n",
      "    epoch          : 246\n",
      "    loss           : -627369.9778125\n",
      "    val_loss       : -631976.6932861328\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -506073.937500\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -510485.437500\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -746634.750000\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -698666.937500\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -471306.843750\n",
      "    epoch          : 247\n",
      "    loss           : -628238.8184375\n",
      "    val_loss       : -628424.962475586\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -488324.906250\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -649369.125000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -553177.687500\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -553610.687500\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -756267.750000\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-04.\n",
      "    epoch          : 248\n",
      "    loss           : -629732.783125\n",
      "    val_loss       : -632176.7097900391\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -782630.250000\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -549960.250000\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -783906.875000\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -500252.937500\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -564379.375000\n",
      "    epoch          : 249\n",
      "    loss           : -632967.23875\n",
      "    val_loss       : -632909.4330566407\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -508903.125000\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -783163.437500\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -565384.937500\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -710153.250000\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -553286.875000\n",
      "    epoch          : 250\n",
      "    loss           : -633346.714375\n",
      "    val_loss       : -634304.0912353515\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0805_171053/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -516637.406250\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -522466.781250\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -711020.625000\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -557315.875000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -657553.687500\n",
      "    epoch          : 251\n",
      "    loss           : -633394.66875\n",
      "    val_loss       : -633473.3241699219\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -779781.312500\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -559446.500000\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -507236.281250\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -500033.375000\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -760297.812500\n",
      "    epoch          : 252\n",
      "    loss           : -633154.0790625\n",
      "    val_loss       : -633800.9820800781\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -762187.750000\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -556630.812500\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -755144.250000\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -655004.000000\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -521864.500000\n",
      "    epoch          : 253\n",
      "    loss           : -633165.5409375\n",
      "    val_loss       : -633776.4794921875\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -788372.000000\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -789929.625000\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -553092.625000\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -793447.125000\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -496393.812500\n",
      "    epoch          : 254\n",
      "    loss           : -633519.0634375\n",
      "    val_loss       : -633412.0712646485\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -528019.750000\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -551402.250000\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -514209.812500\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -656508.250000\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -509754.843750\n",
      "    epoch          : 255\n",
      "    loss           : -633404.4021875\n",
      "    val_loss       : -633459.8186767579\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -506011.406250\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -712218.437500\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -513067.468750\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -558997.937500\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -701988.000000\n",
      "    epoch          : 256\n",
      "    loss           : -633502.2315625\n",
      "    val_loss       : -633541.2751220703\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -507349.218750\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -711725.375000\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -556647.562500\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -753165.625000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -558817.687500\n",
      "    epoch          : 257\n",
      "    loss           : -633442.68\n",
      "    val_loss       : -633265.5654296875\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -507425.687500\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -505073.000000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -506029.781250\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -654075.312500\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -713608.875000\n",
      "    epoch          : 258\n",
      "    loss           : -633515.20625\n",
      "    val_loss       : -634300.8588378907\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -708462.437500\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -755953.875000\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -506971.968750\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -658410.375000\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -787077.375000\n",
      "    epoch          : 259\n",
      "    loss           : -633531.1421875\n",
      "    val_loss       : -634608.1283935547\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -502030.437500\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -550417.250000\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -557130.125000\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -503197.500000\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -657366.125000\n",
      "    epoch          : 260\n",
      "    loss           : -633325.5909375\n",
      "    val_loss       : -634574.9685546875\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -507068.781250\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -789044.125000\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -752892.437500\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -660829.187500\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -519696.375000\n",
      "    epoch          : 261\n",
      "    loss           : -633616.5334375\n",
      "    val_loss       : -635192.4731201172\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -561460.812500\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -654681.000000\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -778764.500000\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -557700.500000\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -761348.750000\n",
      "    epoch          : 262\n",
      "    loss           : -633635.405\n",
      "    val_loss       : -634565.0076171875\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -515930.468750\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -559505.687500\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -787105.875000\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -563747.250000\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -561416.062500\n",
      "    epoch          : 263\n",
      "    loss           : -633512.3659375\n",
      "    val_loss       : -635250.0731689453\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -556423.000000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -558531.625000\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -557364.375000\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -705683.000000\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -780074.375000\n",
      "    epoch          : 264\n",
      "    loss           : -633542.2196875\n",
      "    val_loss       : -634246.2433837891\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -527633.000000\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -758302.187500\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -511827.375000\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -555177.125000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -519263.500000\n",
      "    epoch          : 265\n",
      "    loss           : -633734.4684375\n",
      "    val_loss       : -635869.3501953125\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -556768.187500\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -530676.625000\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -699430.875000\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -516129.250000\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -657029.812500\n",
      "    epoch          : 266\n",
      "    loss           : -633660.4953125\n",
      "    val_loss       : -635210.6944091797\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -551144.500000\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -508615.687500\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -790785.187500\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -558870.625000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -560248.187500\n",
      "    epoch          : 267\n",
      "    loss           : -633558.809375\n",
      "    val_loss       : -634431.8701660156\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -786032.250000\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -653539.812500\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -788587.062500\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -789226.750000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -509118.500000\n",
      "    epoch          : 268\n",
      "    loss           : -633852.093125\n",
      "    val_loss       : -635229.0868408203\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -557128.500000\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -702168.750000\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -522389.812500\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -518631.062500\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -747958.375000\n",
      "    epoch          : 269\n",
      "    loss           : -633424.75875\n",
      "    val_loss       : -634430.2096435546\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -757827.375000\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -716234.000000\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -783191.375000\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -706392.562500\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -761401.750000\n",
      "    epoch          : 270\n",
      "    loss           : -633773.1359375\n",
      "    val_loss       : -634465.4163085937\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -515876.531250\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -788478.562500\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -552159.625000\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -755706.000000\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -508720.281250\n",
      "    epoch          : 271\n",
      "    loss           : -633605.328125\n",
      "    val_loss       : -634247.6478027344\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -554115.500000\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -777547.437500\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -788916.437500\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -512954.906250\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -499709.062500\n",
      "    epoch          : 272\n",
      "    loss           : -633570.0609375\n",
      "    val_loss       : -633855.341821289\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -796677.500000\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -794267.937500\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -501354.031250\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -510647.437500\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -703309.187500\n",
      "    epoch          : 273\n",
      "    loss           : -633762.4403125\n",
      "    val_loss       : -635149.6089111328\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -517188.656250\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -656413.437500\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -508475.375000\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -503902.218750\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -555219.187500\n",
      "    epoch          : 274\n",
      "    loss           : -633953.4\n",
      "    val_loss       : -634412.2524414062\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -501155.125000\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -504692.625000\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -510479.593750\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -562765.875000\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -522449.562500\n",
      "    epoch          : 275\n",
      "    loss           : -633710.463125\n",
      "    val_loss       : -634646.2100097656\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -704191.750000\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -702525.062500\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -506135.750000\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -555640.625000\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -776367.000000\n",
      "    epoch          : 276\n",
      "    loss           : -633546.749375\n",
      "    val_loss       : -633469.7405761719\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -508784.375000\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -707374.625000\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -507392.093750\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -556427.125000\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -502789.343750\n",
      "    epoch          : 277\n",
      "    loss           : -633573.4440625\n",
      "    val_loss       : -634266.8674316406\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -507745.250000\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -510276.125000\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -705650.250000\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -781025.625000\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -714162.312500\n",
      "    epoch          : 278\n",
      "    loss           : -633775.48625\n",
      "    val_loss       : -634902.0458496094\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -558509.687500\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -525983.062500\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -702107.750000\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -553447.375000\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -506544.625000\n",
      "    epoch          : 279\n",
      "    loss           : -633959.4009375\n",
      "    val_loss       : -634760.6500976563\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -563081.125000\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -515670.031250\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -509971.562500\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -555561.125000\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -762448.250000\n",
      "    epoch          : 280\n",
      "    loss           : -633908.639375\n",
      "    val_loss       : -634782.3806884766\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -508086.468750\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -751987.625000\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -507600.500000\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -754928.062500\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -754913.187500\n",
      "    epoch          : 281\n",
      "    loss           : -633870.2475\n",
      "    val_loss       : -635627.5880371094\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -501239.125000\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -506721.187500\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -747708.062500\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -511899.718750\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -513704.687500\n",
      "    epoch          : 282\n",
      "    loss           : -633756.1896875\n",
      "    val_loss       : -635675.8330566406\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -509898.812500\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -786572.562500\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -559506.562500\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -554933.500000\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -504883.125000\n",
      "    epoch          : 283\n",
      "    loss           : -633652.823125\n",
      "    val_loss       : -634222.7698974609\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -787987.437500\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -788313.562500\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -754968.250000\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -657701.250000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -715861.125000\n",
      "    epoch          : 284\n",
      "    loss           : -633740.740625\n",
      "    val_loss       : -634677.1159912109\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -781218.250000\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -560590.375000\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -519729.937500\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -504177.187500\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -706717.875000\n",
      "    epoch          : 285\n",
      "    loss           : -633731.626875\n",
      "    val_loss       : -633987.5250488281\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -657145.187500\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -788084.750000\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -787680.562500\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -498343.437500\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -760821.250000\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "    epoch          : 286\n",
      "    loss           : -633696.81625\n",
      "    val_loss       : -635073.3992919922\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -508788.531250\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -712494.125000\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -706577.250000\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -548707.250000\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -701559.812500\n",
      "    epoch          : 287\n",
      "    loss           : -634014.37875\n",
      "    val_loss       : -634814.0247314454\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -788174.562500\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -659178.187500\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -557514.562500\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -697941.750000\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -655098.625000\n",
      "    epoch          : 288\n",
      "    loss           : -633825.4571875\n",
      "    val_loss       : -634748.5919921875\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -707469.312500\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -555289.312500\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -784347.750000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -713500.937500\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -701365.562500\n",
      "    epoch          : 289\n",
      "    loss           : -633754.4996875\n",
      "    val_loss       : -634370.8432617188\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -500734.000000\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -501010.781250\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -556164.625000\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -556041.250000\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -662601.125000\n",
      "    epoch          : 290\n",
      "    loss           : -633764.819375\n",
      "    val_loss       : -634085.017163086\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -505170.375000\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -501647.218750\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -522859.500000\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -658021.125000\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -752878.500000\n",
      "    epoch          : 291\n",
      "    loss           : -633768.484375\n",
      "    val_loss       : -634790.8759765625\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -784663.937500\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -501207.968750\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -556855.250000\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -755031.312500\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -511788.062500\n",
      "    epoch          : 292\n",
      "    loss           : -633787.2653125\n",
      "    val_loss       : -634132.2253417969\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -553889.125000\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -711889.125000\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -557595.812500\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -785512.875000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -758870.500000\n",
      "    epoch          : 293\n",
      "    loss           : -633987.3359375\n",
      "    val_loss       : -635459.7462890625\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -554121.875000\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -516838.625000\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -508886.812500\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -559959.875000\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -754380.562500\n",
      "    epoch          : 294\n",
      "    loss           : -633894.984375\n",
      "    val_loss       : -633360.8425537109\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -782043.625000\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -783818.500000\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -503718.500000\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -557536.375000\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -509758.437500\n",
      "    epoch          : 295\n",
      "    loss           : -634041.064375\n",
      "    val_loss       : -634855.4137695313\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -517068.750000\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -788921.750000\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -559988.625000\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -498660.437500\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -512931.343750\n",
      "    epoch          : 296\n",
      "    loss           : -633911.8228125\n",
      "    val_loss       : -635511.9814697265\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -511334.812500\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -785472.500000\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -708022.000000\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -792464.312500\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -660228.875000\n",
      "    epoch          : 297\n",
      "    loss           : -633796.999375\n",
      "    val_loss       : -633984.2408935546\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -785175.437500\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -513336.781250\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -554927.750000\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -655310.062500\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -711732.187500\n",
      "    epoch          : 298\n",
      "    loss           : -633916.468125\n",
      "    val_loss       : -635141.5763916016\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -786313.687500\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -550881.750000\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -655250.312500\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -658914.500000\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -791035.750000\n",
      "    epoch          : 299\n",
      "    loss           : -633819.0296875\n",
      "    val_loss       : -634512.1743164062\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -503143.187500\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -778609.625000\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -503831.437500\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -560543.062500\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -751689.000000\n",
      "    epoch          : 300\n",
      "    loss           : -634016.1390625\n",
      "    val_loss       : -634215.2439941406\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0805_171053/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -504211.312500\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -506630.875000\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -503919.906250\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -702467.750000\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -660289.500000\n",
      "    epoch          : 301\n",
      "    loss           : -633885.139375\n",
      "    val_loss       : -634593.7889160156\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -776106.500000\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -500023.687500\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -777827.500000\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -789570.250000\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -780734.375000\n",
      "    epoch          : 302\n",
      "    loss           : -633806.0471875\n",
      "    val_loss       : -634306.2007324218\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -499556.687500\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -519534.250000\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -512541.781250\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -660681.500000\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -704987.437500\n",
      "    epoch          : 303\n",
      "    loss           : -633767.0159375\n",
      "    val_loss       : -634716.448046875\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -560734.125000\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -517415.375000\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -713203.125000\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -516577.000000\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -753710.312500\n",
      "    epoch          : 304\n",
      "    loss           : -633858.7165625\n",
      "    val_loss       : -634284.1283447265\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -783165.250000\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -705313.625000\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -520092.875000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -744835.375000\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -521650.937500\n",
      "    epoch          : 305\n",
      "    loss           : -634151.6\n",
      "    val_loss       : -634644.1364746094\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -655793.437500\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -516585.875000\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -513569.718750\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -761269.000000\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -648767.687500\n",
      "    epoch          : 306\n",
      "    loss           : -633882.1046875\n",
      "    val_loss       : -634790.8299072266\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -782036.750000\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -713654.375000\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -514290.250000\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -555320.500000\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -514804.781250\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "    epoch          : 307\n",
      "    loss           : -633899.57625\n",
      "    val_loss       : -634680.4962646484\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -752035.625000\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -701341.750000\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -760825.125000\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -755429.687500\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -510738.187500\n",
      "    epoch          : 308\n",
      "    loss           : -633917.105625\n",
      "    val_loss       : -634878.459008789\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -514473.187500\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -522134.250000\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -556027.062500\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -708428.500000\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -756010.687500\n",
      "    epoch          : 309\n",
      "    loss           : -633893.7584375\n",
      "    val_loss       : -634989.5556884765\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -562769.312500\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -790821.562500\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -779972.062500\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -789501.687500\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -498010.375000\n",
      "    epoch          : 310\n",
      "    loss           : -633893.960625\n",
      "    val_loss       : -634095.7826660157\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -504476.437500\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -788457.625000\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -660996.500000\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -762179.500000\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -705650.125000\n",
      "    epoch          : 311\n",
      "    loss           : -633925.798125\n",
      "    val_loss       : -634882.1735595703\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -787076.625000\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -755168.250000\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -520256.125000\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -784412.000000\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -503928.250000\n",
      "    epoch          : 312\n",
      "    loss           : -633754.4478125\n",
      "    val_loss       : -635255.8530029297\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -654748.937500\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -761395.812500\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -498535.781250\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -563540.500000\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -787456.375000\n",
      "    epoch          : 313\n",
      "    loss           : -634013.7975\n",
      "    val_loss       : -634454.6935546875\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -557980.375000\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -657891.250000\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -511056.687500\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -660805.687500\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -711291.000000\n",
      "    epoch          : 314\n",
      "    loss           : -633920.5228125\n",
      "    val_loss       : -634427.101977539\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -560844.500000\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -657929.625000\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -496836.531250\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -664109.125000\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -663439.750000\n",
      "    epoch          : 315\n",
      "    loss           : -633737.278125\n",
      "    val_loss       : -635726.8170166016\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -786600.687500\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -707128.750000\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -502158.406250\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -783437.187500\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -653088.375000\n",
      "    epoch          : 316\n",
      "    loss           : -633918.2646875\n",
      "    val_loss       : -634834.2320556641\n",
      "Validation performance didn't improve for 50 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAECategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_1_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=136, bias=True)\n",
       "        (1): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=136, out_features=136, bias=True)\n",
       "        (4): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=136, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_3_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=136, bias=True)\n",
       "        (1): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=136, out_features=136, bias=True)\n",
       "        (4): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=136, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=264, bias=True)\n",
       "        (1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=264, out_features=264, bias=True)\n",
       "        (4): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=264, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=264, bias=True)\n",
       "        (1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=264, out_features=264, bias=True)\n",
       "        (4): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=264, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=400, bias=True)\n",
       "        (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (4): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=400, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_5_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "        (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (4): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=400, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_7_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=144, bias=True)\n",
       "        (1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=144, out_features=144, bias=True)\n",
       "        (4): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=144, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=144, bias=True)\n",
       "        (1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=144, out_features=144, bias=True)\n",
       "        (4): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=144, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=272, bias=True)\n",
       "        (1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=272, out_features=272, bias=True)\n",
       "        (4): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=272, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_9_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=272, bias=True)\n",
       "        (1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=272, out_features=272, bias=True)\n",
       "        (4): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=272, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_10): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=408, bias=True)\n",
       "        (1): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=408, out_features=408, bias=True)\n",
       "        (4): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=408, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_10_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=408, bias=True)\n",
       "        (1): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=408, out_features=408, bias=True)\n",
       "        (4): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=408, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_11): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_11_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_12): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_12_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_13): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_13_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_14): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_14_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_15): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_15_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_16): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_16_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_17): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_17_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_18): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_18_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_19): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_19_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_20): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_20_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "    (global_element_6): StandardNormal()\n",
       "  )\n",
       "  (guide_embedding): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (guide_confidences): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_distances): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=28, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAT5ElEQVR4nO3de7CcdX3H8feHk0NIQpBEIIaQEBAoRMSgEW1hLBqxwFTBVqyMOqGlhpmq1Zaill5ArcJQFUEt0wgZEq4ycssoCBiEjEWBiBBAbgEChMQkhFsAzeWcb//YJ7o5nP3tOXt79uT3ec2cObv7fS7f3ZxPnmf32ef5KSIws+3fDmU3YGad4bCbZcJhN8uEw26WCYfdLBMOu1kmHPbtjKQzJV3a4Lx/IunXkjZI+sdW99ZqkqZJekVST9m9jAQOe4tIOkLSHZJekvS8pP+T9M6y+xqmLwC3RcT4iDi/7GbqiYinI2LniOgru5eRwGFvAUm7AD8CvgNMBKYAXwY2ltlXA/YGHqxV7KYtqKRRZc4/EjnsrXEAQERcERF9EfG7iLg5IpYBSHqzpFslrZf0nKTLJO26dWZJKySdJmmZpFclXSRpkqQbi13qn0qaUEw7XVJImitplaTVkk6t1Zikdxd7HC9Kuk/SkTWmuxV4L/DdYtf4AEkXS7pA0g2SXgXeK+kNkhZKWifpKUn/LmmHYhknFXs05xbre0LSnxWPPyNpraQ5iV5vk3SWpLuKPaTrJU0c8LxPlvQ0cGvVY6OKafaUtKjYs1ou6VNVyz5T0g8lXSrpZeCkIf3Lbk8iwj9N/gC7AOuBBcAxwIQB9f2Ao4DRwO7AEuDbVfUVwC+BSVT2CtYC9wCHFvPcCpxRTDsdCOAKYBzwVmAd8P6ifiZwaXF7StHXsVT+Yz+quL97jedxG/D3VfcvBl4CDi/m3wlYCFwPjC96eRQ4uZj+JGAL8LdAD/BfwNPA94rn8QFgA7BzYv3PAgcXz+3qquey9XkvLGpjqh4bVUxzO/A/RZ8zi9dldtXrshk4vnguY8r+u+n432nZDWwvP8BBRThWFn/wi4BJNaY9Hvh11f0VwMer7l8NXFB1/7PAdcXtrX/gB1bVzwEuKm5Xh/2LwCUD1n0TMKdGX4OFfWHV/R4qb01mVD12CpX3+VvD/lhV7a1Fr5OqHlsPzEys/+yq+zOATcV6tz7vfavqfwg7MBXoA8ZX1c8CLq56XZaU/XdS5o9341skIh6KiJMiYi8qW6Y9gW8DSNpD0pWSni12IS8FdhuwiDVVt383yP2dB0z/TNXtp4r1DbQ3cEKxS/2ipBeBI4DJw3hq1evZDdixWF/1uqdU3R/YNxFR77nUWt9TQC/bvlbPMLg9gecjYkOit1rzZsFhb4OIeJjKVvHg4qGzqGyBDomIXYBPAGpyNVOrbk8DVg0yzTNUtuy7Vv2Mi4izh7Ge6tMin6OyK7z3gHU/O4zl1TPweW0u1jtYP9VWARMljU/0lvUpng57C0g6UNKpkvYq7k8FTqTyPhwq729fAV6UNAU4rQWr/Q9JYyW9hcp75B8MMs2lwAcl/YWkHkk7STpya5/DFZVDXFcBX5M0XtLewD8X62mVT0iaIWks8BXghzGEQ2sR8QxwB3BW8TwPAU4GLmthbyOaw94aG4B3AXcWn1r/EngA2Pop+ZeBt1P5sOvHwDUtWOftwHJgMfCNiLh54ARFAI4DTqfyYdUzVP6jaebf/bPAq8ATwM+By4H5TSxvoEuo7BX9lsoHbcP5cs+JVN7HrwKupfKh5i0t7G1EU/HhhY0QkqYDTwK9EbGl3G5aS9JtVD5cvLDsXrZH3rKbZcJhN8uEd+PNMuEtu1kmOnoywI4aHTsxrpOrNMvK73mVTbFx0O9wNHvm0NHAeVS+znhhvS9r7MQ43qXZzazSzBLujMU1aw3vxhenO36PyokfM4ATJc1odHlm1l7NvGc/DFgeEU9ExCbgSipf4DCzLtRM2Kew7YkFK9n2pAMAivOul0paunnEXcvBbPvRTNgH+xDgdcfxImJeRMyKiFm9jG5idWbWjGbCvpJtz1Dai8HPvDKzLtBM2O8G9pe0j6QdgY9RuWCDmXWhhg+9RcQWSZ+hcuWTHmB+RNS8WKG1z5jbJ9WsXbf/TR3sZHiuezV1DQu4YP/9OtRJHpo6zh4RNwA3tKgXM2sjf13WLBMOu1kmHHazTDjsZplw2M0y4bCbZSK7we22Ry9/tfaVofsW9Cfn7VFz/9+/0v/7ZH2HxPZk/qoj6iz9tw10ZLV4y26WCYfdLBMOu1kmHHazTDjsZplw2M0y4UNv24GDz15Ws9bsobV6xmjHZP13salmbf6+VyfnvXP5G5P18/c7MFm3bXnLbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwsfZR4Adxo9P1r+wx42Javpyze3Wq56atb7XDyC0jaPHvJasn99QR/nylt0sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4SPs48AKxdOTdYn94ztUCevV/d8+cSh9J0Tx+CHsmyNHp1e9caNyXpumgq7pBXABqAP2BIRs1rRlJm1Xiu27O+NiOdasBwzayO/ZzfLRLNhD+BmSb+SNHewCSTNlbRU0tLN+D2UWVma3Y0/PCJWSdoDuEXSwxGxpHqCiJgHzAPYRRPTZz6YWds0tWWPiFXF77XAtcBhrWjKzFqv4bBLGidp/NbbwAeAB1rVmJm1VjO78ZOAayVtXc7lEfGTlnRl27ju7fOS9R6Ve856o5q9pv3v33dIsj76xrubWv72puGwR8QTwNta2IuZtZEPvZllwmE3y4TDbpYJh90sEw67WSZ8iusIMG3UmNLW/eV1M5L1M3b/TbLeziGj1xzWm6xPS11hO0PesptlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmfBx9hEgNexxu938tfck62d8O32cvZ163vZSaeseibxlN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4ePslrTLwy+W3UJNR05dnqw/1qE+Rgpv2c0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTPg4u6U9+WzZHdS05Mp3JOuTuaNDnYwMdbfskuZLWivpgarHJkq6RdJjxe8J7W3TzJo1lN34i4GjBzz2JWBxROwPLC7um1kXqxv2iFgCPD/g4eOABcXtBcDxLe7LzFqs0Q/oJkXEaoDi9x61JpQ0V9JSSUs3s7HB1ZlZs9r+aXxEzIuIWRExq5fR7V6dmdXQaNjXSJoMUPxe27qWzKwdGg37ImBOcXsOcH1r2jGzdql7nF3SFcCRwG6SVgJnAGcDV0k6GXgaOKGdTeauL/qT9XaOgd6/YUPblt2sPc+7K1mPDvUxUtQNe0ScWKM0u8W9mFkb+euyZplw2M0y4bCbZcJhN8uEw26WCZ/iOgL01zmIVN6AzuWKLVvKbqGmjce8M1kfe9fjNWt96weeitIa3rKbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZpnwcfYR4MFN6ePJM0dvn0faV255pbR198w4ID1BX/q042n/+Uiyvupf96u97tt8nN3MmuCwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0z4OPsIcM6qgeNqbuvyfX7W8LLLPJZdz4e+flqyvju/aNu6n/zIbul1L0t/9+GF89Pzjxm1uWatXd+a8JbdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEj7OPANPHrm/bshe++I62LbtZk3/0dLLezqvGf+5j1yfr5+yb/u7DrhNfStYPnfxUzdqT6w5Kztt/30PJei11t+yS5ktaK+mBqsfOlPSspHuLn2MbWruZdcxQduMvBgb7b+zciJhZ/NzQ2rbMrNXqhj0ilgDtuU6OmXVMMx/QfUbSsmI3f0KtiSTNlbRU0tLNbGxidWbWjEbDfgHwZmAmsBr4Zq0JI2JeRMyKiFm9jG5wdWbWrIbCHhFrIqIvIvqB7wOHtbYtM2u1hsIuaXLV3Q8DD9Sa1sy6Q93j7JKuAI4EdpO0EjgDOFLSTCCAFcApbewxe7eurnMN80nLGl72JdfMTtancUeyvjFqn5cNMFq9w+5pqxg/tuF5m/W+cY8m6x+Zna7f9Nq0ZP0tO66qWbvryvT3C64+aI9kvZa6YY+IEwd5+KKG1mZmpfHXZc0y4bCbZcJhN8uEw26WCYfdLBM+xXUEmPhPdSZo/ErS7HPF2mS9r878579wYLJ+2sTHh9nRH8XTtQ9Ptdtnpx+RrI+alD78tWVN+nVVzz6JYr1t8KY69cF5y26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcLH2UeA/rFtvMLPqOYGCJ7a28bLE0a0b9lNrnvL2ueamj+21L4Qdv+fH5qcd4fbf51ed635GprLzEYch90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwsfZR4BHT965bcuee92Pk/UL9t8vWX//2JV11jBumB39kcbslJ7gtdcaXnY9O4xNX8b6xuXpS2zve8vfJeuT9qg9pPOSQy5MzvuXUxobZttbdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sE0MZsnkqsBB4E9APzIuI8yRNBH4ATKcybPNHI+KF9rWar4O+9lR6gr9qfNkLV/9pnSnWJasff/RvkvWbDvrRMDv6o771bTxXvo7+OsfwX+tPX7v9wK+moxDjan+H4Kj//uvkvKNZkazXMpQt+xbg1Ig4CHg38GlJM4AvAYsjYn9gcXHfzLpU3bBHxOqIuKe4vQF4CJgCHAcsKCZbABzfribNrHnDes8uaTpwKHAnMCkiVkPlPwQgPR6OmZVqyGGXtDNwNfD5iHh5GPPNlbRU0tLNbGykRzNrgSGFXVIvlaBfFhHXFA+vkTS5qE8GBh3JLiLmRcSsiJjVSxsvnGhmSXXDLknARcBDEfGtqtIiYE5xew5wfevbM7NWGcoprocDnwTul3Rv8djpwNnAVZJOBp4GTmhPi9a3rs5li5swe7eHk/VFvDFZf2r9hIbXvTE2Nzxv2b7zwluS9b7HVyTrqSGfv7LfTcl5v957WO3iZtVeZ3KpQET8HKi1hNn15jez7uBv0JllwmE3y4TDbpYJh90sEw67WSYcdrNMbD+Xklbt44sAGtWbru+U/nZf/4YNw26pVVLD+zbru1d+MFmfRp1LJp9S51LSD9YuHbd3vdNr2/e8m/UvEx9J1hed8A/J+i6P1L6U9GMb35ScNzYnTq9NDBXtLbtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulont6Dh7+v+tHXZ9Q7K+/pj00MS7LvzFsFvqlJVbXqlZ22tUerjn6efen6z311l33wvpSyYf+54P16zFlifrLL179dT5ezvhjJ8k6/971bE1a/91+4eS8x7AXcl6Ld6ym2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZ2H6Os/f3Jct969JDD0+4vM4Qu8NuqHM+dcD7a9bmP/rT5LztPk+/b/nIPZaesrbv1WR9wQW1j6MDTL/wnpq1/k3tuZ6+t+xmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYUietMA0iaCiwE3kTl9OZ5EXGepDOBTwFbD2CfHhE3pJa1iybGu+RRnm3ki8NnJuu64746C2jPNzfujMW8HM8POojCUL5UswU4NSLukTQe+JWkW4rauRHxjVY1ambtUzfsEbEaWF3c3iDpIWBKuxszs9Ya1nt2SdOBQ4E7i4c+I2mZpPmSJtSYZ66kpZKWbmZjU82aWeOGHHZJOwNXA5+PiJeBC4A3AzOpbPm/Odh8ETEvImZFxKxe0uOpmVn7DCnsknqpBP2yiLgGICLWRERfRPQD3wcOa1+bZtasumGXJOAi4KGI+FbV45OrJvsw8EDr2zOzVhnKp/GHA58E7pd0b/HY6cCJkmZSOftzBXBKWzo060I99z6WrPe36dBaM4byafzPgcGO2yWPqZtZd/E36Mwy4bCbZcJhN8uEw26WCYfdLBMOu1kmtp9LSZt1UP+r6UtJdyNv2c0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTNS9lHRLVyatA56qemg34LmONTA83dpbt/YF7q1Rrext74jYfbBCR8P+upVLSyNiVmkNJHRrb93aF7i3RnWqN+/Gm2XCYTfLRNlhn1fy+lO6tbdu7QvcW6M60lup79nNrHPK3rKbWYc47GaZKCXsko6W9Iik5ZK+VEYPtUhaIel+SfdKWlpyL/MlrZX0QNVjEyXdIumx4vegY+yV1NuZkp4tXrt7JR1bUm9TJf1M0kOSHpT0ueLxUl+7RF8ded06/p5dUg/wKHAUsBK4GzgxIn7T0UZqkLQCmBURpX8BQ9J7gFeAhRFxcPHYOcDzEXF28R/lhIj4Ypf0dibwStnDeBejFU2uHmYcOB44iRJfu0RfH6UDr1sZW/bDgOUR8UREbAKuBI4roY+uFxFLgOcHPHwcsKC4vYDKH0vH1eitK0TE6oi4p7i9Adg6zHipr12ir44oI+xTgGeq7q+ku8Z7D+BmSb+SNLfsZgYxKSJWQ+WPB9ij5H4GqjuMdycNGGa8a167RoY/b1YZYR9sKKluOv53eES8HTgG+HSxu2pDM6RhvDtlkGHGu0Kjw583q4ywrwSmVt3fC1hVQh+DiohVxe+1wLV031DUa7aOoFv8XltyP3/QTcN4DzbMOF3w2pU5/HkZYb8b2F/SPpJ2BD4GLCqhj9eRNK744ARJ44AP0H1DUS8C5hS35wDXl9jLNrplGO9aw4xT8mtX+vDnEdHxH+BYKp/IPw78Wxk91OhrX+C+4ufBsnsDrqCyW7eZyh7RycAbgcXAY8XviV3U2yXA/cAyKsGaXFJvR1B5a7gMuLf4Obbs1y7RV0deN39d1iwT/gadWSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpaJ/wfFFu8iFXIiawAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
