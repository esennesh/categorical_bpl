{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 123022.734375\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -13182.923828\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -59926.851562\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -88543.773438\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -72896.320312\n",
      "    epoch          : 1\n",
      "    loss           : -30908.17462890625\n",
      "    val_loss       : -51892.01701452732\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -90590.140625\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -65399.484375\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -19490.914062\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -12175.691406\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -76670.164062\n",
      "    epoch          : 2\n",
      "    loss           : -62538.03803710938\n",
      "    val_loss       : -59981.28604630232\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -72825.835938\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -86733.890625\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -79996.656250\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -83638.679688\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -34764.695312\n",
      "    epoch          : 3\n",
      "    loss           : -73715.571796875\n",
      "    val_loss       : -76430.721875\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -65444.382812\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -87733.835938\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -78124.804688\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -82368.601562\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -76973.859375\n",
      "    epoch          : 4\n",
      "    loss           : -76240.7257421875\n",
      "    val_loss       : -78871.69453125\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -77133.804688\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -100427.695312\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -98141.195312\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -75145.570312\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -83423.296875\n",
      "    epoch          : 5\n",
      "    loss           : -78326.3664453125\n",
      "    val_loss       : -80892.7921875\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -85637.054688\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -96231.445312\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -41721.335938\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -39573.710938\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -90934.476562\n",
      "    epoch          : 6\n",
      "    loss           : -81662.835390625\n",
      "    val_loss       : -84224.53984375\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -97242.250000\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -97557.039062\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -55296.984375\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -41111.132812\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -90662.648438\n",
      "    epoch          : 7\n",
      "    loss           : -83783.901328125\n",
      "    val_loss       : -84665.53359375\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -100382.710938\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -85822.218750\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -76509.218750\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -86163.859375\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -101915.179688\n",
      "    epoch          : 8\n",
      "    loss           : -85859.6046875\n",
      "    val_loss       : -87581.2484375\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -108172.921875\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -87156.468750\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -77238.632812\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -109179.156250\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -102580.375000\n",
      "    epoch          : 9\n",
      "    loss           : -87877.369140625\n",
      "    val_loss       : -89198.652734375\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -79368.375000\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -79036.031250\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -91445.039062\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -110409.437500\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -47786.117188\n",
      "    epoch          : 10\n",
      "    loss           : -89142.035703125\n",
      "    val_loss       : -90650.508203125\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -97565.328125\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -92863.664062\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -104459.054688\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -49508.250000\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -96839.078125\n",
      "    epoch          : 11\n",
      "    loss           : -90503.66046875\n",
      "    val_loss       : -91785.05078125\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -90246.750000\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -107827.976562\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -86449.164062\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -94690.898438\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -107854.265625\n",
      "    epoch          : 12\n",
      "    loss           : -92228.81546875\n",
      "    val_loss       : -91724.93359375\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -95855.750000\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -113634.906250\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -109034.062500\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -111354.953125\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -111554.273438\n",
      "    epoch          : 13\n",
      "    loss           : -93863.1484375\n",
      "    val_loss       : -95408.687890625\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -89526.406250\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -115036.578125\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -54718.046875\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -84339.093750\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -96704.734375\n",
      "    epoch          : 14\n",
      "    loss           : -96180.44046875\n",
      "    val_loss       : -97732.43984375\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -85480.875000\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -84723.164062\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -84665.523438\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -59742.734375\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -112357.609375\n",
      "    epoch          : 15\n",
      "    loss           : -96547.515390625\n",
      "    val_loss       : -99491.529296875\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -95080.843750\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -62571.125000\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -119146.921875\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -62825.070312\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -61507.835938\n",
      "    epoch          : 16\n",
      "    loss           : -99755.412265625\n",
      "    val_loss       : -101032.692578125\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -61273.289062\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -95213.968750\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -117917.085938\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -101256.132812\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -101655.062500\n",
      "    epoch          : 17\n",
      "    loss           : -101666.95078125\n",
      "    val_loss       : -102942.667578125\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -96815.609375\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -95720.585938\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -102844.375000\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -98492.250000\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -64449.812500\n",
      "    epoch          : 18\n",
      "    loss           : -103229.20625\n",
      "    val_loss       : -104128.768359375\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -120074.320312\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -92658.351562\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -96890.304688\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -96094.179688\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -63666.406250\n",
      "    epoch          : 19\n",
      "    loss           : -104040.598125\n",
      "    val_loss       : -104325.276953125\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -121795.929688\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -122688.296875\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -118983.781250\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -117856.953125\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -120758.031250\n",
      "    epoch          : 20\n",
      "    loss           : -105626.803046875\n",
      "    val_loss       : -106884.237890625\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -120413.421875\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -108071.859375\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -104561.687500\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -122840.203125\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -124398.453125\n",
      "    epoch          : 21\n",
      "    loss           : -107300.904453125\n",
      "    val_loss       : -108409.783203125\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -110042.828125\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -126834.828125\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -126301.710938\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -71841.640625\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -106648.609375\n",
      "    epoch          : 22\n",
      "    loss           : -109099.62109375\n",
      "    val_loss       : -110155.871875\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -124292.726562\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -102073.617188\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -127476.890625\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -120804.156250\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -73794.593750\n",
      "    epoch          : 23\n",
      "    loss           : -110322.932109375\n",
      "    val_loss       : -111627.087109375\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -108469.421875\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -103343.156250\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -130091.710938\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -127494.625000\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -128093.875000\n",
      "    epoch          : 24\n",
      "    loss           : -112196.202109375\n",
      "    val_loss       : -113340.149609375\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -74483.382812\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -74251.351562\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -107714.187500\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -132329.500000\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -76879.890625\n",
      "    epoch          : 25\n",
      "    loss           : -113752.086953125\n",
      "    val_loss       : -115437.28828125\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -105472.664062\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -75103.070312\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -110331.218750\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -78977.539062\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -113440.695312\n",
      "    epoch          : 26\n",
      "    loss           : -114900.386015625\n",
      "    val_loss       : -116124.94921875\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -111550.234375\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -131313.750000\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -108980.500000\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -132519.843750\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -132429.578125\n",
      "    epoch          : 27\n",
      "    loss           : -116327.3190625\n",
      "    val_loss       : -117761.754296875\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -118949.031250\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -115347.968750\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -133989.218750\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -79519.382812\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -130598.867188\n",
      "    epoch          : 28\n",
      "    loss           : -117828.94765625\n",
      "    val_loss       : -118903.349609375\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -107865.281250\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -115726.203125\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -137317.265625\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -133435.156250\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -116055.406250\n",
      "    epoch          : 29\n",
      "    loss           : -119320.967109375\n",
      "    val_loss       : -120297.628125\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -135850.250000\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -137136.093750\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -111238.734375\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -136559.328125\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -121780.250000\n",
      "    epoch          : 30\n",
      "    loss           : -120503.598671875\n",
      "    val_loss       : -121096.365234375\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -122092.625000\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -121523.328125\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -83480.382812\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -137472.625000\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -116006.679688\n",
      "    epoch          : 31\n",
      "    loss           : -122023.06609375\n",
      "    val_loss       : -123105.90859375\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -113508.828125\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -139590.781250\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -121741.687500\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -136150.375000\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -136008.578125\n",
      "    epoch          : 32\n",
      "    loss           : -123505.033671875\n",
      "    val_loss       : -124435.29609375\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -142410.531250\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -118362.585938\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -121217.992188\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -120901.828125\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -117129.515625\n",
      "    epoch          : 33\n",
      "    loss           : -124844.126875\n",
      "    val_loss       : -126002.68203125\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -141480.828125\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -124429.375000\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -145630.218750\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -140997.078125\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -122052.132812\n",
      "    epoch          : 34\n",
      "    loss           : -126251.973203125\n",
      "    val_loss       : -127033.25546875\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -88706.289062\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -141770.281250\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -139514.796875\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -122204.359375\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -143459.125000\n",
      "    epoch          : 35\n",
      "    loss           : -127397.25078125\n",
      "    val_loss       : -128627.878515625\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -123293.359375\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -128909.859375\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -142780.250000\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -124090.304688\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -124327.703125\n",
      "    epoch          : 36\n",
      "    loss           : -128428.9940625\n",
      "    val_loss       : -129675.562890625\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -122365.859375\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -124416.523438\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -144814.203125\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -143480.781250\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -93656.273438\n",
      "    epoch          : 37\n",
      "    loss           : -129953.92328125\n",
      "    val_loss       : -131261.762890625\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -144663.328125\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -119869.921875\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -148745.343750\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -129738.039062\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -146000.562500\n",
      "    epoch          : 38\n",
      "    loss           : -131305.99265625\n",
      "    val_loss       : -132335.2875\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -119855.296875\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -123368.046875\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -94804.101562\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -152731.484375\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -131095.484375\n",
      "    epoch          : 39\n",
      "    loss           : -132375.190703125\n",
      "    val_loss       : -133498.075390625\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -146241.656250\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -125260.367188\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -133498.171875\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -131051.328125\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -98005.257812\n",
      "    epoch          : 40\n",
      "    loss           : -133571.059921875\n",
      "    val_loss       : -134679.997265625\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -133358.203125\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -149421.656250\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -153210.562500\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -152067.453125\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -148001.546875\n",
      "    epoch          : 41\n",
      "    loss           : -134292.8565625\n",
      "    val_loss       : -134604.522265625\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -148402.562500\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -129577.218750\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -127793.703125\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -135220.000000\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -128634.539062\n",
      "    epoch          : 42\n",
      "    loss           : -135785.383828125\n",
      "    val_loss       : -136491.31015625\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -149355.937500\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -147418.921875\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -148863.093750\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -131289.000000\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -157033.515625\n",
      "    epoch          : 43\n",
      "    loss           : -136803.966796875\n",
      "    val_loss       : -138039.671484375\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -130672.937500\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -136720.875000\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -137577.906250\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -154502.500000\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -103698.023438\n",
      "    epoch          : 44\n",
      "    loss           : -138607.8621875\n",
      "    val_loss       : -139667.728515625\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -160374.312500\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -128226.843750\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -139227.187500\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -133088.843750\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -133093.578125\n",
      "    epoch          : 45\n",
      "    loss           : -139876.93109375\n",
      "    val_loss       : -140714.270703125\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -135992.593750\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -155860.781250\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -132521.468750\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -155146.203125\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -106706.593750\n",
      "    epoch          : 46\n",
      "    loss           : -141037.778359375\n",
      "    val_loss       : -142158.216015625\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -126610.632812\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -133366.859375\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -130699.875000\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -161314.734375\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -107330.187500\n",
      "    epoch          : 47\n",
      "    loss           : -142201.698984375\n",
      "    val_loss       : -143011.00234375\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -159259.937500\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -142110.140625\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -160529.406250\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -143813.968750\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -108238.156250\n",
      "    epoch          : 48\n",
      "    loss           : -143502.030078125\n",
      "    val_loss       : -144840.895703125\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -161304.515625\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -143264.000000\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -110559.875000\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -110530.578125\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -108983.921875\n",
      "    epoch          : 49\n",
      "    loss           : -144676.741953125\n",
      "    val_loss       : -145285.563671875\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -145608.703125\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -138008.890625\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -161807.046875\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -158367.125000\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -139115.625000\n",
      "    epoch          : 50\n",
      "    loss           : -145887.946796875\n",
      "    val_loss       : -146721.977734375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -169927.125000\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -112264.140625\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -132764.625000\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -133920.859375\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -112420.937500\n",
      "    epoch          : 51\n",
      "    loss           : -146814.247578125\n",
      "    val_loss       : -148240.35\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -113051.234375\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -136759.218750\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -139461.000000\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -170346.296875\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -138160.968750\n",
      "    epoch          : 52\n",
      "    loss           : -148180.72578125\n",
      "    val_loss       : -149152.26796875\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -132360.062500\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -167277.437500\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -135801.671875\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -160809.531250\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -165533.031250\n",
      "    epoch          : 53\n",
      "    loss           : -149412.711875\n",
      "    val_loss       : -150521.98671875\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -138296.937500\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -166859.250000\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -151107.859375\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -169754.093750\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -144058.812500\n",
      "    epoch          : 54\n",
      "    loss           : -150587.26375\n",
      "    val_loss       : -151696.361328125\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -163590.109375\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -139585.046875\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -175903.390625\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -135795.921875\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -143965.562500\n",
      "    epoch          : 55\n",
      "    loss           : -151721.478984375\n",
      "    val_loss       : -152504.9234375\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -153976.796875\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -140879.218750\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -138253.312500\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -118291.265625\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -144513.312500\n",
      "    epoch          : 56\n",
      "    loss           : -152801.523125\n",
      "    val_loss       : -153928.196875\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -143546.343750\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -145271.781250\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -146105.890625\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -146034.250000\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -170785.140625\n",
      "    epoch          : 57\n",
      "    loss           : -153903.046953125\n",
      "    val_loss       : -154966.68984375\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -154188.312500\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -156678.437500\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -176206.937500\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -178500.875000\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -173859.656250\n",
      "    epoch          : 58\n",
      "    loss           : -154962.828515625\n",
      "    val_loss       : -156044.51484375\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -153807.750000\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -154428.812500\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -146940.562500\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -172578.140625\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -171495.656250\n",
      "    epoch          : 59\n",
      "    loss           : -156238.896171875\n",
      "    val_loss       : -157113.719921875\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -149160.671875\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -153802.656250\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -174299.781250\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -148313.250000\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -125301.304688\n",
      "    epoch          : 60\n",
      "    loss           : -157181.849296875\n",
      "    val_loss       : -158314.852734375\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -175769.453125\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -182626.281250\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -149254.968750\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -142302.125000\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -126683.390625\n",
      "    epoch          : 61\n",
      "    loss           : -158512.26296875\n",
      "    val_loss       : -159564.635546875\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -157396.406250\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -124887.140625\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -151131.562500\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -144271.218750\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -186299.500000\n",
      "    epoch          : 62\n",
      "    loss           : -159464.35578125\n",
      "    val_loss       : -160528.059765625\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -141198.046875\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -149434.000000\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -152564.250000\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -177412.765625\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -152252.906250\n",
      "    epoch          : 63\n",
      "    loss           : -160707.46390625\n",
      "    val_loss       : -161838.616796875\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -178782.656250\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -165098.687500\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -187640.312500\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -171730.468750\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -151788.500000\n",
      "    epoch          : 64\n",
      "    loss           : -161921.911640625\n",
      "    val_loss       : -162682.898046875\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -180013.937500\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -151851.687500\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -156061.468750\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -182529.625000\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -153178.375000\n",
      "    epoch          : 65\n",
      "    loss           : -163114.160859375\n",
      "    val_loss       : -164202.687109375\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -153332.437500\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -155648.593750\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -146873.125000\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -151925.687500\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -183067.812500\n",
      "    epoch          : 66\n",
      "    loss           : -164218.53765625\n",
      "    val_loss       : -165231.008984375\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -167585.250000\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -153360.312500\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -192812.484375\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -161533.531250\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -184104.640625\n",
      "    epoch          : 67\n",
      "    loss           : -165068.60265625\n",
      "    val_loss       : -165983.36484375\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -154445.953125\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -191516.562500\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -183601.218750\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -152362.000000\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -158594.593750\n",
      "    epoch          : 68\n",
      "    loss           : -166266.57\n",
      "    val_loss       : -167329.48125\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -163366.078125\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -193379.468750\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -156101.078125\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -178395.968750\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -183575.406250\n",
      "    epoch          : 69\n",
      "    loss           : -167490.42984375\n",
      "    val_loss       : -168563.61640625\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -156486.593750\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -172207.453125\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -184964.906250\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -156947.718750\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -186051.312500\n",
      "    epoch          : 70\n",
      "    loss           : -168500.921875\n",
      "    val_loss       : -169491.378515625\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -179019.828125\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -165277.593750\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -164388.187500\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -180860.218750\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -189388.484375\n",
      "    epoch          : 71\n",
      "    loss           : -169780.3771875\n",
      "    val_loss       : -170189.594921875\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -139027.562500\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -161894.968750\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -165952.531250\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -167803.937500\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -138444.656250\n",
      "    epoch          : 72\n",
      "    loss           : -170658.3928125\n",
      "    val_loss       : -172050.00234375\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -167238.703125\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -157768.687500\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -176923.812500\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -141263.812500\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -158605.515625\n",
      "    epoch          : 73\n",
      "    loss           : -171967.05953125\n",
      "    val_loss       : -173165.016796875\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -198782.343750\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -200494.500000\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -162858.968750\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -182121.046875\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -143629.093750\n",
      "    epoch          : 74\n",
      "    loss           : -173097.204375\n",
      "    val_loss       : -174285.537890625\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -192534.796875\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -155951.031250\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -182878.109375\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -192033.968750\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -145643.718750\n",
      "    epoch          : 75\n",
      "    loss           : -174240.00265625\n",
      "    val_loss       : -175220.609375\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -145979.546875\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -155751.656250\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -161808.531250\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -144151.156250\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -144434.921875\n",
      "    epoch          : 76\n",
      "    loss           : -175356.1690625\n",
      "    val_loss       : -176529.773828125\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -145565.625000\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -157491.171875\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -163314.421875\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -162528.500000\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -166650.484375\n",
      "    epoch          : 77\n",
      "    loss           : -176301.989375\n",
      "    val_loss       : -177653.780859375\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -193990.546875\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -147457.343750\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -148266.328125\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -185148.687500\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -165835.750000\n",
      "    epoch          : 78\n",
      "    loss           : -177662.99671875\n",
      "    val_loss       : -178800.699609375\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -167969.953125\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -171338.125000\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -175223.843750\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -198423.781250\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -185832.828125\n",
      "    epoch          : 79\n",
      "    loss           : -178187.3\n",
      "    val_loss       : -178986.0015625\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -165627.500000\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -206637.593750\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -187714.343750\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -208494.375000\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -176998.812500\n",
      "    epoch          : 80\n",
      "    loss           : -179191.15703125\n",
      "    val_loss       : -179833.087109375\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -161861.859375\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -150514.359375\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -161422.437500\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -209023.718750\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -201722.703125\n",
      "    epoch          : 81\n",
      "    loss           : -180750.95375\n",
      "    val_loss       : -182045.66484375\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -201977.796875\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -171398.750000\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -201033.906250\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -200832.281250\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -152520.156250\n",
      "    epoch          : 82\n",
      "    loss           : -182087.120625\n",
      "    val_loss       : -183101.06484375\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -208573.250000\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -202342.468750\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -188336.046875\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -172145.750000\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -190454.281250\n",
      "    epoch          : 83\n",
      "    loss           : -183203.47453125\n",
      "    val_loss       : -184010.41796875\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -169566.093750\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -190044.968750\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -164999.312500\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -177931.093750\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -154936.140625\n",
      "    epoch          : 84\n",
      "    loss           : -184042.27421875\n",
      "    val_loss       : -184445.63125\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -177441.218750\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -212399.937500\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -163374.843750\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -200963.812500\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -194427.109375\n",
      "    epoch          : 85\n",
      "    loss           : -184627.6284375\n",
      "    val_loss       : -185534.28984375\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -171644.734375\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -192699.546875\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -180420.875000\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -195054.687500\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -206310.281250\n",
      "    epoch          : 86\n",
      "    loss           : -185887.61359375\n",
      "    val_loss       : -187196.95703125\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -156513.281250\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -164251.156250\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -166031.984375\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -182671.375000\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -171416.062500\n",
      "    epoch          : 87\n",
      "    loss           : -186954.74421875\n",
      "    val_loss       : -187680.219921875\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -165089.703125\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -180694.281250\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -206730.296875\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -206995.328125\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -208663.640625\n",
      "    epoch          : 88\n",
      "    loss           : -188352.44515625\n",
      "    val_loss       : -189324.309375\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -166331.593750\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -176246.265625\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -196174.406250\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -175950.078125\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -176097.328125\n",
      "    epoch          : 89\n",
      "    loss           : -189514.8746875\n",
      "    val_loss       : -190380.591796875\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -159468.125000\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -168913.328125\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -208875.515625\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -218363.593750\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -209441.328125\n",
      "    epoch          : 90\n",
      "    loss           : -190619.82859375\n",
      "    val_loss       : -192014.709765625\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -210878.406250\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -210617.859375\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -162099.406250\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -210546.250000\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -212265.531250\n",
      "    epoch          : 91\n",
      "    loss           : -191792.13171875\n",
      "    val_loss       : -192295.776171875\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -184576.609375\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -188736.906250\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -187981.718750\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -198859.906250\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -165934.140625\n",
      "    epoch          : 92\n",
      "    loss           : -192755.20609375\n",
      "    val_loss       : -193822.830078125\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -198805.703125\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -172407.531250\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -174078.140625\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -182696.765625\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -212731.468750\n",
      "    epoch          : 93\n",
      "    loss           : -193626.9328125\n",
      "    val_loss       : -194017.060546875\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -202517.453125\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -177041.343750\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -213255.906250\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -214263.000000\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -178315.796875\n",
      "    epoch          : 94\n",
      "    loss           : -194057.14390625\n",
      "    val_loss       : -195531.264453125\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -215735.562500\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -215010.281250\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -167704.781250\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -226156.343750\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -176768.250000\n",
      "    epoch          : 95\n",
      "    loss           : -195632.43328125\n",
      "    val_loss       : -196785.561328125\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -182056.421875\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -215230.875000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -204131.796875\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -179295.984375\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -202644.796875\n",
      "    epoch          : 96\n",
      "    loss           : -196554.2171875\n",
      "    val_loss       : -197491.26875\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -168702.578125\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -176104.734375\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -204947.546875\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -205057.734375\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -216977.531250\n",
      "    epoch          : 97\n",
      "    loss           : -197416.92515625\n",
      "    val_loss       : -198531.14296875\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -183106.296875\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -193867.562500\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -221201.281250\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -219855.218750\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -181657.500000\n",
      "    epoch          : 98\n",
      "    loss           : -198456.41703125\n",
      "    val_loss       : -199707.1625\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -180971.781250\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -175733.015625\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -183567.968750\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -221300.765625\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -194613.343750\n",
      "    epoch          : 99\n",
      "    loss           : -199866.45453125\n",
      "    val_loss       : -200770.423046875\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -222115.125000\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -206858.765625\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -189168.359375\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -172949.406250\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -185070.406250\n",
      "    epoch          : 100\n",
      "    loss           : -200966.43171875\n",
      "    val_loss       : -201723.464453125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -208327.156250\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -206443.875000\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -219419.093750\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -223521.515625\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -182951.968750\n",
      "    epoch          : 101\n",
      "    loss           : -201876.16828125\n",
      "    val_loss       : -202625.9890625\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -189116.937500\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -187633.687500\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -210242.953125\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -221034.375000\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -221191.453125\n",
      "    epoch          : 102\n",
      "    loss           : -201652.94765625\n",
      "    val_loss       : -202354.36328125\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -187616.468750\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -181312.875000\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -236713.343750\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -176620.718750\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -184503.812500\n",
      "    epoch          : 103\n",
      "    loss           : -202668.98125\n",
      "    val_loss       : -204199.1125\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -183243.062500\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -211253.984375\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -209232.765625\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -181394.468750\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -229096.562500\n",
      "    epoch          : 104\n",
      "    loss           : -204727.77875\n",
      "    val_loss       : -205662.1265625\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -189393.015625\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -191317.968750\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -198567.828125\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -190400.343750\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -238762.484375\n",
      "    epoch          : 105\n",
      "    loss           : -205907.76015625\n",
      "    val_loss       : -206655.17265625\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -216730.281250\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -190945.312500\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -182076.843750\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -184765.281250\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -232423.828125\n",
      "    epoch          : 106\n",
      "    loss           : -206710.0896875\n",
      "    val_loss       : -207680.191015625\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -213284.390625\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -180712.437500\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -199615.250000\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -213209.171875\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -188674.093750\n",
      "    epoch          : 107\n",
      "    loss           : -207792.42359375\n",
      "    val_loss       : -208730.726953125\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -238896.921875\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -186642.093750\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -212384.968750\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -218227.000000\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -229275.218750\n",
      "    epoch          : 108\n",
      "    loss           : -208868.875\n",
      "    val_loss       : -209770.74140625\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -201461.031250\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -191228.875000\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -183971.765625\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -194157.468750\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -192977.750000\n",
      "    epoch          : 109\n",
      "    loss           : -209785.4521875\n",
      "    val_loss       : -210607.96953125\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -186253.437500\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -222042.765625\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -243700.953125\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -195351.375000\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -233254.375000\n",
      "    epoch          : 110\n",
      "    loss           : -210683.6984375\n",
      "    val_loss       : -211629.245703125\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -229350.203125\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -221336.593750\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -190748.906250\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -214241.062500\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -234977.687500\n",
      "    epoch          : 111\n",
      "    loss           : -211433.80671875\n",
      "    val_loss       : -212117.553125\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -229338.968750\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -234243.734375\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -201081.625000\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -210256.859375\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -216187.640625\n",
      "    epoch          : 112\n",
      "    loss           : -210461.86390625\n",
      "    val_loss       : -212146.46328125\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -218395.718750\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -196266.781250\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -188230.562500\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -237306.750000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -234717.937500\n",
      "    epoch          : 113\n",
      "    loss           : -212324.18859375\n",
      "    val_loss       : -213028.708984375\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -190798.187500\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -190259.218750\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -245949.765625\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -237319.093750\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -237570.125000\n",
      "    epoch          : 114\n",
      "    loss           : -213334.8003125\n",
      "    val_loss       : -214871.377734375\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -221006.843750\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -207105.750000\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -199495.500000\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -217308.625000\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -238026.484375\n",
      "    epoch          : 115\n",
      "    loss           : -215022.01953125\n",
      "    val_loss       : -216020.382421875\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -206014.812500\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -226986.468750\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -248467.750000\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -208232.156250\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -235270.531250\n",
      "    epoch          : 116\n",
      "    loss           : -215983.89375\n",
      "    val_loss       : -216738.832421875\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -196438.562500\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -220617.640625\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -224510.781250\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -250707.859375\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -250172.234375\n",
      "    epoch          : 117\n",
      "    loss           : -216505.30859375\n",
      "    val_loss       : -217280.36640625\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -229372.718750\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -210661.031250\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -236513.812500\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -236846.531250\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -195645.281250\n",
      "    epoch          : 118\n",
      "    loss           : -217228.69078125\n",
      "    val_loss       : -218398.380078125\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -220253.062500\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -236542.546875\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -200908.218750\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -204678.406250\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -222092.125000\n",
      "    epoch          : 119\n",
      "    loss           : -218588.87015625\n",
      "    val_loss       : -219327.13671875\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -212244.171875\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -231834.843750\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -198868.750000\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -219599.625000\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -255354.234375\n",
      "    epoch          : 120\n",
      "    loss           : -219440.5796875\n",
      "    val_loss       : -220063.01953125\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -222806.531250\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -231374.000000\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -197617.718750\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -196678.000000\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -196712.109375\n",
      "    epoch          : 121\n",
      "    loss           : -219555.0653125\n",
      "    val_loss       : -220309.3\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -255265.250000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -203105.109375\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -245324.031250\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -240779.296875\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -256557.375000\n",
      "    epoch          : 122\n",
      "    loss           : -220424.36328125\n",
      "    val_loss       : -221652.298828125\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -226058.359375\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -256995.468750\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -198468.765625\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -200443.171875\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -201182.875000\n",
      "    epoch          : 123\n",
      "    loss           : -221668.154375\n",
      "    val_loss       : -222653.799609375\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -197994.968750\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -197598.890625\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -246523.921875\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -244169.500000\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -246048.343750\n",
      "    epoch          : 124\n",
      "    loss           : -222486.6765625\n",
      "    val_loss       : -223869.76953125\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -245426.218750\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -258498.062500\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -187212.906250\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -214301.812500\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -244772.328125\n",
      "    epoch          : 125\n",
      "    loss           : -223163.4084375\n",
      "    val_loss       : -223833.59765625\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -213679.781250\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -231136.437500\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -199853.437500\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -207503.750000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -201802.609375\n",
      "    epoch          : 126\n",
      "    loss           : -223285.33921875\n",
      "    val_loss       : -224657.94765625\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -261499.312500\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -260451.328125\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -244565.421875\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -203661.312500\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -204160.593750\n",
      "    epoch          : 127\n",
      "    loss           : -224190.72953125\n",
      "    val_loss       : -226097.91328125\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -261311.359375\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -225238.812500\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -247679.375000\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -203016.781250\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -201906.781250\n",
      "    epoch          : 128\n",
      "    loss           : -225157.235625\n",
      "    val_loss       : -226441.26484375\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -203185.187500\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -227317.687500\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -244792.656250\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -250197.343750\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -251422.968750\n",
      "    epoch          : 129\n",
      "    loss           : -226408.1609375\n",
      "    val_loss       : -227982.547265625\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -248635.562500\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -218096.359375\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -206746.203125\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -249915.281250\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -248145.500000\n",
      "    epoch          : 130\n",
      "    loss           : -227234.50609375\n",
      "    val_loss       : -228449.90859375\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -221160.500000\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -209761.843750\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -232765.218750\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -232313.531250\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -206458.468750\n",
      "    epoch          : 131\n",
      "    loss           : -227176.8128125\n",
      "    val_loss       : -228390.90859375\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -263650.312500\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -250173.968750\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -265686.937500\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -232050.109375\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -250438.640625\n",
      "    epoch          : 132\n",
      "    loss           : -228348.26796875\n",
      "    val_loss       : -230330.138671875\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -209628.875000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -261810.593750\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -205803.546875\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -215010.437500\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -205190.609375\n",
      "    epoch          : 133\n",
      "    loss           : -229326.61125\n",
      "    val_loss       : -230830.25\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -253596.312500\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -255603.937500\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -255328.843750\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -201716.843750\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -210711.937500\n",
      "    epoch          : 134\n",
      "    loss           : -230168.1034375\n",
      "    val_loss       : -231420.481640625\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -265930.875000\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -255531.125000\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -252220.875000\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -205501.265625\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -254241.843750\n",
      "    epoch          : 135\n",
      "    loss           : -231284.36046875\n",
      "    val_loss       : -232555.669921875\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -203696.953125\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -252873.531250\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -246747.781250\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -252899.796875\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -211633.875000\n",
      "    epoch          : 136\n",
      "    loss           : -231647.21828125\n",
      "    val_loss       : -232762.6125\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -253314.906250\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -271495.750000\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -215392.312500\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -204352.000000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -256274.937500\n",
      "    epoch          : 137\n",
      "    loss           : -232392.20140625\n",
      "    val_loss       : -232790.872265625\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -234492.062500\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -225459.906250\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -259544.890625\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -208347.484375\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -208300.687500\n",
      "    epoch          : 138\n",
      "    loss           : -233248.0940625\n",
      "    val_loss       : -234049.043359375\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -251423.718750\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -215493.671875\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -254950.843750\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -254570.843750\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -214452.250000\n",
      "    epoch          : 139\n",
      "    loss           : -234325.8403125\n",
      "    val_loss       : -235840.64921875\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -226669.156250\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -272125.375000\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -256916.171875\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -251508.781250\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -216177.390625\n",
      "    epoch          : 140\n",
      "    loss           : -235415.7615625\n",
      "    val_loss       : -236675.2953125\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -207508.609375\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -217649.687500\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -223756.906250\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -216597.500000\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -219085.531250\n",
      "    epoch          : 141\n",
      "    loss           : -236214.0715625\n",
      "    val_loss       : -237180.068359375\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -257756.015625\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -238271.750000\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -229619.281250\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -220708.765625\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -262008.218750\n",
      "    epoch          : 142\n",
      "    loss           : -236671.453125\n",
      "    val_loss       : -238037.328515625\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -218821.625000\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -209742.750000\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -254302.421875\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -240865.281250\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -261746.625000\n",
      "    epoch          : 143\n",
      "    loss           : -237354.3275\n",
      "    val_loss       : -236526.157421875\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -212438.468750\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -210976.171875\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -211606.765625\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -254933.484375\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -218231.953125\n",
      "    epoch          : 144\n",
      "    loss           : -236948.70515625\n",
      "    val_loss       : -239210.103125\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -263490.062500\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -259182.671875\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -212709.593750\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -220775.125000\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -220534.484375\n",
      "    epoch          : 145\n",
      "    loss           : -238849.1784375\n",
      "    val_loss       : -240199.491015625\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -264587.812500\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -219774.343750\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -229247.281250\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -240448.250000\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -215141.390625\n",
      "    epoch          : 146\n",
      "    loss           : -239824.47453125\n",
      "    val_loss       : -241156.923828125\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -231275.234375\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -209111.375000\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -221131.625000\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -258448.390625\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -258523.000000\n",
      "    epoch          : 147\n",
      "    loss           : -240397.25765625\n",
      "    val_loss       : -242178.4125\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -250099.250000\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -222846.156250\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -282643.187500\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -273849.812500\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -207861.390625\n",
      "    epoch          : 148\n",
      "    loss           : -239659.8478125\n",
      "    val_loss       : -241728.238671875\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -281031.156250\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -222040.812500\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -224210.078125\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -282349.531250\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -223342.406250\n",
      "    epoch          : 149\n",
      "    loss           : -242289.38703125\n",
      "    val_loss       : -243463.612109375\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -212841.906250\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -216882.484375\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -261968.921875\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -233428.046875\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -267666.531250\n",
      "    epoch          : 150\n",
      "    loss           : -242906.0940625\n",
      "    val_loss       : -243604.7328125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch150.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -213640.843750\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -225538.281250\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -246881.406250\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -231213.187500\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -245982.250000\n",
      "    epoch          : 151\n",
      "    loss           : -243388.5578125\n",
      "    val_loss       : -244351.19453125\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -227930.875000\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -262534.750000\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -246130.031250\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -217387.531250\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -284784.000000\n",
      "    epoch          : 152\n",
      "    loss           : -244075.2890625\n",
      "    val_loss       : -245604.03984375\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -224092.046875\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -226131.859375\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -227765.843750\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -245012.828125\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -227194.578125\n",
      "    epoch          : 153\n",
      "    loss           : -245361.1740625\n",
      "    val_loss       : -246133.17109375\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -266023.343750\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -268625.187500\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -272657.687500\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -219164.593750\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -271830.812500\n",
      "    epoch          : 154\n",
      "    loss           : -245674.11640625\n",
      "    val_loss       : -247239.639453125\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -265729.031250\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -254619.515625\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -217711.718750\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -225985.453125\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -231354.312500\n",
      "    epoch          : 155\n",
      "    loss           : -246607.24171875\n",
      "    val_loss       : -247612.0234375\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -217623.218750\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -236795.187500\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -265497.125000\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -265114.218750\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -263906.968750\n",
      "    epoch          : 156\n",
      "    loss           : -246419.30015625\n",
      "    val_loss       : -247192.31875\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -285080.812500\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -267457.875000\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -231646.625000\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -226565.812500\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -268599.656250\n",
      "    epoch          : 157\n",
      "    loss           : -247504.01921875\n",
      "    val_loss       : -249410.625390625\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -274672.656250\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -220887.265625\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -268352.375000\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -268482.687500\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -221155.968750\n",
      "    epoch          : 158\n",
      "    loss           : -248544.143125\n",
      "    val_loss       : -249814.975390625\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -228618.156250\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -227294.750000\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -236967.937500\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -273113.343750\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -223987.875000\n",
      "    epoch          : 159\n",
      "    loss           : -249570.82703125\n",
      "    val_loss       : -250711.78203125\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -228639.343750\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -270375.562500\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -276670.375000\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -228994.031250\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -272375.156250\n",
      "    epoch          : 160\n",
      "    loss           : -250104.66375\n",
      "    val_loss       : -251408.823828125\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -256597.062500\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -234470.015625\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -271755.937500\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -235018.093750\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -274968.687500\n",
      "    epoch          : 161\n",
      "    loss           : -250490.4953125\n",
      "    val_loss       : -252085.971484375\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -229222.406250\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -271623.093750\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -280224.937500\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -219579.343750\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -235075.531250\n",
      "    epoch          : 162\n",
      "    loss           : -251737.95109375\n",
      "    val_loss       : -252953.315234375\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -270900.625000\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -222445.593750\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -274816.156250\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -228857.390625\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -272873.375000\n",
      "    epoch          : 163\n",
      "    loss           : -251941.9565625\n",
      "    val_loss       : -251955.08046875\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -221484.187500\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -222877.750000\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -273742.750000\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -228923.156250\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -278023.125000\n",
      "    epoch          : 164\n",
      "    loss           : -252666.7496875\n",
      "    val_loss       : -253863.038671875\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -225022.640625\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -295399.625000\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -234748.953125\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -253697.171875\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -278094.406250\n",
      "    epoch          : 165\n",
      "    loss           : -252935.24453125\n",
      "    val_loss       : -252764.1609375\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -220994.484375\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -273282.750000\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -221948.937500\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -274234.593750\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -226900.343750\n",
      "    epoch          : 166\n",
      "    loss           : -253711.23046875\n",
      "    val_loss       : -255570.025\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -224035.000000\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -297754.281250\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -244998.890625\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -225826.328125\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -275533.375000\n",
      "    epoch          : 167\n",
      "    loss           : -255285.87453125\n",
      "    val_loss       : -255688.396484375\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -263913.593750\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -224808.265625\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -230830.843750\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -239952.015625\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -274288.906250\n",
      "    epoch          : 168\n",
      "    loss           : -253904.39453125\n",
      "    val_loss       : -256382.61171875\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -259452.687500\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -232967.718750\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -222082.000000\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -283427.468750\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -278975.312500\n",
      "    epoch          : 169\n",
      "    loss           : -256488.183125\n",
      "    val_loss       : -258019.3765625\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -278633.812500\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -278068.281250\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -223395.046875\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -225266.765625\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -227768.218750\n",
      "    epoch          : 170\n",
      "    loss           : -257367.363125\n",
      "    val_loss       : -258774.087890625\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -280125.718750\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -233729.640625\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -283537.437500\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -257731.531250\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -244422.953125\n",
      "    epoch          : 171\n",
      "    loss           : -258152.0165625\n",
      "    val_loss       : -259404.5296875\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -231318.640625\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -232745.343750\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -280609.562500\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -281015.781250\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -233471.875000\n",
      "    epoch          : 172\n",
      "    loss           : -258822.7303125\n",
      "    val_loss       : -260208.803515625\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -258212.625000\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -301710.000000\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -281061.750000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -250915.015625\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -286237.187500\n",
      "    epoch          : 173\n",
      "    loss           : -259302.07984375\n",
      "    val_loss       : -260531.94921875\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -245401.875000\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -280157.687500\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -280747.500000\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -261381.421875\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -227508.546875\n",
      "    epoch          : 174\n",
      "    loss           : -259299.72125\n",
      "    val_loss       : -260263.944140625\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -230659.125000\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -236998.984375\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -233336.500000\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -259745.468750\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -246877.656250\n",
      "    epoch          : 175\n",
      "    loss           : -259549.92484375\n",
      "    val_loss       : -261155.4875\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -259584.000000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -245946.343750\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -283359.781250\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -246993.000000\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -287018.343750\n",
      "    epoch          : 176\n",
      "    loss           : -260134.64640625\n",
      "    val_loss       : -262227.386328125\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -283036.562500\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -235103.062500\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -241103.937500\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -247291.718750\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -260353.265625\n",
      "    epoch          : 177\n",
      "    loss           : -261758.9184375\n",
      "    val_loss       : -263013.978125\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -283356.312500\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -283587.187500\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -233967.531250\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -306422.312500\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -233629.843750\n",
      "    epoch          : 178\n",
      "    loss           : -262571.4065625\n",
      "    val_loss       : -261352.458203125\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -225509.484375\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -231686.734375\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -249262.531250\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -249533.921875\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -290192.906250\n",
      "    epoch          : 179\n",
      "    loss           : -262936.41546875\n",
      "    val_loss       : -264349.999609375\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -291601.187500\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -263742.718750\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -233839.500000\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -254412.750000\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -233373.343750\n",
      "    epoch          : 180\n",
      "    loss           : -263980.30328125\n",
      "    val_loss       : -265231.65546875\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -287333.656250\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -287802.062500\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -310326.812500\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -252069.125000\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -250534.875000\n",
      "    epoch          : 181\n",
      "    loss           : -264316.25015625\n",
      "    val_loss       : -265713.93515625\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -309409.687500\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -288579.781250\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -249839.421875\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -252890.359375\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -288256.656250\n",
      "    epoch          : 182\n",
      "    loss           : -264880.1096875\n",
      "    val_loss       : -266921.901171875\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -239069.343750\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -233440.187500\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -285817.468750\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -289829.937500\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -220931.640625\n",
      "    epoch          : 183\n",
      "    loss           : -266071.1903125\n",
      "    val_loss       : -267458.25546875\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -288309.312500\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -310216.500000\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -240244.875000\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -265990.187500\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -234457.125000\n",
      "    epoch          : 184\n",
      "    loss           : -266659.59921875\n",
      "    val_loss       : -267905.33828125\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -265665.218750\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -312489.812500\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -255063.765625\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -253885.234375\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -288903.250000\n",
      "    epoch          : 185\n",
      "    loss           : -266859.84328125\n",
      "    val_loss       : -268142.147265625\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -255760.484375\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -241224.625000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -294808.625000\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -238536.734375\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -274153.218750\n",
      "    epoch          : 186\n",
      "    loss           : -267524.35203125\n",
      "    val_loss       : -269084.13671875\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -235249.515625\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -290329.250000\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -266454.062500\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -239658.421875\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -293048.000000\n",
      "    epoch          : 187\n",
      "    loss           : -268748.5490625\n",
      "    val_loss       : -269860.187890625\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -291200.125000\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -256261.593750\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -300181.750000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -295689.687500\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -234690.281250\n",
      "    epoch          : 188\n",
      "    loss           : -268897.009375\n",
      "    val_loss       : -270326.812890625\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -298134.312500\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -294318.062500\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -235664.265625\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -256852.625000\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -292831.562500\n",
      "    epoch          : 189\n",
      "    loss           : -269140.12359375\n",
      "    val_loss       : -269725.4765625\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -257077.734375\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -314185.437500\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -246093.171875\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -259215.625000\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -253823.671875\n",
      "    epoch          : 190\n",
      "    loss           : -270157.44609375\n",
      "    val_loss       : -271825.86328125\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -291242.687500\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -236455.218750\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -249469.828125\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -299240.343750\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -258943.875000\n",
      "    epoch          : 191\n",
      "    loss           : -271115.58125\n",
      "    val_loss       : -272886.269921875\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -240534.156250\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -240237.375000\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -244575.281250\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -254953.718750\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -299865.156250\n",
      "    epoch          : 192\n",
      "    loss           : -271858.67546875\n",
      "    val_loss       : -272312.5453125\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -246091.625000\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -297855.437500\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -259287.750000\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -293871.281250\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -295368.062500\n",
      "    epoch          : 193\n",
      "    loss           : -272059.073125\n",
      "    val_loss       : -273409.647265625\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -237936.421875\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -237511.968750\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -318079.750000\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -239621.500000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -261704.500000\n",
      "    epoch          : 194\n",
      "    loss           : -273076.3453125\n",
      "    val_loss       : -274213.610546875\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -298561.625000\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -239896.718750\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -302701.468750\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -317744.343750\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -293234.812500\n",
      "    epoch          : 195\n",
      "    loss           : -272247.59875\n",
      "    val_loss       : -274698.19140625\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -240586.406250\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -239615.406250\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -247641.609375\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -271287.000000\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -295587.937500\n",
      "    epoch          : 196\n",
      "    loss           : -273923.7740625\n",
      "    val_loss       : -275737.6484375\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -263911.968750\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -243077.156250\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -262599.343750\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -259763.515625\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -296971.718750\n",
      "    epoch          : 197\n",
      "    loss           : -275004.2359375\n",
      "    val_loss       : -276545.57734375\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -243572.781250\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -320889.718750\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -224510.687500\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -245691.687500\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -297281.812500\n",
      "    epoch          : 198\n",
      "    loss           : -275650.02484375\n",
      "    val_loss       : -276949.155078125\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -245513.875000\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -252456.031250\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -265908.625000\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -246033.750000\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -298766.625000\n",
      "    epoch          : 199\n",
      "    loss           : -276380.76171875\n",
      "    val_loss       : -277609.062109375\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -246362.468750\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -241664.875000\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -324366.687500\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -296430.000000\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -303153.093750\n",
      "    epoch          : 200\n",
      "    loss           : -276372.5503125\n",
      "    val_loss       : -277480.005859375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -251027.375000\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -263188.250000\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -248834.187500\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -243935.515625\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -245279.218750\n",
      "    epoch          : 201\n",
      "    loss           : -276330.62296875\n",
      "    val_loss       : -277983.293359375\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -243707.031250\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -307820.312500\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -302598.875000\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -299739.187500\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -265593.781250\n",
      "    epoch          : 202\n",
      "    loss           : -276835.3909375\n",
      "    val_loss       : -279076.699609375\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -308528.562500\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -247620.687500\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -320265.843750\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -263368.187500\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -269216.562500\n",
      "    epoch          : 203\n",
      "    loss           : -277743.63625\n",
      "    val_loss       : -279384.160546875\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -253549.312500\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -307116.375000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -304395.125000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -304960.531250\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -269632.281250\n",
      "    epoch          : 204\n",
      "    loss           : -279355.6878125\n",
      "    val_loss       : -280581.34609375\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -277722.062500\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -269502.781250\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -293233.750000\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -255362.343750\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -250219.703125\n",
      "    epoch          : 205\n",
      "    loss           : -280042.77859375\n",
      "    val_loss       : -280904.913671875\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -327330.687500\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -289812.718750\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -326238.812500\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -247510.937500\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -310619.500000\n",
      "    epoch          : 206\n",
      "    loss           : -279685.465\n",
      "    val_loss       : -280431.39140625\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -262353.062500\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -288392.843750\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -250040.937500\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -255127.031250\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -309372.343750\n",
      "    epoch          : 207\n",
      "    loss           : -280153.65046875\n",
      "    val_loss       : -281530.265234375\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -247637.156250\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -246457.000000\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -310124.687500\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -255242.546875\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -245482.312500\n",
      "    epoch          : 208\n",
      "    loss           : -281071.4553125\n",
      "    val_loss       : -281231.982421875\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -249383.296875\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -247226.531250\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -258002.750000\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -311461.000000\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -272764.031250\n",
      "    epoch          : 209\n",
      "    loss           : -281512.64921875\n",
      "    val_loss       : -283527.439453125\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -310449.468750\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -250875.734375\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -280149.500000\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -330192.593750\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -251857.187500\n",
      "    epoch          : 210\n",
      "    loss           : -282979.0240625\n",
      "    val_loss       : -284387.436328125\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -310656.437500\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -309937.968750\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -249032.203125\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -303666.656250\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -274097.937500\n",
      "    epoch          : 211\n",
      "    loss           : -283365.3578125\n",
      "    val_loss       : -285162.880078125\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -311115.250000\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -312539.406250\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -311856.687500\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -330060.593750\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -332101.562500\n",
      "    epoch          : 212\n",
      "    loss           : -284001.32203125\n",
      "    val_loss       : -285238.3265625\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -264947.531250\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -249650.625000\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -304769.750000\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -261020.375000\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -284039.968750\n",
      "    epoch          : 213\n",
      "    loss           : -284100.03328125\n",
      "    val_loss       : -285527.72421875\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -310757.500000\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -250326.859375\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -247595.812500\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -250973.859375\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -315527.750000\n",
      "    epoch          : 214\n",
      "    loss           : -285391.05484375\n",
      "    val_loss       : -286179.93671875\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -313036.312500\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -253710.187500\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -251032.937500\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -277379.562500\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -249951.093750\n",
      "    epoch          : 215\n",
      "    loss           : -285788.17796875\n",
      "    val_loss       : -287359.92578125\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -237207.500000\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -259040.500000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -314176.281250\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -249256.343750\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -254365.781250\n",
      "    epoch          : 216\n",
      "    loss           : -285911.70953125\n",
      "    val_loss       : -287172.71640625\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -247019.718750\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -276579.875000\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -314793.093750\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -275421.500000\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -252727.031250\n",
      "    epoch          : 217\n",
      "    loss           : -286642.77515625\n",
      "    val_loss       : -288072.4953125\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -259410.531250\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -250786.343750\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -283281.687500\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -309613.218750\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -254363.734375\n",
      "    epoch          : 218\n",
      "    loss           : -286902.05828125\n",
      "    val_loss       : -288825.495703125\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -253136.937500\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -254677.937500\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -316912.218750\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -262411.000000\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -311297.781250\n",
      "    epoch          : 219\n",
      "    loss           : -288435.18453125\n",
      "    val_loss       : -289449.2453125\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -270034.250000\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -284730.687500\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -254456.187500\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -310194.875000\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -338311.187500\n",
      "    epoch          : 220\n",
      "    loss           : -287861.4346875\n",
      "    val_loss       : -288306.8171875\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -317938.156250\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -274219.187500\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -338052.781250\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -316278.250000\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -279807.218750\n",
      "    epoch          : 221\n",
      "    loss           : -287520.17390625\n",
      "    val_loss       : -289584.8953125\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -317923.031250\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -317204.156250\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -262864.406250\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -254010.812500\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -276454.531250\n",
      "    epoch          : 222\n",
      "    loss           : -289306.1809375\n",
      "    val_loss       : -290502.132421875\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -253026.375000\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -310072.031250\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -319821.093750\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -283917.562500\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -283739.875000\n",
      "    epoch          : 223\n",
      "    loss           : -290113.82578125\n",
      "    val_loss       : -291780.069921875\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -320673.687500\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -259960.406250\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -282705.218750\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -277173.093750\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -323816.031250\n",
      "    epoch          : 224\n",
      "    loss           : -290989.56671875\n",
      "    val_loss       : -292333.2234375\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -253745.906250\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -282819.656250\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -340971.562500\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -319155.875000\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -284811.656250\n",
      "    epoch          : 225\n",
      "    loss           : -291696.77671875\n",
      "    val_loss       : -292984.171875\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -288164.375000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -263320.187500\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -282845.625000\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -322573.812500\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -322337.062500\n",
      "    epoch          : 226\n",
      "    loss           : -292438.806875\n",
      "    val_loss       : -293503.6375\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -257339.156250\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -260976.062500\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -291350.093750\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -324333.718750\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -324941.750000\n",
      "    epoch          : 227\n",
      "    loss           : -292672.22921875\n",
      "    val_loss       : -294101.36484375\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -266217.468750\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -259123.125000\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -289896.343750\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -254387.375000\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -256991.609375\n",
      "    epoch          : 228\n",
      "    loss           : -293333.69609375\n",
      "    val_loss       : -294005.4640625\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -256843.781250\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -258686.687500\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -291414.562500\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -315059.656250\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -286920.062500\n",
      "    epoch          : 229\n",
      "    loss           : -293249.68765625\n",
      "    val_loss       : -294873.50859375\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -319011.531250\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -291051.062500\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -324262.343750\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -323654.187500\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -325546.531250\n",
      "    epoch          : 230\n",
      "    loss           : -294311.34546875\n",
      "    val_loss       : -295877.57890625\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -288026.906250\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -281516.250000\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -279158.062500\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -325583.156250\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -288917.000000\n",
      "    epoch          : 231\n",
      "    loss           : -294906.9103125\n",
      "    val_loss       : -296462.95703125\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -264762.375000\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -327641.312500\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -286608.500000\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -320452.437500\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -260746.343750\n",
      "    epoch          : 232\n",
      "    loss           : -293166.54734375\n",
      "    val_loss       : -296652.85390625\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -280369.125000\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -263384.656250\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -289222.937500\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -288572.875000\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -319638.562500\n",
      "    epoch          : 233\n",
      "    loss           : -295954.35515625\n",
      "    val_loss       : -297082.10390625\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -281160.750000\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -260776.578125\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -261099.265625\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -262746.125000\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -325991.281250\n",
      "    epoch          : 234\n",
      "    loss           : -295864.48515625\n",
      "    val_loss       : -297491.13515625\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -328616.500000\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -319164.812500\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -318594.062500\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -268063.875000\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -237842.859375\n",
      "    epoch          : 235\n",
      "    loss           : -296687.01203125\n",
      "    val_loss       : -297654.82109375\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -288191.125000\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -259546.218750\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -347233.031250\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -259955.562500\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -257913.656250\n",
      "    epoch          : 236\n",
      "    loss           : -296198.759375\n",
      "    val_loss       : -295979.228125\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -277614.656250\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -351923.718750\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -280795.843750\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -267818.531250\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -293852.093750\n",
      "    epoch          : 237\n",
      "    loss           : -297351.74859375\n",
      "    val_loss       : -299567.859375\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -292669.562500\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -291207.656250\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -330122.125000\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -283448.093750\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -316939.750000\n",
      "    epoch          : 238\n",
      "    loss           : -298577.17328125\n",
      "    val_loss       : -299859.86015625\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -329690.125000\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -294180.937500\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -293452.187500\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -256136.531250\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -322809.718750\n",
      "    epoch          : 239\n",
      "    loss           : -299603.3446875\n",
      "    val_loss       : -300855.73046875\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -265347.875000\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -262518.500000\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -284764.593750\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -320520.093750\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -294356.218750\n",
      "    epoch          : 240\n",
      "    loss           : -299442.9978125\n",
      "    val_loss       : -300673.5671875\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -271134.000000\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -285323.187500\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -334440.968750\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -293311.625000\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -333057.500000\n",
      "    epoch          : 241\n",
      "    loss           : -299901.9846875\n",
      "    val_loss       : -301133.82421875\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -350507.906250\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -352333.187500\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -349102.093750\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -263803.125000\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -267911.843750\n",
      "    epoch          : 242\n",
      "    loss           : -300057.87984375\n",
      "    val_loss       : -302300.4171875\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -295919.375000\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -296003.031250\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -295418.375000\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -264155.312500\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -322367.718750\n",
      "    epoch          : 243\n",
      "    loss           : -300858.04984375\n",
      "    val_loss       : -300715.02421875\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -290237.750000\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -294078.781250\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -259815.640625\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -314631.687500\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -262339.718750\n",
      "    epoch          : 244\n",
      "    loss           : -298368.8528125\n",
      "    val_loss       : -302580.37734375\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -296985.593750\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -268136.531250\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -323916.812500\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -297746.937500\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -299247.562500\n",
      "    epoch          : 245\n",
      "    loss           : -302157.9784375\n",
      "    val_loss       : -303738.07890625\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -266151.093750\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -297736.937500\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -284100.593750\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -266258.156250\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -267193.343750\n",
      "    epoch          : 246\n",
      "    loss           : -302775.67859375\n",
      "    val_loss       : -304186.7921875\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -325813.343750\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -286647.000000\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -354055.125000\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -338413.343750\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -268939.812500\n",
      "    epoch          : 247\n",
      "    loss           : -303066.3215625\n",
      "    val_loss       : -304702.49296875\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -296308.500000\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -264781.625000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -356389.250000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -295646.218750\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -297439.781250\n",
      "    epoch          : 248\n",
      "    loss           : -304262.725\n",
      "    val_loss       : -305479.790625\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -273647.187500\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -266285.812500\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -287645.250000\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -271906.062500\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -332892.031250\n",
      "    epoch          : 249\n",
      "    loss           : -304169.61109375\n",
      "    val_loss       : -304205.71328125\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -269251.812500\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -337539.562500\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -285805.000000\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -334349.843750\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -267262.468750\n",
      "    epoch          : 250\n",
      "    loss           : -303541.51421875\n",
      "    val_loss       : -304998.0703125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -354604.312500\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -356093.687500\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -354513.125000\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -332367.312500\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -268338.625000\n",
      "    epoch          : 251\n",
      "    loss           : -303481.31453125\n",
      "    val_loss       : -304185.33359375\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -357368.406250\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -286246.718750\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -302849.000000\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -297757.812500\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -353945.500000\n",
      "    epoch          : 252\n",
      "    loss           : -305327.2084375\n",
      "    val_loss       : -307023.56796875\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -268392.906250\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -276283.593750\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -246232.828125\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -327057.375000\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -300124.468750\n",
      "    epoch          : 253\n",
      "    loss           : -306627.57015625\n",
      "    val_loss       : -307596.1015625\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -339927.312500\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -257658.406250\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -276079.375000\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -328637.406250\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -271430.000000\n",
      "    epoch          : 254\n",
      "    loss           : -307188.57203125\n",
      "    val_loss       : -308823.2421875\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -330389.343750\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -300748.000000\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -341330.906250\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -278293.625000\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -337411.250000\n",
      "    epoch          : 255\n",
      "    loss           : -307708.56359375\n",
      "    val_loss       : -309055.5546875\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -266680.531250\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -357427.781250\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -302319.000000\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -298403.406250\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -327251.437500\n",
      "    epoch          : 256\n",
      "    loss           : -307283.38671875\n",
      "    val_loss       : -308103.40234375\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -360271.687500\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -359818.843750\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -260665.046875\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -302737.656250\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -300723.625000\n",
      "    epoch          : 257\n",
      "    loss           : -308518.20046875\n",
      "    val_loss       : -310155.51015625\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -360949.781250\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -250024.125000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -306350.218750\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -291277.781250\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -334015.093750\n",
      "    epoch          : 258\n",
      "    loss           : -309201.125\n",
      "    val_loss       : -310817.89296875\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -304362.187500\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -282775.875000\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -306642.375000\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -305598.781250\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -300666.000000\n",
      "    epoch          : 259\n",
      "    loss           : -309726.4059375\n",
      "    val_loss       : -311358.81171875\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -363112.500000\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -342208.218750\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -342905.250000\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -267973.531250\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -307712.062500\n",
      "    epoch          : 260\n",
      "    loss           : -309944.2925\n",
      "    val_loss       : -311712.64921875\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -340544.968750\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -363244.093750\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -276758.312500\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -273629.156250\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -333544.625000\n",
      "    epoch          : 261\n",
      "    loss           : -310473.464375\n",
      "    val_loss       : -311667.24765625\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -302971.250000\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -280879.000000\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -281211.593750\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -296174.687500\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -308770.718750\n",
      "    epoch          : 262\n",
      "    loss           : -310858.97125\n",
      "    val_loss       : -312025.93046875\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -280842.625000\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -343565.500000\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -308485.875000\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -296026.156250\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -334560.937500\n",
      "    epoch          : 263\n",
      "    loss           : -311382.86234375\n",
      "    val_loss       : -312816.5921875\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -337065.625000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -298351.000000\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -280743.625000\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -345861.906250\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -271585.656250\n",
      "    epoch          : 264\n",
      "    loss           : -312469.9684375\n",
      "    val_loss       : -313861.390625\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -291527.468750\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -347831.500000\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -335458.437500\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -310235.906250\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -340657.218750\n",
      "    epoch          : 265\n",
      "    loss           : -312533.433125\n",
      "    val_loss       : -312932.1859375\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -364836.625000\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -305049.312500\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -273863.937500\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -336833.281250\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -293620.250000\n",
      "    epoch          : 266\n",
      "    loss           : -312431.34578125\n",
      "    val_loss       : -314219.52578125\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -346949.750000\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -367999.250000\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -348683.687500\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -296460.937500\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -272135.312500\n",
      "    epoch          : 267\n",
      "    loss           : -313343.7165625\n",
      "    val_loss       : -315036.69609375\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -328580.437500\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -349110.187500\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -347694.593750\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -366324.875000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -306507.937500\n",
      "    epoch          : 268\n",
      "    loss           : -312857.45453125\n",
      "    val_loss       : -313247.4578125\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -278598.531250\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -278904.781250\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -293602.062500\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -281668.468750\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -344752.000000\n",
      "    epoch          : 269\n",
      "    loss           : -312717.685\n",
      "    val_loss       : -315803.515625\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -306404.687500\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -348989.062500\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -298933.500000\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -346634.875000\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -335582.187500\n",
      "    epoch          : 270\n",
      "    loss           : -315047.1240625\n",
      "    val_loss       : -316900.05\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -294980.812500\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -274429.843750\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -279522.531250\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -275746.718750\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -310467.437500\n",
      "    epoch          : 271\n",
      "    loss           : -315953.719375\n",
      "    val_loss       : -316785.515625\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -333847.656250\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -370681.750000\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -276142.156250\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -278751.500000\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -279568.781250\n",
      "    epoch          : 272\n",
      "    loss           : -315800.3696875\n",
      "    val_loss       : -317190.2609375\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -301690.468750\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -298518.375000\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -272671.625000\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -275961.250000\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -274695.375000\n",
      "    epoch          : 273\n",
      "    loss           : -316542.360625\n",
      "    val_loss       : -317859.1234375\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -283804.156250\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -339821.687500\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -274317.625000\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -346972.656250\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -336659.718750\n",
      "    epoch          : 274\n",
      "    loss           : -316049.86546875\n",
      "    val_loss       : -317960.1546875\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -347980.125000\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -349058.437500\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -371309.062500\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -308328.687500\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -274258.437500\n",
      "    epoch          : 275\n",
      "    loss           : -316940.5925\n",
      "    val_loss       : -318406.02265625\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -368839.250000\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -352540.125000\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -273254.062500\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -351897.187500\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -346169.500000\n",
      "    epoch          : 276\n",
      "    loss           : -316806.480625\n",
      "    val_loss       : -318884.7859375\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -318612.687500\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -299594.281250\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -308670.562500\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -335594.500000\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -314867.125000\n",
      "    epoch          : 277\n",
      "    loss           : -315474.5178125\n",
      "    val_loss       : -316930.22734375\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -352498.875000\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -315856.687500\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -340364.375000\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -341579.750000\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -318036.843750\n",
      "    epoch          : 278\n",
      "    loss           : -318578.145625\n",
      "    val_loss       : -320242.10625\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -301239.968750\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -279459.750000\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -318534.375000\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -320490.656250\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -341284.312500\n",
      "    epoch          : 279\n",
      "    loss           : -319527.7859375\n",
      "    val_loss       : -321305.60625\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -277055.000000\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -353762.156250\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -304302.562500\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -371244.468750\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -355662.937500\n",
      "    epoch          : 280\n",
      "    loss           : -319839.7603125\n",
      "    val_loss       : -321090.68125\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -280180.750000\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -286908.468750\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -301320.125000\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -278079.812500\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -374588.875000\n",
      "    epoch          : 281\n",
      "    loss           : -319707.599375\n",
      "    val_loss       : -319707.553125\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -281476.718750\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -275776.000000\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -286079.031250\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -308460.968750\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -374527.062500\n",
      "    epoch          : 282\n",
      "    loss           : -320068.2528125\n",
      "    val_loss       : -322635.3171875\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -311881.375000\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -314223.125000\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -286412.062500\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -275841.437500\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -354539.781250\n",
      "    epoch          : 283\n",
      "    loss           : -321378.758125\n",
      "    val_loss       : -322990.28046875\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -375058.562500\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -281910.531250\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -306113.625000\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -292177.437500\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -344262.656250\n",
      "    epoch          : 284\n",
      "    loss           : -321775.4953125\n",
      "    val_loss       : -323034.80234375\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -354192.406250\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -284482.437500\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -287453.656250\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -344144.812500\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -344329.937500\n",
      "    epoch          : 285\n",
      "    loss           : -321790.751875\n",
      "    val_loss       : -323526.41328125\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -359725.906250\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -292363.500000\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -379248.281250\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -355440.125000\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -280900.312500\n",
      "    epoch          : 286\n",
      "    loss           : -322315.55546875\n",
      "    val_loss       : -322457.834375\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -360798.312500\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -378064.937500\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -281717.687500\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -355133.906250\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -282997.687500\n",
      "    epoch          : 287\n",
      "    loss           : -322062.52953125\n",
      "    val_loss       : -324289.7921875\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -283543.781250\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -290093.687500\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -357528.218750\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -346602.968750\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -345090.000000\n",
      "    epoch          : 288\n",
      "    loss           : -323421.41640625\n",
      "    val_loss       : -324338.3109375\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -303017.093750\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -283471.281250\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -279566.312500\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -360023.687500\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -359357.375000\n",
      "    epoch          : 289\n",
      "    loss           : -323464.881875\n",
      "    val_loss       : -325234.13984375\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -282011.906250\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -359745.531250\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -378930.437500\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -285026.562500\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -260440.750000\n",
      "    epoch          : 290\n",
      "    loss           : -323914.2678125\n",
      "    val_loss       : -325630.80546875\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -317774.968750\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -380486.375000\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -324388.125000\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -309113.093750\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -295283.812500\n",
      "    epoch          : 291\n",
      "    loss           : -325147.1325\n",
      "    val_loss       : -326386.83203125\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -285044.906250\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -285301.375000\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -316374.187500\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -321581.500000\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -340231.406250\n",
      "    epoch          : 292\n",
      "    loss           : -323068.4909375\n",
      "    val_loss       : -324709.25\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -382814.281250\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -361613.156250\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -357491.062500\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -361435.375000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -288821.656250\n",
      "    epoch          : 293\n",
      "    loss           : -325133.255625\n",
      "    val_loss       : -326631.16328125\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -347319.718750\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -347938.218750\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -295122.625000\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -299160.062500\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -268017.125000\n",
      "    epoch          : 294\n",
      "    loss           : -326029.4759375\n",
      "    val_loss       : -327978.4984375\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -362902.281250\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -305657.562500\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -317602.812500\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -262865.718750\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -278827.093750\n",
      "    epoch          : 295\n",
      "    loss           : -324444.3171875\n",
      "    val_loss       : -326683.58046875\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -310878.625000\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -327306.031250\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -325714.531250\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -298003.562500\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -349468.406250\n",
      "    epoch          : 296\n",
      "    loss           : -326405.3634375\n",
      "    val_loss       : -328567.71796875\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -349714.750000\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -292502.500000\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -328515.125000\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -365321.812500\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -288760.437500\n",
      "    epoch          : 297\n",
      "    loss           : -327468.610625\n",
      "    val_loss       : -329150.99140625\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -309155.750000\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -365413.062500\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -322177.968750\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -286593.000000\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -282027.187500\n",
      "    epoch          : 298\n",
      "    loss           : -328141.2275\n",
      "    val_loss       : -328999.66328125\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -383148.781250\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -285234.500000\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -300293.062500\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -284444.406250\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -284625.000000\n",
      "    epoch          : 299\n",
      "    loss           : -328203.9615625\n",
      "    val_loss       : -329760.51875\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -355680.062500\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -350355.125000\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -363357.500000\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -348668.500000\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -281624.156250\n",
      "    epoch          : 300\n",
      "    loss           : -328560.510625\n",
      "    val_loss       : -329973.28828125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch300.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -367236.406250\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -366103.656250\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -320182.000000\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -366877.250000\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -319813.062500\n",
      "    epoch          : 301\n",
      "    loss           : -328421.4728125\n",
      "    val_loss       : -330342.546875\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -324358.781250\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -332234.312500\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -260606.218750\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -366485.375000\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -347744.656250\n",
      "    epoch          : 302\n",
      "    loss           : -328707.56125\n",
      "    val_loss       : -329331.4609375\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -293149.531250\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -367415.781250\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -309934.906250\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -352126.375000\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -365271.406250\n",
      "    epoch          : 303\n",
      "    loss           : -329717.230625\n",
      "    val_loss       : -331674.5234375\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -298661.562500\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -293853.656250\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -290114.875000\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -309072.000000\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -335065.750000\n",
      "    epoch          : 304\n",
      "    loss           : -331110.3559375\n",
      "    val_loss       : -332147.19296875\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -367988.187500\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -287557.687500\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -298013.750000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -354420.343750\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -354344.750000\n",
      "    epoch          : 305\n",
      "    loss           : -331657.85\n",
      "    val_loss       : -332869.2109375\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -370461.000000\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -349116.937500\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -333476.000000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -333437.250000\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -353016.937500\n",
      "    epoch          : 306\n",
      "    loss           : -331582.6571875\n",
      "    val_loss       : -332702.39296875\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -357587.156250\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -353857.062500\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -314149.750000\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -352840.843750\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -289628.343750\n",
      "    epoch          : 307\n",
      "    loss           : -332143.3815625\n",
      "    val_loss       : -333413.1375\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -370334.937500\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -370541.500000\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -353009.156250\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -357609.625000\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -367824.062500\n",
      "    epoch          : 308\n",
      "    loss           : -332451.8575\n",
      "    val_loss       : -334147.83828125\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -290166.312500\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -314610.625000\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -344423.593750\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -324628.437500\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -354326.875000\n",
      "    epoch          : 309\n",
      "    loss           : -332587.0865625\n",
      "    val_loss       : -334631.76796875\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -356458.218750\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -347238.218750\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -336062.562500\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -368189.875000\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -333804.093750\n",
      "    epoch          : 310\n",
      "    loss           : -331681.97\n",
      "    val_loss       : -334298.32734375\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -354434.312500\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -336357.000000\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -389413.937500\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -357708.750000\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -293031.000000\n",
      "    epoch          : 311\n",
      "    loss           : -333303.249375\n",
      "    val_loss       : -335075.16484375\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -312929.593750\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -388708.687500\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -394362.093750\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -293410.125000\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -288208.375000\n",
      "    epoch          : 312\n",
      "    loss           : -333848.3903125\n",
      "    val_loss       : -335751.125\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -290292.937500\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -358255.375000\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -372968.281250\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -357960.468750\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -368359.156250\n",
      "    epoch          : 313\n",
      "    loss           : -334493.2025\n",
      "    val_loss       : -335970.22421875\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -300334.562500\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -374223.750000\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -289887.500000\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -325346.468750\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -294466.687500\n",
      "    epoch          : 314\n",
      "    loss           : -335148.251875\n",
      "    val_loss       : -336597.56328125\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -290103.437500\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -356818.625000\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -373264.468750\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -371895.531250\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -323479.281250\n",
      "    epoch          : 315\n",
      "    loss           : -335147.7909375\n",
      "    val_loss       : -336507.32421875\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -287404.250000\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -313757.906250\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -374814.750000\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -376105.968750\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -315481.187500\n",
      "    epoch          : 316\n",
      "    loss           : -335569.4840625\n",
      "    val_loss       : -337043.4609375\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -317873.093750\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -315113.000000\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -302521.468750\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -396049.500000\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -288721.031250\n",
      "    epoch          : 317\n",
      "    loss           : -335230.120625\n",
      "    val_loss       : -335639.71796875\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -285803.656250\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -372818.593750\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -328751.687500\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -357920.843750\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -339835.937500\n",
      "    epoch          : 318\n",
      "    loss           : -336355.015\n",
      "    val_loss       : -338137.821875\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -290959.750000\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -326787.500000\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -377079.093750\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -360490.562500\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -317900.218750\n",
      "    epoch          : 319\n",
      "    loss           : -336525.564375\n",
      "    val_loss       : -337639.99296875\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -353125.843750\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -376227.625000\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -292765.187500\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -290176.843750\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -396190.000000\n",
      "    epoch          : 320\n",
      "    loss           : -337589.4059375\n",
      "    val_loss       : -339033.01796875\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -377495.093750\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -319922.156250\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -296978.437500\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -342897.375000\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -297882.125000\n",
      "    epoch          : 321\n",
      "    loss           : -338295.3525\n",
      "    val_loss       : -340045.00546875\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -396158.062500\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -379438.625000\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -315912.718750\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -290341.031250\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -341607.312500\n",
      "    epoch          : 322\n",
      "    loss           : -338166.6296875\n",
      "    val_loss       : -337039.0390625\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -314507.687500\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -291036.812500\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -378733.218750\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -302573.062500\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -286565.125000\n",
      "    epoch          : 323\n",
      "    loss           : -337624.581875\n",
      "    val_loss       : -339263.20078125\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -321354.187500\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -321537.656250\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -296680.562500\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -327304.000000\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -297883.250000\n",
      "    epoch          : 324\n",
      "    loss           : -338918.25375\n",
      "    val_loss       : -340813.5375\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -320687.937500\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -378890.187500\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -373642.312500\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -364134.031250\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -362391.312500\n",
      "    epoch          : 325\n",
      "    loss           : -339998.88125\n",
      "    val_loss       : -341090.33671875\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -323317.500000\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -304905.375000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -292200.812500\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -319722.875000\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -327410.437500\n",
      "    epoch          : 326\n",
      "    loss           : -339838.0696875\n",
      "    val_loss       : -341785.3640625\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -364386.062500\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -396609.968750\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -308458.812500\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -381008.218750\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -299332.406250\n",
      "    epoch          : 327\n",
      "    loss           : -340673.161875\n",
      "    val_loss       : -341849.2015625\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -398807.875000\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -295370.625000\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -321538.875000\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -294310.312500\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -362937.281250\n",
      "    epoch          : 328\n",
      "    loss           : -341070.1303125\n",
      "    val_loss       : -342298.19921875\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -299409.000000\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -399824.093750\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -377751.562500\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -297018.625000\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -295974.875000\n",
      "    epoch          : 329\n",
      "    loss           : -341380.7965625\n",
      "    val_loss       : -342119.6234375\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -382042.625000\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -305833.500000\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -297910.187500\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -298306.093750\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -357191.031250\n",
      "    epoch          : 330\n",
      "    loss           : -339421.6228125\n",
      "    val_loss       : -339867.12265625\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -363794.781250\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -380295.375000\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -381155.125000\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -296717.156250\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -297833.031250\n",
      "    epoch          : 331\n",
      "    loss           : -340959.8834375\n",
      "    val_loss       : -342574.97421875\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -400374.937500\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -296846.093750\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -347569.281250\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -347682.593750\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -347269.062500\n",
      "    epoch          : 332\n",
      "    loss           : -342497.15\n",
      "    val_loss       : -344338.653125\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -382938.937500\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -347643.406250\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -379239.531250\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -325796.968750\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -322175.250000\n",
      "    epoch          : 333\n",
      "    loss           : -343145.069375\n",
      "    val_loss       : -344705.00859375\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -304816.468750\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -330606.375000\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -305083.687500\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -347746.156250\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -348191.281250\n",
      "    epoch          : 334\n",
      "    loss           : -343533.560625\n",
      "    val_loss       : -344853.378125\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -323519.937500\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -301812.343750\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -296224.781250\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -385187.687500\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -400112.437500\n",
      "    epoch          : 335\n",
      "    loss           : -343442.65125\n",
      "    val_loss       : -344853.72890625\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -403888.468750\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -323618.937500\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -307586.187500\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -352384.843750\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -348618.500000\n",
      "    epoch          : 336\n",
      "    loss           : -344247.9784375\n",
      "    val_loss       : -345534.80234375\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -348507.750000\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -385985.500000\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -349736.406250\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -349638.312500\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -301319.937500\n",
      "    epoch          : 337\n",
      "    loss           : -344539.88625\n",
      "    val_loss       : -344908.503125\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -407298.406250\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -299911.718750\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -383932.531250\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -382670.593750\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -383119.656250\n",
      "    epoch          : 338\n",
      "    loss           : -344415.77125\n",
      "    val_loss       : -345885.45390625\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -350589.906250\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -323539.812500\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -351388.625000\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -301694.250000\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -359929.562500\n",
      "    epoch          : 339\n",
      "    loss           : -345425.6609375\n",
      "    val_loss       : -346853.65234375\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -386595.656250\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -298986.062500\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -349297.687500\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -382699.312500\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -338997.375000\n",
      "    epoch          : 340\n",
      "    loss           : -346174.834375\n",
      "    val_loss       : -347473.00078125\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -407183.781250\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -385498.156250\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -384295.937500\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -387678.218750\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -387776.312500\n",
      "    epoch          : 341\n",
      "    loss           : -346387.9490625\n",
      "    val_loss       : -347415.56484375\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -381411.781250\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -307728.718750\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -369027.218750\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -299254.406250\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -389164.062500\n",
      "    epoch          : 342\n",
      "    loss           : -346420.906875\n",
      "    val_loss       : -348025.88203125\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -324185.750000\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -368679.593750\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -390314.781250\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -302120.968750\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -372287.250000\n",
      "    epoch          : 343\n",
      "    loss           : -347169.62125\n",
      "    val_loss       : -348306.3953125\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -385305.437500\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -389411.656250\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -299541.812500\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -372305.968750\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -351948.468750\n",
      "    epoch          : 344\n",
      "    loss           : -346543.2203125\n",
      "    val_loss       : -341251.5421875\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -378396.312500\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -326854.562500\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -379982.062500\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -302485.000000\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -346582.187500\n",
      "    epoch          : 345\n",
      "    loss           : -341938.410625\n",
      "    val_loss       : -344711.765625\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -383176.031250\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -354122.968750\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -305345.750000\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -299137.687500\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -354716.093750\n",
      "    epoch          : 346\n",
      "    loss           : -347387.6578125\n",
      "    val_loss       : -350229.36171875\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -374049.843750\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -336437.750000\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -303071.687500\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -281776.937500\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -373482.312500\n",
      "    epoch          : 347\n",
      "    loss           : -348735.9990625\n",
      "    val_loss       : -350422.70390625\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -303931.906250\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -370521.437500\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -309479.937500\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -285465.562500\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -326972.562500\n",
      "    epoch          : 348\n",
      "    loss           : -349342.7796875\n",
      "    val_loss       : -351121.65625\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -390700.125000\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -304890.375000\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -386686.156250\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -303592.437500\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -407957.500000\n",
      "    epoch          : 349\n",
      "    loss           : -349552.1390625\n",
      "    val_loss       : -351129.26484375\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -371027.625000\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -303219.593750\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -331779.125000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -302164.656250\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -368024.750000\n",
      "    epoch          : 350\n",
      "    loss           : -346672.156875\n",
      "    val_loss       : -348823.75546875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -370220.687500\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -411052.187500\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -356124.187500\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -328079.468750\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -370250.593750\n",
      "    epoch          : 351\n",
      "    loss           : -348144.579375\n",
      "    val_loss       : -351180.2921875\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -391738.718750\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -316993.625000\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -303408.125000\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -303680.250000\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -385377.625000\n",
      "    epoch          : 352\n",
      "    loss           : -350630.2571875\n",
      "    val_loss       : -352048.890625\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -313020.437500\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -391570.000000\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -332470.125000\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -329841.000000\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -360019.656250\n",
      "    epoch          : 353\n",
      "    loss           : -351306.9834375\n",
      "    val_loss       : -352398.7265625\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -412697.343750\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -330091.937500\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -292606.625000\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -374867.187500\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -390357.718750\n",
      "    epoch          : 354\n",
      "    loss           : -351863.705\n",
      "    val_loss       : -352874.065625\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -375537.250000\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -304422.000000\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -314542.000000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -304216.468750\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -391466.781250\n",
      "    epoch          : 355\n",
      "    loss           : -352224.42375\n",
      "    val_loss       : -353398.4265625\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -304968.031250\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -307122.968750\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -314198.062500\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -360362.312500\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -359367.437500\n",
      "    epoch          : 356\n",
      "    loss           : -352533.205625\n",
      "    val_loss       : -354102.0640625\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -414307.531250\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -361020.937500\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -354531.906250\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -410663.343750\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -307337.000000\n",
      "    epoch          : 357\n",
      "    loss           : -352942.9346875\n",
      "    val_loss       : -353639.678125\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -414020.531250\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -397141.093750\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -340847.625000\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -414980.875000\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -391596.718750\n",
      "    epoch          : 358\n",
      "    loss           : -353169.90875\n",
      "    val_loss       : -354642.03984375\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -306474.125000\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -416404.125000\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -395217.531250\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -377748.562500\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -392517.562500\n",
      "    epoch          : 359\n",
      "    loss           : -353542.2715625\n",
      "    val_loss       : -354781.1484375\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -307097.062500\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -414634.687500\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -332686.062500\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -395512.968750\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -376644.718750\n",
      "    epoch          : 360\n",
      "    loss           : -353909.995625\n",
      "    val_loss       : -355082.515625\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -332805.437500\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -333548.687500\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -322258.062500\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -308643.187500\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -301288.250000\n",
      "    epoch          : 361\n",
      "    loss           : -352231.7546875\n",
      "    val_loss       : -353702.8140625\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -359890.500000\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -373267.875000\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -394895.062500\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -345293.531250\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -362434.468750\n",
      "    epoch          : 362\n",
      "    loss           : -353457.5125\n",
      "    val_loss       : -355995.60859375\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -310323.312500\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -372907.312500\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -308366.750000\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -363756.437500\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -393751.906250\n",
      "    epoch          : 363\n",
      "    loss           : -354922.104375\n",
      "    val_loss       : -356482.14609375\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -340433.250000\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -338516.906250\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -396651.562500\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -318290.437500\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -392727.312500\n",
      "    epoch          : 364\n",
      "    loss           : -355617.3659375\n",
      "    val_loss       : -356796.91015625\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -316603.437500\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -379424.843750\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -364836.812500\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -308675.000000\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -415845.593750\n",
      "    epoch          : 365\n",
      "    loss           : -355427.316875\n",
      "    val_loss       : -355920.8125\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -343448.593750\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -344161.593750\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -326066.875000\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -410595.312500\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -392718.125000\n",
      "    epoch          : 366\n",
      "    loss           : -350167.1128125\n",
      "    val_loss       : -355419.88125\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -376574.406250\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -307789.468750\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -310438.500000\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -312358.281250\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -313156.437500\n",
      "    epoch          : 367\n",
      "    loss           : -355837.7078125\n",
      "    val_loss       : -357673.3921875\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -308200.875000\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -308651.937500\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -310513.531250\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -365735.312500\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -308403.031250\n",
      "    epoch          : 368\n",
      "    loss           : -357209.70625\n",
      "    val_loss       : -358100.72734375\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -371806.187500\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -332348.468750\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -318113.718750\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -310181.062500\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -397131.656250\n",
      "    epoch          : 369\n",
      "    loss           : -357116.584375\n",
      "    val_loss       : -359250.67421875\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -312403.500000\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -312329.593750\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -312715.843750\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -309409.781250\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -366873.875000\n",
      "    epoch          : 370\n",
      "    loss           : -357925.4146875\n",
      "    val_loss       : -359458.7390625\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -320415.718750\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -400918.343750\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -383361.406250\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -396158.531250\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -311799.250000\n",
      "    epoch          : 371\n",
      "    loss           : -357931.60125\n",
      "    val_loss       : -359110.728125\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -322695.312500\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -419012.781250\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -402198.750000\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -309341.625000\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -366809.062500\n",
      "    epoch          : 372\n",
      "    loss           : -358428.62875\n",
      "    val_loss       : -360173.0203125\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -309642.000000\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -397657.156250\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -311856.218750\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -314730.750000\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -421479.156250\n",
      "    epoch          : 373\n",
      "    loss           : -358717.5453125\n",
      "    val_loss       : -360359.43828125\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -314927.593750\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -382385.687500\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -424429.718750\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -336162.343750\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -383022.593750\n",
      "    epoch          : 374\n",
      "    loss           : -359255.924375\n",
      "    val_loss       : -360163.3125\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -310793.031250\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -313082.187500\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -403731.125000\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -368408.625000\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -370464.562500\n",
      "    epoch          : 375\n",
      "    loss           : -359336.9003125\n",
      "    val_loss       : -360376.36015625\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -323802.406250\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -335511.156250\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -383454.906250\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -312153.750000\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -339226.531250\n",
      "    epoch          : 376\n",
      "    loss           : -359494.621875\n",
      "    val_loss       : -361339.15859375\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -311920.812500\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -337702.531250\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -369450.187500\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -350307.875000\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -314944.156250\n",
      "    epoch          : 377\n",
      "    loss           : -359913.5725\n",
      "    val_loss       : -360804.7046875\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -421138.156250\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -331789.531250\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -334973.125000\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -397702.750000\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -422384.250000\n",
      "    epoch          : 378\n",
      "    loss           : -357711.785\n",
      "    val_loss       : -361398.9421875\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -320777.687500\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -334731.250000\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -321521.562500\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -370154.625000\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -396149.968750\n",
      "    epoch          : 379\n",
      "    loss           : -360508.516875\n",
      "    val_loss       : -361714.35625\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -310859.625000\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -335852.406250\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -321275.250000\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -310856.500000\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -385378.750000\n",
      "    epoch          : 380\n",
      "    loss           : -361149.3609375\n",
      "    val_loss       : -362391.82421875\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -423231.906250\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -346216.625000\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -398546.687500\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -315044.843750\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -385575.625000\n",
      "    epoch          : 381\n",
      "    loss           : -361402.925\n",
      "    val_loss       : -362837.66171875\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -405314.062500\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -399471.187500\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -338656.312500\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -385614.187500\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -395363.375000\n",
      "    epoch          : 382\n",
      "    loss           : -361252.4028125\n",
      "    val_loss       : -362811.98828125\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -320998.562500\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -371652.375000\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -314369.968750\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -404246.062500\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -385725.437500\n",
      "    epoch          : 383\n",
      "    loss           : -362265.7\n",
      "    val_loss       : -363192.87578125\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -327344.500000\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -343438.625000\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -314723.625000\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -318634.187500\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -405984.437500\n",
      "    epoch          : 384\n",
      "    loss           : -362854.736875\n",
      "    val_loss       : -363721.91640625\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -426095.312500\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -314682.468750\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -312157.375000\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -338840.562500\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -401408.562500\n",
      "    epoch          : 385\n",
      "    loss           : -363053.764375\n",
      "    val_loss       : -364307.678125\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -341235.593750\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -349494.562500\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -427711.187500\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -371952.125000\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -401455.906250\n",
      "    epoch          : 386\n",
      "    loss           : -363037.4009375\n",
      "    val_loss       : -364359.040625\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -316849.500000\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -313885.656250\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -408146.031250\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -351466.250000\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -390295.000000\n",
      "    epoch          : 387\n",
      "    loss           : -362973.8234375\n",
      "    val_loss       : -363869.44453125\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -397571.718750\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -352136.406250\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -382192.093750\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -312066.812500\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -318176.625000\n",
      "    epoch          : 388\n",
      "    loss           : -362782.446875\n",
      "    val_loss       : -365668.76875\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -315020.062500\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -387095.843750\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -374565.500000\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -325418.062500\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -399954.156250\n",
      "    epoch          : 389\n",
      "    loss           : -364199.891875\n",
      "    val_loss       : -365640.68359375\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -374702.125000\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -388721.125000\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -403373.250000\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -373638.531250\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -352807.968750\n",
      "    epoch          : 390\n",
      "    loss           : -364960.5846875\n",
      "    val_loss       : -366240.20390625\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -351160.718750\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -344978.750000\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -410490.250000\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -388129.750000\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -293529.687500\n",
      "    epoch          : 391\n",
      "    loss           : -364926.0578125\n",
      "    val_loss       : -365542.7\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -313866.500000\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -383956.375000\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -310483.062500\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -313924.093750\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -374311.062500\n",
      "    epoch          : 392\n",
      "    loss           : -363338.068125\n",
      "    val_loss       : -363853.38203125\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -397500.125000\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -341625.093750\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -410204.656250\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -428314.687500\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -315015.062500\n",
      "    epoch          : 393\n",
      "    loss           : -364620.6984375\n",
      "    val_loss       : -367232.740625\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -409952.250000\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -411162.500000\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -410380.312500\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -325074.156250\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -385356.656250\n",
      "    epoch          : 394\n",
      "    loss           : -366275.195625\n",
      "    val_loss       : -367231.5328125\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -387920.625000\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -313889.625000\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -411026.437500\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -382791.375000\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -430337.750000\n",
      "    epoch          : 395\n",
      "    loss           : -365722.6321875\n",
      "    val_loss       : -367130.98828125\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -410099.125000\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -347257.718750\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -433615.937500\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -392471.250000\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -376328.281250\n",
      "    epoch          : 396\n",
      "    loss           : -366565.6059375\n",
      "    val_loss       : -368585.1171875\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -326155.000000\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -322200.281250\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -311762.406250\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -328256.000000\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -377676.562500\n",
      "    epoch          : 397\n",
      "    loss           : -367688.0134375\n",
      "    val_loss       : -368510.82734375\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -412047.812500\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -321044.562500\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -378033.781250\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -377337.562500\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -363509.718750\n",
      "    epoch          : 398\n",
      "    loss           : -367767.615625\n",
      "    val_loss       : -369340.86796875\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -325526.875000\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -408265.593750\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -346707.562500\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -388557.468750\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -394246.406250\n",
      "    epoch          : 399\n",
      "    loss           : -367383.8084375\n",
      "    val_loss       : -368461.1984375\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -429122.937500\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -429788.593750\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -354616.687500\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -322502.562500\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -373330.468750\n",
      "    epoch          : 400\n",
      "    loss           : -364669.8915625\n",
      "    val_loss       : -365390.1859375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -348393.187500\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -313104.062500\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -387745.093750\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -357081.968750\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -352402.281250\n",
      "    epoch          : 401\n",
      "    loss           : -366041.2340625\n",
      "    val_loss       : -369615.89765625\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -318660.968750\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -323592.062500\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -414608.250000\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -378512.437500\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -340590.750000\n",
      "    epoch          : 402\n",
      "    loss           : -368870.3203125\n",
      "    val_loss       : -369724.709375\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -359969.312500\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -414922.656250\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -429488.093750\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -406889.125000\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -316979.812500\n",
      "    epoch          : 403\n",
      "    loss           : -367163.595\n",
      "    val_loss       : -368944.02890625\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -434242.531250\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -328344.000000\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -345508.968750\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -391425.625000\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -393519.250000\n",
      "    epoch          : 404\n",
      "    loss           : -369530.5134375\n",
      "    val_loss       : -371505.66796875\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -402875.968750\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -417956.656250\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -413037.093750\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -327004.937500\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -320292.812500\n",
      "    epoch          : 405\n",
      "    loss           : -370209.414375\n",
      "    val_loss       : -372112.3796875\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -333590.875000\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -382176.437500\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -294877.625000\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -416891.437500\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -355627.656250\n",
      "    epoch          : 406\n",
      "    loss           : -370565.3784375\n",
      "    val_loss       : -371201.6375\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -353349.875000\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -329250.406250\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -393168.375000\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -326827.718750\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -409055.937500\n",
      "    epoch          : 407\n",
      "    loss           : -370532.6003125\n",
      "    val_loss       : -372301.8703125\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -347535.218750\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -329592.218750\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -313729.937500\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -407816.687500\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -332740.500000\n",
      "    epoch          : 408\n",
      "    loss           : -371027.741875\n",
      "    val_loss       : -371847.0484375\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -384989.000000\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -323382.906250\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -380679.156250\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -345244.625000\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -395696.937500\n",
      "    epoch          : 409\n",
      "    loss           : -371712.738125\n",
      "    val_loss       : -372941.68515625\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -317941.187500\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -393004.562500\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -438973.156250\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -325706.343750\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -382238.250000\n",
      "    epoch          : 410\n",
      "    loss           : -371181.2853125\n",
      "    val_loss       : -372344.353125\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -319339.093750\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -360690.218750\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -329362.937500\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -381752.750000\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -382219.062500\n",
      "    epoch          : 411\n",
      "    loss           : -371336.99\n",
      "    val_loss       : -373254.88359375\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -323856.750000\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -326633.093750\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -344603.750000\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -351180.062500\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -319250.093750\n",
      "    epoch          : 412\n",
      "    loss           : -371355.1840625\n",
      "    val_loss       : -372632.10390625\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -414482.562500\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -323045.250000\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -388182.062500\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -360673.843750\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -411618.625000\n",
      "    epoch          : 413\n",
      "    loss           : -372622.366875\n",
      "    val_loss       : -374448.03828125\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -411936.250000\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -390209.093750\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -358113.906250\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -384927.593750\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -438135.562500\n",
      "    epoch          : 414\n",
      "    loss           : -373244.60375\n",
      "    val_loss       : -374231.896875\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -355211.312500\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -300840.062500\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -422230.000000\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -397687.531250\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -383617.968750\n",
      "    epoch          : 415\n",
      "    loss           : -373427.3525\n",
      "    val_loss       : -375081.58359375\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -420900.218750\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -439588.625000\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -322592.750000\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -350810.718750\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -323139.281250\n",
      "    epoch          : 416\n",
      "    loss           : -374025.9028125\n",
      "    val_loss       : -375424.46875\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -413058.500000\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -323184.437500\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -323693.812500\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -332148.250000\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -387078.000000\n",
      "    epoch          : 417\n",
      "    loss           : -373445.2453125\n",
      "    val_loss       : -374259.1625\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -321436.718750\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -414601.937500\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -327872.625000\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -408159.937500\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -389103.031250\n",
      "    epoch          : 418\n",
      "    loss           : -373871.0534375\n",
      "    val_loss       : -376015.56875\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -339550.812500\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -436346.750000\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -398542.031250\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -325065.937500\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -321668.406250\n",
      "    epoch          : 419\n",
      "    loss           : -374970.141875\n",
      "    val_loss       : -375564.91796875\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -335291.875000\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -327422.906250\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -420045.875000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -328419.218750\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -416786.500000\n",
      "    epoch          : 420\n",
      "    loss           : -374679.9990625\n",
      "    val_loss       : -375430.64921875\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -342964.312500\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -421490.312500\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -437023.968750\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -351098.437500\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -399534.250000\n",
      "    epoch          : 421\n",
      "    loss           : -374039.329375\n",
      "    val_loss       : -376771.725\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -423180.031250\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -396923.187500\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -327026.968750\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -400009.406250\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -413254.250000\n",
      "    epoch          : 422\n",
      "    loss           : -375833.08\n",
      "    val_loss       : -377132.77734375\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -444701.937500\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -401327.562500\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -325246.343750\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -331535.062500\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -360724.968750\n",
      "    epoch          : 423\n",
      "    loss           : -376132.5934375\n",
      "    val_loss       : -377388.98359375\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -404558.562500\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -331437.312500\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -400778.812500\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -423976.937500\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -327071.812500\n",
      "    epoch          : 424\n",
      "    loss           : -376668.5575\n",
      "    val_loss       : -378368.94296875\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -323214.906250\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -360358.625000\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -355332.437500\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -387705.937500\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -391729.968750\n",
      "    epoch          : 425\n",
      "    loss           : -376609.4925\n",
      "    val_loss       : -378278.4671875\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -353553.437500\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -362339.656250\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -390221.187500\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -363490.750000\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -443223.437500\n",
      "    epoch          : 426\n",
      "    loss           : -375782.0828125\n",
      "    val_loss       : -377372.0953125\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -349523.437500\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -318568.562500\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -390753.937500\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -330403.937500\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -326409.375000\n",
      "    epoch          : 427\n",
      "    loss           : -375465.4946875\n",
      "    val_loss       : -376947.39609375\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -340931.343750\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -356710.937500\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -362672.093750\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -325739.625000\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -338538.250000\n",
      "    epoch          : 428\n",
      "    loss           : -377085.6990625\n",
      "    val_loss       : -378900.71171875\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -353364.937500\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -330007.812500\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -399288.000000\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -389094.250000\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -352432.531250\n",
      "    epoch          : 429\n",
      "    loss           : -378153.3271875\n",
      "    val_loss       : -379868.009375\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -393103.187500\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -329240.250000\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -351359.000000\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -426031.593750\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -335078.281250\n",
      "    epoch          : 430\n",
      "    loss           : -377817.2503125\n",
      "    val_loss       : -379003.64140625\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -424773.812500\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -425575.000000\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -358076.125000\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -330006.906250\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -393546.406250\n",
      "    epoch          : 431\n",
      "    loss           : -378303.6359375\n",
      "    val_loss       : -380154.41875\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -404841.812500\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -419741.968750\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -445972.718750\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -392758.968750\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -393196.406250\n",
      "    epoch          : 432\n",
      "    loss           : -379060.2665625\n",
      "    val_loss       : -380394.1421875\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -333292.125000\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -363671.406250\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -426006.000000\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -445586.937500\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -405286.718750\n",
      "    epoch          : 433\n",
      "    loss           : -379059.973125\n",
      "    val_loss       : -380548.9609375\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -341689.625000\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -402749.375000\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -449040.687500\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -426265.000000\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -362417.406250\n",
      "    epoch          : 434\n",
      "    loss           : -379523.396875\n",
      "    val_loss       : -379589.06015625\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -336741.437500\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -444982.656250\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -406701.875000\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -363766.656250\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -403772.125000\n",
      "    epoch          : 435\n",
      "    loss           : -379159.4521875\n",
      "    val_loss       : -380825.9234375\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -326548.375000\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -356932.187500\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -355582.343750\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -448070.062500\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -326736.187500\n",
      "    epoch          : 436\n",
      "    loss           : -380035.796875\n",
      "    val_loss       : -381692.19140625\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -335008.625000\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -448747.375000\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -428744.968750\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -327380.000000\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -420868.656250\n",
      "    epoch          : 437\n",
      "    loss           : -380488.7775\n",
      "    val_loss       : -382073.36171875\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -447734.468750\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -395551.562500\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -342709.500000\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -420451.718750\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -392155.625000\n",
      "    epoch          : 438\n",
      "    loss           : -380548.9653125\n",
      "    val_loss       : -382011.025\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -418406.218750\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -332585.656250\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -329199.656250\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -392495.656250\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -327689.375000\n",
      "    epoch          : 439\n",
      "    loss           : -380738.86\n",
      "    val_loss       : -382438.9625\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -355723.218750\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -365985.156250\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -331540.875000\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -447570.468750\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -324790.531250\n",
      "    epoch          : 440\n",
      "    loss           : -381196.945625\n",
      "    val_loss       : -382783.76171875\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -326105.906250\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -356465.593750\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -356436.156250\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -354253.250000\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -332043.843750\n",
      "    epoch          : 441\n",
      "    loss           : -380789.86\n",
      "    val_loss       : -383365.6390625\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -444737.531250\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -338540.625000\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -331348.375000\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -417513.312500\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -419206.312500\n",
      "    epoch          : 442\n",
      "    loss           : -381558.5009375\n",
      "    val_loss       : -381509.571875\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -361440.250000\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -421932.656250\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -397158.937500\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -395823.593750\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -342578.875000\n",
      "    epoch          : 443\n",
      "    loss           : -381833.5353125\n",
      "    val_loss       : -382615.88359375\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -431336.781250\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -343602.625000\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -353501.906250\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -328115.187500\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -398780.000000\n",
      "    epoch          : 444\n",
      "    loss           : -381955.22875\n",
      "    val_loss       : -383020.3578125\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -364492.031250\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -451099.656250\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -332889.000000\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -334292.406250\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -366908.968750\n",
      "    epoch          : 445\n",
      "    loss           : -382427.4840625\n",
      "    val_loss       : -384448.54140625\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -401576.937500\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -425428.875000\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -328232.375000\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -451274.125000\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -316451.812500\n",
      "    epoch          : 446\n",
      "    loss           : -383644.1009375\n",
      "    val_loss       : -385297.9390625\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -341264.187500\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -411301.281250\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -333323.281250\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -401443.687500\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -396243.187500\n",
      "    epoch          : 447\n",
      "    loss           : -384212.165\n",
      "    val_loss       : -385389.4546875\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -334090.125000\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -335592.750000\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -400419.781250\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -333500.968750\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -409389.750000\n",
      "    epoch          : 448\n",
      "    loss           : -384139.1975\n",
      "    val_loss       : -385400.34921875\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -451737.437500\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -423942.500000\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -451555.156250\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -409033.687500\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -432706.031250\n",
      "    epoch          : 449\n",
      "    loss           : -384456.271875\n",
      "    val_loss       : -385421.29453125\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -424706.437500\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -371671.875000\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -451159.656250\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -332966.250000\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -315816.875000\n",
      "    epoch          : 450\n",
      "    loss           : -384044.0409375\n",
      "    val_loss       : -385634.1515625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch450.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -331033.312500\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -409680.031250\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -454233.125000\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -399706.031250\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -338518.781250\n",
      "    epoch          : 451\n",
      "    loss           : -384257.1596875\n",
      "    val_loss       : -385799.84140625\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -399007.000000\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -399027.937500\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -397505.187500\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -318736.125000\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -329872.062500\n",
      "    epoch          : 452\n",
      "    loss           : -384181.7453125\n",
      "    val_loss       : -383708.959375\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -327360.437500\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -416678.218750\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -332077.406250\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -424613.562500\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -408650.750000\n",
      "    epoch          : 453\n",
      "    loss           : -383417.931875\n",
      "    val_loss       : -386314.646875\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -366552.781250\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -450679.187500\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -454073.625000\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -367059.437500\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -422005.937500\n",
      "    epoch          : 454\n",
      "    loss           : -385088.670625\n",
      "    val_loss       : -387649.02421875\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -434811.781250\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -333994.625000\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -433915.531250\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -426566.625000\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -423550.562500\n",
      "    epoch          : 455\n",
      "    loss           : -385956.334375\n",
      "    val_loss       : -388382.8453125\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -343420.500000\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -403614.406250\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -338276.531250\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -371629.562500\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -411230.562500\n",
      "    epoch          : 456\n",
      "    loss           : -386651.413125\n",
      "    val_loss       : -387080.028125\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -427801.750000\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -342100.500000\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -337024.093750\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -434258.625000\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -334881.750000\n",
      "    epoch          : 457\n",
      "    loss           : -386608.84125\n",
      "    val_loss       : -387839.98515625\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -437141.531250\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -404222.281250\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -347658.343750\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -427516.218750\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -411835.375000\n",
      "    epoch          : 458\n",
      "    loss           : -387098.7546875\n",
      "    val_loss       : -388411.8203125\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -341531.312500\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -411239.812500\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -368672.437500\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -415219.125000\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -403242.125000\n",
      "    epoch          : 459\n",
      "    loss           : -387278.8625\n",
      "    val_loss       : -388486.96171875\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -457566.781250\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -453776.687500\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -409838.312500\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -342680.906250\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -403417.343750\n",
      "    epoch          : 460\n",
      "    loss           : -387266.595625\n",
      "    val_loss       : -388928.32421875\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -340901.187500\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -344211.125000\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -404927.875000\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -365818.656250\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -329558.437500\n",
      "    epoch          : 461\n",
      "    loss           : -386866.8375\n",
      "    val_loss       : -386104.30390625\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -343325.000000\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -345399.125000\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -455684.062500\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -429774.812500\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -334158.687500\n",
      "    epoch          : 462\n",
      "    loss           : -387160.824375\n",
      "    val_loss       : -389189.5125\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -430576.187500\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -412162.062500\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -453142.687500\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -428111.812500\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -416174.437500\n",
      "    epoch          : 463\n",
      "    loss           : -388620.1825\n",
      "    val_loss       : -390288.725\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -427255.718750\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -439709.343750\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -341046.406250\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -375582.593750\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -413659.281250\n",
      "    epoch          : 464\n",
      "    loss           : -389254.531875\n",
      "    val_loss       : -390862.2203125\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -365152.312500\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -344162.250000\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -309830.406250\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -403965.343750\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -459745.500000\n",
      "    epoch          : 465\n",
      "    loss           : -389211.3921875\n",
      "    val_loss       : -390674.328125\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -345345.968750\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -397842.125000\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -341173.687500\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -438409.593750\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -364791.062500\n",
      "    epoch          : 466\n",
      "    loss           : -388870.885625\n",
      "    val_loss       : -391115.11328125\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -345858.875000\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -349186.125000\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -430312.250000\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -406001.187500\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -342768.218750\n",
      "    epoch          : 467\n",
      "    loss           : -390127.2453125\n",
      "    val_loss       : -391189.89375\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -336058.687500\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -404934.562500\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -425233.062500\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -431747.312500\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -411989.187500\n",
      "    epoch          : 468\n",
      "    loss           : -390145.9784375\n",
      "    val_loss       : -392140.46328125\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -440803.625000\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -413741.625000\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -413400.000000\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -339618.656250\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -432852.937500\n",
      "    epoch          : 469\n",
      "    loss           : -390594.85\n",
      "    val_loss       : -391863.846875\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -406020.687500\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -337126.312500\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -378428.625000\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -333808.687500\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -375852.687500\n",
      "    epoch          : 470\n",
      "    loss           : -390868.7609375\n",
      "    val_loss       : -392668.76171875\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -406793.718750\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -334501.125000\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -415192.218750\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -457472.812500\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -407278.375000\n",
      "    epoch          : 471\n",
      "    loss           : -388959.1428125\n",
      "    val_loss       : -389922.7296875\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -372520.343750\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -376600.437500\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -408206.375000\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -334846.343750\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -436925.250000\n",
      "    epoch          : 472\n",
      "    loss           : -391221.4640625\n",
      "    val_loss       : -392816.55703125\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -440982.375000\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -367708.718750\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -416133.062500\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -457259.750000\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -411202.468750\n",
      "    epoch          : 473\n",
      "    loss           : -391256.0540625\n",
      "    val_loss       : -393163.70390625\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -442002.093750\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -459989.812500\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -374705.687500\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -414834.125000\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -405382.562500\n",
      "    epoch          : 474\n",
      "    loss           : -392143.6471875\n",
      "    val_loss       : -393621.28984375\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -339344.875000\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -440503.500000\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -415272.562500\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -334097.687500\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -405698.781250\n",
      "    epoch          : 475\n",
      "    loss           : -391108.0515625\n",
      "    val_loss       : -390794.8203125\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -414748.750000\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -334425.687500\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -407848.656250\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -408733.000000\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -416243.437500\n",
      "    epoch          : 476\n",
      "    loss           : -391663.3909375\n",
      "    val_loss       : -394452.8375\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -416167.593750\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -340034.843750\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -367500.687500\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -336561.562500\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -344500.437500\n",
      "    epoch          : 477\n",
      "    loss           : -392995.7790625\n",
      "    val_loss       : -394369.73828125\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -340018.781250\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -367685.625000\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -463299.406250\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -464303.593750\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -408711.250000\n",
      "    epoch          : 478\n",
      "    loss           : -393212.1684375\n",
      "    val_loss       : -394380.07265625\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -365686.218750\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -364734.781250\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -410369.000000\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -339583.406250\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -411082.218750\n",
      "    epoch          : 479\n",
      "    loss           : -393551.4209375\n",
      "    val_loss       : -394246.67890625\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -390100.250000\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -350476.062500\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -417715.812500\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -338899.562500\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -417810.625000\n",
      "    epoch          : 480\n",
      "    loss           : -392269.6403125\n",
      "    val_loss       : -394363.40078125\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -417078.156250\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -340345.937500\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -461318.750000\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -410575.500000\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -409414.593750\n",
      "    epoch          : 481\n",
      "    loss           : -393081.419375\n",
      "    val_loss       : -394426.1875\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -340604.156250\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -461608.625000\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -436830.218750\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -414458.500000\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -416319.062500\n",
      "    epoch          : 482\n",
      "    loss           : -394015.1790625\n",
      "    val_loss       : -394996.61171875\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -460852.187500\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -432905.531250\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -414241.937500\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -433884.375000\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -413510.906250\n",
      "    epoch          : 483\n",
      "    loss           : -392291.751875\n",
      "    val_loss       : -394500.26640625\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -463316.968750\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -420504.687500\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -335360.312500\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -339922.968750\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -433459.000000\n",
      "    epoch          : 484\n",
      "    loss           : -394025.806875\n",
      "    val_loss       : -396437.9484375\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -348334.437500\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -412511.281250\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -365513.875000\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -434658.187500\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -439054.156250\n",
      "    epoch          : 485\n",
      "    loss           : -395094.72\n",
      "    val_loss       : -395575.16953125\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -417863.656250\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -363501.062500\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -412447.750000\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -365773.343750\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -414194.437500\n",
      "    epoch          : 486\n",
      "    loss           : -394866.6603125\n",
      "    val_loss       : -391461.19765625\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -424586.156250\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -368479.937500\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -341902.000000\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -416280.843750\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -348705.906250\n",
      "    epoch          : 487\n",
      "    loss           : -393552.4403125\n",
      "    val_loss       : -396660.61796875\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -446370.812500\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -446110.968750\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -371175.750000\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -374541.531250\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -342728.312500\n",
      "    epoch          : 488\n",
      "    loss           : -396191.726875\n",
      "    val_loss       : -397875.63203125\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -350076.812500\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -349776.500000\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -353406.375000\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -413377.500000\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -344127.406250\n",
      "    epoch          : 489\n",
      "    loss           : -396333.5315625\n",
      "    val_loss       : -397656.7609375\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -447266.687500\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -340875.125000\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -353048.562500\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -340833.093750\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -340665.687500\n",
      "    epoch          : 490\n",
      "    loss           : -396406.5890625\n",
      "    val_loss       : -397520.93203125\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -436723.906250\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -342700.250000\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -371329.250000\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -423866.968750\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -440399.406250\n",
      "    epoch          : 491\n",
      "    loss           : -396832.5475\n",
      "    val_loss       : -397480.70859375\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -415283.000000\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -377407.187500\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -370245.218750\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -436515.125000\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -414340.750000\n",
      "    epoch          : 492\n",
      "    loss           : -396158.1178125\n",
      "    val_loss       : -397928.22890625\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -341354.593750\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -352402.875000\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -448401.375000\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -342427.750000\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -436712.937500\n",
      "    epoch          : 493\n",
      "    loss           : -397194.5378125\n",
      "    val_loss       : -399044.6984375\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -437598.937500\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -466956.781250\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -347685.656250\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -382740.625000\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -351019.562500\n",
      "    epoch          : 494\n",
      "    loss           : -397221.01375\n",
      "    val_loss       : -397957.36328125\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -414956.031250\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -345554.250000\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -369529.562500\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -340850.125000\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -411691.625000\n",
      "    epoch          : 495\n",
      "    loss           : -397453.39375\n",
      "    val_loss       : -399007.1296875\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -368556.375000\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -418585.687500\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -370580.312500\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -380229.468750\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -382185.718750\n",
      "    epoch          : 496\n",
      "    loss           : -397847.284375\n",
      "    val_loss       : -398611.2234375\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -355616.531250\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -369475.312500\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -352802.968750\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -469841.750000\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -415542.250000\n",
      "    epoch          : 497\n",
      "    loss           : -398767.2809375\n",
      "    val_loss       : -400533.93671875\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -419404.656250\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -342437.593750\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -353695.562500\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -438257.906250\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -419540.093750\n",
      "    epoch          : 498\n",
      "    loss           : -399406.4175\n",
      "    val_loss       : -400672.4921875\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -350767.781250\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -355446.906250\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -372300.187500\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -424709.968750\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -468281.687500\n",
      "    epoch          : 499\n",
      "    loss           : -399810.8978125\n",
      "    val_loss       : -400633.8296875\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -425648.500000\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -420913.468750\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -371980.468750\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -467888.187500\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -347191.968750\n",
      "    epoch          : 500\n",
      "    loss           : -396364.44625\n",
      "    val_loss       : -399649.7953125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch500.pth ...\n",
      "Train Epoch: 501 [512/54000 (1%)] Loss: -421528.250000\n",
      "Train Epoch: 501 [11776/54000 (22%)] Loss: -471102.000000\n",
      "Train Epoch: 501 [23040/54000 (43%)] Loss: -386519.062500\n",
      "Train Epoch: 501 [34304/54000 (64%)] Loss: -438739.687500\n",
      "Train Epoch: 501 [45568/54000 (84%)] Loss: -442724.375000\n",
      "    epoch          : 501\n",
      "    loss           : -399586.0984375\n",
      "    val_loss       : -400504.14140625\n",
      "Train Epoch: 502 [512/54000 (1%)] Loss: -346982.625000\n",
      "Train Epoch: 502 [11776/54000 (22%)] Loss: -339248.906250\n",
      "Train Epoch: 502 [23040/54000 (43%)] Loss: -440243.656250\n",
      "Train Epoch: 502 [34304/54000 (64%)] Loss: -374789.812500\n",
      "Train Epoch: 502 [45568/54000 (84%)] Loss: -359323.937500\n",
      "    epoch          : 502\n",
      "    loss           : -400097.903125\n",
      "    val_loss       : -401428.96171875\n",
      "Train Epoch: 503 [512/54000 (1%)] Loss: -451838.750000\n",
      "Train Epoch: 503 [11776/54000 (22%)] Loss: -349129.000000\n",
      "Train Epoch: 503 [23040/54000 (43%)] Loss: -352580.937500\n",
      "Train Epoch: 503 [34304/54000 (64%)] Loss: -429607.250000\n",
      "Train Epoch: 503 [45568/54000 (84%)] Loss: -344579.875000\n",
      "    epoch          : 503\n",
      "    loss           : -400563.6953125\n",
      "    val_loss       : -402338.68203125\n",
      "Train Epoch: 504 [512/54000 (1%)] Loss: -343996.906250\n",
      "Train Epoch: 504 [11776/54000 (22%)] Loss: -359006.187500\n",
      "Train Epoch: 504 [23040/54000 (43%)] Loss: -422822.718750\n",
      "Train Epoch: 504 [34304/54000 (64%)] Loss: -401515.843750\n",
      "Train Epoch: 504 [45568/54000 (84%)] Loss: -470635.625000\n",
      "    epoch          : 504\n",
      "    loss           : -400733.6153125\n",
      "    val_loss       : -402168.771875\n",
      "Train Epoch: 505 [512/54000 (1%)] Loss: -343337.468750\n",
      "Train Epoch: 505 [11776/54000 (22%)] Loss: -452707.750000\n",
      "Train Epoch: 505 [23040/54000 (43%)] Loss: -341775.718750\n",
      "Train Epoch: 505 [34304/54000 (64%)] Loss: -386622.562500\n",
      "Train Epoch: 505 [45568/54000 (84%)] Loss: -441020.031250\n",
      "    epoch          : 505\n",
      "    loss           : -400324.2453125\n",
      "    val_loss       : -402076.07578125\n",
      "Train Epoch: 506 [512/54000 (1%)] Loss: -454173.562500\n",
      "Train Epoch: 506 [11776/54000 (22%)] Loss: -370717.781250\n",
      "Train Epoch: 506 [23040/54000 (43%)] Loss: -417931.500000\n",
      "Train Epoch: 506 [34304/54000 (64%)] Loss: -347292.937500\n",
      "Train Epoch: 506 [45568/54000 (84%)] Loss: -416800.000000\n",
      "    epoch          : 506\n",
      "    loss           : -400735.4759375\n",
      "    val_loss       : -401730.1296875\n",
      "Train Epoch: 507 [512/54000 (1%)] Loss: -443001.593750\n",
      "Train Epoch: 507 [11776/54000 (22%)] Loss: -385461.656250\n",
      "Train Epoch: 507 [23040/54000 (43%)] Loss: -343104.718750\n",
      "Train Epoch: 507 [34304/54000 (64%)] Loss: -375247.125000\n",
      "Train Epoch: 507 [45568/54000 (84%)] Loss: -443521.343750\n",
      "    epoch          : 507\n",
      "    loss           : -400292.9415625\n",
      "    val_loss       : -400739.575\n",
      "Train Epoch: 508 [512/54000 (1%)] Loss: -364970.812500\n",
      "Train Epoch: 508 [11776/54000 (22%)] Loss: -422328.718750\n",
      "Train Epoch: 508 [23040/54000 (43%)] Loss: -421066.750000\n",
      "Train Epoch: 508 [34304/54000 (64%)] Loss: -444487.781250\n",
      "Train Epoch: 508 [45568/54000 (84%)] Loss: -424306.593750\n",
      "    epoch          : 508\n",
      "    loss           : -399741.07625\n",
      "    val_loss       : -401352.75234375\n",
      "Train Epoch: 509 [512/54000 (1%)] Loss: -473983.156250\n",
      "Train Epoch: 509 [11776/54000 (22%)] Loss: -345978.906250\n",
      "Train Epoch: 509 [23040/54000 (43%)] Loss: -346307.968750\n",
      "Train Epoch: 509 [34304/54000 (64%)] Loss: -374675.312500\n",
      "Train Epoch: 509 [45568/54000 (84%)] Loss: -349064.500000\n",
      "    epoch          : 509\n",
      "    loss           : -401060.996875\n",
      "    val_loss       : -403502.38515625\n",
      "Train Epoch: 510 [512/54000 (1%)] Loss: -389083.531250\n",
      "Train Epoch: 510 [11776/54000 (22%)] Loss: -477019.937500\n",
      "Train Epoch: 510 [23040/54000 (43%)] Loss: -405874.500000\n",
      "Train Epoch: 510 [34304/54000 (64%)] Loss: -346602.625000\n",
      "Train Epoch: 510 [45568/54000 (84%)] Loss: -425359.312500\n",
      "    epoch          : 510\n",
      "    loss           : -402676.7534375\n",
      "    val_loss       : -404087.3859375\n",
      "Train Epoch: 511 [512/54000 (1%)] Loss: -455708.062500\n",
      "Train Epoch: 511 [11776/54000 (22%)] Loss: -419832.406250\n",
      "Train Epoch: 511 [23040/54000 (43%)] Loss: -377573.781250\n",
      "Train Epoch: 511 [34304/54000 (64%)] Loss: -378781.500000\n",
      "Train Epoch: 511 [45568/54000 (84%)] Loss: -349179.562500\n",
      "    epoch          : 511\n",
      "    loss           : -402429.7840625\n",
      "    val_loss       : -402790.14140625\n",
      "Train Epoch: 512 [512/54000 (1%)] Loss: -473814.250000\n",
      "Train Epoch: 512 [11776/54000 (22%)] Loss: -439048.875000\n",
      "Train Epoch: 512 [23040/54000 (43%)] Loss: -374754.031250\n",
      "Train Epoch: 512 [34304/54000 (64%)] Loss: -357621.375000\n",
      "Train Epoch: 512 [45568/54000 (84%)] Loss: -349972.531250\n",
      "    epoch          : 512\n",
      "    loss           : -401689.95125\n",
      "    val_loss       : -404409.484375\n",
      "Train Epoch: 513 [512/54000 (1%)] Loss: -423695.468750\n",
      "Train Epoch: 513 [11776/54000 (22%)] Loss: -455661.875000\n",
      "Train Epoch: 513 [23040/54000 (43%)] Loss: -405771.718750\n",
      "Train Epoch: 513 [34304/54000 (64%)] Loss: -342599.000000\n",
      "Train Epoch: 513 [45568/54000 (84%)] Loss: -427838.312500\n",
      "    epoch          : 513\n",
      "    loss           : -403337.25375\n",
      "    val_loss       : -404191.50859375\n",
      "Train Epoch: 514 [512/54000 (1%)] Loss: -349283.593750\n",
      "Train Epoch: 514 [11776/54000 (22%)] Loss: -320988.906250\n",
      "Train Epoch: 514 [23040/54000 (43%)] Loss: -347453.312500\n",
      "Train Epoch: 514 [34304/54000 (64%)] Loss: -355125.187500\n",
      "Train Epoch: 514 [45568/54000 (84%)] Loss: -420493.937500\n",
      "    epoch          : 514\n",
      "    loss           : -401184.4459375\n",
      "    val_loss       : -402439.2609375\n",
      "Train Epoch: 515 [512/54000 (1%)] Loss: -474821.375000\n",
      "Train Epoch: 515 [11776/54000 (22%)] Loss: -441479.218750\n",
      "Train Epoch: 515 [23040/54000 (43%)] Loss: -347355.968750\n",
      "Train Epoch: 515 [34304/54000 (64%)] Loss: -376804.468750\n",
      "Train Epoch: 515 [45568/54000 (84%)] Loss: -443091.218750\n",
      "    epoch          : 515\n",
      "    loss           : -401224.9440625\n",
      "    val_loss       : -405708.26875\n",
      "Train Epoch: 516 [512/54000 (1%)] Loss: -425300.875000\n",
      "Train Epoch: 516 [11776/54000 (22%)] Loss: -357827.343750\n",
      "Train Epoch: 516 [23040/54000 (43%)] Loss: -454506.750000\n",
      "Train Epoch: 516 [34304/54000 (64%)] Loss: -383577.531250\n",
      "Train Epoch: 516 [45568/54000 (84%)] Loss: -348232.250000\n",
      "    epoch          : 516\n",
      "    loss           : -404065.33375\n",
      "    val_loss       : -404589.38359375\n",
      "Train Epoch: 517 [512/54000 (1%)] Loss: -428783.687500\n",
      "Train Epoch: 517 [11776/54000 (22%)] Loss: -430124.125000\n",
      "Train Epoch: 517 [23040/54000 (43%)] Loss: -399182.562500\n",
      "Train Epoch: 517 [34304/54000 (64%)] Loss: -450202.093750\n",
      "Train Epoch: 517 [45568/54000 (84%)] Loss: -426146.906250\n",
      "    epoch          : 517\n",
      "    loss           : -402842.05625\n",
      "    val_loss       : -405280.5609375\n",
      "Train Epoch: 518 [512/54000 (1%)] Loss: -427815.375000\n",
      "Train Epoch: 518 [11776/54000 (22%)] Loss: -327520.812500\n",
      "Train Epoch: 518 [23040/54000 (43%)] Loss: -445230.625000\n",
      "Train Epoch: 518 [34304/54000 (64%)] Loss: -386088.687500\n",
      "Train Epoch: 518 [45568/54000 (84%)] Loss: -352100.843750\n",
      "    epoch          : 518\n",
      "    loss           : -404223.2453125\n",
      "    val_loss       : -406121.2046875\n",
      "Train Epoch: 519 [512/54000 (1%)] Loss: -348164.312500\n",
      "Train Epoch: 519 [11776/54000 (22%)] Loss: -320595.875000\n",
      "Train Epoch: 519 [23040/54000 (43%)] Loss: -430396.843750\n",
      "Train Epoch: 519 [34304/54000 (64%)] Loss: -426748.437500\n",
      "Train Epoch: 519 [45568/54000 (84%)] Loss: -387136.750000\n",
      "    epoch          : 519\n",
      "    loss           : -404831.1978125\n",
      "    val_loss       : -406310.24296875\n",
      "Train Epoch: 520 [512/54000 (1%)] Loss: -456852.218750\n",
      "Train Epoch: 520 [11776/54000 (22%)] Loss: -428009.406250\n",
      "Train Epoch: 520 [23040/54000 (43%)] Loss: -459077.937500\n",
      "Train Epoch: 520 [34304/54000 (64%)] Loss: -374414.093750\n",
      "Train Epoch: 520 [45568/54000 (84%)] Loss: -444220.687500\n",
      "    epoch          : 520\n",
      "    loss           : -404015.2725\n",
      "    val_loss       : -405900.39453125\n",
      "Train Epoch: 521 [512/54000 (1%)] Loss: -388056.093750\n",
      "Train Epoch: 521 [11776/54000 (22%)] Loss: -389392.000000\n",
      "Train Epoch: 521 [23040/54000 (43%)] Loss: -389027.062500\n",
      "Train Epoch: 521 [34304/54000 (64%)] Loss: -446349.156250\n",
      "Train Epoch: 521 [45568/54000 (84%)] Loss: -428264.343750\n",
      "    epoch          : 521\n",
      "    loss           : -405454.67375\n",
      "    val_loss       : -407157.471875\n",
      "Train Epoch: 522 [512/54000 (1%)] Loss: -393286.187500\n",
      "Train Epoch: 522 [11776/54000 (22%)] Loss: -477739.000000\n",
      "Train Epoch: 522 [23040/54000 (43%)] Loss: -474817.843750\n",
      "Train Epoch: 522 [34304/54000 (64%)] Loss: -424566.437500\n",
      "Train Epoch: 522 [45568/54000 (84%)] Loss: -446179.468750\n",
      "    epoch          : 522\n",
      "    loss           : -405853.00875\n",
      "    val_loss       : -407226.9453125\n",
      "Train Epoch: 523 [512/54000 (1%)] Loss: -448933.250000\n",
      "Train Epoch: 523 [11776/54000 (22%)] Loss: -458351.781250\n",
      "Train Epoch: 523 [23040/54000 (43%)] Loss: -359713.656250\n",
      "Train Epoch: 523 [34304/54000 (64%)] Loss: -428782.250000\n",
      "Train Epoch: 523 [45568/54000 (84%)] Loss: -425348.312500\n",
      "    epoch          : 523\n",
      "    loss           : -406061.2971875\n",
      "    val_loss       : -407101.553125\n",
      "Train Epoch: 524 [512/54000 (1%)] Loss: -430662.750000\n",
      "Train Epoch: 524 [11776/54000 (22%)] Loss: -432656.781250\n",
      "Train Epoch: 524 [23040/54000 (43%)] Loss: -360900.687500\n",
      "Train Epoch: 524 [34304/54000 (64%)] Loss: -427473.781250\n",
      "Train Epoch: 524 [45568/54000 (84%)] Loss: -448994.156250\n",
      "    epoch          : 524\n",
      "    loss           : -406612.9371875\n",
      "    val_loss       : -407462.39296875\n",
      "Train Epoch: 525 [512/54000 (1%)] Loss: -320459.500000\n",
      "Train Epoch: 525 [11776/54000 (22%)] Loss: -358874.250000\n",
      "Train Epoch: 525 [23040/54000 (43%)] Loss: -360583.062500\n",
      "Train Epoch: 525 [34304/54000 (64%)] Loss: -359608.750000\n",
      "Train Epoch: 525 [45568/54000 (84%)] Loss: -378425.406250\n",
      "    epoch          : 525\n",
      "    loss           : -406191.2984375\n",
      "    val_loss       : -408023.28125\n",
      "Train Epoch: 526 [512/54000 (1%)] Loss: -361480.937500\n",
      "Train Epoch: 526 [11776/54000 (22%)] Loss: -357430.062500\n",
      "Train Epoch: 526 [23040/54000 (43%)] Loss: -457657.375000\n",
      "Train Epoch: 526 [34304/54000 (64%)] Loss: -458633.187500\n",
      "Train Epoch: 526 [45568/54000 (84%)] Loss: -347718.781250\n",
      "    epoch          : 526\n",
      "    loss           : -406011.761875\n",
      "    val_loss       : -407106.3046875\n",
      "Train Epoch: 527 [512/54000 (1%)] Loss: -346563.937500\n",
      "Train Epoch: 527 [11776/54000 (22%)] Loss: -355673.687500\n",
      "Train Epoch: 527 [23040/54000 (43%)] Loss: -349148.406250\n",
      "Train Epoch: 527 [34304/54000 (64%)] Loss: -428337.125000\n",
      "Train Epoch: 527 [45568/54000 (84%)] Loss: -429741.468750\n",
      "    epoch          : 527\n",
      "    loss           : -404940.66\n",
      "    val_loss       : -408299.82890625\n",
      "Train Epoch: 528 [512/54000 (1%)] Loss: -398352.812500\n",
      "Train Epoch: 528 [11776/54000 (22%)] Loss: -425469.125000\n",
      "Train Epoch: 528 [23040/54000 (43%)] Loss: -347547.937500\n",
      "Train Epoch: 528 [34304/54000 (64%)] Loss: -451445.500000\n",
      "Train Epoch: 528 [45568/54000 (84%)] Loss: -428077.937500\n",
      "    epoch          : 528\n",
      "    loss           : -406539.77\n",
      "    val_loss       : -409309.83828125\n",
      "Train Epoch: 529 [512/54000 (1%)] Loss: -426900.906250\n",
      "Train Epoch: 529 [11776/54000 (22%)] Loss: -427563.125000\n",
      "Train Epoch: 529 [23040/54000 (43%)] Loss: -357677.750000\n",
      "Train Epoch: 529 [34304/54000 (64%)] Loss: -446039.156250\n",
      "Train Epoch: 529 [45568/54000 (84%)] Loss: -431064.437500\n",
      "    epoch          : 529\n",
      "    loss           : -407186.6578125\n",
      "    val_loss       : -408859.33203125\n",
      "Train Epoch: 530 [512/54000 (1%)] Loss: -452795.562500\n",
      "Train Epoch: 530 [11776/54000 (22%)] Loss: -390734.437500\n",
      "Train Epoch: 530 [23040/54000 (43%)] Loss: -388142.718750\n",
      "Train Epoch: 530 [34304/54000 (64%)] Loss: -447317.812500\n",
      "Train Epoch: 530 [45568/54000 (84%)] Loss: -423534.625000\n",
      "    epoch          : 530\n",
      "    loss           : -404856.215625\n",
      "    val_loss       : -407305.453125\n",
      "Train Epoch: 531 [512/54000 (1%)] Loss: -426273.875000\n",
      "Train Epoch: 531 [11776/54000 (22%)] Loss: -429911.187500\n",
      "Train Epoch: 531 [23040/54000 (43%)] Loss: -430415.125000\n",
      "Train Epoch: 531 [34304/54000 (64%)] Loss: -481503.875000\n",
      "Train Epoch: 531 [45568/54000 (84%)] Loss: -429186.875000\n",
      "    epoch          : 531\n",
      "    loss           : -407722.415\n",
      "    val_loss       : -409072.3515625\n",
      "Train Epoch: 532 [512/54000 (1%)] Loss: -341067.375000\n",
      "Train Epoch: 532 [11776/54000 (22%)] Loss: -361196.500000\n",
      "Train Epoch: 532 [23040/54000 (43%)] Loss: -349992.250000\n",
      "Train Epoch: 532 [34304/54000 (64%)] Loss: -428360.250000\n",
      "Train Epoch: 532 [45568/54000 (84%)] Loss: -390194.437500\n",
      "    epoch          : 532\n",
      "    loss           : -407164.36125\n",
      "    val_loss       : -407977.56328125\n",
      "Train Epoch: 533 [512/54000 (1%)] Loss: -445896.625000\n",
      "Train Epoch: 533 [11776/54000 (22%)] Loss: -351492.062500\n",
      "Train Epoch: 533 [23040/54000 (43%)] Loss: -353748.593750\n",
      "Train Epoch: 533 [34304/54000 (64%)] Loss: -456474.500000\n",
      "Train Epoch: 533 [45568/54000 (84%)] Loss: -351623.750000\n",
      "    epoch          : 533\n",
      "    loss           : -408687.171875\n",
      "    val_loss       : -410230.3734375\n",
      "Train Epoch: 534 [512/54000 (1%)] Loss: -396246.875000\n",
      "Train Epoch: 534 [11776/54000 (22%)] Loss: -347228.156250\n",
      "Train Epoch: 534 [23040/54000 (43%)] Loss: -377952.250000\n",
      "Train Epoch: 534 [34304/54000 (64%)] Loss: -353885.437500\n",
      "Train Epoch: 534 [45568/54000 (84%)] Loss: -430835.812500\n",
      "    epoch          : 534\n",
      "    loss           : -409216.143125\n",
      "    val_loss       : -410615.3625\n",
      "Train Epoch: 535 [512/54000 (1%)] Loss: -432626.312500\n",
      "Train Epoch: 535 [11776/54000 (22%)] Loss: -430144.625000\n",
      "Train Epoch: 535 [23040/54000 (43%)] Loss: -435390.281250\n",
      "Train Epoch: 535 [34304/54000 (64%)] Loss: -352195.000000\n",
      "Train Epoch: 535 [45568/54000 (84%)] Loss: -454784.750000\n",
      "    epoch          : 535\n",
      "    loss           : -409326.7803125\n",
      "    val_loss       : -410294.6875\n",
      "Train Epoch: 536 [512/54000 (1%)] Loss: -352598.312500\n",
      "Train Epoch: 536 [11776/54000 (22%)] Loss: -351822.437500\n",
      "Train Epoch: 536 [23040/54000 (43%)] Loss: -350741.593750\n",
      "Train Epoch: 536 [34304/54000 (64%)] Loss: -352204.687500\n",
      "Train Epoch: 536 [45568/54000 (84%)] Loss: -445879.156250\n",
      "    epoch          : 536\n",
      "    loss           : -409173.5059375\n",
      "    val_loss       : -408976.56015625\n",
      "Train Epoch: 537 [512/54000 (1%)] Loss: -482162.718750\n",
      "Train Epoch: 537 [11776/54000 (22%)] Loss: -435163.812500\n",
      "Train Epoch: 537 [23040/54000 (43%)] Loss: -481025.687500\n",
      "Train Epoch: 537 [34304/54000 (64%)] Loss: -482755.781250\n",
      "Train Epoch: 537 [45568/54000 (84%)] Loss: -393931.656250\n",
      "    epoch          : 537\n",
      "    loss           : -409234.3415625\n",
      "    val_loss       : -409699.2203125\n",
      "Train Epoch: 538 [512/54000 (1%)] Loss: -377702.250000\n",
      "Train Epoch: 538 [11776/54000 (22%)] Loss: -374633.187500\n",
      "Train Epoch: 538 [23040/54000 (43%)] Loss: -461002.125000\n",
      "Train Epoch: 538 [34304/54000 (64%)] Loss: -353984.125000\n",
      "Train Epoch: 538 [45568/54000 (84%)] Loss: -432730.250000\n",
      "    epoch          : 538\n",
      "    loss           : -407581.745\n",
      "    val_loss       : -406952.12421875\n",
      "Train Epoch: 539 [512/54000 (1%)] Loss: -377547.218750\n",
      "Train Epoch: 539 [11776/54000 (22%)] Loss: -354119.156250\n",
      "Train Epoch: 539 [23040/54000 (43%)] Loss: -354160.500000\n",
      "Train Epoch: 539 [34304/54000 (64%)] Loss: -356951.312500\n",
      "Train Epoch: 539 [45568/54000 (84%)] Loss: -432434.250000\n",
      "    epoch          : 539\n",
      "    loss           : -407614.5525\n",
      "    val_loss       : -409884.0765625\n",
      "Train Epoch: 540 [512/54000 (1%)] Loss: -463788.000000\n",
      "Train Epoch: 540 [11776/54000 (22%)] Loss: -381788.468750\n",
      "Train Epoch: 540 [23040/54000 (43%)] Loss: -357129.562500\n",
      "Train Epoch: 540 [34304/54000 (64%)] Loss: -389497.375000\n",
      "Train Epoch: 540 [45568/54000 (84%)] Loss: -457016.625000\n",
      "    epoch          : 540\n",
      "    loss           : -409807.4396875\n",
      "    val_loss       : -411167.91484375\n",
      "Train Epoch: 541 [512/54000 (1%)] Loss: -436010.625000\n",
      "Train Epoch: 541 [11776/54000 (22%)] Loss: -434949.312500\n",
      "Train Epoch: 541 [23040/54000 (43%)] Loss: -464589.250000\n",
      "Train Epoch: 541 [34304/54000 (64%)] Loss: -433152.843750\n",
      "Train Epoch: 541 [45568/54000 (84%)] Loss: -431138.156250\n",
      "    epoch          : 541\n",
      "    loss           : -410240.97\n",
      "    val_loss       : -411562.27109375\n",
      "Train Epoch: 542 [512/54000 (1%)] Loss: -362265.187500\n",
      "Train Epoch: 542 [11776/54000 (22%)] Loss: -353905.625000\n",
      "Train Epoch: 542 [23040/54000 (43%)] Loss: -354247.375000\n",
      "Train Epoch: 542 [34304/54000 (64%)] Loss: -356267.000000\n",
      "Train Epoch: 542 [45568/54000 (84%)] Loss: -452959.281250\n",
      "    epoch          : 542\n",
      "    loss           : -410179.2328125\n",
      "    val_loss       : -411656.0078125\n",
      "Train Epoch: 543 [512/54000 (1%)] Loss: -432849.937500\n",
      "Train Epoch: 543 [11776/54000 (22%)] Loss: -434844.093750\n",
      "Train Epoch: 543 [23040/54000 (43%)] Loss: -453855.968750\n",
      "Train Epoch: 543 [34304/54000 (64%)] Loss: -362087.906250\n",
      "Train Epoch: 543 [45568/54000 (84%)] Loss: -433131.281250\n",
      "    epoch          : 543\n",
      "    loss           : -410615.46125\n",
      "    val_loss       : -411770.5171875\n",
      "Train Epoch: 544 [512/54000 (1%)] Loss: -362669.187500\n",
      "Train Epoch: 544 [11776/54000 (22%)] Loss: -454107.562500\n",
      "Train Epoch: 544 [23040/54000 (43%)] Loss: -392860.375000\n",
      "Train Epoch: 544 [34304/54000 (64%)] Loss: -434241.812500\n",
      "Train Epoch: 544 [45568/54000 (84%)] Loss: -352164.593750\n",
      "    epoch          : 544\n",
      "    loss           : -410813.7425\n",
      "    val_loss       : -411935.50234375\n",
      "Train Epoch: 545 [512/54000 (1%)] Loss: -484477.375000\n",
      "Train Epoch: 545 [11776/54000 (22%)] Loss: -437593.562500\n",
      "Train Epoch: 545 [23040/54000 (43%)] Loss: -382357.250000\n",
      "Train Epoch: 545 [34304/54000 (64%)] Loss: -434807.343750\n",
      "Train Epoch: 545 [45568/54000 (84%)] Loss: -453375.312500\n",
      "    epoch          : 545\n",
      "    loss           : -410879.7684375\n",
      "    val_loss       : -412720.50859375\n",
      "Train Epoch: 546 [512/54000 (1%)] Loss: -486759.437500\n",
      "Train Epoch: 546 [11776/54000 (22%)] Loss: -436017.812500\n",
      "Train Epoch: 546 [23040/54000 (43%)] Loss: -357032.812500\n",
      "Train Epoch: 546 [34304/54000 (64%)] Loss: -484295.593750\n",
      "Train Epoch: 546 [45568/54000 (84%)] Loss: -435924.406250\n",
      "    epoch          : 546\n",
      "    loss           : -411240.5153125\n",
      "    val_loss       : -413137.084375\n",
      "Train Epoch: 547 [512/54000 (1%)] Loss: -486444.062500\n",
      "Train Epoch: 547 [11776/54000 (22%)] Loss: -391116.218750\n",
      "Train Epoch: 547 [23040/54000 (43%)] Loss: -434502.437500\n",
      "Train Epoch: 547 [34304/54000 (64%)] Loss: -486724.281250\n",
      "Train Epoch: 547 [45568/54000 (84%)] Loss: -328960.031250\n",
      "    epoch          : 547\n",
      "    loss           : -411932.9125\n",
      "    val_loss       : -414099.0015625\n",
      "Train Epoch: 548 [512/54000 (1%)] Loss: -465630.625000\n",
      "Train Epoch: 548 [11776/54000 (22%)] Loss: -466191.937500\n",
      "Train Epoch: 548 [23040/54000 (43%)] Loss: -355009.437500\n",
      "Train Epoch: 548 [34304/54000 (64%)] Loss: -353449.281250\n",
      "Train Epoch: 548 [45568/54000 (84%)] Loss: -458928.250000\n",
      "    epoch          : 548\n",
      "    loss           : -412598.9184375\n",
      "    val_loss       : -413233.9953125\n",
      "Train Epoch: 549 [512/54000 (1%)] Loss: -433610.500000\n",
      "Train Epoch: 549 [11776/54000 (22%)] Loss: -433033.312500\n",
      "Train Epoch: 549 [23040/54000 (43%)] Loss: -366609.187500\n",
      "Train Epoch: 549 [34304/54000 (64%)] Loss: -468106.781250\n",
      "Train Epoch: 549 [45568/54000 (84%)] Loss: -391580.625000\n",
      "    epoch          : 549\n",
      "    loss           : -411500.8115625\n",
      "    val_loss       : -413305.58984375\n",
      "Train Epoch: 550 [512/54000 (1%)] Loss: -435975.375000\n",
      "Train Epoch: 550 [11776/54000 (22%)] Loss: -453270.343750\n",
      "Train Epoch: 550 [23040/54000 (43%)] Loss: -434930.500000\n",
      "Train Epoch: 550 [34304/54000 (64%)] Loss: -380556.937500\n",
      "Train Epoch: 550 [45568/54000 (84%)] Loss: -434722.218750\n",
      "    epoch          : 550\n",
      "    loss           : -412375.54625\n",
      "    val_loss       : -414738.309375\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch550.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 551 [512/54000 (1%)] Loss: -365604.375000\n",
      "Train Epoch: 551 [11776/54000 (22%)] Loss: -485737.562500\n",
      "Train Epoch: 551 [23040/54000 (43%)] Loss: -466742.593750\n",
      "Train Epoch: 551 [34304/54000 (64%)] Loss: -334649.375000\n",
      "Train Epoch: 551 [45568/54000 (84%)] Loss: -439948.625000\n",
      "    epoch          : 551\n",
      "    loss           : -413512.5728125\n",
      "    val_loss       : -414874.31640625\n",
      "Train Epoch: 552 [512/54000 (1%)] Loss: -467126.812500\n",
      "Train Epoch: 552 [11776/54000 (22%)] Loss: -361206.218750\n",
      "Train Epoch: 552 [23040/54000 (43%)] Loss: -395002.125000\n",
      "Train Epoch: 552 [34304/54000 (64%)] Loss: -362911.968750\n",
      "Train Epoch: 552 [45568/54000 (84%)] Loss: -454765.312500\n",
      "    epoch          : 552\n",
      "    loss           : -412585.0875\n",
      "    val_loss       : -414807.1921875\n",
      "Train Epoch: 553 [512/54000 (1%)] Loss: -378196.062500\n",
      "Train Epoch: 553 [11776/54000 (22%)] Loss: -453123.562500\n",
      "Train Epoch: 553 [23040/54000 (43%)] Loss: -434237.812500\n",
      "Train Epoch: 553 [34304/54000 (64%)] Loss: -367725.250000\n",
      "Train Epoch: 553 [45568/54000 (84%)] Loss: -438709.687500\n",
      "    epoch          : 553\n",
      "    loss           : -414113.3234375\n",
      "    val_loss       : -415133.146875\n",
      "Train Epoch: 554 [512/54000 (1%)] Loss: -364497.562500\n",
      "Train Epoch: 554 [11776/54000 (22%)] Loss: -359477.437500\n",
      "Train Epoch: 554 [23040/54000 (43%)] Loss: -392414.687500\n",
      "Train Epoch: 554 [34304/54000 (64%)] Loss: -454607.187500\n",
      "Train Epoch: 554 [45568/54000 (84%)] Loss: -370334.500000\n",
      "    epoch          : 554\n",
      "    loss           : -413783.94875\n",
      "    val_loss       : -414401.7546875\n",
      "Train Epoch: 555 [512/54000 (1%)] Loss: -454425.781250\n",
      "Train Epoch: 555 [11776/54000 (22%)] Loss: -388224.562500\n",
      "Train Epoch: 555 [23040/54000 (43%)] Loss: -433779.250000\n",
      "Train Epoch: 555 [34304/54000 (64%)] Loss: -331652.375000\n",
      "Train Epoch: 555 [45568/54000 (84%)] Loss: -342279.812500\n",
      "    epoch          : 555\n",
      "    loss           : -412138.735625\n",
      "    val_loss       : -414134.0875\n",
      "Train Epoch: 556 [512/54000 (1%)] Loss: -466778.781250\n",
      "Train Epoch: 556 [11776/54000 (22%)] Loss: -435730.531250\n",
      "Train Epoch: 556 [23040/54000 (43%)] Loss: -356759.875000\n",
      "Train Epoch: 556 [34304/54000 (64%)] Loss: -433412.937500\n",
      "Train Epoch: 556 [45568/54000 (84%)] Loss: -352280.656250\n",
      "    epoch          : 556\n",
      "    loss           : -413642.8959375\n",
      "    val_loss       : -415851.59296875\n",
      "Train Epoch: 557 [512/54000 (1%)] Loss: -487678.375000\n",
      "Train Epoch: 557 [11776/54000 (22%)] Loss: -492563.875000\n",
      "Train Epoch: 557 [23040/54000 (43%)] Loss: -441198.500000\n",
      "Train Epoch: 557 [34304/54000 (64%)] Loss: -384812.500000\n",
      "Train Epoch: 557 [45568/54000 (84%)] Loss: -436103.562500\n",
      "    epoch          : 557\n",
      "    loss           : -414951.2925\n",
      "    val_loss       : -416343.88359375\n",
      "Train Epoch: 558 [512/54000 (1%)] Loss: -360174.906250\n",
      "Train Epoch: 558 [11776/54000 (22%)] Loss: -357457.156250\n",
      "Train Epoch: 558 [23040/54000 (43%)] Loss: -360907.718750\n",
      "Train Epoch: 558 [34304/54000 (64%)] Loss: -439619.906250\n",
      "Train Epoch: 558 [45568/54000 (84%)] Loss: -439557.562500\n",
      "    epoch          : 558\n",
      "    loss           : -415613.3278125\n",
      "    val_loss       : -416615.09140625\n",
      "Train Epoch: 559 [512/54000 (1%)] Loss: -398868.937500\n",
      "Train Epoch: 559 [11776/54000 (22%)] Loss: -489198.562500\n",
      "Train Epoch: 559 [23040/54000 (43%)] Loss: -468063.375000\n",
      "Train Epoch: 559 [34304/54000 (64%)] Loss: -363131.281250\n",
      "Train Epoch: 559 [45568/54000 (84%)] Loss: -442191.906250\n",
      "    epoch          : 559\n",
      "    loss           : -415237.9109375\n",
      "    val_loss       : -416510.50234375\n",
      "Train Epoch: 560 [512/54000 (1%)] Loss: -365236.156250\n",
      "Train Epoch: 560 [11776/54000 (22%)] Loss: -441883.750000\n",
      "Train Epoch: 560 [23040/54000 (43%)] Loss: -399105.218750\n",
      "Train Epoch: 560 [34304/54000 (64%)] Loss: -365113.312500\n",
      "Train Epoch: 560 [45568/54000 (84%)] Loss: -392374.937500\n",
      "    epoch          : 560\n",
      "    loss           : -415872.7971875\n",
      "    val_loss       : -416958.71796875\n",
      "Train Epoch: 561 [512/54000 (1%)] Loss: -457883.812500\n",
      "Train Epoch: 561 [11776/54000 (22%)] Loss: -388955.718750\n",
      "Train Epoch: 561 [23040/54000 (43%)] Loss: -470215.218750\n",
      "Train Epoch: 561 [34304/54000 (64%)] Loss: -437207.343750\n",
      "Train Epoch: 561 [45568/54000 (84%)] Loss: -440011.093750\n",
      "    epoch          : 561\n",
      "    loss           : -415741.899375\n",
      "    val_loss       : -417021.31796875\n",
      "Train Epoch: 562 [512/54000 (1%)] Loss: -488924.937500\n",
      "Train Epoch: 562 [11776/54000 (22%)] Loss: -355995.437500\n",
      "Train Epoch: 562 [23040/54000 (43%)] Loss: -416095.875000\n",
      "Train Epoch: 562 [34304/54000 (64%)] Loss: -363630.906250\n",
      "Train Epoch: 562 [45568/54000 (84%)] Loss: -486257.312500\n",
      "    epoch          : 562\n",
      "    loss           : -416089.26625\n",
      "    val_loss       : -417179.97421875\n",
      "Train Epoch: 563 [512/54000 (1%)] Loss: -441040.312500\n",
      "Train Epoch: 563 [11776/54000 (22%)] Loss: -441194.375000\n",
      "Train Epoch: 563 [23040/54000 (43%)] Loss: -362593.750000\n",
      "Train Epoch: 563 [34304/54000 (64%)] Loss: -389561.000000\n",
      "Train Epoch: 563 [45568/54000 (84%)] Loss: -431116.531250\n",
      "    epoch          : 563\n",
      "    loss           : -415234.1996875\n",
      "    val_loss       : -415719.15546875\n",
      "Train Epoch: 564 [512/54000 (1%)] Loss: -390656.843750\n",
      "Train Epoch: 564 [11776/54000 (22%)] Loss: -491656.343750\n",
      "Train Epoch: 564 [23040/54000 (43%)] Loss: -357238.156250\n",
      "Train Epoch: 564 [34304/54000 (64%)] Loss: -396423.500000\n",
      "Train Epoch: 564 [45568/54000 (84%)] Loss: -459120.062500\n",
      "    epoch          : 564\n",
      "    loss           : -415693.3971875\n",
      "    val_loss       : -417789.2875\n",
      "Train Epoch: 565 [512/54000 (1%)] Loss: -438309.218750\n",
      "Train Epoch: 565 [11776/54000 (22%)] Loss: -461863.250000\n",
      "Train Epoch: 565 [23040/54000 (43%)] Loss: -333059.562500\n",
      "Train Epoch: 565 [34304/54000 (64%)] Loss: -439407.218750\n",
      "Train Epoch: 565 [45568/54000 (84%)] Loss: -491948.312500\n",
      "    epoch          : 565\n",
      "    loss           : -416572.955\n",
      "    val_loss       : -417747.465625\n",
      "Train Epoch: 566 [512/54000 (1%)] Loss: -491119.687500\n",
      "Train Epoch: 566 [11776/54000 (22%)] Loss: -426740.750000\n",
      "Train Epoch: 566 [23040/54000 (43%)] Loss: -472714.875000\n",
      "Train Epoch: 566 [34304/54000 (64%)] Loss: -356987.250000\n",
      "Train Epoch: 566 [45568/54000 (84%)] Loss: -355058.093750\n",
      "    epoch          : 566\n",
      "    loss           : -415895.07375\n",
      "    val_loss       : -416752.34765625\n",
      "Train Epoch: 567 [512/54000 (1%)] Loss: -396236.156250\n",
      "Train Epoch: 567 [11776/54000 (22%)] Loss: -360173.906250\n",
      "Train Epoch: 567 [23040/54000 (43%)] Loss: -386172.593750\n",
      "Train Epoch: 567 [34304/54000 (64%)] Loss: -390252.687500\n",
      "Train Epoch: 567 [45568/54000 (84%)] Loss: -439916.812500\n",
      "    epoch          : 567\n",
      "    loss           : -416255.2871875\n",
      "    val_loss       : -417489.89609375\n",
      "Train Epoch: 568 [512/54000 (1%)] Loss: -357820.375000\n",
      "Train Epoch: 568 [11776/54000 (22%)] Loss: -461397.468750\n",
      "Train Epoch: 568 [23040/54000 (43%)] Loss: -491712.781250\n",
      "Train Epoch: 568 [34304/54000 (64%)] Loss: -370418.843750\n",
      "Train Epoch: 568 [45568/54000 (84%)] Loss: -361851.625000\n",
      "    epoch          : 568\n",
      "    loss           : -417350.9278125\n",
      "    val_loss       : -417968.01015625\n",
      "Train Epoch: 569 [512/54000 (1%)] Loss: -391902.312500\n",
      "Train Epoch: 569 [11776/54000 (22%)] Loss: -472522.656250\n",
      "Train Epoch: 569 [23040/54000 (43%)] Loss: -361995.187500\n",
      "Train Epoch: 569 [34304/54000 (64%)] Loss: -469933.437500\n",
      "Train Epoch: 569 [45568/54000 (84%)] Loss: -470375.281250\n",
      "    epoch          : 569\n",
      "    loss           : -414361.3925\n",
      "    val_loss       : -417073.6703125\n",
      "Train Epoch: 570 [512/54000 (1%)] Loss: -387630.562500\n",
      "Train Epoch: 570 [11776/54000 (22%)] Loss: -433914.937500\n",
      "Train Epoch: 570 [23040/54000 (43%)] Loss: -359398.750000\n",
      "Train Epoch: 570 [34304/54000 (64%)] Loss: -353628.500000\n",
      "Train Epoch: 570 [45568/54000 (84%)] Loss: -495460.781250\n",
      "    epoch          : 570\n",
      "    loss           : -416647.4234375\n",
      "    val_loss       : -420271.6953125\n",
      "Train Epoch: 571 [512/54000 (1%)] Loss: -492095.750000\n",
      "Train Epoch: 571 [11776/54000 (22%)] Loss: -360243.687500\n",
      "Train Epoch: 571 [23040/54000 (43%)] Loss: -394918.062500\n",
      "Train Epoch: 571 [34304/54000 (64%)] Loss: -491701.562500\n",
      "Train Epoch: 571 [45568/54000 (84%)] Loss: -360501.375000\n",
      "    epoch          : 571\n",
      "    loss           : -417891.37625\n",
      "    val_loss       : -419325.2625\n",
      "Train Epoch: 572 [512/54000 (1%)] Loss: -441008.343750\n",
      "Train Epoch: 572 [11776/54000 (22%)] Loss: -397003.562500\n",
      "Train Epoch: 572 [23040/54000 (43%)] Loss: -361136.031250\n",
      "Train Epoch: 572 [34304/54000 (64%)] Loss: -441071.500000\n",
      "Train Epoch: 572 [45568/54000 (84%)] Loss: -366051.843750\n",
      "    epoch          : 572\n",
      "    loss           : -418052.3334375\n",
      "    val_loss       : -419880.43359375\n",
      "Train Epoch: 573 [512/54000 (1%)] Loss: -390714.812500\n",
      "Train Epoch: 573 [11776/54000 (22%)] Loss: -360370.437500\n",
      "Train Epoch: 573 [23040/54000 (43%)] Loss: -386452.812500\n",
      "Train Epoch: 573 [34304/54000 (64%)] Loss: -390314.125000\n",
      "Train Epoch: 573 [45568/54000 (84%)] Loss: -456222.250000\n",
      "    epoch          : 573\n",
      "    loss           : -418881.816875\n",
      "    val_loss       : -420172.31015625\n",
      "Train Epoch: 574 [512/54000 (1%)] Loss: -497716.687500\n",
      "Train Epoch: 574 [11776/54000 (22%)] Loss: -390252.718750\n",
      "Train Epoch: 574 [23040/54000 (43%)] Loss: -441090.687500\n",
      "Train Epoch: 574 [34304/54000 (64%)] Loss: -444106.781250\n",
      "Train Epoch: 574 [45568/54000 (84%)] Loss: -360651.281250\n",
      "    epoch          : 574\n",
      "    loss           : -419216.3084375\n",
      "    val_loss       : -420657.32265625\n",
      "Train Epoch: 575 [512/54000 (1%)] Loss: -359008.750000\n",
      "Train Epoch: 575 [11776/54000 (22%)] Loss: -363096.062500\n",
      "Train Epoch: 575 [23040/54000 (43%)] Loss: -497750.125000\n",
      "Train Epoch: 575 [34304/54000 (64%)] Loss: -359785.218750\n",
      "Train Epoch: 575 [45568/54000 (84%)] Loss: -364762.406250\n",
      "    epoch          : 575\n",
      "    loss           : -419385.7821875\n",
      "    val_loss       : -420832.4078125\n",
      "Train Epoch: 576 [512/54000 (1%)] Loss: -358500.593750\n",
      "Train Epoch: 576 [11776/54000 (22%)] Loss: -369302.156250\n",
      "Train Epoch: 576 [23040/54000 (43%)] Loss: -493924.187500\n",
      "Train Epoch: 576 [34304/54000 (64%)] Loss: -365806.718750\n",
      "Train Epoch: 576 [45568/54000 (84%)] Loss: -443289.656250\n",
      "    epoch          : 576\n",
      "    loss           : -419912.856875\n",
      "    val_loss       : -421385.76875\n",
      "Train Epoch: 577 [512/54000 (1%)] Loss: -366528.500000\n",
      "Train Epoch: 577 [11776/54000 (22%)] Loss: -399888.625000\n",
      "Train Epoch: 577 [23040/54000 (43%)] Loss: -474807.312500\n",
      "Train Epoch: 577 [34304/54000 (64%)] Loss: -404555.312500\n",
      "Train Epoch: 577 [45568/54000 (84%)] Loss: -443121.687500\n",
      "    epoch          : 577\n",
      "    loss           : -419418.5821875\n",
      "    val_loss       : -419997.8671875\n",
      "Train Epoch: 578 [512/54000 (1%)] Loss: -395408.187500\n",
      "Train Epoch: 578 [11776/54000 (22%)] Loss: -497871.937500\n",
      "Train Epoch: 578 [23040/54000 (43%)] Loss: -357771.187500\n",
      "Train Epoch: 578 [34304/54000 (64%)] Loss: -400160.750000\n",
      "Train Epoch: 578 [45568/54000 (84%)] Loss: -398175.187500\n",
      "    epoch          : 578\n",
      "    loss           : -419316.3853125\n",
      "    val_loss       : -420177.85546875\n",
      "Train Epoch: 579 [512/54000 (1%)] Loss: -372254.250000\n",
      "Train Epoch: 579 [11776/54000 (22%)] Loss: -459197.437500\n",
      "Train Epoch: 579 [23040/54000 (43%)] Loss: -468114.250000\n",
      "Train Epoch: 579 [34304/54000 (64%)] Loss: -362386.437500\n",
      "Train Epoch: 579 [45568/54000 (84%)] Loss: -444058.750000\n",
      "    epoch          : 579\n",
      "    loss           : -419538.4109375\n",
      "    val_loss       : -420329.2625\n",
      "Train Epoch: 580 [512/54000 (1%)] Loss: -435314.812500\n",
      "Train Epoch: 580 [11776/54000 (22%)] Loss: -372312.468750\n",
      "Train Epoch: 580 [23040/54000 (43%)] Loss: -358823.062500\n",
      "Train Epoch: 580 [34304/54000 (64%)] Loss: -395423.875000\n",
      "Train Epoch: 580 [45568/54000 (84%)] Loss: -357203.281250\n",
      "    epoch          : 580\n",
      "    loss           : -418733.728125\n",
      "    val_loss       : -421447.45\n",
      "Train Epoch: 581 [512/54000 (1%)] Loss: -358889.218750\n",
      "Train Epoch: 581 [11776/54000 (22%)] Loss: -359881.593750\n",
      "Train Epoch: 581 [23040/54000 (43%)] Loss: -372901.125000\n",
      "Train Epoch: 581 [34304/54000 (64%)] Loss: -444847.375000\n",
      "Train Epoch: 581 [45568/54000 (84%)] Loss: -447755.687500\n",
      "    epoch          : 581\n",
      "    loss           : -419604.7965625\n",
      "    val_loss       : -422036.63046875\n",
      "Train Epoch: 582 [512/54000 (1%)] Loss: -494481.125000\n",
      "Train Epoch: 582 [11776/54000 (22%)] Loss: -391472.062500\n",
      "Train Epoch: 582 [23040/54000 (43%)] Loss: -445427.468750\n",
      "Train Epoch: 582 [34304/54000 (64%)] Loss: -444018.906250\n",
      "Train Epoch: 582 [45568/54000 (84%)] Loss: -461397.187500\n",
      "    epoch          : 582\n",
      "    loss           : -419656.0128125\n",
      "    val_loss       : -422048.71015625\n",
      "Train Epoch: 583 [512/54000 (1%)] Loss: -401405.625000\n",
      "Train Epoch: 583 [11776/54000 (22%)] Loss: -388958.156250\n",
      "Train Epoch: 583 [23040/54000 (43%)] Loss: -494275.812500\n",
      "Train Epoch: 583 [34304/54000 (64%)] Loss: -442191.250000\n",
      "Train Epoch: 583 [45568/54000 (84%)] Loss: -448860.406250\n",
      "    epoch          : 583\n",
      "    loss           : -421696.63625\n",
      "    val_loss       : -422403.1359375\n",
      "Train Epoch: 584 [512/54000 (1%)] Loss: -419176.843750\n",
      "Train Epoch: 584 [11776/54000 (22%)] Loss: -403140.562500\n",
      "Train Epoch: 584 [23040/54000 (43%)] Loss: -445565.312500\n",
      "Train Epoch: 584 [34304/54000 (64%)] Loss: -401384.750000\n",
      "Train Epoch: 584 [45568/54000 (84%)] Loss: -445956.125000\n",
      "    epoch          : 584\n",
      "    loss           : -421181.844375\n",
      "    val_loss       : -422921.71484375\n",
      "Train Epoch: 585 [512/54000 (1%)] Loss: -363346.968750\n",
      "Train Epoch: 585 [11776/54000 (22%)] Loss: -373966.281250\n",
      "Train Epoch: 585 [23040/54000 (43%)] Loss: -447562.187500\n",
      "Train Epoch: 585 [34304/54000 (64%)] Loss: -444622.656250\n",
      "Train Epoch: 585 [45568/54000 (84%)] Loss: -497114.875000\n",
      "    epoch          : 585\n",
      "    loss           : -422007.601875\n",
      "    val_loss       : -423267.234375\n",
      "Train Epoch: 586 [512/54000 (1%)] Loss: -465629.000000\n",
      "Train Epoch: 586 [11776/54000 (22%)] Loss: -374751.125000\n",
      "Train Epoch: 586 [23040/54000 (43%)] Loss: -398611.062500\n",
      "Train Epoch: 586 [34304/54000 (64%)] Loss: -368201.656250\n",
      "Train Epoch: 586 [45568/54000 (84%)] Loss: -466957.625000\n",
      "    epoch          : 586\n",
      "    loss           : -421317.3475\n",
      "    val_loss       : -422925.35\n",
      "Train Epoch: 587 [512/54000 (1%)] Loss: -361011.812500\n",
      "Train Epoch: 587 [11776/54000 (22%)] Loss: -448605.687500\n",
      "Train Epoch: 587 [23040/54000 (43%)] Loss: -475340.750000\n",
      "Train Epoch: 587 [34304/54000 (64%)] Loss: -364994.906250\n",
      "Train Epoch: 587 [45568/54000 (84%)] Loss: -363924.125000\n",
      "    epoch          : 587\n",
      "    loss           : -420292.3334375\n",
      "    val_loss       : -422298.715625\n",
      "Train Epoch: 588 [512/54000 (1%)] Loss: -477213.656250\n",
      "Train Epoch: 588 [11776/54000 (22%)] Loss: -373119.093750\n",
      "Train Epoch: 588 [23040/54000 (43%)] Loss: -376668.062500\n",
      "Train Epoch: 588 [34304/54000 (64%)] Loss: -372774.593750\n",
      "Train Epoch: 588 [45568/54000 (84%)] Loss: -463200.312500\n",
      "    epoch          : 588\n",
      "    loss           : -421692.2640625\n",
      "    val_loss       : -422067.18359375\n",
      "Train Epoch: 589 [512/54000 (1%)] Loss: -478686.187500\n",
      "Train Epoch: 589 [11776/54000 (22%)] Loss: -441831.750000\n",
      "Train Epoch: 589 [23040/54000 (43%)] Loss: -489089.031250\n",
      "Train Epoch: 589 [34304/54000 (64%)] Loss: -401240.593750\n",
      "Train Epoch: 589 [45568/54000 (84%)] Loss: -443413.968750\n",
      "    epoch          : 589\n",
      "    loss           : -420199.455\n",
      "    val_loss       : -422734.48125\n",
      "Train Epoch: 590 [512/54000 (1%)] Loss: -477475.625000\n",
      "Train Epoch: 590 [11776/54000 (22%)] Loss: -495877.937500\n",
      "Train Epoch: 590 [23040/54000 (43%)] Loss: -369193.812500\n",
      "Train Epoch: 590 [34304/54000 (64%)] Loss: -465943.906250\n",
      "Train Epoch: 590 [45568/54000 (84%)] Loss: -368143.593750\n",
      "    epoch          : 590\n",
      "    loss           : -422550.63\n",
      "    val_loss       : -423925.96796875\n",
      "Train Epoch: 591 [512/54000 (1%)] Loss: -452553.156250\n",
      "Train Epoch: 591 [11776/54000 (22%)] Loss: -447734.375000\n",
      "Train Epoch: 591 [23040/54000 (43%)] Loss: -407245.656250\n",
      "Train Epoch: 591 [34304/54000 (64%)] Loss: -405034.531250\n",
      "Train Epoch: 591 [45568/54000 (84%)] Loss: -448353.406250\n",
      "    epoch          : 591\n",
      "    loss           : -422775.8165625\n",
      "    val_loss       : -424085.9640625\n",
      "Train Epoch: 592 [512/54000 (1%)] Loss: -448838.250000\n",
      "Train Epoch: 592 [11776/54000 (22%)] Loss: -393260.500000\n",
      "Train Epoch: 592 [23040/54000 (43%)] Loss: -446520.250000\n",
      "Train Epoch: 592 [34304/54000 (64%)] Loss: -362954.937500\n",
      "Train Epoch: 592 [45568/54000 (84%)] Loss: -402396.281250\n",
      "    epoch          : 592\n",
      "    loss           : -423288.153125\n",
      "    val_loss       : -425145.83984375\n",
      "Train Epoch: 593 [512/54000 (1%)] Loss: -370786.843750\n",
      "Train Epoch: 593 [11776/54000 (22%)] Loss: -467463.312500\n",
      "Train Epoch: 593 [23040/54000 (43%)] Loss: -408358.250000\n",
      "Train Epoch: 593 [34304/54000 (64%)] Loss: -362822.562500\n",
      "Train Epoch: 593 [45568/54000 (84%)] Loss: -469887.687500\n",
      "    epoch          : 593\n",
      "    loss           : -423761.665\n",
      "    val_loss       : -425553.59921875\n",
      "Train Epoch: 594 [512/54000 (1%)] Loss: -470719.812500\n",
      "Train Epoch: 594 [11776/54000 (22%)] Loss: -446418.250000\n",
      "Train Epoch: 594 [23040/54000 (43%)] Loss: -445504.406250\n",
      "Train Epoch: 594 [34304/54000 (64%)] Loss: -406045.781250\n",
      "Train Epoch: 594 [45568/54000 (84%)] Loss: -365499.906250\n",
      "    epoch          : 594\n",
      "    loss           : -423659.719375\n",
      "    val_loss       : -424457.425\n",
      "Train Epoch: 595 [512/54000 (1%)] Loss: -370962.312500\n",
      "Train Epoch: 595 [11776/54000 (22%)] Loss: -390709.062500\n",
      "Train Epoch: 595 [23040/54000 (43%)] Loss: -464236.750000\n",
      "Train Epoch: 595 [34304/54000 (64%)] Loss: -358139.937500\n",
      "Train Epoch: 595 [45568/54000 (84%)] Loss: -442763.750000\n",
      "    epoch          : 595\n",
      "    loss           : -421364.976875\n",
      "    val_loss       : -422123.83359375\n",
      "Train Epoch: 596 [512/54000 (1%)] Loss: -360015.937500\n",
      "Train Epoch: 596 [11776/54000 (22%)] Loss: -364815.437500\n",
      "Train Epoch: 596 [23040/54000 (43%)] Loss: -478126.375000\n",
      "Train Epoch: 596 [34304/54000 (64%)] Loss: -393811.187500\n",
      "Train Epoch: 596 [45568/54000 (84%)] Loss: -502770.906250\n",
      "    epoch          : 596\n",
      "    loss           : -423217.3759375\n",
      "    val_loss       : -426017.48671875\n",
      "Train Epoch: 597 [512/54000 (1%)] Loss: -374118.125000\n",
      "Train Epoch: 597 [11776/54000 (22%)] Loss: -451484.875000\n",
      "Train Epoch: 597 [23040/54000 (43%)] Loss: -413242.312500\n",
      "Train Epoch: 597 [34304/54000 (64%)] Loss: -360473.562500\n",
      "Train Epoch: 597 [45568/54000 (84%)] Loss: -447867.875000\n",
      "    epoch          : 597\n",
      "    loss           : -424647.4028125\n",
      "    val_loss       : -426231.79296875\n",
      "Train Epoch: 598 [512/54000 (1%)] Loss: -376495.687500\n",
      "Train Epoch: 598 [11776/54000 (22%)] Loss: -392492.218750\n",
      "Train Epoch: 598 [23040/54000 (43%)] Loss: -463309.031250\n",
      "Train Epoch: 598 [34304/54000 (64%)] Loss: -448334.093750\n",
      "Train Epoch: 598 [45568/54000 (84%)] Loss: -447780.718750\n",
      "    epoch          : 598\n",
      "    loss           : -423304.454375\n",
      "    val_loss       : -418133.51796875\n",
      "Train Epoch: 599 [512/54000 (1%)] Loss: -493487.406250\n",
      "Train Epoch: 599 [11776/54000 (22%)] Loss: -478690.906250\n",
      "Train Epoch: 599 [23040/54000 (43%)] Loss: -478976.718750\n",
      "Train Epoch: 599 [34304/54000 (64%)] Loss: -404920.312500\n",
      "Train Epoch: 599 [45568/54000 (84%)] Loss: -423969.375000\n",
      "    epoch          : 599\n",
      "    loss           : -423042.7621875\n",
      "    val_loss       : -425950.0484375\n",
      "Train Epoch: 600 [512/54000 (1%)] Loss: -405514.343750\n",
      "Train Epoch: 600 [11776/54000 (22%)] Loss: -368232.687500\n",
      "Train Epoch: 600 [23040/54000 (43%)] Loss: -447908.437500\n",
      "Train Epoch: 600 [34304/54000 (64%)] Loss: -450290.687500\n",
      "Train Epoch: 600 [45568/54000 (84%)] Loss: -469338.625000\n",
      "    epoch          : 600\n",
      "    loss           : -425122.5271875\n",
      "    val_loss       : -426176.0203125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch600.pth ...\n",
      "Train Epoch: 601 [512/54000 (1%)] Loss: -503096.593750\n",
      "Train Epoch: 601 [11776/54000 (22%)] Loss: -392748.812500\n",
      "Train Epoch: 601 [23040/54000 (43%)] Loss: -408876.656250\n",
      "Train Epoch: 601 [34304/54000 (64%)] Loss: -364403.531250\n",
      "Train Epoch: 601 [45568/54000 (84%)] Loss: -506251.875000\n",
      "    epoch          : 601\n",
      "    loss           : -425507.0215625\n",
      "    val_loss       : -426410.49375\n",
      "Train Epoch: 602 [512/54000 (1%)] Loss: -451667.812500\n",
      "Train Epoch: 602 [11776/54000 (22%)] Loss: -402989.500000\n",
      "Train Epoch: 602 [23040/54000 (43%)] Loss: -480904.562500\n",
      "Train Epoch: 602 [34304/54000 (64%)] Loss: -500675.468750\n",
      "Train Epoch: 602 [45568/54000 (84%)] Loss: -428278.125000\n",
      "    epoch          : 602\n",
      "    loss           : -424964.6540625\n",
      "    val_loss       : -425995.93125\n",
      "Train Epoch: 603 [512/54000 (1%)] Loss: -501627.937500\n",
      "Train Epoch: 603 [11776/54000 (22%)] Loss: -467123.562500\n",
      "Train Epoch: 603 [23040/54000 (43%)] Loss: -452007.031250\n",
      "Train Epoch: 603 [34304/54000 (64%)] Loss: -473772.750000\n",
      "Train Epoch: 603 [45568/54000 (84%)] Loss: -449020.218750\n",
      "    epoch          : 603\n",
      "    loss           : -425395.31375\n",
      "    val_loss       : -424710.6921875\n",
      "Train Epoch: 604 [512/54000 (1%)] Loss: -393614.968750\n",
      "Train Epoch: 604 [11776/54000 (22%)] Loss: -373341.093750\n",
      "Train Epoch: 604 [23040/54000 (43%)] Loss: -468966.593750\n",
      "Train Epoch: 604 [34304/54000 (64%)] Loss: -404395.187500\n",
      "Train Epoch: 604 [45568/54000 (84%)] Loss: -451613.250000\n",
      "    epoch          : 604\n",
      "    loss           : -425058.780625\n",
      "    val_loss       : -426840.54921875\n",
      "Train Epoch: 605 [512/54000 (1%)] Loss: -465688.500000\n",
      "Train Epoch: 605 [11776/54000 (22%)] Loss: -400329.000000\n",
      "Train Epoch: 605 [23040/54000 (43%)] Loss: -451474.437500\n",
      "Train Epoch: 605 [34304/54000 (64%)] Loss: -449461.187500\n",
      "Train Epoch: 605 [45568/54000 (84%)] Loss: -362586.156250\n",
      "    epoch          : 605\n",
      "    loss           : -425414.9278125\n",
      "    val_loss       : -426914.2421875\n",
      "Train Epoch: 606 [512/54000 (1%)] Loss: -375170.187500\n",
      "Train Epoch: 606 [11776/54000 (22%)] Loss: -450394.906250\n",
      "Train Epoch: 606 [23040/54000 (43%)] Loss: -450805.812500\n",
      "Train Epoch: 606 [34304/54000 (64%)] Loss: -398305.593750\n",
      "Train Epoch: 606 [45568/54000 (84%)] Loss: -371408.000000\n",
      "    epoch          : 606\n",
      "    loss           : -426375.236875\n",
      "    val_loss       : -428498.71484375\n",
      "Train Epoch: 607 [512/54000 (1%)] Loss: -484788.000000\n",
      "Train Epoch: 607 [11776/54000 (22%)] Loss: -367796.500000\n",
      "Train Epoch: 607 [23040/54000 (43%)] Loss: -396550.062500\n",
      "Train Epoch: 607 [34304/54000 (64%)] Loss: -372148.187500\n",
      "Train Epoch: 607 [45568/54000 (84%)] Loss: -397486.406250\n",
      "    epoch          : 607\n",
      "    loss           : -426735.594375\n",
      "    val_loss       : -427257.60234375\n",
      "Train Epoch: 608 [512/54000 (1%)] Loss: -402907.625000\n",
      "Train Epoch: 608 [11776/54000 (22%)] Loss: -406857.875000\n",
      "Train Epoch: 608 [23040/54000 (43%)] Loss: -368570.750000\n",
      "Train Epoch: 608 [34304/54000 (64%)] Loss: -426252.625000\n",
      "Train Epoch: 608 [45568/54000 (84%)] Loss: -443508.000000\n",
      "    epoch          : 608\n",
      "    loss           : -426273.335\n",
      "    val_loss       : -427351.67265625\n",
      "Train Epoch: 609 [512/54000 (1%)] Loss: -363445.125000\n",
      "Train Epoch: 609 [11776/54000 (22%)] Loss: -371902.656250\n",
      "Train Epoch: 609 [23040/54000 (43%)] Loss: -366106.812500\n",
      "Train Epoch: 609 [34304/54000 (64%)] Loss: -504329.312500\n",
      "Train Epoch: 609 [45568/54000 (84%)] Loss: -446564.375000\n",
      "    epoch          : 609\n",
      "    loss           : -424476.1409375\n",
      "    val_loss       : -419376.61796875\n",
      "Train Epoch: 610 [512/54000 (1%)] Loss: -352893.968750\n",
      "Train Epoch: 610 [11776/54000 (22%)] Loss: -479530.125000\n",
      "Train Epoch: 610 [23040/54000 (43%)] Loss: -471818.375000\n",
      "Train Epoch: 610 [34304/54000 (64%)] Loss: -396592.875000\n",
      "Train Epoch: 610 [45568/54000 (84%)] Loss: -466701.281250\n",
      "    epoch          : 610\n",
      "    loss           : -424066.5196875\n",
      "    val_loss       : -426035.70859375\n",
      "Train Epoch: 611 [512/54000 (1%)] Loss: -394320.593750\n",
      "Train Epoch: 611 [11776/54000 (22%)] Loss: -377871.781250\n",
      "Train Epoch: 611 [23040/54000 (43%)] Loss: -365497.031250\n",
      "Train Epoch: 611 [34304/54000 (64%)] Loss: -365108.156250\n",
      "Train Epoch: 611 [45568/54000 (84%)] Loss: -469144.375000\n",
      "    epoch          : 611\n",
      "    loss           : -427139.2390625\n",
      "    val_loss       : -428718.67890625\n",
      "Train Epoch: 612 [512/54000 (1%)] Loss: -376176.875000\n",
      "Train Epoch: 612 [11776/54000 (22%)] Loss: -453378.531250\n",
      "Train Epoch: 612 [23040/54000 (43%)] Loss: -406459.093750\n",
      "Train Epoch: 612 [34304/54000 (64%)] Loss: -506933.562500\n",
      "Train Epoch: 612 [45568/54000 (84%)] Loss: -454014.718750\n",
      "    epoch          : 612\n",
      "    loss           : -428649.4653125\n",
      "    val_loss       : -430016.3578125\n",
      "Train Epoch: 613 [512/54000 (1%)] Loss: -473318.531250\n",
      "Train Epoch: 613 [11776/54000 (22%)] Loss: -365599.031250\n",
      "Train Epoch: 613 [23040/54000 (43%)] Loss: -454042.000000\n",
      "Train Epoch: 613 [34304/54000 (64%)] Loss: -371153.875000\n",
      "Train Epoch: 613 [45568/54000 (84%)] Loss: -474966.718750\n",
      "    epoch          : 613\n",
      "    loss           : -428585.6753125\n",
      "    val_loss       : -430243.25234375\n",
      "Train Epoch: 614 [512/54000 (1%)] Loss: -456876.968750\n",
      "Train Epoch: 614 [11776/54000 (22%)] Loss: -505589.531250\n",
      "Train Epoch: 614 [23040/54000 (43%)] Loss: -344016.468750\n",
      "Train Epoch: 614 [34304/54000 (64%)] Loss: -372614.812500\n",
      "Train Epoch: 614 [45568/54000 (84%)] Loss: -369021.500000\n",
      "    epoch          : 614\n",
      "    loss           : -428787.05125\n",
      "    val_loss       : -429354.3203125\n",
      "Train Epoch: 615 [512/54000 (1%)] Loss: -370771.093750\n",
      "Train Epoch: 615 [11776/54000 (22%)] Loss: -368172.312500\n",
      "Train Epoch: 615 [23040/54000 (43%)] Loss: -506964.468750\n",
      "Train Epoch: 615 [34304/54000 (64%)] Loss: -401011.531250\n",
      "Train Epoch: 615 [45568/54000 (84%)] Loss: -374779.250000\n",
      "    epoch          : 615\n",
      "    loss           : -428756.4234375\n",
      "    val_loss       : -430096.79453125\n",
      "Train Epoch: 616 [512/54000 (1%)] Loss: -362603.781250\n",
      "Train Epoch: 616 [11776/54000 (22%)] Loss: -455507.593750\n",
      "Train Epoch: 616 [23040/54000 (43%)] Loss: -365283.437500\n",
      "Train Epoch: 616 [34304/54000 (64%)] Loss: -470946.718750\n",
      "Train Epoch: 616 [45568/54000 (84%)] Loss: -372070.906250\n",
      "    epoch          : 616\n",
      "    loss           : -429213.220625\n",
      "    val_loss       : -429701.32578125\n",
      "Train Epoch: 617 [512/54000 (1%)] Loss: -378645.312500\n",
      "Train Epoch: 617 [11776/54000 (22%)] Loss: -455075.750000\n",
      "Train Epoch: 617 [23040/54000 (43%)] Loss: -433483.125000\n",
      "Train Epoch: 617 [34304/54000 (64%)] Loss: -484666.687500\n",
      "Train Epoch: 617 [45568/54000 (84%)] Loss: -453002.250000\n",
      "    epoch          : 617\n",
      "    loss           : -429072.851875\n",
      "    val_loss       : -430364.35078125\n",
      "Train Epoch: 618 [512/54000 (1%)] Loss: -504496.625000\n",
      "Train Epoch: 618 [11776/54000 (22%)] Loss: -445218.125000\n",
      "Train Epoch: 618 [23040/54000 (43%)] Loss: -455441.593750\n",
      "Train Epoch: 618 [34304/54000 (64%)] Loss: -399014.437500\n",
      "Train Epoch: 618 [45568/54000 (84%)] Loss: -454655.937500\n",
      "    epoch          : 618\n",
      "    loss           : -429261.6134375\n",
      "    val_loss       : -430514.2953125\n",
      "Train Epoch: 619 [512/54000 (1%)] Loss: -394703.906250\n",
      "Train Epoch: 619 [11776/54000 (22%)] Loss: -456982.750000\n",
      "Train Epoch: 619 [23040/54000 (43%)] Loss: -367639.500000\n",
      "Train Epoch: 619 [34304/54000 (64%)] Loss: -369449.937500\n",
      "Train Epoch: 619 [45568/54000 (84%)] Loss: -453230.593750\n",
      "    epoch          : 619\n",
      "    loss           : -429739.0115625\n",
      "    val_loss       : -431346.81640625\n",
      "Train Epoch: 620 [512/54000 (1%)] Loss: -455256.781250\n",
      "Train Epoch: 620 [11776/54000 (22%)] Loss: -505422.625000\n",
      "Train Epoch: 620 [23040/54000 (43%)] Loss: -471398.750000\n",
      "Train Epoch: 620 [34304/54000 (64%)] Loss: -452989.437500\n",
      "Train Epoch: 620 [45568/54000 (84%)] Loss: -452879.875000\n",
      "    epoch          : 620\n",
      "    loss           : -429766.2746875\n",
      "    val_loss       : -431247.4546875\n",
      "Train Epoch: 621 [512/54000 (1%)] Loss: -487229.937500\n",
      "Train Epoch: 621 [11776/54000 (22%)] Loss: -454154.937500\n",
      "Train Epoch: 621 [23040/54000 (43%)] Loss: -378828.156250\n",
      "Train Epoch: 621 [34304/54000 (64%)] Loss: -409347.968750\n",
      "Train Epoch: 621 [45568/54000 (84%)] Loss: -373829.312500\n",
      "    epoch          : 621\n",
      "    loss           : -430076.651875\n",
      "    val_loss       : -431417.91796875\n",
      "Train Epoch: 622 [512/54000 (1%)] Loss: -454077.125000\n",
      "Train Epoch: 622 [11776/54000 (22%)] Loss: -480401.562500\n",
      "Train Epoch: 622 [23040/54000 (43%)] Loss: -505015.968750\n",
      "Train Epoch: 622 [34304/54000 (64%)] Loss: -472457.937500\n",
      "Train Epoch: 622 [45568/54000 (84%)] Loss: -452959.187500\n",
      "    epoch          : 622\n",
      "    loss           : -429407.7590625\n",
      "    val_loss       : -430553.453125\n",
      "Train Epoch: 623 [512/54000 (1%)] Loss: -455465.750000\n",
      "Train Epoch: 623 [11776/54000 (22%)] Loss: -423257.562500\n",
      "Train Epoch: 623 [23040/54000 (43%)] Loss: -384203.250000\n",
      "Train Epoch: 623 [34304/54000 (64%)] Loss: -480685.750000\n",
      "Train Epoch: 623 [45568/54000 (84%)] Loss: -473720.250000\n",
      "    epoch          : 623\n",
      "    loss           : -429610.8184375\n",
      "    val_loss       : -431665.20234375\n",
      "Train Epoch: 624 [512/54000 (1%)] Loss: -377945.750000\n",
      "Train Epoch: 624 [11776/54000 (22%)] Loss: -488705.437500\n",
      "Train Epoch: 624 [23040/54000 (43%)] Loss: -456442.281250\n",
      "Train Epoch: 624 [34304/54000 (64%)] Loss: -415065.968750\n",
      "Train Epoch: 624 [45568/54000 (84%)] Loss: -399492.218750\n",
      "    epoch          : 624\n",
      "    loss           : -430485.943125\n",
      "    val_loss       : -432312.75\n",
      "Train Epoch: 625 [512/54000 (1%)] Loss: -456550.781250\n",
      "Train Epoch: 625 [11776/54000 (22%)] Loss: -489013.562500\n",
      "Train Epoch: 625 [23040/54000 (43%)] Loss: -376094.500000\n",
      "Train Epoch: 625 [34304/54000 (64%)] Loss: -472718.250000\n",
      "Train Epoch: 625 [45568/54000 (84%)] Loss: -398397.718750\n",
      "    epoch          : 625\n",
      "    loss           : -430822.340625\n",
      "    val_loss       : -432680.03515625\n",
      "Train Epoch: 626 [512/54000 (1%)] Loss: -387663.937500\n",
      "Train Epoch: 626 [11776/54000 (22%)] Loss: -487208.843750\n",
      "Train Epoch: 626 [23040/54000 (43%)] Loss: -399463.093750\n",
      "Train Epoch: 626 [34304/54000 (64%)] Loss: -370326.750000\n",
      "Train Epoch: 626 [45568/54000 (84%)] Loss: -455860.468750\n",
      "    epoch          : 626\n",
      "    loss           : -431025.45125\n",
      "    val_loss       : -431887.32421875\n",
      "Train Epoch: 627 [512/54000 (1%)] Loss: -405076.312500\n",
      "Train Epoch: 627 [11776/54000 (22%)] Loss: -367790.437500\n",
      "Train Epoch: 627 [23040/54000 (43%)] Loss: -411827.937500\n",
      "Train Epoch: 627 [34304/54000 (64%)] Loss: -485338.250000\n",
      "Train Epoch: 627 [45568/54000 (84%)] Loss: -370761.718750\n",
      "    epoch          : 627\n",
      "    loss           : -430005.95625\n",
      "    val_loss       : -429450.65078125\n",
      "Train Epoch: 628 [512/54000 (1%)] Loss: -457583.625000\n",
      "Train Epoch: 628 [11776/54000 (22%)] Loss: -375551.312500\n",
      "Train Epoch: 628 [23040/54000 (43%)] Loss: -474291.750000\n",
      "Train Epoch: 628 [34304/54000 (64%)] Loss: -458591.593750\n",
      "Train Epoch: 628 [45568/54000 (84%)] Loss: -409373.750000\n",
      "    epoch          : 628\n",
      "    loss           : -431228.0009375\n",
      "    val_loss       : -432826.4890625\n",
      "Train Epoch: 629 [512/54000 (1%)] Loss: -389410.562500\n",
      "Train Epoch: 629 [11776/54000 (22%)] Loss: -463461.562500\n",
      "Train Epoch: 629 [23040/54000 (43%)] Loss: -382641.718750\n",
      "Train Epoch: 629 [34304/54000 (64%)] Loss: -454701.531250\n",
      "Train Epoch: 629 [45568/54000 (84%)] Loss: -369675.968750\n",
      "    epoch          : 629\n",
      "    loss           : -431493.269375\n",
      "    val_loss       : -433224.690625\n",
      "Train Epoch: 630 [512/54000 (1%)] Loss: -488616.750000\n",
      "Train Epoch: 630 [11776/54000 (22%)] Loss: -414308.687500\n",
      "Train Epoch: 630 [23040/54000 (43%)] Loss: -414267.968750\n",
      "Train Epoch: 630 [34304/54000 (64%)] Loss: -410073.437500\n",
      "Train Epoch: 630 [45568/54000 (84%)] Loss: -475647.312500\n",
      "    epoch          : 630\n",
      "    loss           : -432464.3634375\n",
      "    val_loss       : -433682.89375\n",
      "Train Epoch: 631 [512/54000 (1%)] Loss: -415080.937500\n",
      "Train Epoch: 631 [11776/54000 (22%)] Loss: -477507.187500\n",
      "Train Epoch: 631 [23040/54000 (43%)] Loss: -401640.000000\n",
      "Train Epoch: 631 [34304/54000 (64%)] Loss: -368028.062500\n",
      "Train Epoch: 631 [45568/54000 (84%)] Loss: -370628.937500\n",
      "    epoch          : 631\n",
      "    loss           : -432172.95\n",
      "    val_loss       : -433860.9296875\n",
      "Train Epoch: 632 [512/54000 (1%)] Loss: -342114.218750\n",
      "Train Epoch: 632 [11776/54000 (22%)] Loss: -379727.562500\n",
      "Train Epoch: 632 [23040/54000 (43%)] Loss: -377446.906250\n",
      "Train Epoch: 632 [34304/54000 (64%)] Loss: -400838.062500\n",
      "Train Epoch: 632 [45568/54000 (84%)] Loss: -364316.187500\n",
      "    epoch          : 632\n",
      "    loss           : -432068.40625\n",
      "    val_loss       : -434275.42734375\n",
      "Train Epoch: 633 [512/54000 (1%)] Loss: -400264.718750\n",
      "Train Epoch: 633 [11776/54000 (22%)] Loss: -372734.343750\n",
      "Train Epoch: 633 [23040/54000 (43%)] Loss: -489393.000000\n",
      "Train Epoch: 633 [34304/54000 (64%)] Loss: -478889.125000\n",
      "Train Epoch: 633 [45568/54000 (84%)] Loss: -476291.343750\n",
      "    epoch          : 633\n",
      "    loss           : -431491.8425\n",
      "    val_loss       : -430664.20703125\n",
      "Train Epoch: 634 [512/54000 (1%)] Loss: -450139.750000\n",
      "Train Epoch: 634 [11776/54000 (22%)] Loss: -370630.312500\n",
      "Train Epoch: 634 [23040/54000 (43%)] Loss: -482401.437500\n",
      "Train Epoch: 634 [34304/54000 (64%)] Loss: -459611.718750\n",
      "Train Epoch: 634 [45568/54000 (84%)] Loss: -477611.187500\n",
      "    epoch          : 634\n",
      "    loss           : -432215.305\n",
      "    val_loss       : -433460.303125\n",
      "Train Epoch: 635 [512/54000 (1%)] Loss: -510033.062500\n",
      "Train Epoch: 635 [11776/54000 (22%)] Loss: -402048.625000\n",
      "Train Epoch: 635 [23040/54000 (43%)] Loss: -475040.500000\n",
      "Train Epoch: 635 [34304/54000 (64%)] Loss: -480877.000000\n",
      "Train Epoch: 635 [45568/54000 (84%)] Loss: -411710.750000\n",
      "    epoch          : 635\n",
      "    loss           : -433049.851875\n",
      "    val_loss       : -434172.9734375\n",
      "Train Epoch: 636 [512/54000 (1%)] Loss: -374883.031250\n",
      "Train Epoch: 636 [11776/54000 (22%)] Loss: -371533.656250\n",
      "Train Epoch: 636 [23040/54000 (43%)] Loss: -374532.906250\n",
      "Train Epoch: 636 [34304/54000 (64%)] Loss: -473893.500000\n",
      "Train Epoch: 636 [45568/54000 (84%)] Loss: -457910.750000\n",
      "    epoch          : 636\n",
      "    loss           : -432920.154375\n",
      "    val_loss       : -433593.03828125\n",
      "Train Epoch: 637 [512/54000 (1%)] Loss: -490963.593750\n",
      "Train Epoch: 637 [11776/54000 (22%)] Loss: -508218.281250\n",
      "Train Epoch: 637 [23040/54000 (43%)] Loss: -401362.968750\n",
      "Train Epoch: 637 [34304/54000 (64%)] Loss: -405297.500000\n",
      "Train Epoch: 637 [45568/54000 (84%)] Loss: -480159.187500\n",
      "    epoch          : 637\n",
      "    loss           : -433879.219375\n",
      "    val_loss       : -434947.7109375\n",
      "Train Epoch: 638 [512/54000 (1%)] Loss: -372045.781250\n",
      "Train Epoch: 638 [11776/54000 (22%)] Loss: -414121.375000\n",
      "Train Epoch: 638 [23040/54000 (43%)] Loss: -461791.062500\n",
      "Train Epoch: 638 [34304/54000 (64%)] Loss: -401107.656250\n",
      "Train Epoch: 638 [45568/54000 (84%)] Loss: -377515.562500\n",
      "    epoch          : 638\n",
      "    loss           : -434029.5865625\n",
      "    val_loss       : -434882.73203125\n",
      "Train Epoch: 639 [512/54000 (1%)] Loss: -405923.781250\n",
      "Train Epoch: 639 [11776/54000 (22%)] Loss: -372029.531250\n",
      "Train Epoch: 639 [23040/54000 (43%)] Loss: -371066.093750\n",
      "Train Epoch: 639 [34304/54000 (64%)] Loss: -460092.437500\n",
      "Train Epoch: 639 [45568/54000 (84%)] Loss: -460370.656250\n",
      "    epoch          : 639\n",
      "    loss           : -434286.3328125\n",
      "    val_loss       : -436065.93359375\n",
      "Train Epoch: 640 [512/54000 (1%)] Loss: -415570.687500\n",
      "Train Epoch: 640 [11776/54000 (22%)] Loss: -375016.156250\n",
      "Train Epoch: 640 [23040/54000 (43%)] Loss: -479396.000000\n",
      "Train Epoch: 640 [34304/54000 (64%)] Loss: -480388.437500\n",
      "Train Epoch: 640 [45568/54000 (84%)] Loss: -371395.593750\n",
      "    epoch          : 640\n",
      "    loss           : -434151.3571875\n",
      "    val_loss       : -434060.06484375\n",
      "Train Epoch: 641 [512/54000 (1%)] Loss: -404214.937500\n",
      "Train Epoch: 641 [11776/54000 (22%)] Loss: -459953.906250\n",
      "Train Epoch: 641 [23040/54000 (43%)] Loss: -506359.812500\n",
      "Train Epoch: 641 [34304/54000 (64%)] Loss: -414319.468750\n",
      "Train Epoch: 641 [45568/54000 (84%)] Loss: -458454.031250\n",
      "    epoch          : 641\n",
      "    loss           : -433166.05375\n",
      "    val_loss       : -435674.70703125\n",
      "Train Epoch: 642 [512/54000 (1%)] Loss: -463875.187500\n",
      "Train Epoch: 642 [11776/54000 (22%)] Loss: -492669.906250\n",
      "Train Epoch: 642 [23040/54000 (43%)] Loss: -492160.781250\n",
      "Train Epoch: 642 [34304/54000 (64%)] Loss: -377985.250000\n",
      "Train Epoch: 642 [45568/54000 (84%)] Loss: -460034.375000\n",
      "    epoch          : 642\n",
      "    loss           : -433564.4621875\n",
      "    val_loss       : -435100.6\n",
      "Train Epoch: 643 [512/54000 (1%)] Loss: -490534.718750\n",
      "Train Epoch: 643 [11776/54000 (22%)] Loss: -457492.656250\n",
      "Train Epoch: 643 [23040/54000 (43%)] Loss: -490089.250000\n",
      "Train Epoch: 643 [34304/54000 (64%)] Loss: -372672.156250\n",
      "Train Epoch: 643 [45568/54000 (84%)] Loss: -458305.687500\n",
      "    epoch          : 643\n",
      "    loss           : -434689.86\n",
      "    val_loss       : -436005.3328125\n",
      "Train Epoch: 644 [512/54000 (1%)] Loss: -413820.375000\n",
      "Train Epoch: 644 [11776/54000 (22%)] Loss: -375045.093750\n",
      "Train Epoch: 644 [23040/54000 (43%)] Loss: -494423.187500\n",
      "Train Epoch: 644 [34304/54000 (64%)] Loss: -414631.812500\n",
      "Train Epoch: 644 [45568/54000 (84%)] Loss: -459235.406250\n",
      "    epoch          : 644\n",
      "    loss           : -434647.2675\n",
      "    val_loss       : -436037.51953125\n",
      "Train Epoch: 645 [512/54000 (1%)] Loss: -400774.250000\n",
      "Train Epoch: 645 [11776/54000 (22%)] Loss: -404409.625000\n",
      "Train Epoch: 645 [23040/54000 (43%)] Loss: -474630.000000\n",
      "Train Epoch: 645 [34304/54000 (64%)] Loss: -367507.875000\n",
      "Train Epoch: 645 [45568/54000 (84%)] Loss: -461450.343750\n",
      "    epoch          : 645\n",
      "    loss           : -434112.37125\n",
      "    val_loss       : -436064.609375\n",
      "Train Epoch: 646 [512/54000 (1%)] Loss: -479948.687500\n",
      "Train Epoch: 646 [11776/54000 (22%)] Loss: -462777.750000\n",
      "Train Epoch: 646 [23040/54000 (43%)] Loss: -493175.031250\n",
      "Train Epoch: 646 [34304/54000 (64%)] Loss: -513392.562500\n",
      "Train Epoch: 646 [45568/54000 (84%)] Loss: -373703.656250\n",
      "    epoch          : 646\n",
      "    loss           : -435564.790625\n",
      "    val_loss       : -437143.80859375\n",
      "Train Epoch: 647 [512/54000 (1%)] Loss: -373163.156250\n",
      "Train Epoch: 647 [11776/54000 (22%)] Loss: -404048.687500\n",
      "Train Epoch: 647 [23040/54000 (43%)] Loss: -405329.375000\n",
      "Train Epoch: 647 [34304/54000 (64%)] Loss: -460102.593750\n",
      "Train Epoch: 647 [45568/54000 (84%)] Loss: -511998.968750\n",
      "    epoch          : 647\n",
      "    loss           : -435530.3134375\n",
      "    val_loss       : -436733.271875\n",
      "Train Epoch: 648 [512/54000 (1%)] Loss: -375896.250000\n",
      "Train Epoch: 648 [11776/54000 (22%)] Loss: -462579.656250\n",
      "Train Epoch: 648 [23040/54000 (43%)] Loss: -374365.312500\n",
      "Train Epoch: 648 [34304/54000 (64%)] Loss: -514093.218750\n",
      "Train Epoch: 648 [45568/54000 (84%)] Loss: -461794.875000\n",
      "    epoch          : 648\n",
      "    loss           : -435174.1078125\n",
      "    val_loss       : -436209.22421875\n",
      "Train Epoch: 649 [512/54000 (1%)] Loss: -411900.312500\n",
      "Train Epoch: 649 [11776/54000 (22%)] Loss: -377804.562500\n",
      "Train Epoch: 649 [23040/54000 (43%)] Loss: -385878.687500\n",
      "Train Epoch: 649 [34304/54000 (64%)] Loss: -461063.000000\n",
      "Train Epoch: 649 [45568/54000 (84%)] Loss: -460513.375000\n",
      "    epoch          : 649\n",
      "    loss           : -435733.9659375\n",
      "    val_loss       : -437586.73203125\n",
      "Train Epoch: 650 [512/54000 (1%)] Loss: -416847.031250\n",
      "Train Epoch: 650 [11776/54000 (22%)] Loss: -374108.562500\n",
      "Train Epoch: 650 [23040/54000 (43%)] Loss: -419678.031250\n",
      "Train Epoch: 650 [34304/54000 (64%)] Loss: -514150.968750\n",
      "Train Epoch: 650 [45568/54000 (84%)] Loss: -461577.750000\n",
      "    epoch          : 650\n",
      "    loss           : -436399.6715625\n",
      "    val_loss       : -437650.00703125\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch650.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 651 [512/54000 (1%)] Loss: -463488.500000\n",
      "Train Epoch: 651 [11776/54000 (22%)] Loss: -514076.687500\n",
      "Train Epoch: 651 [23040/54000 (43%)] Loss: -465540.000000\n",
      "Train Epoch: 651 [34304/54000 (64%)] Loss: -407376.281250\n",
      "Train Epoch: 651 [45568/54000 (84%)] Loss: -455567.500000\n",
      "    epoch          : 651\n",
      "    loss           : -435941.2209375\n",
      "    val_loss       : -437286.2078125\n",
      "Train Epoch: 652 [512/54000 (1%)] Loss: -493900.656250\n",
      "Train Epoch: 652 [11776/54000 (22%)] Loss: -372627.843750\n",
      "Train Epoch: 652 [23040/54000 (43%)] Loss: -403550.937500\n",
      "Train Epoch: 652 [34304/54000 (64%)] Loss: -465045.062500\n",
      "Train Epoch: 652 [45568/54000 (84%)] Loss: -481330.500000\n",
      "    epoch          : 652\n",
      "    loss           : -435931.6165625\n",
      "    val_loss       : -438298.00078125\n",
      "Train Epoch: 653 [512/54000 (1%)] Loss: -465234.750000\n",
      "Train Epoch: 653 [11776/54000 (22%)] Loss: -513961.875000\n",
      "Train Epoch: 653 [23040/54000 (43%)] Loss: -373191.625000\n",
      "Train Epoch: 653 [34304/54000 (64%)] Loss: -407256.218750\n",
      "Train Epoch: 653 [45568/54000 (84%)] Loss: -467022.000000\n",
      "    epoch          : 653\n",
      "    loss           : -437064.7778125\n",
      "    val_loss       : -438237.65\n",
      "Train Epoch: 654 [512/54000 (1%)] Loss: -408166.218750\n",
      "Train Epoch: 654 [11776/54000 (22%)] Loss: -517065.687500\n",
      "Train Epoch: 654 [23040/54000 (43%)] Loss: -373755.312500\n",
      "Train Epoch: 654 [34304/54000 (64%)] Loss: -517901.656250\n",
      "Train Epoch: 654 [45568/54000 (84%)] Loss: -379414.812500\n",
      "    epoch          : 654\n",
      "    loss           : -437363.7803125\n",
      "    val_loss       : -439344.60078125\n",
      "Train Epoch: 655 [512/54000 (1%)] Loss: -377837.062500\n",
      "Train Epoch: 655 [11776/54000 (22%)] Loss: -377386.906250\n",
      "Train Epoch: 655 [23040/54000 (43%)] Loss: -516467.781250\n",
      "Train Epoch: 655 [34304/54000 (64%)] Loss: -483701.406250\n",
      "Train Epoch: 655 [45568/54000 (84%)] Loss: -381313.812500\n",
      "    epoch          : 655\n",
      "    loss           : -438005.1625\n",
      "    val_loss       : -439026.23125\n",
      "Train Epoch: 656 [512/54000 (1%)] Loss: -479895.937500\n",
      "Train Epoch: 656 [11776/54000 (22%)] Loss: -467153.125000\n",
      "Train Epoch: 656 [23040/54000 (43%)] Loss: -415475.937500\n",
      "Train Epoch: 656 [34304/54000 (64%)] Loss: -497362.562500\n",
      "Train Epoch: 656 [45568/54000 (84%)] Loss: -488030.406250\n",
      "    epoch          : 656\n",
      "    loss           : -437670.5784375\n",
      "    val_loss       : -439085.36953125\n",
      "Train Epoch: 657 [512/54000 (1%)] Loss: -415826.375000\n",
      "Train Epoch: 657 [11776/54000 (22%)] Loss: -419614.531250\n",
      "Train Epoch: 657 [23040/54000 (43%)] Loss: -467545.875000\n",
      "Train Epoch: 657 [34304/54000 (64%)] Loss: -464897.531250\n",
      "Train Epoch: 657 [45568/54000 (84%)] Loss: -460507.187500\n",
      "    epoch          : 657\n",
      "    loss           : -438013.445625\n",
      "    val_loss       : -439195.834375\n",
      "Train Epoch: 658 [512/54000 (1%)] Loss: -388127.125000\n",
      "Train Epoch: 658 [11776/54000 (22%)] Loss: -466024.562500\n",
      "Train Epoch: 658 [23040/54000 (43%)] Loss: -379178.937500\n",
      "Train Epoch: 658 [34304/54000 (64%)] Loss: -464386.468750\n",
      "Train Epoch: 658 [45568/54000 (84%)] Loss: -440809.593750\n",
      "    epoch          : 658\n",
      "    loss           : -437915.3171875\n",
      "    val_loss       : -439351.84609375\n",
      "Train Epoch: 659 [512/54000 (1%)] Loss: -379514.375000\n",
      "Train Epoch: 659 [11776/54000 (22%)] Loss: -388401.000000\n",
      "Train Epoch: 659 [23040/54000 (43%)] Loss: -463303.187500\n",
      "Train Epoch: 659 [34304/54000 (64%)] Loss: -457890.218750\n",
      "Train Epoch: 659 [45568/54000 (84%)] Loss: -462512.406250\n",
      "    epoch          : 659\n",
      "    loss           : -437809.8334375\n",
      "    val_loss       : -439087.74375\n",
      "Train Epoch: 660 [512/54000 (1%)] Loss: -496450.531250\n",
      "Train Epoch: 660 [11776/54000 (22%)] Loss: -464634.531250\n",
      "Train Epoch: 660 [23040/54000 (43%)] Loss: -402390.187500\n",
      "Train Epoch: 660 [34304/54000 (64%)] Loss: -345440.125000\n",
      "Train Epoch: 660 [45568/54000 (84%)] Loss: -468027.750000\n",
      "    epoch          : 660\n",
      "    loss           : -436299.9496875\n",
      "    val_loss       : -438296.0640625\n",
      "Train Epoch: 661 [512/54000 (1%)] Loss: -375265.312500\n",
      "Train Epoch: 661 [11776/54000 (22%)] Loss: -494881.406250\n",
      "Train Epoch: 661 [23040/54000 (43%)] Loss: -377345.375000\n",
      "Train Epoch: 661 [34304/54000 (64%)] Loss: -488766.500000\n",
      "Train Epoch: 661 [45568/54000 (84%)] Loss: -413831.906250\n",
      "    epoch          : 661\n",
      "    loss           : -438015.36875\n",
      "    val_loss       : -439970.30546875\n",
      "Train Epoch: 662 [512/54000 (1%)] Loss: -414993.187500\n",
      "Train Epoch: 662 [11776/54000 (22%)] Loss: -480593.375000\n",
      "Train Epoch: 662 [23040/54000 (43%)] Loss: -466698.375000\n",
      "Train Epoch: 662 [34304/54000 (64%)] Loss: -406715.125000\n",
      "Train Epoch: 662 [45568/54000 (84%)] Loss: -466390.156250\n",
      "    epoch          : 662\n",
      "    loss           : -438770.12\n",
      "    val_loss       : -440264.88046875\n",
      "Train Epoch: 663 [512/54000 (1%)] Loss: -378902.812500\n",
      "Train Epoch: 663 [11776/54000 (22%)] Loss: -460770.656250\n",
      "Train Epoch: 663 [23040/54000 (43%)] Loss: -404541.500000\n",
      "Train Epoch: 663 [34304/54000 (64%)] Loss: -379063.093750\n",
      "Train Epoch: 663 [45568/54000 (84%)] Loss: -518091.718750\n",
      "    epoch          : 663\n",
      "    loss           : -438831.2371875\n",
      "    val_loss       : -439591.74375\n",
      "Train Epoch: 664 [512/54000 (1%)] Loss: -379197.625000\n",
      "Train Epoch: 664 [11776/54000 (22%)] Loss: -496204.500000\n",
      "Train Epoch: 664 [23040/54000 (43%)] Loss: -465558.218750\n",
      "Train Epoch: 664 [34304/54000 (64%)] Loss: -383531.906250\n",
      "Train Epoch: 664 [45568/54000 (84%)] Loss: -380378.531250\n",
      "    epoch          : 664\n",
      "    loss           : -438955.8725\n",
      "    val_loss       : -440417.5953125\n",
      "Train Epoch: 665 [512/54000 (1%)] Loss: -412198.093750\n",
      "Train Epoch: 665 [11776/54000 (22%)] Loss: -417732.875000\n",
      "Train Epoch: 665 [23040/54000 (43%)] Loss: -379391.781250\n",
      "Train Epoch: 665 [34304/54000 (64%)] Loss: -377728.500000\n",
      "Train Epoch: 665 [45568/54000 (84%)] Loss: -375345.781250\n",
      "    epoch          : 665\n",
      "    loss           : -439227.3478125\n",
      "    val_loss       : -441064.7046875\n",
      "Train Epoch: 666 [512/54000 (1%)] Loss: -416719.750000\n",
      "Train Epoch: 666 [11776/54000 (22%)] Loss: -483694.375000\n",
      "Train Epoch: 666 [23040/54000 (43%)] Loss: -437972.906250\n",
      "Train Epoch: 666 [34304/54000 (64%)] Loss: -463458.437500\n",
      "Train Epoch: 666 [45568/54000 (84%)] Loss: -513956.406250\n",
      "    epoch          : 666\n",
      "    loss           : -438759.341875\n",
      "    val_loss       : -440680.459375\n",
      "Train Epoch: 667 [512/54000 (1%)] Loss: -402803.937500\n",
      "Train Epoch: 667 [11776/54000 (22%)] Loss: -467602.437500\n",
      "Train Epoch: 667 [23040/54000 (43%)] Loss: -416384.468750\n",
      "Train Epoch: 667 [34304/54000 (64%)] Loss: -467546.437500\n",
      "Train Epoch: 667 [45568/54000 (84%)] Loss: -383759.031250\n",
      "    epoch          : 667\n",
      "    loss           : -438706.245625\n",
      "    val_loss       : -440567.38125\n",
      "Train Epoch: 668 [512/54000 (1%)] Loss: -481799.062500\n",
      "Train Epoch: 668 [11776/54000 (22%)] Loss: -405750.281250\n",
      "Train Epoch: 668 [23040/54000 (43%)] Loss: -377435.093750\n",
      "Train Epoch: 668 [34304/54000 (64%)] Loss: -375639.375000\n",
      "Train Epoch: 668 [45568/54000 (84%)] Loss: -371992.375000\n",
      "    epoch          : 668\n",
      "    loss           : -438831.6178125\n",
      "    val_loss       : -440917.11796875\n",
      "Train Epoch: 669 [512/54000 (1%)] Loss: -497554.687500\n",
      "Train Epoch: 669 [11776/54000 (22%)] Loss: -410941.750000\n",
      "Train Epoch: 669 [23040/54000 (43%)] Loss: -456918.218750\n",
      "Train Epoch: 669 [34304/54000 (64%)] Loss: -420028.468750\n",
      "Train Epoch: 669 [45568/54000 (84%)] Loss: -466894.343750\n",
      "    epoch          : 669\n",
      "    loss           : -440821.9728125\n",
      "    val_loss       : -441538.96796875\n",
      "Train Epoch: 670 [512/54000 (1%)] Loss: -409635.343750\n",
      "Train Epoch: 670 [11776/54000 (22%)] Loss: -476230.718750\n",
      "Train Epoch: 670 [23040/54000 (43%)] Loss: -420486.375000\n",
      "Train Epoch: 670 [34304/54000 (64%)] Loss: -519626.187500\n",
      "Train Epoch: 670 [45568/54000 (84%)] Loss: -466062.687500\n",
      "    epoch          : 670\n",
      "    loss           : -440230.751875\n",
      "    val_loss       : -441436.55234375\n",
      "Train Epoch: 671 [512/54000 (1%)] Loss: -467480.375000\n",
      "Train Epoch: 671 [11776/54000 (22%)] Loss: -518227.062500\n",
      "Train Epoch: 671 [23040/54000 (43%)] Loss: -411123.562500\n",
      "Train Epoch: 671 [34304/54000 (64%)] Loss: -377538.375000\n",
      "Train Epoch: 671 [45568/54000 (84%)] Loss: -472690.312500\n",
      "    epoch          : 671\n",
      "    loss           : -440489.0553125\n",
      "    val_loss       : -441511.7015625\n",
      "Train Epoch: 672 [512/54000 (1%)] Loss: -377385.875000\n",
      "Train Epoch: 672 [11776/54000 (22%)] Loss: -383471.812500\n",
      "Train Epoch: 672 [23040/54000 (43%)] Loss: -416528.531250\n",
      "Train Epoch: 672 [34304/54000 (64%)] Loss: -499619.718750\n",
      "Train Epoch: 672 [45568/54000 (84%)] Loss: -485394.812500\n",
      "    epoch          : 672\n",
      "    loss           : -440760.8909375\n",
      "    val_loss       : -441470.29140625\n",
      "Train Epoch: 673 [512/54000 (1%)] Loss: -418196.437500\n",
      "Train Epoch: 673 [11776/54000 (22%)] Loss: -387786.531250\n",
      "Train Epoch: 673 [23040/54000 (43%)] Loss: -447257.750000\n",
      "Train Epoch: 673 [34304/54000 (64%)] Loss: -392395.250000\n",
      "Train Epoch: 673 [45568/54000 (84%)] Loss: -472590.750000\n",
      "    epoch          : 673\n",
      "    loss           : -440359.160625\n",
      "    val_loss       : -441232.29765625\n",
      "Train Epoch: 674 [512/54000 (1%)] Loss: -498917.781250\n",
      "Train Epoch: 674 [11776/54000 (22%)] Loss: -381460.687500\n",
      "Train Epoch: 674 [23040/54000 (43%)] Loss: -377462.375000\n",
      "Train Epoch: 674 [34304/54000 (64%)] Loss: -519109.812500\n",
      "Train Epoch: 674 [45568/54000 (84%)] Loss: -465902.125000\n",
      "    epoch          : 674\n",
      "    loss           : -440486.2403125\n",
      "    val_loss       : -442208.38828125\n",
      "Train Epoch: 675 [512/54000 (1%)] Loss: -522454.250000\n",
      "Train Epoch: 675 [11776/54000 (22%)] Loss: -351032.281250\n",
      "Train Epoch: 675 [23040/54000 (43%)] Loss: -499892.718750\n",
      "Train Epoch: 675 [34304/54000 (64%)] Loss: -471157.531250\n",
      "Train Epoch: 675 [45568/54000 (84%)] Loss: -377187.312500\n",
      "    epoch          : 675\n",
      "    loss           : -441430.2078125\n",
      "    val_loss       : -442751.82421875\n",
      "Train Epoch: 676 [512/54000 (1%)] Loss: -465550.593750\n",
      "Train Epoch: 676 [11776/54000 (22%)] Loss: -382903.875000\n",
      "Train Epoch: 676 [23040/54000 (43%)] Loss: -378249.187500\n",
      "Train Epoch: 676 [34304/54000 (64%)] Loss: -391013.562500\n",
      "Train Epoch: 676 [45568/54000 (84%)] Loss: -392618.000000\n",
      "    epoch          : 676\n",
      "    loss           : -441978.2759375\n",
      "    val_loss       : -442901.62109375\n",
      "Train Epoch: 677 [512/54000 (1%)] Loss: -503300.468750\n",
      "Train Epoch: 677 [11776/54000 (22%)] Loss: -378855.437500\n",
      "Train Epoch: 677 [23040/54000 (43%)] Loss: -489283.750000\n",
      "Train Epoch: 677 [34304/54000 (64%)] Loss: -521799.562500\n",
      "Train Epoch: 677 [45568/54000 (84%)] Loss: -419519.156250\n",
      "    epoch          : 677\n",
      "    loss           : -441685.5046875\n",
      "    val_loss       : -439276.42109375\n",
      "Train Epoch: 678 [512/54000 (1%)] Loss: -369471.281250\n",
      "Train Epoch: 678 [11776/54000 (22%)] Loss: -377847.812500\n",
      "Train Epoch: 678 [23040/54000 (43%)] Loss: -492354.187500\n",
      "Train Epoch: 678 [34304/54000 (64%)] Loss: -465801.687500\n",
      "Train Epoch: 678 [45568/54000 (84%)] Loss: -522086.968750\n",
      "    epoch          : 678\n",
      "    loss           : -441737.8903125\n",
      "    val_loss       : -443428.5140625\n",
      "Train Epoch: 679 [512/54000 (1%)] Loss: -524355.625000\n",
      "Train Epoch: 679 [11776/54000 (22%)] Loss: -501225.812500\n",
      "Train Epoch: 679 [23040/54000 (43%)] Loss: -377859.375000\n",
      "Train Epoch: 679 [34304/54000 (64%)] Loss: -483922.750000\n",
      "Train Epoch: 679 [45568/54000 (84%)] Loss: -486936.093750\n",
      "    epoch          : 679\n",
      "    loss           : -441196.1875\n",
      "    val_loss       : -439824.98671875\n",
      "Train Epoch: 680 [512/54000 (1%)] Loss: -414600.687500\n",
      "Train Epoch: 680 [11776/54000 (22%)] Loss: -406260.531250\n",
      "Train Epoch: 680 [23040/54000 (43%)] Loss: -496500.875000\n",
      "Train Epoch: 680 [34304/54000 (64%)] Loss: -408314.406250\n",
      "Train Epoch: 680 [45568/54000 (84%)] Loss: -377833.343750\n",
      "    epoch          : 680\n",
      "    loss           : -440533.318125\n",
      "    val_loss       : -443937.46953125\n",
      "Train Epoch: 681 [512/54000 (1%)] Loss: -457740.375000\n",
      "Train Epoch: 681 [11776/54000 (22%)] Loss: -418116.531250\n",
      "Train Epoch: 681 [23040/54000 (43%)] Loss: -413872.750000\n",
      "Train Epoch: 681 [34304/54000 (64%)] Loss: -492271.937500\n",
      "Train Epoch: 681 [45568/54000 (84%)] Loss: -469621.125000\n",
      "    epoch          : 681\n",
      "    loss           : -442881.768125\n",
      "    val_loss       : -444463.5375\n",
      "Train Epoch: 682 [512/54000 (1%)] Loss: -381079.875000\n",
      "Train Epoch: 682 [11776/54000 (22%)] Loss: -500489.343750\n",
      "Train Epoch: 682 [23040/54000 (43%)] Loss: -380466.031250\n",
      "Train Epoch: 682 [34304/54000 (64%)] Loss: -376696.625000\n",
      "Train Epoch: 682 [45568/54000 (84%)] Loss: -380650.062500\n",
      "    epoch          : 682\n",
      "    loss           : -443498.4396875\n",
      "    val_loss       : -443842.8828125\n",
      "Train Epoch: 683 [512/54000 (1%)] Loss: -502371.437500\n",
      "Train Epoch: 683 [11776/54000 (22%)] Loss: -484873.656250\n",
      "Train Epoch: 683 [23040/54000 (43%)] Loss: -387415.781250\n",
      "Train Epoch: 683 [34304/54000 (64%)] Loss: -524405.562500\n",
      "Train Epoch: 683 [45568/54000 (84%)] Loss: -488802.812500\n",
      "    epoch          : 683\n",
      "    loss           : -443607.1871875\n",
      "    val_loss       : -444295.5140625\n",
      "Train Epoch: 684 [512/54000 (1%)] Loss: -499397.437500\n",
      "Train Epoch: 684 [11776/54000 (22%)] Loss: -381484.437500\n",
      "Train Epoch: 684 [23040/54000 (43%)] Loss: -376058.125000\n",
      "Train Epoch: 684 [34304/54000 (64%)] Loss: -502301.812500\n",
      "Train Epoch: 684 [45568/54000 (84%)] Loss: -419723.125000\n",
      "    epoch          : 684\n",
      "    loss           : -442907.4740625\n",
      "    val_loss       : -443852.11484375\n",
      "Train Epoch: 685 [512/54000 (1%)] Loss: -382167.437500\n",
      "Train Epoch: 685 [11776/54000 (22%)] Loss: -468788.750000\n",
      "Train Epoch: 685 [23040/54000 (43%)] Loss: -387479.125000\n",
      "Train Epoch: 685 [34304/54000 (64%)] Loss: -519657.687500\n",
      "Train Epoch: 685 [45568/54000 (84%)] Loss: -390987.531250\n",
      "    epoch          : 685\n",
      "    loss           : -443425.2825\n",
      "    val_loss       : -445467.93828125\n",
      "Train Epoch: 686 [512/54000 (1%)] Loss: -490583.343750\n",
      "Train Epoch: 686 [11776/54000 (22%)] Loss: -502157.281250\n",
      "Train Epoch: 686 [23040/54000 (43%)] Loss: -421148.375000\n",
      "Train Epoch: 686 [34304/54000 (64%)] Loss: -410848.562500\n",
      "Train Epoch: 686 [45568/54000 (84%)] Loss: -471022.562500\n",
      "    epoch          : 686\n",
      "    loss           : -443686.398125\n",
      "    val_loss       : -445054.6625\n",
      "Train Epoch: 687 [512/54000 (1%)] Loss: -474300.250000\n",
      "Train Epoch: 687 [11776/54000 (22%)] Loss: -473524.125000\n",
      "Train Epoch: 687 [23040/54000 (43%)] Loss: -490334.125000\n",
      "Train Epoch: 687 [34304/54000 (64%)] Loss: -419635.250000\n",
      "Train Epoch: 687 [45568/54000 (84%)] Loss: -487859.937500\n",
      "    epoch          : 687\n",
      "    loss           : -443048.618125\n",
      "    val_loss       : -440493.07890625\n",
      "Train Epoch: 688 [512/54000 (1%)] Loss: -383011.562500\n",
      "Train Epoch: 688 [11776/54000 (22%)] Loss: -419915.312500\n",
      "Train Epoch: 688 [23040/54000 (43%)] Loss: -523947.625000\n",
      "Train Epoch: 688 [34304/54000 (64%)] Loss: -464856.468750\n",
      "Train Epoch: 688 [45568/54000 (84%)] Loss: -468858.562500\n",
      "    epoch          : 688\n",
      "    loss           : -438918.3109375\n",
      "    val_loss       : -444900.24453125\n",
      "Train Epoch: 689 [512/54000 (1%)] Loss: -469770.187500\n",
      "Train Epoch: 689 [11776/54000 (22%)] Loss: -391401.531250\n",
      "Train Epoch: 689 [23040/54000 (43%)] Loss: -506330.031250\n",
      "Train Epoch: 689 [34304/54000 (64%)] Loss: -504058.937500\n",
      "Train Epoch: 689 [45568/54000 (84%)] Loss: -473736.062500\n",
      "    epoch          : 689\n",
      "    loss           : -444606.123125\n",
      "    val_loss       : -446128.365625\n",
      "Train Epoch: 690 [512/54000 (1%)] Loss: -475919.750000\n",
      "Train Epoch: 690 [11776/54000 (22%)] Loss: -471142.718750\n",
      "Train Epoch: 690 [23040/54000 (43%)] Loss: -474378.562500\n",
      "Train Epoch: 690 [34304/54000 (64%)] Loss: -525413.187500\n",
      "Train Epoch: 690 [45568/54000 (84%)] Loss: -469885.312500\n",
      "    epoch          : 690\n",
      "    loss           : -444903.855625\n",
      "    val_loss       : -446291.11015625\n",
      "Train Epoch: 691 [512/54000 (1%)] Loss: -475713.906250\n",
      "Train Epoch: 691 [11776/54000 (22%)] Loss: -382274.937500\n",
      "Train Epoch: 691 [23040/54000 (43%)] Loss: -504447.281250\n",
      "Train Epoch: 691 [34304/54000 (64%)] Loss: -470137.187500\n",
      "Train Epoch: 691 [45568/54000 (84%)] Loss: -483808.812500\n",
      "    epoch          : 691\n",
      "    loss           : -444867.8221875\n",
      "    val_loss       : -445963.71015625\n",
      "Train Epoch: 692 [512/54000 (1%)] Loss: -425432.687500\n",
      "Train Epoch: 692 [11776/54000 (22%)] Loss: -385010.281250\n",
      "Train Epoch: 692 [23040/54000 (43%)] Loss: -419877.875000\n",
      "Train Epoch: 692 [34304/54000 (64%)] Loss: -418503.250000\n",
      "Train Epoch: 692 [45568/54000 (84%)] Loss: -380458.562500\n",
      "    epoch          : 692\n",
      "    loss           : -444019.076875\n",
      "    val_loss       : -443582.58515625\n",
      "Train Epoch: 693 [512/54000 (1%)] Loss: -475188.250000\n",
      "Train Epoch: 693 [11776/54000 (22%)] Loss: -502132.375000\n",
      "Train Epoch: 693 [23040/54000 (43%)] Loss: -376874.750000\n",
      "Train Epoch: 693 [34304/54000 (64%)] Loss: -384012.812500\n",
      "Train Epoch: 693 [45568/54000 (84%)] Loss: -379405.218750\n",
      "    epoch          : 693\n",
      "    loss           : -444464.5721875\n",
      "    val_loss       : -446644.2140625\n",
      "Train Epoch: 694 [512/54000 (1%)] Loss: -379876.343750\n",
      "Train Epoch: 694 [11776/54000 (22%)] Loss: -395107.312500\n",
      "Train Epoch: 694 [23040/54000 (43%)] Loss: -471358.500000\n",
      "Train Epoch: 694 [34304/54000 (64%)] Loss: -416048.625000\n",
      "Train Epoch: 694 [45568/54000 (84%)] Loss: -486987.375000\n",
      "    epoch          : 694\n",
      "    loss           : -446161.568125\n",
      "    val_loss       : -447087.69296875\n",
      "Train Epoch: 695 [512/54000 (1%)] Loss: -422703.750000\n",
      "Train Epoch: 695 [11776/54000 (22%)] Loss: -470724.125000\n",
      "Train Epoch: 695 [23040/54000 (43%)] Loss: -494835.031250\n",
      "Train Epoch: 695 [34304/54000 (64%)] Loss: -493955.062500\n",
      "Train Epoch: 695 [45568/54000 (84%)] Loss: -479668.125000\n",
      "    epoch          : 695\n",
      "    loss           : -446092.1315625\n",
      "    val_loss       : -447476.32578125\n",
      "Train Epoch: 696 [512/54000 (1%)] Loss: -487262.625000\n",
      "Train Epoch: 696 [11776/54000 (22%)] Loss: -506966.187500\n",
      "Train Epoch: 696 [23040/54000 (43%)] Loss: -505198.812500\n",
      "Train Epoch: 696 [34304/54000 (64%)] Loss: -421204.562500\n",
      "Train Epoch: 696 [45568/54000 (84%)] Loss: -416137.750000\n",
      "    epoch          : 696\n",
      "    loss           : -446167.72625\n",
      "    val_loss       : -446542.95\n",
      "Train Epoch: 697 [512/54000 (1%)] Loss: -504765.937500\n",
      "Train Epoch: 697 [11776/54000 (22%)] Loss: -467959.843750\n",
      "Train Epoch: 697 [23040/54000 (43%)] Loss: -488565.718750\n",
      "Train Epoch: 697 [34304/54000 (64%)] Loss: -422333.875000\n",
      "Train Epoch: 697 [45568/54000 (84%)] Loss: -487802.875000\n",
      "    epoch          : 697\n",
      "    loss           : -444366.77625\n",
      "    val_loss       : -447147.540625\n",
      "Train Epoch: 698 [512/54000 (1%)] Loss: -385056.781250\n",
      "Train Epoch: 698 [11776/54000 (22%)] Loss: -379935.937500\n",
      "Train Epoch: 698 [23040/54000 (43%)] Loss: -412681.218750\n",
      "Train Epoch: 698 [34304/54000 (64%)] Loss: -472565.406250\n",
      "Train Epoch: 698 [45568/54000 (84%)] Loss: -472285.125000\n",
      "    epoch          : 698\n",
      "    loss           : -446248.6303125\n",
      "    val_loss       : -447508.0703125\n",
      "Train Epoch: 699 [512/54000 (1%)] Loss: -469834.062500\n",
      "Train Epoch: 699 [11776/54000 (22%)] Loss: -479893.562500\n",
      "Train Epoch: 699 [23040/54000 (43%)] Loss: -427297.875000\n",
      "Train Epoch: 699 [34304/54000 (64%)] Loss: -522058.718750\n",
      "Train Epoch: 699 [45568/54000 (84%)] Loss: -475643.906250\n",
      "    epoch          : 699\n",
      "    loss           : -445257.4890625\n",
      "    val_loss       : -444922.74765625\n",
      "Train Epoch: 700 [512/54000 (1%)] Loss: -491309.593750\n",
      "Train Epoch: 700 [11776/54000 (22%)] Loss: -504880.375000\n",
      "Train Epoch: 700 [23040/54000 (43%)] Loss: -386819.437500\n",
      "Train Epoch: 700 [34304/54000 (64%)] Loss: -422574.343750\n",
      "Train Epoch: 700 [45568/54000 (84%)] Loss: -491131.625000\n",
      "    epoch          : 700\n",
      "    loss           : -445516.6896875\n",
      "    val_loss       : -447871.21875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch700.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 701 [512/54000 (1%)] Loss: -392515.687500\n",
      "Train Epoch: 701 [11776/54000 (22%)] Loss: -507558.000000\n",
      "Train Epoch: 701 [23040/54000 (43%)] Loss: -505263.500000\n",
      "Train Epoch: 701 [34304/54000 (64%)] Loss: -529188.000000\n",
      "Train Epoch: 701 [45568/54000 (84%)] Loss: -477828.000000\n",
      "    epoch          : 701\n",
      "    loss           : -446965.48125\n",
      "    val_loss       : -448385.71484375\n",
      "Train Epoch: 702 [512/54000 (1%)] Loss: -416459.187500\n",
      "Train Epoch: 702 [11776/54000 (22%)] Loss: -392089.687500\n",
      "Train Epoch: 702 [23040/54000 (43%)] Loss: -381117.093750\n",
      "Train Epoch: 702 [34304/54000 (64%)] Loss: -478217.250000\n",
      "Train Epoch: 702 [45568/54000 (84%)] Loss: -494564.375000\n",
      "    epoch          : 702\n",
      "    loss           : -446938.1871875\n",
      "    val_loss       : -449142.71875\n",
      "Train Epoch: 703 [512/54000 (1%)] Loss: -378300.875000\n",
      "Train Epoch: 703 [11776/54000 (22%)] Loss: -490230.937500\n",
      "Train Epoch: 703 [23040/54000 (43%)] Loss: -493837.375000\n",
      "Train Epoch: 703 [34304/54000 (64%)] Loss: -426233.500000\n",
      "Train Epoch: 703 [45568/54000 (84%)] Loss: -442615.531250\n",
      "    epoch          : 703\n",
      "    loss           : -446582.3696875\n",
      "    val_loss       : -446255.8921875\n",
      "Train Epoch: 704 [512/54000 (1%)] Loss: -422791.687500\n",
      "Train Epoch: 704 [11776/54000 (22%)] Loss: -504593.562500\n",
      "Train Epoch: 704 [23040/54000 (43%)] Loss: -468089.312500\n",
      "Train Epoch: 704 [34304/54000 (64%)] Loss: -381066.312500\n",
      "Train Epoch: 704 [45568/54000 (84%)] Loss: -475130.937500\n",
      "    epoch          : 704\n",
      "    loss           : -446692.8965625\n",
      "    val_loss       : -446848.57421875\n",
      "Train Epoch: 705 [512/54000 (1%)] Loss: -409756.187500\n",
      "Train Epoch: 705 [11776/54000 (22%)] Loss: -473554.375000\n",
      "Train Epoch: 705 [23040/54000 (43%)] Loss: -478472.500000\n",
      "Train Epoch: 705 [34304/54000 (64%)] Loss: -426080.093750\n",
      "Train Epoch: 705 [45568/54000 (84%)] Loss: -489634.031250\n",
      "    epoch          : 705\n",
      "    loss           : -447007.5396875\n",
      "    val_loss       : -449174.21953125\n",
      "Train Epoch: 706 [512/54000 (1%)] Loss: -509925.812500\n",
      "Train Epoch: 706 [11776/54000 (22%)] Loss: -506618.000000\n",
      "Train Epoch: 706 [23040/54000 (43%)] Loss: -383792.375000\n",
      "Train Epoch: 706 [34304/54000 (64%)] Loss: -391067.000000\n",
      "Train Epoch: 706 [45568/54000 (84%)] Loss: -451059.750000\n",
      "    epoch          : 706\n",
      "    loss           : -447948.854375\n",
      "    val_loss       : -448427.08828125\n",
      "Train Epoch: 707 [512/54000 (1%)] Loss: -425879.781250\n",
      "Train Epoch: 707 [11776/54000 (22%)] Loss: -383725.687500\n",
      "Train Epoch: 707 [23040/54000 (43%)] Loss: -390107.531250\n",
      "Train Epoch: 707 [34304/54000 (64%)] Loss: -392395.687500\n",
      "Train Epoch: 707 [45568/54000 (84%)] Loss: -474174.750000\n",
      "    epoch          : 707\n",
      "    loss           : -448618.2684375\n",
      "    val_loss       : -449717.70625\n",
      "Train Epoch: 708 [512/54000 (1%)] Loss: -479293.281250\n",
      "Train Epoch: 708 [11776/54000 (22%)] Loss: -471176.593750\n",
      "Train Epoch: 708 [23040/54000 (43%)] Loss: -472489.281250\n",
      "Train Epoch: 708 [34304/54000 (64%)] Loss: -371502.281250\n",
      "Train Epoch: 708 [45568/54000 (84%)] Loss: -385260.531250\n",
      "    epoch          : 708\n",
      "    loss           : -445134.6228125\n",
      "    val_loss       : -444910.82890625\n",
      "Train Epoch: 709 [512/54000 (1%)] Loss: -477989.062500\n",
      "Train Epoch: 709 [11776/54000 (22%)] Loss: -412293.437500\n",
      "Train Epoch: 709 [23040/54000 (43%)] Loss: -478474.031250\n",
      "Train Epoch: 709 [34304/54000 (64%)] Loss: -492409.781250\n",
      "Train Epoch: 709 [45568/54000 (84%)] Loss: -498169.875000\n",
      "    epoch          : 709\n",
      "    loss           : -448131.9925\n",
      "    val_loss       : -449905.1\n",
      "Train Epoch: 710 [512/54000 (1%)] Loss: -507174.625000\n",
      "Train Epoch: 710 [11776/54000 (22%)] Loss: -472972.562500\n",
      "Train Epoch: 710 [23040/54000 (43%)] Loss: -473088.500000\n",
      "Train Epoch: 710 [34304/54000 (64%)] Loss: -480904.312500\n",
      "Train Epoch: 710 [45568/54000 (84%)] Loss: -400505.062500\n",
      "    epoch          : 710\n",
      "    loss           : -448619.0365625\n",
      "    val_loss       : -449180.4140625\n",
      "Train Epoch: 711 [512/54000 (1%)] Loss: -387691.937500\n",
      "Train Epoch: 711 [11776/54000 (22%)] Loss: -384047.312500\n",
      "Train Epoch: 711 [23040/54000 (43%)] Loss: -390024.625000\n",
      "Train Epoch: 711 [34304/54000 (64%)] Loss: -353973.750000\n",
      "Train Epoch: 711 [45568/54000 (84%)] Loss: -375310.000000\n",
      "    epoch          : 711\n",
      "    loss           : -448343.0053125\n",
      "    val_loss       : -448606.02578125\n",
      "Train Epoch: 712 [512/54000 (1%)] Loss: -527186.562500\n",
      "Train Epoch: 712 [11776/54000 (22%)] Loss: -509873.875000\n",
      "Train Epoch: 712 [23040/54000 (43%)] Loss: -495835.031250\n",
      "Train Epoch: 712 [34304/54000 (64%)] Loss: -505994.781250\n",
      "Train Epoch: 712 [45568/54000 (84%)] Loss: -489504.218750\n",
      "    epoch          : 712\n",
      "    loss           : -448518.0953125\n",
      "    val_loss       : -449989.61875\n",
      "Train Epoch: 713 [512/54000 (1%)] Loss: -422579.812500\n",
      "Train Epoch: 713 [11776/54000 (22%)] Loss: -475207.906250\n",
      "Train Epoch: 713 [23040/54000 (43%)] Loss: -473345.312500\n",
      "Train Epoch: 713 [34304/54000 (64%)] Loss: -509673.000000\n",
      "Train Epoch: 713 [45568/54000 (84%)] Loss: -482791.937500\n",
      "    epoch          : 713\n",
      "    loss           : -448706.3825\n",
      "    val_loss       : -450368.3234375\n",
      "Train Epoch: 714 [512/54000 (1%)] Loss: -528504.500000\n",
      "Train Epoch: 714 [11776/54000 (22%)] Loss: -400053.437500\n",
      "Train Epoch: 714 [23040/54000 (43%)] Loss: -492943.125000\n",
      "Train Epoch: 714 [34304/54000 (64%)] Loss: -479399.375000\n",
      "Train Epoch: 714 [45568/54000 (84%)] Loss: -386385.281250\n",
      "    epoch          : 714\n",
      "    loss           : -449628.61\n",
      "    val_loss       : -451368.65859375\n",
      "Train Epoch: 715 [512/54000 (1%)] Loss: -476470.531250\n",
      "Train Epoch: 715 [11776/54000 (22%)] Loss: -415615.312500\n",
      "Train Epoch: 715 [23040/54000 (43%)] Loss: -387650.062500\n",
      "Train Epoch: 715 [34304/54000 (64%)] Loss: -423851.281250\n",
      "Train Epoch: 715 [45568/54000 (84%)] Loss: -473950.156250\n",
      "    epoch          : 715\n",
      "    loss           : -448898.1534375\n",
      "    val_loss       : -450482.3515625\n",
      "Train Epoch: 716 [512/54000 (1%)] Loss: -388967.187500\n",
      "Train Epoch: 716 [11776/54000 (22%)] Loss: -477053.875000\n",
      "Train Epoch: 716 [23040/54000 (43%)] Loss: -389751.937500\n",
      "Train Epoch: 716 [34304/54000 (64%)] Loss: -529413.937500\n",
      "Train Epoch: 716 [45568/54000 (84%)] Loss: -476283.812500\n",
      "    epoch          : 716\n",
      "    loss           : -448923.2046875\n",
      "    val_loss       : -450274.403125\n",
      "Train Epoch: 717 [512/54000 (1%)] Loss: -476966.687500\n",
      "Train Epoch: 717 [11776/54000 (22%)] Loss: -429674.125000\n",
      "Train Epoch: 717 [23040/54000 (43%)] Loss: -384585.468750\n",
      "Train Epoch: 717 [34304/54000 (64%)] Loss: -381126.218750\n",
      "Train Epoch: 717 [45568/54000 (84%)] Loss: -385873.062500\n",
      "    epoch          : 717\n",
      "    loss           : -449276.384375\n",
      "    val_loss       : -450878.97578125\n",
      "Train Epoch: 718 [512/54000 (1%)] Loss: -394652.000000\n",
      "Train Epoch: 718 [11776/54000 (22%)] Loss: -420195.625000\n",
      "Train Epoch: 718 [23040/54000 (43%)] Loss: -463056.750000\n",
      "Train Epoch: 718 [34304/54000 (64%)] Loss: -419029.156250\n",
      "Train Epoch: 718 [45568/54000 (84%)] Loss: -500980.906250\n",
      "    epoch          : 718\n",
      "    loss           : -450244.2634375\n",
      "    val_loss       : -451490.11015625\n",
      "Train Epoch: 719 [512/54000 (1%)] Loss: -386752.062500\n",
      "Train Epoch: 719 [11776/54000 (22%)] Loss: -500044.875000\n",
      "Train Epoch: 719 [23040/54000 (43%)] Loss: -384773.718750\n",
      "Train Epoch: 719 [34304/54000 (64%)] Loss: -483304.937500\n",
      "Train Epoch: 719 [45568/54000 (84%)] Loss: -390570.687500\n",
      "    epoch          : 719\n",
      "    loss           : -450260.5390625\n",
      "    val_loss       : -451126.790625\n",
      "Train Epoch: 720 [512/54000 (1%)] Loss: -536450.750000\n",
      "Train Epoch: 720 [11776/54000 (22%)] Loss: -498596.687500\n",
      "Train Epoch: 720 [23040/54000 (43%)] Loss: -508516.500000\n",
      "Train Epoch: 720 [34304/54000 (64%)] Loss: -533532.750000\n",
      "Train Epoch: 720 [45568/54000 (84%)] Loss: -382526.875000\n",
      "    epoch          : 720\n",
      "    loss           : -450571.4790625\n",
      "    val_loss       : -452108.38671875\n",
      "Train Epoch: 721 [512/54000 (1%)] Loss: -384099.750000\n",
      "Train Epoch: 721 [11776/54000 (22%)] Loss: -510629.750000\n",
      "Train Epoch: 721 [23040/54000 (43%)] Loss: -450583.812500\n",
      "Train Epoch: 721 [34304/54000 (64%)] Loss: -484349.406250\n",
      "Train Epoch: 721 [45568/54000 (84%)] Loss: -482871.375000\n",
      "    epoch          : 721\n",
      "    loss           : -451383.8771875\n",
      "    val_loss       : -452427.14765625\n",
      "Train Epoch: 722 [512/54000 (1%)] Loss: -388720.468750\n",
      "Train Epoch: 722 [11776/54000 (22%)] Loss: -362992.687500\n",
      "Train Epoch: 722 [23040/54000 (43%)] Loss: -511074.687500\n",
      "Train Epoch: 722 [34304/54000 (64%)] Loss: -383684.375000\n",
      "Train Epoch: 722 [45568/54000 (84%)] Loss: -481533.781250\n",
      "    epoch          : 722\n",
      "    loss           : -451347.4165625\n",
      "    val_loss       : -452461.9484375\n",
      "Train Epoch: 723 [512/54000 (1%)] Loss: -385595.312500\n",
      "Train Epoch: 723 [11776/54000 (22%)] Loss: -395356.406250\n",
      "Train Epoch: 723 [23040/54000 (43%)] Loss: -510591.500000\n",
      "Train Epoch: 723 [34304/54000 (64%)] Loss: -512160.000000\n",
      "Train Epoch: 723 [45568/54000 (84%)] Loss: -478785.093750\n",
      "    epoch          : 723\n",
      "    loss           : -451233.7425\n",
      "    val_loss       : -452291.121875\n",
      "Train Epoch: 724 [512/54000 (1%)] Loss: -381106.250000\n",
      "Train Epoch: 724 [11776/54000 (22%)] Loss: -431075.250000\n",
      "Train Epoch: 724 [23040/54000 (43%)] Loss: -431923.781250\n",
      "Train Epoch: 724 [34304/54000 (64%)] Loss: -424155.062500\n",
      "Train Epoch: 724 [45568/54000 (84%)] Loss: -499315.406250\n",
      "    epoch          : 724\n",
      "    loss           : -450937.7321875\n",
      "    val_loss       : -452267.703125\n",
      "Train Epoch: 725 [512/54000 (1%)] Loss: -484992.437500\n",
      "Train Epoch: 725 [11776/54000 (22%)] Loss: -391503.250000\n",
      "Train Epoch: 725 [23040/54000 (43%)] Loss: -452046.593750\n",
      "Train Epoch: 725 [34304/54000 (64%)] Loss: -386969.218750\n",
      "Train Epoch: 725 [45568/54000 (84%)] Loss: -393085.312500\n",
      "    epoch          : 725\n",
      "    loss           : -451582.560625\n",
      "    val_loss       : -452799.81015625\n",
      "Train Epoch: 726 [512/54000 (1%)] Loss: -422257.406250\n",
      "Train Epoch: 726 [11776/54000 (22%)] Loss: -367640.343750\n",
      "Train Epoch: 726 [23040/54000 (43%)] Loss: -531120.375000\n",
      "Train Epoch: 726 [34304/54000 (64%)] Loss: -481331.406250\n",
      "Train Epoch: 726 [45568/54000 (84%)] Loss: -531754.500000\n",
      "    epoch          : 726\n",
      "    loss           : -450855.705\n",
      "    val_loss       : -449947.80546875\n",
      "Train Epoch: 727 [512/54000 (1%)] Loss: -531442.375000\n",
      "Train Epoch: 727 [11776/54000 (22%)] Loss: -357912.906250\n",
      "Train Epoch: 727 [23040/54000 (43%)] Loss: -382885.500000\n",
      "Train Epoch: 727 [34304/54000 (64%)] Loss: -501527.218750\n",
      "Train Epoch: 727 [45568/54000 (84%)] Loss: -433507.593750\n",
      "    epoch          : 727\n",
      "    loss           : -448734.674375\n",
      "    val_loss       : -452450.3015625\n",
      "Train Epoch: 728 [512/54000 (1%)] Loss: -429197.687500\n",
      "Train Epoch: 728 [11776/54000 (22%)] Loss: -514143.937500\n",
      "Train Epoch: 728 [23040/54000 (43%)] Loss: -476045.562500\n",
      "Train Epoch: 728 [34304/54000 (64%)] Loss: -399305.656250\n",
      "Train Epoch: 728 [45568/54000 (84%)] Loss: -474616.312500\n",
      "    epoch          : 728\n",
      "    loss           : -451836.275\n",
      "    val_loss       : -453501.05078125\n",
      "Train Epoch: 729 [512/54000 (1%)] Loss: -424009.312500\n",
      "Train Epoch: 729 [11776/54000 (22%)] Loss: -497741.468750\n",
      "Train Epoch: 729 [23040/54000 (43%)] Loss: -498777.562500\n",
      "Train Epoch: 729 [34304/54000 (64%)] Loss: -428426.531250\n",
      "Train Epoch: 729 [45568/54000 (84%)] Loss: -497745.375000\n",
      "    epoch          : 729\n",
      "    loss           : -452339.6478125\n",
      "    val_loss       : -452796.26015625\n",
      "Train Epoch: 730 [512/54000 (1%)] Loss: -384413.000000\n",
      "Train Epoch: 730 [11776/54000 (22%)] Loss: -498482.500000\n",
      "Train Epoch: 730 [23040/54000 (43%)] Loss: -532504.875000\n",
      "Train Epoch: 730 [34304/54000 (64%)] Loss: -421792.062500\n",
      "Train Epoch: 730 [45568/54000 (84%)] Loss: -387768.781250\n",
      "    epoch          : 730\n",
      "    loss           : -450200.023125\n",
      "    val_loss       : -452136.0390625\n",
      "Train Epoch: 731 [512/54000 (1%)] Loss: -476353.187500\n",
      "Train Epoch: 731 [11776/54000 (22%)] Loss: -475076.406250\n",
      "Train Epoch: 731 [23040/54000 (43%)] Loss: -495948.125000\n",
      "Train Epoch: 731 [34304/54000 (64%)] Loss: -540121.312500\n",
      "Train Epoch: 731 [45568/54000 (84%)] Loss: -497557.750000\n",
      "    epoch          : 731\n",
      "    loss           : -452203.490625\n",
      "    val_loss       : -453757.75859375\n",
      "Train Epoch: 732 [512/54000 (1%)] Loss: -514533.093750\n",
      "Train Epoch: 732 [11776/54000 (22%)] Loss: -394296.937500\n",
      "Train Epoch: 732 [23040/54000 (43%)] Loss: -420474.812500\n",
      "Train Epoch: 732 [34304/54000 (64%)] Loss: -396115.687500\n",
      "Train Epoch: 732 [45568/54000 (84%)] Loss: -487683.187500\n",
      "    epoch          : 732\n",
      "    loss           : -452898.1365625\n",
      "    val_loss       : -452876.846875\n",
      "Train Epoch: 733 [512/54000 (1%)] Loss: -451602.375000\n",
      "Train Epoch: 733 [11776/54000 (22%)] Loss: -512765.687500\n",
      "Train Epoch: 733 [23040/54000 (43%)] Loss: -513787.250000\n",
      "Train Epoch: 733 [34304/54000 (64%)] Loss: -430784.343750\n",
      "Train Epoch: 733 [45568/54000 (84%)] Loss: -533022.125000\n",
      "    epoch          : 733\n",
      "    loss           : -450685.5528125\n",
      "    val_loss       : -451729.1390625\n",
      "Train Epoch: 734 [512/54000 (1%)] Loss: -504137.531250\n",
      "Train Epoch: 734 [11776/54000 (22%)] Loss: -398771.593750\n",
      "Train Epoch: 734 [23040/54000 (43%)] Loss: -512660.750000\n",
      "Train Epoch: 734 [34304/54000 (64%)] Loss: -485967.843750\n",
      "Train Epoch: 734 [45568/54000 (84%)] Loss: -498476.656250\n",
      "    epoch          : 734\n",
      "    loss           : -452674.25625\n",
      "    val_loss       : -454289.51015625\n",
      "Train Epoch: 735 [512/54000 (1%)] Loss: -501393.312500\n",
      "Train Epoch: 735 [11776/54000 (22%)] Loss: -389799.187500\n",
      "Train Epoch: 735 [23040/54000 (43%)] Loss: -420592.343750\n",
      "Train Epoch: 735 [34304/54000 (64%)] Loss: -485021.250000\n",
      "Train Epoch: 735 [45568/54000 (84%)] Loss: -477918.875000\n",
      "    epoch          : 735\n",
      "    loss           : -453438.2365625\n",
      "    val_loss       : -454028.66328125\n",
      "Train Epoch: 736 [512/54000 (1%)] Loss: -361746.625000\n",
      "Train Epoch: 736 [11776/54000 (22%)] Loss: -394227.125000\n",
      "Train Epoch: 736 [23040/54000 (43%)] Loss: -421581.281250\n",
      "Train Epoch: 736 [34304/54000 (64%)] Loss: -497437.062500\n",
      "Train Epoch: 736 [45568/54000 (84%)] Loss: -383931.187500\n",
      "    epoch          : 736\n",
      "    loss           : -453211.7596875\n",
      "    val_loss       : -454902.7828125\n",
      "Train Epoch: 737 [512/54000 (1%)] Loss: -514288.500000\n",
      "Train Epoch: 737 [11776/54000 (22%)] Loss: -515848.562500\n",
      "Train Epoch: 737 [23040/54000 (43%)] Loss: -396548.750000\n",
      "Train Epoch: 737 [34304/54000 (64%)] Loss: -380693.593750\n",
      "Train Epoch: 737 [45568/54000 (84%)] Loss: -482742.687500\n",
      "    epoch          : 737\n",
      "    loss           : -453461.7421875\n",
      "    val_loss       : -454581.98203125\n",
      "Train Epoch: 738 [512/54000 (1%)] Loss: -485256.718750\n",
      "Train Epoch: 738 [11776/54000 (22%)] Loss: -418951.968750\n",
      "Train Epoch: 738 [23040/54000 (43%)] Loss: -515651.000000\n",
      "Train Epoch: 738 [34304/54000 (64%)] Loss: -501776.187500\n",
      "Train Epoch: 738 [45568/54000 (84%)] Loss: -480247.562500\n",
      "    epoch          : 738\n",
      "    loss           : -453564.58875\n",
      "    val_loss       : -455149.58046875\n",
      "Train Epoch: 739 [512/54000 (1%)] Loss: -387273.125000\n",
      "Train Epoch: 739 [11776/54000 (22%)] Loss: -395842.000000\n",
      "Train Epoch: 739 [23040/54000 (43%)] Loss: -417876.625000\n",
      "Train Epoch: 739 [34304/54000 (64%)] Loss: -538461.250000\n",
      "Train Epoch: 739 [45568/54000 (84%)] Loss: -484240.500000\n",
      "    epoch          : 739\n",
      "    loss           : -454588.0971875\n",
      "    val_loss       : -455335.77890625\n",
      "Train Epoch: 740 [512/54000 (1%)] Loss: -504845.812500\n",
      "Train Epoch: 740 [11776/54000 (22%)] Loss: -492155.062500\n",
      "Train Epoch: 740 [23040/54000 (43%)] Loss: -400206.750000\n",
      "Train Epoch: 740 [34304/54000 (64%)] Loss: -481357.437500\n",
      "Train Epoch: 740 [45568/54000 (84%)] Loss: -478925.562500\n",
      "    epoch          : 740\n",
      "    loss           : -454154.35625\n",
      "    val_loss       : -454895.5484375\n",
      "Train Epoch: 741 [512/54000 (1%)] Loss: -384086.750000\n",
      "Train Epoch: 741 [11776/54000 (22%)] Loss: -387998.500000\n",
      "Train Epoch: 741 [23040/54000 (43%)] Loss: -488952.625000\n",
      "Train Epoch: 741 [34304/54000 (64%)] Loss: -428043.375000\n",
      "Train Epoch: 741 [45568/54000 (84%)] Loss: -483037.093750\n",
      "    epoch          : 741\n",
      "    loss           : -454574.2284375\n",
      "    val_loss       : -455665.821875\n",
      "Train Epoch: 742 [512/54000 (1%)] Loss: -390196.843750\n",
      "Train Epoch: 742 [11776/54000 (22%)] Loss: -390755.343750\n",
      "Train Epoch: 742 [23040/54000 (43%)] Loss: -388821.937500\n",
      "Train Epoch: 742 [34304/54000 (64%)] Loss: -418993.343750\n",
      "Train Epoch: 742 [45568/54000 (84%)] Loss: -539346.125000\n",
      "    epoch          : 742\n",
      "    loss           : -454750.591875\n",
      "    val_loss       : -455510.41796875\n",
      "Train Epoch: 743 [512/54000 (1%)] Loss: -539056.187500\n",
      "Train Epoch: 743 [11776/54000 (22%)] Loss: -482047.812500\n",
      "Train Epoch: 743 [23040/54000 (43%)] Loss: -388663.937500\n",
      "Train Epoch: 743 [34304/54000 (64%)] Loss: -484132.656250\n",
      "Train Epoch: 743 [45568/54000 (84%)] Loss: -495519.875000\n",
      "    epoch          : 743\n",
      "    loss           : -454933.64125\n",
      "    val_loss       : -455989.94375\n",
      "Train Epoch: 744 [512/54000 (1%)] Loss: -502953.937500\n",
      "Train Epoch: 744 [11776/54000 (22%)] Loss: -395431.906250\n",
      "Train Epoch: 744 [23040/54000 (43%)] Loss: -499706.437500\n",
      "Train Epoch: 744 [34304/54000 (64%)] Loss: -503330.000000\n",
      "Train Epoch: 744 [45568/54000 (84%)] Loss: -387753.687500\n",
      "    epoch          : 744\n",
      "    loss           : -455467.144375\n",
      "    val_loss       : -456517.746875\n",
      "Train Epoch: 745 [512/54000 (1%)] Loss: -385838.968750\n",
      "Train Epoch: 745 [11776/54000 (22%)] Loss: -394598.812500\n",
      "Train Epoch: 745 [23040/54000 (43%)] Loss: -487263.375000\n",
      "Train Epoch: 745 [34304/54000 (64%)] Loss: -476673.468750\n",
      "Train Epoch: 745 [45568/54000 (84%)] Loss: -391504.312500\n",
      "    epoch          : 745\n",
      "    loss           : -455534.3103125\n",
      "    val_loss       : -457273.5515625\n",
      "Train Epoch: 746 [512/54000 (1%)] Loss: -425940.062500\n",
      "Train Epoch: 746 [11776/54000 (22%)] Loss: -426213.031250\n",
      "Train Epoch: 746 [23040/54000 (43%)] Loss: -392823.093750\n",
      "Train Epoch: 746 [34304/54000 (64%)] Loss: -427674.750000\n",
      "Train Epoch: 746 [45568/54000 (84%)] Loss: -478139.312500\n",
      "    epoch          : 746\n",
      "    loss           : -455879.1559375\n",
      "    val_loss       : -456911.10859375\n",
      "Train Epoch: 747 [512/54000 (1%)] Loss: -494940.187500\n",
      "Train Epoch: 747 [11776/54000 (22%)] Loss: -542133.250000\n",
      "Train Epoch: 747 [23040/54000 (43%)] Loss: -396420.312500\n",
      "Train Epoch: 747 [34304/54000 (64%)] Loss: -397147.312500\n",
      "Train Epoch: 747 [45568/54000 (84%)] Loss: -474286.062500\n",
      "    epoch          : 747\n",
      "    loss           : -455110.2153125\n",
      "    val_loss       : -456593.31953125\n",
      "Train Epoch: 748 [512/54000 (1%)] Loss: -516621.312500\n",
      "Train Epoch: 748 [11776/54000 (22%)] Loss: -517556.250000\n",
      "Train Epoch: 748 [23040/54000 (43%)] Loss: -485387.437500\n",
      "Train Epoch: 748 [34304/54000 (64%)] Loss: -498001.187500\n",
      "Train Epoch: 748 [45568/54000 (84%)] Loss: -500800.468750\n",
      "    epoch          : 748\n",
      "    loss           : -454532.21125\n",
      "    val_loss       : -456107.50390625\n",
      "Train Epoch: 749 [512/54000 (1%)] Loss: -540404.625000\n",
      "Train Epoch: 749 [11776/54000 (22%)] Loss: -514559.125000\n",
      "Train Epoch: 749 [23040/54000 (43%)] Loss: -478353.656250\n",
      "Train Epoch: 749 [34304/54000 (64%)] Loss: -532946.625000\n",
      "Train Epoch: 749 [45568/54000 (84%)] Loss: -488025.187500\n",
      "    epoch          : 749\n",
      "    loss           : -453003.4625\n",
      "    val_loss       : -456076.6515625\n",
      "Train Epoch: 750 [512/54000 (1%)] Loss: -491821.750000\n",
      "Train Epoch: 750 [11776/54000 (22%)] Loss: -391989.562500\n",
      "Train Epoch: 750 [23040/54000 (43%)] Loss: -393830.312500\n",
      "Train Epoch: 750 [34304/54000 (64%)] Loss: -539796.625000\n",
      "Train Epoch: 750 [45568/54000 (84%)] Loss: -495738.656250\n",
      "    epoch          : 750\n",
      "    loss           : -455374.2871875\n",
      "    val_loss       : -456974.82421875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch750.pth ...\n",
      "Train Epoch: 751 [512/54000 (1%)] Loss: -498734.968750\n",
      "Train Epoch: 751 [11776/54000 (22%)] Loss: -391072.812500\n",
      "Train Epoch: 751 [23040/54000 (43%)] Loss: -502728.718750\n",
      "Train Epoch: 751 [34304/54000 (64%)] Loss: -391832.125000\n",
      "Train Epoch: 751 [45568/54000 (84%)] Loss: -481834.625000\n",
      "    epoch          : 751\n",
      "    loss           : -456215.3071875\n",
      "    val_loss       : -457153.57421875\n",
      "Train Epoch: 752 [512/54000 (1%)] Loss: -396914.062500\n",
      "Train Epoch: 752 [11776/54000 (22%)] Loss: -398152.718750\n",
      "Train Epoch: 752 [23040/54000 (43%)] Loss: -432470.093750\n",
      "Train Epoch: 752 [34304/54000 (64%)] Loss: -504005.781250\n",
      "Train Epoch: 752 [45568/54000 (84%)] Loss: -490522.906250\n",
      "    epoch          : 752\n",
      "    loss           : -456058.5603125\n",
      "    val_loss       : -456147.22109375\n",
      "Train Epoch: 753 [512/54000 (1%)] Loss: -391316.781250\n",
      "Train Epoch: 753 [11776/54000 (22%)] Loss: -520656.250000\n",
      "Train Epoch: 753 [23040/54000 (43%)] Loss: -422019.968750\n",
      "Train Epoch: 753 [34304/54000 (64%)] Loss: -477171.750000\n",
      "Train Epoch: 753 [45568/54000 (84%)] Loss: -425751.156250\n",
      "    epoch          : 753\n",
      "    loss           : -456341.4940625\n",
      "    val_loss       : -458282.221875\n",
      "Train Epoch: 754 [512/54000 (1%)] Loss: -515880.750000\n",
      "Train Epoch: 754 [11776/54000 (22%)] Loss: -403475.812500\n",
      "Train Epoch: 754 [23040/54000 (43%)] Loss: -389455.468750\n",
      "Train Epoch: 754 [34304/54000 (64%)] Loss: -427329.562500\n",
      "Train Epoch: 754 [45568/54000 (84%)] Loss: -431469.906250\n",
      "    epoch          : 754\n",
      "    loss           : -456707.7121875\n",
      "    val_loss       : -457250.48203125\n",
      "Train Epoch: 755 [512/54000 (1%)] Loss: -505285.125000\n",
      "Train Epoch: 755 [11776/54000 (22%)] Loss: -501945.500000\n",
      "Train Epoch: 755 [23040/54000 (43%)] Loss: -389640.187500\n",
      "Train Epoch: 755 [34304/54000 (64%)] Loss: -389713.250000\n",
      "Train Epoch: 755 [45568/54000 (84%)] Loss: -364251.062500\n",
      "    epoch          : 755\n",
      "    loss           : -456105.4346875\n",
      "    val_loss       : -457963.63359375\n",
      "Train Epoch: 756 [512/54000 (1%)] Loss: -389963.812500\n",
      "Train Epoch: 756 [11776/54000 (22%)] Loss: -420202.062500\n",
      "Train Epoch: 756 [23040/54000 (43%)] Loss: -491128.781250\n",
      "Train Epoch: 756 [34304/54000 (64%)] Loss: -540831.500000\n",
      "Train Epoch: 756 [45568/54000 (84%)] Loss: -485317.562500\n",
      "    epoch          : 756\n",
      "    loss           : -457329.348125\n",
      "    val_loss       : -458474.41171875\n",
      "Train Epoch: 757 [512/54000 (1%)] Loss: -397100.437500\n",
      "Train Epoch: 757 [11776/54000 (22%)] Loss: -540307.875000\n",
      "Train Epoch: 757 [23040/54000 (43%)] Loss: -426490.031250\n",
      "Train Epoch: 757 [34304/54000 (64%)] Loss: -518282.125000\n",
      "Train Epoch: 757 [45568/54000 (84%)] Loss: -518283.312500\n",
      "    epoch          : 757\n",
      "    loss           : -456741.49\n",
      "    val_loss       : -457891.84609375\n",
      "Train Epoch: 758 [512/54000 (1%)] Loss: -438462.687500\n",
      "Train Epoch: 758 [11776/54000 (22%)] Loss: -422489.093750\n",
      "Train Epoch: 758 [23040/54000 (43%)] Loss: -544355.625000\n",
      "Train Epoch: 758 [34304/54000 (64%)] Loss: -539892.875000\n",
      "Train Epoch: 758 [45568/54000 (84%)] Loss: -362961.093750\n",
      "    epoch          : 758\n",
      "    loss           : -456907.0496875\n",
      "    val_loss       : -458724.209375\n",
      "Train Epoch: 759 [512/54000 (1%)] Loss: -400175.531250\n",
      "Train Epoch: 759 [11776/54000 (22%)] Loss: -507048.000000\n",
      "Train Epoch: 759 [23040/54000 (43%)] Loss: -504172.562500\n",
      "Train Epoch: 759 [34304/54000 (64%)] Loss: -463520.687500\n",
      "Train Epoch: 759 [45568/54000 (84%)] Loss: -492572.531250\n",
      "    epoch          : 759\n",
      "    loss           : -458102.85875\n",
      "    val_loss       : -458981.58671875\n",
      "Train Epoch: 760 [512/54000 (1%)] Loss: -420941.625000\n",
      "Train Epoch: 760 [11776/54000 (22%)] Loss: -522762.843750\n",
      "Train Epoch: 760 [23040/54000 (43%)] Loss: -423101.062500\n",
      "Train Epoch: 760 [34304/54000 (64%)] Loss: -431461.843750\n",
      "Train Epoch: 760 [45568/54000 (84%)] Loss: -493806.656250\n",
      "    epoch          : 760\n",
      "    loss           : -457818.23875\n",
      "    val_loss       : -458339.1703125\n",
      "Train Epoch: 761 [512/54000 (1%)] Loss: -484403.187500\n",
      "Train Epoch: 761 [11776/54000 (22%)] Loss: -508272.218750\n",
      "Train Epoch: 761 [23040/54000 (43%)] Loss: -504779.937500\n",
      "Train Epoch: 761 [34304/54000 (64%)] Loss: -426690.125000\n",
      "Train Epoch: 761 [45568/54000 (84%)] Loss: -480153.437500\n",
      "    epoch          : 761\n",
      "    loss           : -457949.4859375\n",
      "    val_loss       : -458980.00390625\n",
      "Train Epoch: 762 [512/54000 (1%)] Loss: -423858.312500\n",
      "Train Epoch: 762 [11776/54000 (22%)] Loss: -540439.187500\n",
      "Train Epoch: 762 [23040/54000 (43%)] Loss: -433944.062500\n",
      "Train Epoch: 762 [34304/54000 (64%)] Loss: -520033.500000\n",
      "Train Epoch: 762 [45568/54000 (84%)] Loss: -362693.718750\n",
      "    epoch          : 762\n",
      "    loss           : -458236.4471875\n",
      "    val_loss       : -458980.27109375\n",
      "Train Epoch: 763 [512/54000 (1%)] Loss: -425857.343750\n",
      "Train Epoch: 763 [11776/54000 (22%)] Loss: -517269.500000\n",
      "Train Epoch: 763 [23040/54000 (43%)] Loss: -522524.687500\n",
      "Train Epoch: 763 [34304/54000 (64%)] Loss: -357558.281250\n",
      "Train Epoch: 763 [45568/54000 (84%)] Loss: -491598.000000\n",
      "    epoch          : 763\n",
      "    loss           : -458516.17375\n",
      "    val_loss       : -459407.375\n",
      "Train Epoch: 764 [512/54000 (1%)] Loss: -494839.562500\n",
      "Train Epoch: 764 [11776/54000 (22%)] Loss: -423323.500000\n",
      "Train Epoch: 764 [23040/54000 (43%)] Loss: -520632.062500\n",
      "Train Epoch: 764 [34304/54000 (64%)] Loss: -480709.656250\n",
      "Train Epoch: 764 [45568/54000 (84%)] Loss: -485259.187500\n",
      "    epoch          : 764\n",
      "    loss           : -458856.3671875\n",
      "    val_loss       : -459538.36875\n",
      "Train Epoch: 765 [512/54000 (1%)] Loss: -518186.312500\n",
      "Train Epoch: 765 [11776/54000 (22%)] Loss: -520329.187500\n",
      "Train Epoch: 765 [23040/54000 (43%)] Loss: -521236.062500\n",
      "Train Epoch: 765 [34304/54000 (64%)] Loss: -440714.750000\n",
      "Train Epoch: 765 [45568/54000 (84%)] Loss: -494513.937500\n",
      "    epoch          : 765\n",
      "    loss           : -458945.24125\n",
      "    val_loss       : -459876.8890625\n",
      "Train Epoch: 766 [512/54000 (1%)] Loss: -426399.000000\n",
      "Train Epoch: 766 [11776/54000 (22%)] Loss: -544411.500000\n",
      "Train Epoch: 766 [23040/54000 (43%)] Loss: -401282.062500\n",
      "Train Epoch: 766 [34304/54000 (64%)] Loss: -392824.500000\n",
      "Train Epoch: 766 [45568/54000 (84%)] Loss: -398843.812500\n",
      "    epoch          : 766\n",
      "    loss           : -459117.7528125\n",
      "    val_loss       : -460302.24765625\n",
      "Train Epoch: 767 [512/54000 (1%)] Loss: -485211.031250\n",
      "Train Epoch: 767 [11776/54000 (22%)] Loss: -393502.281250\n",
      "Train Epoch: 767 [23040/54000 (43%)] Loss: -423352.156250\n",
      "Train Epoch: 767 [34304/54000 (64%)] Loss: -542402.750000\n",
      "Train Epoch: 767 [45568/54000 (84%)] Loss: -494413.187500\n",
      "    epoch          : 767\n",
      "    loss           : -458701.584375\n",
      "    val_loss       : -460539.90859375\n",
      "Train Epoch: 768 [512/54000 (1%)] Loss: -391896.437500\n",
      "Train Epoch: 768 [11776/54000 (22%)] Loss: -521088.562500\n",
      "Train Epoch: 768 [23040/54000 (43%)] Loss: -520076.937500\n",
      "Train Epoch: 768 [34304/54000 (64%)] Loss: -521441.531250\n",
      "Train Epoch: 768 [45568/54000 (84%)] Loss: -497955.031250\n",
      "    epoch          : 768\n",
      "    loss           : -460034.02875\n",
      "    val_loss       : -460757.121875\n",
      "Train Epoch: 769 [512/54000 (1%)] Loss: -397891.687500\n",
      "Train Epoch: 769 [11776/54000 (22%)] Loss: -396273.437500\n",
      "Train Epoch: 769 [23040/54000 (43%)] Loss: -545131.000000\n",
      "Train Epoch: 769 [34304/54000 (64%)] Loss: -387528.718750\n",
      "Train Epoch: 769 [45568/54000 (84%)] Loss: -494111.343750\n",
      "    epoch          : 769\n",
      "    loss           : -459680.480625\n",
      "    val_loss       : -460613.77265625\n",
      "Train Epoch: 770 [512/54000 (1%)] Loss: -521978.000000\n",
      "Train Epoch: 770 [11776/54000 (22%)] Loss: -520436.812500\n",
      "Train Epoch: 770 [23040/54000 (43%)] Loss: -407586.156250\n",
      "Train Epoch: 770 [34304/54000 (64%)] Loss: -390952.781250\n",
      "Train Epoch: 770 [45568/54000 (84%)] Loss: -437852.000000\n",
      "    epoch          : 770\n",
      "    loss           : -459565.5378125\n",
      "    val_loss       : -461111.42734375\n",
      "Train Epoch: 771 [512/54000 (1%)] Loss: -543819.187500\n",
      "Train Epoch: 771 [11776/54000 (22%)] Loss: -488555.125000\n",
      "Train Epoch: 771 [23040/54000 (43%)] Loss: -504183.187500\n",
      "Train Epoch: 771 [34304/54000 (64%)] Loss: -492548.812500\n",
      "Train Epoch: 771 [45568/54000 (84%)] Loss: -425851.437500\n",
      "    epoch          : 771\n",
      "    loss           : -456866.958125\n",
      "    val_loss       : -451274.01015625\n",
      "Train Epoch: 772 [512/54000 (1%)] Loss: -418021.812500\n",
      "Train Epoch: 772 [11776/54000 (22%)] Loss: -539615.625000\n",
      "Train Epoch: 772 [23040/54000 (43%)] Loss: -390709.593750\n",
      "Train Epoch: 772 [34304/54000 (64%)] Loss: -491102.250000\n",
      "Train Epoch: 772 [45568/54000 (84%)] Loss: -391496.375000\n",
      "    epoch          : 772\n",
      "    loss           : -456890.38625\n",
      "    val_loss       : -461247.2046875\n",
      "Train Epoch: 773 [512/54000 (1%)] Loss: -491106.937500\n",
      "Train Epoch: 773 [11776/54000 (22%)] Loss: -484506.406250\n",
      "Train Epoch: 773 [23040/54000 (43%)] Loss: -390848.093750\n",
      "Train Epoch: 773 [34304/54000 (64%)] Loss: -521810.625000\n",
      "Train Epoch: 773 [45568/54000 (84%)] Loss: -482196.750000\n",
      "    epoch          : 773\n",
      "    loss           : -460491.11375\n",
      "    val_loss       : -461521.25390625\n",
      "Train Epoch: 774 [512/54000 (1%)] Loss: -423801.250000\n",
      "Train Epoch: 774 [11776/54000 (22%)] Loss: -378865.437500\n",
      "Train Epoch: 774 [23040/54000 (43%)] Loss: -393858.875000\n",
      "Train Epoch: 774 [34304/54000 (64%)] Loss: -392968.875000\n",
      "Train Epoch: 774 [45568/54000 (84%)] Loss: -402462.156250\n",
      "    epoch          : 774\n",
      "    loss           : -459820.4584375\n",
      "    val_loss       : -460055.375\n",
      "Train Epoch: 775 [512/54000 (1%)] Loss: -493730.218750\n",
      "Train Epoch: 775 [11776/54000 (22%)] Loss: -504973.000000\n",
      "Train Epoch: 775 [23040/54000 (43%)] Loss: -393240.937500\n",
      "Train Epoch: 775 [34304/54000 (64%)] Loss: -433952.343750\n",
      "Train Epoch: 775 [45568/54000 (84%)] Loss: -496657.468750\n",
      "    epoch          : 775\n",
      "    loss           : -458286.4515625\n",
      "    val_loss       : -459411.68828125\n",
      "Train Epoch: 776 [512/54000 (1%)] Loss: -435449.656250\n",
      "Train Epoch: 776 [11776/54000 (22%)] Loss: -545460.250000\n",
      "Train Epoch: 776 [23040/54000 (43%)] Loss: -425356.500000\n",
      "Train Epoch: 776 [34304/54000 (64%)] Loss: -481511.437500\n",
      "Train Epoch: 776 [45568/54000 (84%)] Loss: -494669.000000\n",
      "    epoch          : 776\n",
      "    loss           : -459601.31375\n",
      "    val_loss       : -461738.85859375\n",
      "Train Epoch: 777 [512/54000 (1%)] Loss: -524797.625000\n",
      "Train Epoch: 777 [11776/54000 (22%)] Loss: -429572.156250\n",
      "Train Epoch: 777 [23040/54000 (43%)] Loss: -407824.875000\n",
      "Train Epoch: 777 [34304/54000 (64%)] Loss: -512079.312500\n",
      "Train Epoch: 777 [45568/54000 (84%)] Loss: -498213.250000\n",
      "    epoch          : 777\n",
      "    loss           : -460495.600625\n",
      "    val_loss       : -459585.37109375\n",
      "Train Epoch: 778 [512/54000 (1%)] Loss: -490366.937500\n",
      "Train Epoch: 778 [11776/54000 (22%)] Loss: -518712.812500\n",
      "Train Epoch: 778 [23040/54000 (43%)] Loss: -392594.437500\n",
      "Train Epoch: 778 [34304/54000 (64%)] Loss: -546159.250000\n",
      "Train Epoch: 778 [45568/54000 (84%)] Loss: -492643.937500\n",
      "    epoch          : 778\n",
      "    loss           : -460032.52375\n",
      "    val_loss       : -460678.20390625\n",
      "Train Epoch: 779 [512/54000 (1%)] Loss: -389583.500000\n",
      "Train Epoch: 779 [11776/54000 (22%)] Loss: -369719.437500\n",
      "Train Epoch: 779 [23040/54000 (43%)] Loss: -401539.187500\n",
      "Train Epoch: 779 [34304/54000 (64%)] Loss: -394797.687500\n",
      "Train Epoch: 779 [45568/54000 (84%)] Loss: -439027.062500\n",
      "    epoch          : 779\n",
      "    loss           : -460912.9996875\n",
      "    val_loss       : -463707.2765625\n",
      "Train Epoch: 780 [512/54000 (1%)] Loss: -406975.500000\n",
      "Train Epoch: 780 [11776/54000 (22%)] Loss: -436507.687500\n",
      "Train Epoch: 780 [23040/54000 (43%)] Loss: -497116.437500\n",
      "Train Epoch: 780 [34304/54000 (64%)] Loss: -398064.875000\n",
      "Train Epoch: 780 [45568/54000 (84%)] Loss: -498140.312500\n",
      "    epoch          : 780\n",
      "    loss           : -461709.6403125\n",
      "    val_loss       : -462956.75546875\n",
      "Train Epoch: 781 [512/54000 (1%)] Loss: -403715.437500\n",
      "Train Epoch: 781 [11776/54000 (22%)] Loss: -399553.531250\n",
      "Train Epoch: 781 [23040/54000 (43%)] Loss: -496895.437500\n",
      "Train Epoch: 781 [34304/54000 (64%)] Loss: -486119.218750\n",
      "Train Epoch: 781 [45568/54000 (84%)] Loss: -523071.875000\n",
      "    epoch          : 781\n",
      "    loss           : -461684.74625\n",
      "    val_loss       : -462674.3953125\n",
      "Train Epoch: 782 [512/54000 (1%)] Loss: -399809.562500\n",
      "Train Epoch: 782 [11776/54000 (22%)] Loss: -396348.718750\n",
      "Train Epoch: 782 [23040/54000 (43%)] Loss: -505543.812500\n",
      "Train Epoch: 782 [34304/54000 (64%)] Loss: -392110.875000\n",
      "Train Epoch: 782 [45568/54000 (84%)] Loss: -398854.875000\n",
      "    epoch          : 782\n",
      "    loss           : -461221.4209375\n",
      "    val_loss       : -463118.3703125\n",
      "Train Epoch: 783 [512/54000 (1%)] Loss: -506579.937500\n",
      "Train Epoch: 783 [11776/54000 (22%)] Loss: -524175.312500\n",
      "Train Epoch: 783 [23040/54000 (43%)] Loss: -507163.687500\n",
      "Train Epoch: 783 [34304/54000 (64%)] Loss: -408278.687500\n",
      "Train Epoch: 783 [45568/54000 (84%)] Loss: -497739.812500\n",
      "    epoch          : 783\n",
      "    loss           : -461881.5978125\n",
      "    val_loss       : -462296.00859375\n",
      "Train Epoch: 784 [512/54000 (1%)] Loss: -505217.281250\n",
      "Train Epoch: 784 [11776/54000 (22%)] Loss: -394494.968750\n",
      "Train Epoch: 784 [23040/54000 (43%)] Loss: -523174.062500\n",
      "Train Epoch: 784 [34304/54000 (64%)] Loss: -398133.750000\n",
      "Train Epoch: 784 [45568/54000 (84%)] Loss: -496594.000000\n",
      "    epoch          : 784\n",
      "    loss           : -461791.27625\n",
      "    val_loss       : -462173.65234375\n",
      "Train Epoch: 785 [512/54000 (1%)] Loss: -524529.562500\n",
      "Train Epoch: 785 [11776/54000 (22%)] Loss: -395921.406250\n",
      "Train Epoch: 785 [23040/54000 (43%)] Loss: -433681.312500\n",
      "Train Epoch: 785 [34304/54000 (64%)] Loss: -489560.125000\n",
      "Train Epoch: 785 [45568/54000 (84%)] Loss: -488930.250000\n",
      "    epoch          : 785\n",
      "    loss           : -461904.4653125\n",
      "    val_loss       : -463965.5140625\n",
      "Train Epoch: 786 [512/54000 (1%)] Loss: -438067.250000\n",
      "Train Epoch: 786 [11776/54000 (22%)] Loss: -427063.968750\n",
      "Train Epoch: 786 [23040/54000 (43%)] Loss: -425404.125000\n",
      "Train Epoch: 786 [34304/54000 (64%)] Loss: -548476.500000\n",
      "Train Epoch: 786 [45568/54000 (84%)] Loss: -508807.718750\n",
      "    epoch          : 786\n",
      "    loss           : -462225.6859375\n",
      "    val_loss       : -462116.62734375\n",
      "Train Epoch: 787 [512/54000 (1%)] Loss: -487441.375000\n",
      "Train Epoch: 787 [11776/54000 (22%)] Loss: -428124.187500\n",
      "Train Epoch: 787 [23040/54000 (43%)] Loss: -428442.843750\n",
      "Train Epoch: 787 [34304/54000 (64%)] Loss: -497832.562500\n",
      "Train Epoch: 787 [45568/54000 (84%)] Loss: -498156.750000\n",
      "    epoch          : 787\n",
      "    loss           : -462248.1665625\n",
      "    val_loss       : -464213.78125\n",
      "Train Epoch: 788 [512/54000 (1%)] Loss: -377531.906250\n",
      "Train Epoch: 788 [11776/54000 (22%)] Loss: -498442.875000\n",
      "Train Epoch: 788 [23040/54000 (43%)] Loss: -525389.250000\n",
      "Train Epoch: 788 [34304/54000 (64%)] Loss: -510960.375000\n",
      "Train Epoch: 788 [45568/54000 (84%)] Loss: -394502.031250\n",
      "    epoch          : 788\n",
      "    loss           : -463282.0434375\n",
      "    val_loss       : -465118.47109375\n",
      "Train Epoch: 789 [512/54000 (1%)] Loss: -396863.625000\n",
      "Train Epoch: 789 [11776/54000 (22%)] Loss: -550479.750000\n",
      "Train Epoch: 789 [23040/54000 (43%)] Loss: -491345.718750\n",
      "Train Epoch: 789 [34304/54000 (64%)] Loss: -429467.375000\n",
      "Train Epoch: 789 [45568/54000 (84%)] Loss: -496553.000000\n",
      "    epoch          : 789\n",
      "    loss           : -462781.8009375\n",
      "    val_loss       : -461685.93125\n",
      "Train Epoch: 790 [512/54000 (1%)] Loss: -487428.500000\n",
      "Train Epoch: 790 [11776/54000 (22%)] Loss: -396067.437500\n",
      "Train Epoch: 790 [23040/54000 (43%)] Loss: -432384.656250\n",
      "Train Epoch: 790 [34304/54000 (64%)] Loss: -508221.000000\n",
      "Train Epoch: 790 [45568/54000 (84%)] Loss: -401504.125000\n",
      "    epoch          : 790\n",
      "    loss           : -462198.7259375\n",
      "    val_loss       : -464525.45546875\n",
      "Train Epoch: 791 [512/54000 (1%)] Loss: -398542.968750\n",
      "Train Epoch: 791 [11776/54000 (22%)] Loss: -501678.750000\n",
      "Train Epoch: 791 [23040/54000 (43%)] Loss: -429173.000000\n",
      "Train Epoch: 791 [34304/54000 (64%)] Loss: -496490.625000\n",
      "Train Epoch: 791 [45568/54000 (84%)] Loss: -437223.375000\n",
      "    epoch          : 791\n",
      "    loss           : -463927.58125\n",
      "    val_loss       : -463641.42265625\n",
      "Train Epoch: 792 [512/54000 (1%)] Loss: -495465.187500\n",
      "Train Epoch: 792 [11776/54000 (22%)] Loss: -388956.062500\n",
      "Train Epoch: 792 [23040/54000 (43%)] Loss: -434580.718750\n",
      "Train Epoch: 792 [34304/54000 (64%)] Loss: -506980.718750\n",
      "Train Epoch: 792 [45568/54000 (84%)] Loss: -507662.875000\n",
      "    epoch          : 792\n",
      "    loss           : -462496.17375\n",
      "    val_loss       : -464397.06484375\n",
      "Train Epoch: 793 [512/54000 (1%)] Loss: -405581.687500\n",
      "Train Epoch: 793 [11776/54000 (22%)] Loss: -409233.812500\n",
      "Train Epoch: 793 [23040/54000 (43%)] Loss: -511752.437500\n",
      "Train Epoch: 793 [34304/54000 (64%)] Loss: -425593.375000\n",
      "Train Epoch: 793 [45568/54000 (84%)] Loss: -487702.937500\n",
      "    epoch          : 793\n",
      "    loss           : -463413.5040625\n",
      "    val_loss       : -462345.103125\n",
      "Train Epoch: 794 [512/54000 (1%)] Loss: -510087.437500\n",
      "Train Epoch: 794 [11776/54000 (22%)] Loss: -395206.531250\n",
      "Train Epoch: 794 [23040/54000 (43%)] Loss: -544836.875000\n",
      "Train Epoch: 794 [34304/54000 (64%)] Loss: -402801.062500\n",
      "Train Epoch: 794 [45568/54000 (84%)] Loss: -411189.437500\n",
      "    epoch          : 794\n",
      "    loss           : -462040.385\n",
      "    val_loss       : -463598.4359375\n",
      "Train Epoch: 795 [512/54000 (1%)] Loss: -507874.875000\n",
      "Train Epoch: 795 [11776/54000 (22%)] Loss: -547524.625000\n",
      "Train Epoch: 795 [23040/54000 (43%)] Loss: -505335.687500\n",
      "Train Epoch: 795 [34304/54000 (64%)] Loss: -526712.812500\n",
      "Train Epoch: 795 [45568/54000 (84%)] Loss: -488963.906250\n",
      "    epoch          : 795\n",
      "    loss           : -460990.6315625\n",
      "    val_loss       : -462976.9015625\n",
      "Train Epoch: 796 [512/54000 (1%)] Loss: -496101.750000\n",
      "Train Epoch: 796 [11776/54000 (22%)] Loss: -401804.593750\n",
      "Train Epoch: 796 [23040/54000 (43%)] Loss: -439299.437500\n",
      "Train Epoch: 796 [34304/54000 (64%)] Loss: -408491.062500\n",
      "Train Epoch: 796 [45568/54000 (84%)] Loss: -503036.500000\n",
      "    epoch          : 796\n",
      "    loss           : -463337.096875\n",
      "    val_loss       : -465345.14921875\n",
      "Train Epoch: 797 [512/54000 (1%)] Loss: -526259.187500\n",
      "Train Epoch: 797 [11776/54000 (22%)] Loss: -394834.312500\n",
      "Train Epoch: 797 [23040/54000 (43%)] Loss: -405115.437500\n",
      "Train Epoch: 797 [34304/54000 (64%)] Loss: -510907.187500\n",
      "Train Epoch: 797 [45568/54000 (84%)] Loss: -487512.687500\n",
      "    epoch          : 797\n",
      "    loss           : -464296.573125\n",
      "    val_loss       : -465336.98828125\n",
      "Train Epoch: 798 [512/54000 (1%)] Loss: -388993.062500\n",
      "Train Epoch: 798 [11776/54000 (22%)] Loss: -490301.812500\n",
      "Train Epoch: 798 [23040/54000 (43%)] Loss: -399261.218750\n",
      "Train Epoch: 798 [34304/54000 (64%)] Loss: -396499.937500\n",
      "Train Epoch: 798 [45568/54000 (84%)] Loss: -501335.437500\n",
      "    epoch          : 798\n",
      "    loss           : -464626.275625\n",
      "    val_loss       : -465095.87109375\n",
      "Train Epoch: 799 [512/54000 (1%)] Loss: -404758.875000\n",
      "Train Epoch: 799 [11776/54000 (22%)] Loss: -489958.156250\n",
      "Train Epoch: 799 [23040/54000 (43%)] Loss: -514447.843750\n",
      "Train Epoch: 799 [34304/54000 (64%)] Loss: -489364.187500\n",
      "Train Epoch: 799 [45568/54000 (84%)] Loss: -500656.062500\n",
      "    epoch          : 799\n",
      "    loss           : -464571.20875\n",
      "    val_loss       : -465709.10234375\n",
      "Train Epoch: 800 [512/54000 (1%)] Loss: -549863.312500\n",
      "Train Epoch: 800 [11776/54000 (22%)] Loss: -407827.718750\n",
      "Train Epoch: 800 [23040/54000 (43%)] Loss: -552398.375000\n",
      "Train Epoch: 800 [34304/54000 (64%)] Loss: -500658.625000\n",
      "Train Epoch: 800 [45568/54000 (84%)] Loss: -498543.593750\n",
      "    epoch          : 800\n",
      "    loss           : -465112.9690625\n",
      "    val_loss       : -465705.76796875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch800.pth ...\n",
      "Train Epoch: 801 [512/54000 (1%)] Loss: -511565.250000\n",
      "Train Epoch: 801 [11776/54000 (22%)] Loss: -430013.250000\n",
      "Train Epoch: 801 [23040/54000 (43%)] Loss: -514191.968750\n",
      "Train Epoch: 801 [34304/54000 (64%)] Loss: -490160.875000\n",
      "Train Epoch: 801 [45568/54000 (84%)] Loss: -516096.406250\n",
      "    epoch          : 801\n",
      "    loss           : -465473.20625\n",
      "    val_loss       : -466213.63984375\n",
      "Train Epoch: 802 [512/54000 (1%)] Loss: -402753.531250\n",
      "Train Epoch: 802 [11776/54000 (22%)] Loss: -441338.562500\n",
      "Train Epoch: 802 [23040/54000 (43%)] Loss: -551318.437500\n",
      "Train Epoch: 802 [34304/54000 (64%)] Loss: -442167.718750\n",
      "Train Epoch: 802 [45568/54000 (84%)] Loss: -510134.312500\n",
      "    epoch          : 802\n",
      "    loss           : -465773.7184375\n",
      "    val_loss       : -466858.16015625\n",
      "Train Epoch: 803 [512/54000 (1%)] Loss: -391925.250000\n",
      "Train Epoch: 803 [11776/54000 (22%)] Loss: -528103.812500\n",
      "Train Epoch: 803 [23040/54000 (43%)] Loss: -399754.093750\n",
      "Train Epoch: 803 [34304/54000 (64%)] Loss: -492018.000000\n",
      "Train Epoch: 803 [45568/54000 (84%)] Loss: -491393.718750\n",
      "    epoch          : 803\n",
      "    loss           : -465561.341875\n",
      "    val_loss       : -464717.72109375\n",
      "Train Epoch: 804 [512/54000 (1%)] Loss: -431892.781250\n",
      "Train Epoch: 804 [11776/54000 (22%)] Loss: -429373.437500\n",
      "Train Epoch: 804 [23040/54000 (43%)] Loss: -391918.093750\n",
      "Train Epoch: 804 [34304/54000 (64%)] Loss: -492242.375000\n",
      "Train Epoch: 804 [45568/54000 (84%)] Loss: -394018.031250\n",
      "    epoch          : 804\n",
      "    loss           : -464934.61125\n",
      "    val_loss       : -467391.65546875\n",
      "Train Epoch: 805 [512/54000 (1%)] Loss: -397236.875000\n",
      "Train Epoch: 805 [11776/54000 (22%)] Loss: -409386.875000\n",
      "Train Epoch: 805 [23040/54000 (43%)] Loss: -406906.156250\n",
      "Train Epoch: 805 [34304/54000 (64%)] Loss: -491379.906250\n",
      "Train Epoch: 805 [45568/54000 (84%)] Loss: -495939.375000\n",
      "    epoch          : 805\n",
      "    loss           : -465972.0546875\n",
      "    val_loss       : -466694.03828125\n",
      "Train Epoch: 806 [512/54000 (1%)] Loss: -446627.187500\n",
      "Train Epoch: 806 [11776/54000 (22%)] Loss: -519707.000000\n",
      "Train Epoch: 806 [23040/54000 (43%)] Loss: -444104.250000\n",
      "Train Epoch: 806 [34304/54000 (64%)] Loss: -527311.500000\n",
      "Train Epoch: 806 [45568/54000 (84%)] Loss: -513703.375000\n",
      "    epoch          : 806\n",
      "    loss           : -466323.0228125\n",
      "    val_loss       : -467166.67578125\n",
      "Train Epoch: 807 [512/54000 (1%)] Loss: -501763.156250\n",
      "Train Epoch: 807 [11776/54000 (22%)] Loss: -530453.500000\n",
      "Train Epoch: 807 [23040/54000 (43%)] Loss: -489802.468750\n",
      "Train Epoch: 807 [34304/54000 (64%)] Loss: -392432.156250\n",
      "Train Epoch: 807 [45568/54000 (84%)] Loss: -492566.468750\n",
      "    epoch          : 807\n",
      "    loss           : -466470.2928125\n",
      "    val_loss       : -467690.01171875\n",
      "Train Epoch: 808 [512/54000 (1%)] Loss: -515849.562500\n",
      "Train Epoch: 808 [11776/54000 (22%)] Loss: -432597.875000\n",
      "Train Epoch: 808 [23040/54000 (43%)] Loss: -503269.500000\n",
      "Train Epoch: 808 [34304/54000 (64%)] Loss: -528741.500000\n",
      "Train Epoch: 808 [45568/54000 (84%)] Loss: -394050.906250\n",
      "    epoch          : 808\n",
      "    loss           : -465638.745\n",
      "    val_loss       : -465900.471875\n",
      "Train Epoch: 809 [512/54000 (1%)] Loss: -398466.687500\n",
      "Train Epoch: 809 [11776/54000 (22%)] Loss: -433499.750000\n",
      "Train Epoch: 809 [23040/54000 (43%)] Loss: -550777.625000\n",
      "Train Epoch: 809 [34304/54000 (64%)] Loss: -511475.687500\n",
      "Train Epoch: 809 [45568/54000 (84%)] Loss: -511289.312500\n",
      "    epoch          : 809\n",
      "    loss           : -462181.670625\n",
      "    val_loss       : -462682.546875\n",
      "Train Epoch: 810 [512/54000 (1%)] Loss: -527423.562500\n",
      "Train Epoch: 810 [11776/54000 (22%)] Loss: -429014.281250\n",
      "Train Epoch: 810 [23040/54000 (43%)] Loss: -398157.625000\n",
      "Train Epoch: 810 [34304/54000 (64%)] Loss: -510340.312500\n",
      "Train Epoch: 810 [45568/54000 (84%)] Loss: -478136.437500\n",
      "    epoch          : 810\n",
      "    loss           : -465406.4046875\n",
      "    val_loss       : -467966.41015625\n",
      "Train Epoch: 811 [512/54000 (1%)] Loss: -511312.500000\n",
      "Train Epoch: 811 [11776/54000 (22%)] Loss: -432983.312500\n",
      "Train Epoch: 811 [23040/54000 (43%)] Loss: -530188.750000\n",
      "Train Epoch: 811 [34304/54000 (64%)] Loss: -522921.812500\n",
      "Train Epoch: 811 [45568/54000 (84%)] Loss: -446817.687500\n",
      "    epoch          : 811\n",
      "    loss           : -467167.0709375\n",
      "    val_loss       : -468262.34140625\n",
      "Train Epoch: 812 [512/54000 (1%)] Loss: -432222.625000\n",
      "Train Epoch: 812 [11776/54000 (22%)] Loss: -551926.750000\n",
      "Train Epoch: 812 [23040/54000 (43%)] Loss: -492626.531250\n",
      "Train Epoch: 812 [34304/54000 (64%)] Loss: -444554.468750\n",
      "Train Epoch: 812 [45568/54000 (84%)] Loss: -515613.375000\n",
      "    epoch          : 812\n",
      "    loss           : -467125.873125\n",
      "    val_loss       : -467398.1578125\n",
      "Train Epoch: 813 [512/54000 (1%)] Loss: -409841.968750\n",
      "Train Epoch: 813 [11776/54000 (22%)] Loss: -547983.562500\n",
      "Train Epoch: 813 [23040/54000 (43%)] Loss: -432229.500000\n",
      "Train Epoch: 813 [34304/54000 (64%)] Loss: -528508.187500\n",
      "Train Epoch: 813 [45568/54000 (84%)] Loss: -552453.125000\n",
      "    epoch          : 813\n",
      "    loss           : -467067.4525\n",
      "    val_loss       : -468701.08515625\n",
      "Train Epoch: 814 [512/54000 (1%)] Loss: -509053.468750\n",
      "Train Epoch: 814 [11776/54000 (22%)] Loss: -402882.531250\n",
      "Train Epoch: 814 [23040/54000 (43%)] Loss: -552818.250000\n",
      "Train Epoch: 814 [34304/54000 (64%)] Loss: -401355.156250\n",
      "Train Epoch: 814 [45568/54000 (84%)] Loss: -469869.375000\n",
      "    epoch          : 814\n",
      "    loss           : -466907.431875\n",
      "    val_loss       : -468209.9296875\n",
      "Train Epoch: 815 [512/54000 (1%)] Loss: -439130.875000\n",
      "Train Epoch: 815 [11776/54000 (22%)] Loss: -395031.562500\n",
      "Train Epoch: 815 [23040/54000 (43%)] Loss: -411161.656250\n",
      "Train Epoch: 815 [34304/54000 (64%)] Loss: -409317.062500\n",
      "Train Epoch: 815 [45568/54000 (84%)] Loss: -430443.875000\n",
      "    epoch          : 815\n",
      "    loss           : -467878.19875\n",
      "    val_loss       : -468723.40703125\n",
      "Train Epoch: 816 [512/54000 (1%)] Loss: -411819.875000\n",
      "Train Epoch: 816 [11776/54000 (22%)] Loss: -512569.500000\n",
      "Train Epoch: 816 [23040/54000 (43%)] Loss: -405757.125000\n",
      "Train Epoch: 816 [34304/54000 (64%)] Loss: -469305.125000\n",
      "Train Epoch: 816 [45568/54000 (84%)] Loss: -493239.187500\n",
      "    epoch          : 816\n",
      "    loss           : -468221.4084375\n",
      "    val_loss       : -468995.2109375\n",
      "Train Epoch: 817 [512/54000 (1%)] Loss: -429897.750000\n",
      "Train Epoch: 817 [11776/54000 (22%)] Loss: -443319.000000\n",
      "Train Epoch: 817 [23040/54000 (43%)] Loss: -434517.375000\n",
      "Train Epoch: 817 [34304/54000 (64%)] Loss: -400421.312500\n",
      "Train Epoch: 817 [45568/54000 (84%)] Loss: -432648.250000\n",
      "    epoch          : 817\n",
      "    loss           : -467493.9378125\n",
      "    val_loss       : -467969.02578125\n",
      "Train Epoch: 818 [512/54000 (1%)] Loss: -529945.312500\n",
      "Train Epoch: 818 [11776/54000 (22%)] Loss: -556597.312500\n",
      "Train Epoch: 818 [23040/54000 (43%)] Loss: -432278.625000\n",
      "Train Epoch: 818 [34304/54000 (64%)] Loss: -514572.375000\n",
      "Train Epoch: 818 [45568/54000 (84%)] Loss: -400897.218750\n",
      "    epoch          : 818\n",
      "    loss           : -467740.6209375\n",
      "    val_loss       : -467259.54921875\n",
      "Train Epoch: 819 [512/54000 (1%)] Loss: -547337.187500\n",
      "Train Epoch: 819 [11776/54000 (22%)] Loss: -518114.000000\n",
      "Train Epoch: 819 [23040/54000 (43%)] Loss: -555400.625000\n",
      "Train Epoch: 819 [34304/54000 (64%)] Loss: -438663.625000\n",
      "Train Epoch: 819 [45568/54000 (84%)] Loss: -405134.312500\n",
      "    epoch          : 819\n",
      "    loss           : -466862.9765625\n",
      "    val_loss       : -469070.7515625\n",
      "Train Epoch: 820 [512/54000 (1%)] Loss: -517363.500000\n",
      "Train Epoch: 820 [11776/54000 (22%)] Loss: -409332.625000\n",
      "Train Epoch: 820 [23040/54000 (43%)] Loss: -533031.875000\n",
      "Train Epoch: 820 [34304/54000 (64%)] Loss: -412429.281250\n",
      "Train Epoch: 820 [45568/54000 (84%)] Loss: -398363.093750\n",
      "    epoch          : 820\n",
      "    loss           : -467611.041875\n",
      "    val_loss       : -467062.52578125\n",
      "Train Epoch: 821 [512/54000 (1%)] Loss: -509978.468750\n",
      "Train Epoch: 821 [11776/54000 (22%)] Loss: -493877.593750\n",
      "Train Epoch: 821 [23040/54000 (43%)] Loss: -531007.500000\n",
      "Train Epoch: 821 [34304/54000 (64%)] Loss: -437573.437500\n",
      "Train Epoch: 821 [45568/54000 (84%)] Loss: -411771.937500\n",
      "    epoch          : 821\n",
      "    loss           : -468171.950625\n",
      "    val_loss       : -469655.93359375\n",
      "Train Epoch: 822 [512/54000 (1%)] Loss: -402958.562500\n",
      "Train Epoch: 822 [11776/54000 (22%)] Loss: -532075.500000\n",
      "Train Epoch: 822 [23040/54000 (43%)] Loss: -414160.312500\n",
      "Train Epoch: 822 [34304/54000 (64%)] Loss: -520180.031250\n",
      "Train Epoch: 822 [45568/54000 (84%)] Loss: -554738.375000\n",
      "    epoch          : 822\n",
      "    loss           : -468613.6825\n",
      "    val_loss       : -468444.43515625\n",
      "Train Epoch: 823 [512/54000 (1%)] Loss: -406704.500000\n",
      "Train Epoch: 823 [11776/54000 (22%)] Loss: -409685.156250\n",
      "Train Epoch: 823 [23040/54000 (43%)] Loss: -401657.625000\n",
      "Train Epoch: 823 [34304/54000 (64%)] Loss: -388665.218750\n",
      "Train Epoch: 823 [45568/54000 (84%)] Loss: -485183.343750\n",
      "    epoch          : 823\n",
      "    loss           : -466018.6134375\n",
      "    val_loss       : -469274.7078125\n",
      "Train Epoch: 824 [512/54000 (1%)] Loss: -505628.406250\n",
      "Train Epoch: 824 [11776/54000 (22%)] Loss: -532098.625000\n",
      "Train Epoch: 824 [23040/54000 (43%)] Loss: -405989.500000\n",
      "Train Epoch: 824 [34304/54000 (64%)] Loss: -502791.375000\n",
      "Train Epoch: 824 [45568/54000 (84%)] Loss: -437902.500000\n",
      "    epoch          : 824\n",
      "    loss           : -468652.4425\n",
      "    val_loss       : -470116.6765625\n",
      "Train Epoch: 825 [512/54000 (1%)] Loss: -531834.562500\n",
      "Train Epoch: 825 [11776/54000 (22%)] Loss: -400074.406250\n",
      "Train Epoch: 825 [23040/54000 (43%)] Loss: -413959.750000\n",
      "Train Epoch: 825 [34304/54000 (64%)] Loss: -405437.250000\n",
      "Train Epoch: 825 [45568/54000 (84%)] Loss: -414444.875000\n",
      "    epoch          : 825\n",
      "    loss           : -469795.7940625\n",
      "    val_loss       : -470795.28515625\n",
      "Train Epoch: 826 [512/54000 (1%)] Loss: -399570.031250\n",
      "Train Epoch: 826 [11776/54000 (22%)] Loss: -370643.500000\n",
      "Train Epoch: 826 [23040/54000 (43%)] Loss: -445613.187500\n",
      "Train Epoch: 826 [34304/54000 (64%)] Loss: -556797.750000\n",
      "Train Epoch: 826 [45568/54000 (84%)] Loss: -519522.125000\n",
      "    epoch          : 826\n",
      "    loss           : -469663.6271875\n",
      "    val_loss       : -470866.55625\n",
      "Train Epoch: 827 [512/54000 (1%)] Loss: -495049.125000\n",
      "Train Epoch: 827 [11776/54000 (22%)] Loss: -403260.437500\n",
      "Train Epoch: 827 [23040/54000 (43%)] Loss: -417646.000000\n",
      "Train Epoch: 827 [34304/54000 (64%)] Loss: -403166.718750\n",
      "Train Epoch: 827 [45568/54000 (84%)] Loss: -506078.875000\n",
      "    epoch          : 827\n",
      "    loss           : -469528.531875\n",
      "    val_loss       : -470569.8078125\n",
      "Train Epoch: 828 [512/54000 (1%)] Loss: -410820.968750\n",
      "Train Epoch: 828 [11776/54000 (22%)] Loss: -400177.031250\n",
      "Train Epoch: 828 [23040/54000 (43%)] Loss: -497268.781250\n",
      "Train Epoch: 828 [34304/54000 (64%)] Loss: -493884.875000\n",
      "Train Epoch: 828 [45568/54000 (84%)] Loss: -494763.843750\n",
      "    epoch          : 828\n",
      "    loss           : -469408.640625\n",
      "    val_loss       : -469969.425\n",
      "Train Epoch: 829 [512/54000 (1%)] Loss: -402305.656250\n",
      "Train Epoch: 829 [11776/54000 (22%)] Loss: -401640.343750\n",
      "Train Epoch: 829 [23040/54000 (43%)] Loss: -473768.875000\n",
      "Train Epoch: 829 [34304/54000 (64%)] Loss: -398019.812500\n",
      "Train Epoch: 829 [45568/54000 (84%)] Loss: -505690.312500\n",
      "    epoch          : 829\n",
      "    loss           : -469436.195625\n",
      "    val_loss       : -470419.07421875\n",
      "Train Epoch: 830 [512/54000 (1%)] Loss: -446034.531250\n",
      "Train Epoch: 830 [11776/54000 (22%)] Loss: -480390.562500\n",
      "Train Epoch: 830 [23040/54000 (43%)] Loss: -400318.968750\n",
      "Train Epoch: 830 [34304/54000 (64%)] Loss: -398331.843750\n",
      "Train Epoch: 830 [45568/54000 (84%)] Loss: -516787.125000\n",
      "    epoch          : 830\n",
      "    loss           : -469609.18125\n",
      "    val_loss       : -470800.5\n",
      "Train Epoch: 831 [512/54000 (1%)] Loss: -414782.312500\n",
      "Train Epoch: 831 [11776/54000 (22%)] Loss: -376664.687500\n",
      "Train Epoch: 831 [23040/54000 (43%)] Loss: -533251.937500\n",
      "Train Epoch: 831 [34304/54000 (64%)] Loss: -409110.656250\n",
      "Train Epoch: 831 [45568/54000 (84%)] Loss: -507481.812500\n",
      "    epoch          : 831\n",
      "    loss           : -470068.27\n",
      "    val_loss       : -470741.11640625\n",
      "Train Epoch: 832 [512/54000 (1%)] Loss: -499531.812500\n",
      "Train Epoch: 832 [11776/54000 (22%)] Loss: -557214.375000\n",
      "Train Epoch: 832 [23040/54000 (43%)] Loss: -407127.687500\n",
      "Train Epoch: 832 [34304/54000 (64%)] Loss: -395342.968750\n",
      "Train Epoch: 832 [45568/54000 (84%)] Loss: -509123.500000\n",
      "    epoch          : 832\n",
      "    loss           : -470102.8521875\n",
      "    val_loss       : -470507.340625\n",
      "Train Epoch: 833 [512/54000 (1%)] Loss: -443303.781250\n",
      "Train Epoch: 833 [11776/54000 (22%)] Loss: -533419.125000\n",
      "Train Epoch: 833 [23040/54000 (43%)] Loss: -433937.500000\n",
      "Train Epoch: 833 [34304/54000 (64%)] Loss: -445313.875000\n",
      "Train Epoch: 833 [45568/54000 (84%)] Loss: -552369.500000\n",
      "    epoch          : 833\n",
      "    loss           : -470301.7034375\n",
      "    val_loss       : -471982.4703125\n",
      "Train Epoch: 834 [512/54000 (1%)] Loss: -433937.781250\n",
      "Train Epoch: 834 [11776/54000 (22%)] Loss: -521438.250000\n",
      "Train Epoch: 834 [23040/54000 (43%)] Loss: -448664.718750\n",
      "Train Epoch: 834 [34304/54000 (64%)] Loss: -503059.093750\n",
      "Train Epoch: 834 [45568/54000 (84%)] Loss: -403799.156250\n",
      "    epoch          : 834\n",
      "    loss           : -469767.2234375\n",
      "    val_loss       : -470881.67265625\n",
      "Train Epoch: 835 [512/54000 (1%)] Loss: -417571.593750\n",
      "Train Epoch: 835 [11776/54000 (22%)] Loss: -517155.125000\n",
      "Train Epoch: 835 [23040/54000 (43%)] Loss: -555632.437500\n",
      "Train Epoch: 835 [34304/54000 (64%)] Loss: -488757.593750\n",
      "Train Epoch: 835 [45568/54000 (84%)] Loss: -403054.687500\n",
      "    epoch          : 835\n",
      "    loss           : -469805.8834375\n",
      "    val_loss       : -470872.378125\n",
      "Train Epoch: 836 [512/54000 (1%)] Loss: -442764.968750\n",
      "Train Epoch: 836 [11776/54000 (22%)] Loss: -437679.812500\n",
      "Train Epoch: 836 [23040/54000 (43%)] Loss: -558495.187500\n",
      "Train Epoch: 836 [34304/54000 (64%)] Loss: -554347.375000\n",
      "Train Epoch: 836 [45568/54000 (84%)] Loss: -506203.812500\n",
      "    epoch          : 836\n",
      "    loss           : -469945.278125\n",
      "    val_loss       : -471550.79453125\n",
      "Train Epoch: 837 [512/54000 (1%)] Loss: -374154.375000\n",
      "Train Epoch: 837 [11776/54000 (22%)] Loss: -517197.437500\n",
      "Train Epoch: 837 [23040/54000 (43%)] Loss: -508422.718750\n",
      "Train Epoch: 837 [34304/54000 (64%)] Loss: -509869.781250\n",
      "Train Epoch: 837 [45568/54000 (84%)] Loss: -436343.843750\n",
      "    epoch          : 837\n",
      "    loss           : -470579.9784375\n",
      "    val_loss       : -471932.4703125\n",
      "Train Epoch: 838 [512/54000 (1%)] Loss: -442984.437500\n",
      "Train Epoch: 838 [11776/54000 (22%)] Loss: -519867.281250\n",
      "Train Epoch: 838 [23040/54000 (43%)] Loss: -443210.687500\n",
      "Train Epoch: 838 [34304/54000 (64%)] Loss: -443104.125000\n",
      "Train Epoch: 838 [45568/54000 (84%)] Loss: -494494.000000\n",
      "    epoch          : 838\n",
      "    loss           : -471294.8271875\n",
      "    val_loss       : -472410.32421875\n",
      "Train Epoch: 839 [512/54000 (1%)] Loss: -418311.937500\n",
      "Train Epoch: 839 [11776/54000 (22%)] Loss: -535931.125000\n",
      "Train Epoch: 839 [23040/54000 (43%)] Loss: -445583.406250\n",
      "Train Epoch: 839 [34304/54000 (64%)] Loss: -519752.625000\n",
      "Train Epoch: 839 [45568/54000 (84%)] Loss: -512908.500000\n",
      "    epoch          : 839\n",
      "    loss           : -470504.8165625\n",
      "    val_loss       : -472199.4390625\n",
      "Train Epoch: 840 [512/54000 (1%)] Loss: -480471.281250\n",
      "Train Epoch: 840 [11776/54000 (22%)] Loss: -500010.812500\n",
      "Train Epoch: 840 [23040/54000 (43%)] Loss: -407846.562500\n",
      "Train Epoch: 840 [34304/54000 (64%)] Loss: -533912.500000\n",
      "Train Epoch: 840 [45568/54000 (84%)] Loss: -507932.093750\n",
      "    epoch          : 840\n",
      "    loss           : -471555.62625\n",
      "    val_loss       : -472095.83046875\n",
      "Train Epoch: 841 [512/54000 (1%)] Loss: -498093.500000\n",
      "Train Epoch: 841 [11776/54000 (22%)] Loss: -556520.062500\n",
      "Train Epoch: 841 [23040/54000 (43%)] Loss: -562658.750000\n",
      "Train Epoch: 841 [34304/54000 (64%)] Loss: -494008.375000\n",
      "Train Epoch: 841 [45568/54000 (84%)] Loss: -495558.031250\n",
      "    epoch          : 841\n",
      "    loss           : -470466.799375\n",
      "    val_loss       : -472374.26015625\n",
      "Train Epoch: 842 [512/54000 (1%)] Loss: -451308.968750\n",
      "Train Epoch: 842 [11776/54000 (22%)] Loss: -406230.687500\n",
      "Train Epoch: 842 [23040/54000 (43%)] Loss: -523229.750000\n",
      "Train Epoch: 842 [34304/54000 (64%)] Loss: -507675.750000\n",
      "Train Epoch: 842 [45568/54000 (84%)] Loss: -513931.562500\n",
      "    epoch          : 842\n",
      "    loss           : -472141.0775\n",
      "    val_loss       : -473062.625\n",
      "Train Epoch: 843 [512/54000 (1%)] Loss: -415831.031250\n",
      "Train Epoch: 843 [11776/54000 (22%)] Loss: -434931.031250\n",
      "Train Epoch: 843 [23040/54000 (43%)] Loss: -407706.593750\n",
      "Train Epoch: 843 [34304/54000 (64%)] Loss: -406027.125000\n",
      "Train Epoch: 843 [45568/54000 (84%)] Loss: -518203.500000\n",
      "    epoch          : 843\n",
      "    loss           : -472717.969375\n",
      "    val_loss       : -472908.88125\n",
      "Train Epoch: 844 [512/54000 (1%)] Loss: -412259.187500\n",
      "Train Epoch: 844 [11776/54000 (22%)] Loss: -409981.687500\n",
      "Train Epoch: 844 [23040/54000 (43%)] Loss: -558718.250000\n",
      "Train Epoch: 844 [34304/54000 (64%)] Loss: -556044.125000\n",
      "Train Epoch: 844 [45568/54000 (84%)] Loss: -410146.250000\n",
      "    epoch          : 844\n",
      "    loss           : -472169.94125\n",
      "    val_loss       : -473646.246875\n",
      "Train Epoch: 845 [512/54000 (1%)] Loss: -403052.781250\n",
      "Train Epoch: 845 [11776/54000 (22%)] Loss: -561931.625000\n",
      "Train Epoch: 845 [23040/54000 (43%)] Loss: -501211.375000\n",
      "Train Epoch: 845 [34304/54000 (64%)] Loss: -408532.062500\n",
      "Train Epoch: 845 [45568/54000 (84%)] Loss: -511569.375000\n",
      "    epoch          : 845\n",
      "    loss           : -473031.90375\n",
      "    val_loss       : -473328.81796875\n",
      "Train Epoch: 846 [512/54000 (1%)] Loss: -560553.875000\n",
      "Train Epoch: 846 [11776/54000 (22%)] Loss: -525432.125000\n",
      "Train Epoch: 846 [23040/54000 (43%)] Loss: -406431.750000\n",
      "Train Epoch: 846 [34304/54000 (64%)] Loss: -404375.625000\n",
      "Train Epoch: 846 [45568/54000 (84%)] Loss: -414951.312500\n",
      "    epoch          : 846\n",
      "    loss           : -472754.143125\n",
      "    val_loss       : -473510.346875\n",
      "Train Epoch: 847 [512/54000 (1%)] Loss: -510426.531250\n",
      "Train Epoch: 847 [11776/54000 (22%)] Loss: -563672.812500\n",
      "Train Epoch: 847 [23040/54000 (43%)] Loss: -496702.500000\n",
      "Train Epoch: 847 [34304/54000 (64%)] Loss: -512587.062500\n",
      "Train Epoch: 847 [45568/54000 (84%)] Loss: -523048.625000\n",
      "    epoch          : 847\n",
      "    loss           : -472701.1421875\n",
      "    val_loss       : -473775.65859375\n",
      "Train Epoch: 848 [512/54000 (1%)] Loss: -401856.750000\n",
      "Train Epoch: 848 [11776/54000 (22%)] Loss: -409805.250000\n",
      "Train Epoch: 848 [23040/54000 (43%)] Loss: -536391.250000\n",
      "Train Epoch: 848 [34304/54000 (64%)] Loss: -445011.062500\n",
      "Train Epoch: 848 [45568/54000 (84%)] Loss: -405379.500000\n",
      "    epoch          : 848\n",
      "    loss           : -472639.1428125\n",
      "    val_loss       : -472896.875\n",
      "Train Epoch: 849 [512/54000 (1%)] Loss: -409544.250000\n",
      "Train Epoch: 849 [11776/54000 (22%)] Loss: -541265.250000\n",
      "Train Epoch: 849 [23040/54000 (43%)] Loss: -448528.812500\n",
      "Train Epoch: 849 [34304/54000 (64%)] Loss: -435602.562500\n",
      "Train Epoch: 849 [45568/54000 (84%)] Loss: -402094.062500\n",
      "    epoch          : 849\n",
      "    loss           : -472002.3503125\n",
      "    val_loss       : -472865.5921875\n",
      "Train Epoch: 850 [512/54000 (1%)] Loss: -537457.937500\n",
      "Train Epoch: 850 [11776/54000 (22%)] Loss: -445759.062500\n",
      "Train Epoch: 850 [23040/54000 (43%)] Loss: -560624.875000\n",
      "Train Epoch: 850 [34304/54000 (64%)] Loss: -407553.187500\n",
      "Train Epoch: 850 [45568/54000 (84%)] Loss: -498953.687500\n",
      "    epoch          : 850\n",
      "    loss           : -473054.0175\n",
      "    val_loss       : -473716.921875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch850.pth ...\n",
      "Train Epoch: 851 [512/54000 (1%)] Loss: -403678.625000\n",
      "Train Epoch: 851 [11776/54000 (22%)] Loss: -518611.625000\n",
      "Train Epoch: 851 [23040/54000 (43%)] Loss: -410584.187500\n",
      "Train Epoch: 851 [34304/54000 (64%)] Loss: -508060.375000\n",
      "Train Epoch: 851 [45568/54000 (84%)] Loss: -399215.593750\n",
      "    epoch          : 851\n",
      "    loss           : -472783.7034375\n",
      "    val_loss       : -474112.5171875\n",
      "Train Epoch: 852 [512/54000 (1%)] Loss: -405256.937500\n",
      "Train Epoch: 852 [11776/54000 (22%)] Loss: -437535.406250\n",
      "Train Epoch: 852 [23040/54000 (43%)] Loss: -560336.875000\n",
      "Train Epoch: 852 [34304/54000 (64%)] Loss: -509118.812500\n",
      "Train Epoch: 852 [45568/54000 (84%)] Loss: -407292.937500\n",
      "    epoch          : 852\n",
      "    loss           : -471094.57\n",
      "    val_loss       : -472851.503125\n",
      "Train Epoch: 853 [512/54000 (1%)] Loss: -448863.000000\n",
      "Train Epoch: 853 [11776/54000 (22%)] Loss: -439623.062500\n",
      "Train Epoch: 853 [23040/54000 (43%)] Loss: -408810.312500\n",
      "Train Epoch: 853 [34304/54000 (64%)] Loss: -413560.875000\n",
      "Train Epoch: 853 [45568/54000 (84%)] Loss: -402638.218750\n",
      "    epoch          : 853\n",
      "    loss           : -472231.5475\n",
      "    val_loss       : -473999.02421875\n",
      "Train Epoch: 854 [512/54000 (1%)] Loss: -562586.750000\n",
      "Train Epoch: 854 [11776/54000 (22%)] Loss: -405834.625000\n",
      "Train Epoch: 854 [23040/54000 (43%)] Loss: -437421.406250\n",
      "Train Epoch: 854 [34304/54000 (64%)] Loss: -407548.937500\n",
      "Train Epoch: 854 [45568/54000 (84%)] Loss: -520610.312500\n",
      "    epoch          : 854\n",
      "    loss           : -473546.0034375\n",
      "    val_loss       : -473917.2046875\n",
      "Train Epoch: 855 [512/54000 (1%)] Loss: -418473.812500\n",
      "Train Epoch: 855 [11776/54000 (22%)] Loss: -497643.343750\n",
      "Train Epoch: 855 [23040/54000 (43%)] Loss: -504065.062500\n",
      "Train Epoch: 855 [34304/54000 (64%)] Loss: -502139.281250\n",
      "Train Epoch: 855 [45568/54000 (84%)] Loss: -400489.562500\n",
      "    epoch          : 855\n",
      "    loss           : -473851.7003125\n",
      "    val_loss       : -474695.484375\n",
      "Train Epoch: 856 [512/54000 (1%)] Loss: -541012.500000\n",
      "Train Epoch: 856 [11776/54000 (22%)] Loss: -432595.281250\n",
      "Train Epoch: 856 [23040/54000 (43%)] Loss: -556306.375000\n",
      "Train Epoch: 856 [34304/54000 (64%)] Loss: -511177.781250\n",
      "Train Epoch: 856 [45568/54000 (84%)] Loss: -448236.843750\n",
      "    epoch          : 856\n",
      "    loss           : -472053.4484375\n",
      "    val_loss       : -475055.13046875\n",
      "Train Epoch: 857 [512/54000 (1%)] Loss: -511095.656250\n",
      "Train Epoch: 857 [11776/54000 (22%)] Loss: -509969.656250\n",
      "Train Epoch: 857 [23040/54000 (43%)] Loss: -560418.687500\n",
      "Train Epoch: 857 [34304/54000 (64%)] Loss: -449710.000000\n",
      "Train Epoch: 857 [45568/54000 (84%)] Loss: -402282.687500\n",
      "    epoch          : 857\n",
      "    loss           : -473517.9609375\n",
      "    val_loss       : -475107.9046875\n",
      "Train Epoch: 858 [512/54000 (1%)] Loss: -499086.875000\n",
      "Train Epoch: 858 [11776/54000 (22%)] Loss: -406896.062500\n",
      "Train Epoch: 858 [23040/54000 (43%)] Loss: -444863.312500\n",
      "Train Epoch: 858 [34304/54000 (64%)] Loss: -507115.843750\n",
      "Train Epoch: 858 [45568/54000 (84%)] Loss: -498769.343750\n",
      "    epoch          : 858\n",
      "    loss           : -473014.583125\n",
      "    val_loss       : -474706.26796875\n",
      "Train Epoch: 859 [512/54000 (1%)] Loss: -409365.125000\n",
      "Train Epoch: 859 [11776/54000 (22%)] Loss: -538422.187500\n",
      "Train Epoch: 859 [23040/54000 (43%)] Loss: -435529.875000\n",
      "Train Epoch: 859 [34304/54000 (64%)] Loss: -513448.937500\n",
      "Train Epoch: 859 [45568/54000 (84%)] Loss: -474849.031250\n",
      "    epoch          : 859\n",
      "    loss           : -473965.37875\n",
      "    val_loss       : -474190.78046875\n",
      "Train Epoch: 860 [512/54000 (1%)] Loss: -447175.093750\n",
      "Train Epoch: 860 [11776/54000 (22%)] Loss: -446751.562500\n",
      "Train Epoch: 860 [23040/54000 (43%)] Loss: -436812.750000\n",
      "Train Epoch: 860 [34304/54000 (64%)] Loss: -402263.687500\n",
      "Train Epoch: 860 [45568/54000 (84%)] Loss: -522209.750000\n",
      "    epoch          : 860\n",
      "    loss           : -474375.9328125\n",
      "    val_loss       : -475093.74765625\n",
      "Train Epoch: 861 [512/54000 (1%)] Loss: -440882.625000\n",
      "Train Epoch: 861 [11776/54000 (22%)] Loss: -561875.000000\n",
      "Train Epoch: 861 [23040/54000 (43%)] Loss: -541697.750000\n",
      "Train Epoch: 861 [34304/54000 (64%)] Loss: -404299.687500\n",
      "Train Epoch: 861 [45568/54000 (84%)] Loss: -399566.437500\n",
      "    epoch          : 861\n",
      "    loss           : -473835.1753125\n",
      "    val_loss       : -474309.64921875\n",
      "Train Epoch: 862 [512/54000 (1%)] Loss: -414964.812500\n",
      "Train Epoch: 862 [11776/54000 (22%)] Loss: -512856.375000\n",
      "Train Epoch: 862 [23040/54000 (43%)] Loss: -435690.718750\n",
      "Train Epoch: 862 [34304/54000 (64%)] Loss: -444923.187500\n",
      "Train Epoch: 862 [45568/54000 (84%)] Loss: -469548.656250\n",
      "    epoch          : 862\n",
      "    loss           : -474933.495\n",
      "    val_loss       : -475611.61640625\n",
      "Train Epoch: 863 [512/54000 (1%)] Loss: -504526.812500\n",
      "Train Epoch: 863 [11776/54000 (22%)] Loss: -515177.312500\n",
      "Train Epoch: 863 [23040/54000 (43%)] Loss: -437455.250000\n",
      "Train Epoch: 863 [34304/54000 (64%)] Loss: -533128.812500\n",
      "Train Epoch: 863 [45568/54000 (84%)] Loss: -513824.375000\n",
      "    epoch          : 863\n",
      "    loss           : -475623.19625\n",
      "    val_loss       : -476426.8859375\n",
      "Train Epoch: 864 [512/54000 (1%)] Loss: -420922.781250\n",
      "Train Epoch: 864 [11776/54000 (22%)] Loss: -448655.250000\n",
      "Train Epoch: 864 [23040/54000 (43%)] Loss: -498778.375000\n",
      "Train Epoch: 864 [34304/54000 (64%)] Loss: -560663.500000\n",
      "Train Epoch: 864 [45568/54000 (84%)] Loss: -510704.781250\n",
      "    epoch          : 864\n",
      "    loss           : -475073.6646875\n",
      "    val_loss       : -476293.8671875\n",
      "Train Epoch: 865 [512/54000 (1%)] Loss: -405414.312500\n",
      "Train Epoch: 865 [11776/54000 (22%)] Loss: -439772.312500\n",
      "Train Epoch: 865 [23040/54000 (43%)] Loss: -517540.812500\n",
      "Train Epoch: 865 [34304/54000 (64%)] Loss: -441881.625000\n",
      "Train Epoch: 865 [45568/54000 (84%)] Loss: -506911.718750\n",
      "    epoch          : 865\n",
      "    loss           : -474639.3228125\n",
      "    val_loss       : -473619.4671875\n",
      "Train Epoch: 866 [512/54000 (1%)] Loss: -413171.156250\n",
      "Train Epoch: 866 [11776/54000 (22%)] Loss: -514055.968750\n",
      "Train Epoch: 866 [23040/54000 (43%)] Loss: -502460.656250\n",
      "Train Epoch: 866 [34304/54000 (64%)] Loss: -541435.125000\n",
      "Train Epoch: 866 [45568/54000 (84%)] Loss: -380364.500000\n",
      "    epoch          : 866\n",
      "    loss           : -473755.2675\n",
      "    val_loss       : -473447.58984375\n",
      "Train Epoch: 867 [512/54000 (1%)] Loss: -559624.250000\n",
      "Train Epoch: 867 [11776/54000 (22%)] Loss: -436045.625000\n",
      "Train Epoch: 867 [23040/54000 (43%)] Loss: -409478.406250\n",
      "Train Epoch: 867 [34304/54000 (64%)] Loss: -406287.031250\n",
      "Train Epoch: 867 [45568/54000 (84%)] Loss: -514501.031250\n",
      "    epoch          : 867\n",
      "    loss           : -474294.8284375\n",
      "    val_loss       : -477217.40078125\n",
      "Train Epoch: 868 [512/54000 (1%)] Loss: -439171.062500\n",
      "Train Epoch: 868 [11776/54000 (22%)] Loss: -442474.000000\n",
      "Train Epoch: 868 [23040/54000 (43%)] Loss: -415916.312500\n",
      "Train Epoch: 868 [34304/54000 (64%)] Loss: -415897.125000\n",
      "Train Epoch: 868 [45568/54000 (84%)] Loss: -513132.250000\n",
      "    epoch          : 868\n",
      "    loss           : -475557.336875\n",
      "    val_loss       : -476362.078125\n",
      "Train Epoch: 869 [512/54000 (1%)] Loss: -524558.375000\n",
      "Train Epoch: 869 [11776/54000 (22%)] Loss: -503082.968750\n",
      "Train Epoch: 869 [23040/54000 (43%)] Loss: -421490.656250\n",
      "Train Epoch: 869 [34304/54000 (64%)] Loss: -540959.625000\n",
      "Train Epoch: 869 [45568/54000 (84%)] Loss: -504106.000000\n",
      "    epoch          : 869\n",
      "    loss           : -476155.9434375\n",
      "    val_loss       : -477212.38515625\n",
      "Train Epoch: 870 [512/54000 (1%)] Loss: -541092.562500\n",
      "Train Epoch: 870 [11776/54000 (22%)] Loss: -443042.718750\n",
      "Train Epoch: 870 [23040/54000 (43%)] Loss: -540719.125000\n",
      "Train Epoch: 870 [34304/54000 (64%)] Loss: -443838.000000\n",
      "Train Epoch: 870 [45568/54000 (84%)] Loss: -407716.625000\n",
      "    epoch          : 870\n",
      "    loss           : -476725.2453125\n",
      "    val_loss       : -476859.20859375\n",
      "Train Epoch: 871 [512/54000 (1%)] Loss: -408508.500000\n",
      "Train Epoch: 871 [11776/54000 (22%)] Loss: -441986.875000\n",
      "Train Epoch: 871 [23040/54000 (43%)] Loss: -565060.875000\n",
      "Train Epoch: 871 [34304/54000 (64%)] Loss: -414894.031250\n",
      "Train Epoch: 871 [45568/54000 (84%)] Loss: -514791.625000\n",
      "    epoch          : 871\n",
      "    loss           : -476091.9221875\n",
      "    val_loss       : -477490.10234375\n",
      "Train Epoch: 872 [512/54000 (1%)] Loss: -516163.250000\n",
      "Train Epoch: 872 [11776/54000 (22%)] Loss: -403547.875000\n",
      "Train Epoch: 872 [23040/54000 (43%)] Loss: -412194.125000\n",
      "Train Epoch: 872 [34304/54000 (64%)] Loss: -513647.937500\n",
      "Train Epoch: 872 [45568/54000 (84%)] Loss: -501452.062500\n",
      "    epoch          : 872\n",
      "    loss           : -476687.1046875\n",
      "    val_loss       : -477972.88671875\n",
      "Train Epoch: 873 [512/54000 (1%)] Loss: -516914.625000\n",
      "Train Epoch: 873 [11776/54000 (22%)] Loss: -433909.218750\n",
      "Train Epoch: 873 [23040/54000 (43%)] Loss: -526202.062500\n",
      "Train Epoch: 873 [34304/54000 (64%)] Loss: -406084.312500\n",
      "Train Epoch: 873 [45568/54000 (84%)] Loss: -504072.218750\n",
      "    epoch          : 873\n",
      "    loss           : -476590.0390625\n",
      "    val_loss       : -477015.175\n",
      "Train Epoch: 874 [512/54000 (1%)] Loss: -564447.000000\n",
      "Train Epoch: 874 [11776/54000 (22%)] Loss: -495446.125000\n",
      "Train Epoch: 874 [23040/54000 (43%)] Loss: -385061.343750\n",
      "Train Epoch: 874 [34304/54000 (64%)] Loss: -412397.375000\n",
      "Train Epoch: 874 [45568/54000 (84%)] Loss: -503485.625000\n",
      "    epoch          : 874\n",
      "    loss           : -476201.7034375\n",
      "    val_loss       : -475477.27578125\n",
      "Train Epoch: 875 [512/54000 (1%)] Loss: -564837.812500\n",
      "Train Epoch: 875 [11776/54000 (22%)] Loss: -417968.281250\n",
      "Train Epoch: 875 [23040/54000 (43%)] Loss: -419337.437500\n",
      "Train Epoch: 875 [34304/54000 (64%)] Loss: -523305.562500\n",
      "Train Epoch: 875 [45568/54000 (84%)] Loss: -500977.375000\n",
      "    epoch          : 875\n",
      "    loss           : -476233.2765625\n",
      "    val_loss       : -478378.39140625\n",
      "Train Epoch: 876 [512/54000 (1%)] Loss: -515560.937500\n",
      "Train Epoch: 876 [11776/54000 (22%)] Loss: -541637.875000\n",
      "Train Epoch: 876 [23040/54000 (43%)] Loss: -416634.250000\n",
      "Train Epoch: 876 [34304/54000 (64%)] Loss: -515334.937500\n",
      "Train Epoch: 876 [45568/54000 (84%)] Loss: -449731.437500\n",
      "    epoch          : 876\n",
      "    loss           : -476884.9078125\n",
      "    val_loss       : -477625.20078125\n",
      "Train Epoch: 877 [512/54000 (1%)] Loss: -541745.562500\n",
      "Train Epoch: 877 [11776/54000 (22%)] Loss: -567474.625000\n",
      "Train Epoch: 877 [23040/54000 (43%)] Loss: -493083.937500\n",
      "Train Epoch: 877 [34304/54000 (64%)] Loss: -540670.125000\n",
      "Train Epoch: 877 [45568/54000 (84%)] Loss: -505440.218750\n",
      "    epoch          : 877\n",
      "    loss           : -477306.0740625\n",
      "    val_loss       : -477837.2875\n",
      "Train Epoch: 878 [512/54000 (1%)] Loss: -516956.250000\n",
      "Train Epoch: 878 [11776/54000 (22%)] Loss: -384258.500000\n",
      "Train Epoch: 878 [23040/54000 (43%)] Loss: -409833.000000\n",
      "Train Epoch: 878 [34304/54000 (64%)] Loss: -453213.062500\n",
      "Train Epoch: 878 [45568/54000 (84%)] Loss: -449916.687500\n",
      "    epoch          : 878\n",
      "    loss           : -476961.3921875\n",
      "    val_loss       : -477659.2671875\n",
      "Train Epoch: 879 [512/54000 (1%)] Loss: -453795.843750\n",
      "Train Epoch: 879 [11776/54000 (22%)] Loss: -454075.093750\n",
      "Train Epoch: 879 [23040/54000 (43%)] Loss: -524586.937500\n",
      "Train Epoch: 879 [34304/54000 (64%)] Loss: -526918.000000\n",
      "Train Epoch: 879 [45568/54000 (84%)] Loss: -515785.625000\n",
      "    epoch          : 879\n",
      "    loss           : -477085.0\n",
      "    val_loss       : -478118.565625\n",
      "Train Epoch: 880 [512/54000 (1%)] Loss: -531087.062500\n",
      "Train Epoch: 880 [11776/54000 (22%)] Loss: -405527.062500\n",
      "Train Epoch: 880 [23040/54000 (43%)] Loss: -518891.062500\n",
      "Train Epoch: 880 [34304/54000 (64%)] Loss: -448839.250000\n",
      "Train Epoch: 880 [45568/54000 (84%)] Loss: -518418.812500\n",
      "    epoch          : 880\n",
      "    loss           : -477841.843125\n",
      "    val_loss       : -479009.79453125\n",
      "Train Epoch: 881 [512/54000 (1%)] Loss: -561991.562500\n",
      "Train Epoch: 881 [11776/54000 (22%)] Loss: -421526.375000\n",
      "Train Epoch: 881 [23040/54000 (43%)] Loss: -536718.375000\n",
      "Train Epoch: 881 [34304/54000 (64%)] Loss: -435736.875000\n",
      "Train Epoch: 881 [45568/54000 (84%)] Loss: -514242.156250\n",
      "    epoch          : 881\n",
      "    loss           : -478039.2971875\n",
      "    val_loss       : -479024.26328125\n",
      "Train Epoch: 882 [512/54000 (1%)] Loss: -454325.375000\n",
      "Train Epoch: 882 [11776/54000 (22%)] Loss: -380601.687500\n",
      "Train Epoch: 882 [23040/54000 (43%)] Loss: -407206.812500\n",
      "Train Epoch: 882 [34304/54000 (64%)] Loss: -516702.406250\n",
      "Train Epoch: 882 [45568/54000 (84%)] Loss: -405990.031250\n",
      "    epoch          : 882\n",
      "    loss           : -477354.3628125\n",
      "    val_loss       : -477780.14609375\n",
      "Train Epoch: 883 [512/54000 (1%)] Loss: -523129.250000\n",
      "Train Epoch: 883 [11776/54000 (22%)] Loss: -406490.250000\n",
      "Train Epoch: 883 [23040/54000 (43%)] Loss: -521681.687500\n",
      "Train Epoch: 883 [34304/54000 (64%)] Loss: -505481.875000\n",
      "Train Epoch: 883 [45568/54000 (84%)] Loss: -448769.187500\n",
      "    epoch          : 883\n",
      "    loss           : -477430.50375\n",
      "    val_loss       : -478788.77890625\n",
      "Train Epoch: 884 [512/54000 (1%)] Loss: -519555.687500\n",
      "Train Epoch: 884 [11776/54000 (22%)] Loss: -440998.250000\n",
      "Train Epoch: 884 [23040/54000 (43%)] Loss: -446300.750000\n",
      "Train Epoch: 884 [34304/54000 (64%)] Loss: -444776.875000\n",
      "Train Epoch: 884 [45568/54000 (84%)] Loss: -410037.343750\n",
      "    epoch          : 884\n",
      "    loss           : -477328.48875\n",
      "    val_loss       : -477291.37578125\n",
      "Train Epoch: 885 [512/54000 (1%)] Loss: -527098.500000\n",
      "Train Epoch: 885 [11776/54000 (22%)] Loss: -421737.531250\n",
      "Train Epoch: 885 [23040/54000 (43%)] Loss: -407718.500000\n",
      "Train Epoch: 885 [34304/54000 (64%)] Loss: -527364.937500\n",
      "Train Epoch: 885 [45568/54000 (84%)] Loss: -530490.562500\n",
      "    epoch          : 885\n",
      "    loss           : -477566.36\n",
      "    val_loss       : -479588.4265625\n",
      "Train Epoch: 886 [512/54000 (1%)] Loss: -518457.187500\n",
      "Train Epoch: 886 [11776/54000 (22%)] Loss: -455763.687500\n",
      "Train Epoch: 886 [23040/54000 (43%)] Loss: -447374.000000\n",
      "Train Epoch: 886 [34304/54000 (64%)] Loss: -569322.750000\n",
      "Train Epoch: 886 [45568/54000 (84%)] Loss: -530868.375000\n",
      "    epoch          : 886\n",
      "    loss           : -478700.51\n",
      "    val_loss       : -479547.95234375\n",
      "Train Epoch: 887 [512/54000 (1%)] Loss: -417843.312500\n",
      "Train Epoch: 887 [11776/54000 (22%)] Loss: -546060.250000\n",
      "Train Epoch: 887 [23040/54000 (43%)] Loss: -520188.062500\n",
      "Train Epoch: 887 [34304/54000 (64%)] Loss: -445165.906250\n",
      "Train Epoch: 887 [45568/54000 (84%)] Loss: -520724.687500\n",
      "    epoch          : 887\n",
      "    loss           : -478763.7203125\n",
      "    val_loss       : -478908.5484375\n",
      "Train Epoch: 888 [512/54000 (1%)] Loss: -412622.562500\n",
      "Train Epoch: 888 [11776/54000 (22%)] Loss: -566304.375000\n",
      "Train Epoch: 888 [23040/54000 (43%)] Loss: -566261.437500\n",
      "Train Epoch: 888 [34304/54000 (64%)] Loss: -529105.937500\n",
      "Train Epoch: 888 [45568/54000 (84%)] Loss: -444172.593750\n",
      "    epoch          : 888\n",
      "    loss           : -478182.4253125\n",
      "    val_loss       : -479417.11171875\n",
      "Train Epoch: 889 [512/54000 (1%)] Loss: -377399.562500\n",
      "Train Epoch: 889 [11776/54000 (22%)] Loss: -520607.937500\n",
      "Train Epoch: 889 [23040/54000 (43%)] Loss: -415499.125000\n",
      "Train Epoch: 889 [34304/54000 (64%)] Loss: -446341.250000\n",
      "Train Epoch: 889 [45568/54000 (84%)] Loss: -517173.937500\n",
      "    epoch          : 889\n",
      "    loss           : -478904.5221875\n",
      "    val_loss       : -480482.46640625\n",
      "Train Epoch: 890 [512/54000 (1%)] Loss: -454420.750000\n",
      "Train Epoch: 890 [11776/54000 (22%)] Loss: -444493.062500\n",
      "Train Epoch: 890 [23040/54000 (43%)] Loss: -570065.937500\n",
      "Train Epoch: 890 [34304/54000 (64%)] Loss: -438805.218750\n",
      "Train Epoch: 890 [45568/54000 (84%)] Loss: -519802.125000\n",
      "    epoch          : 890\n",
      "    loss           : -479757.3340625\n",
      "    val_loss       : -479162.83828125\n",
      "Train Epoch: 891 [512/54000 (1%)] Loss: -404841.437500\n",
      "Train Epoch: 891 [11776/54000 (22%)] Loss: -520239.937500\n",
      "Train Epoch: 891 [23040/54000 (43%)] Loss: -546266.812500\n",
      "Train Epoch: 891 [34304/54000 (64%)] Loss: -517160.125000\n",
      "Train Epoch: 891 [45568/54000 (84%)] Loss: -410140.375000\n",
      "    epoch          : 891\n",
      "    loss           : -476800.5346875\n",
      "    val_loss       : -479774.8984375\n",
      "Train Epoch: 892 [512/54000 (1%)] Loss: -411604.656250\n",
      "Train Epoch: 892 [11776/54000 (22%)] Loss: -447828.937500\n",
      "Train Epoch: 892 [23040/54000 (43%)] Loss: -410363.781250\n",
      "Train Epoch: 892 [34304/54000 (64%)] Loss: -525891.375000\n",
      "Train Epoch: 892 [45568/54000 (84%)] Loss: -506514.687500\n",
      "    epoch          : 892\n",
      "    loss           : -478017.4328125\n",
      "    val_loss       : -480135.69609375\n",
      "Train Epoch: 893 [512/54000 (1%)] Loss: -441373.250000\n",
      "Train Epoch: 893 [11776/54000 (22%)] Loss: -546510.437500\n",
      "Train Epoch: 893 [23040/54000 (43%)] Loss: -565813.125000\n",
      "Train Epoch: 893 [34304/54000 (64%)] Loss: -528757.750000\n",
      "Train Epoch: 893 [45568/54000 (84%)] Loss: -504134.656250\n",
      "    epoch          : 893\n",
      "    loss           : -479421.1725\n",
      "    val_loss       : -480638.1875\n",
      "Train Epoch: 894 [512/54000 (1%)] Loss: -546428.000000\n",
      "Train Epoch: 894 [11776/54000 (22%)] Loss: -405523.750000\n",
      "Train Epoch: 894 [23040/54000 (43%)] Loss: -569524.687500\n",
      "Train Epoch: 894 [34304/54000 (64%)] Loss: -407163.687500\n",
      "Train Epoch: 894 [45568/54000 (84%)] Loss: -518799.687500\n",
      "    epoch          : 894\n",
      "    loss           : -479935.54375\n",
      "    val_loss       : -480962.12734375\n",
      "Train Epoch: 895 [512/54000 (1%)] Loss: -545238.000000\n",
      "Train Epoch: 895 [11776/54000 (22%)] Loss: -410674.843750\n",
      "Train Epoch: 895 [23040/54000 (43%)] Loss: -443945.375000\n",
      "Train Epoch: 895 [34304/54000 (64%)] Loss: -422096.750000\n",
      "Train Epoch: 895 [45568/54000 (84%)] Loss: -518228.250000\n",
      "    epoch          : 895\n",
      "    loss           : -480652.73625\n",
      "    val_loss       : -481594.77734375\n",
      "Train Epoch: 896 [512/54000 (1%)] Loss: -411620.781250\n",
      "Train Epoch: 896 [11776/54000 (22%)] Loss: -410175.718750\n",
      "Train Epoch: 896 [23040/54000 (43%)] Loss: -445516.750000\n",
      "Train Epoch: 896 [34304/54000 (64%)] Loss: -504978.187500\n",
      "Train Epoch: 896 [45568/54000 (84%)] Loss: -521133.937500\n",
      "    epoch          : 896\n",
      "    loss           : -480217.65375\n",
      "    val_loss       : -482243.03515625\n",
      "Train Epoch: 897 [512/54000 (1%)] Loss: -416746.125000\n",
      "Train Epoch: 897 [11776/54000 (22%)] Loss: -545297.000000\n",
      "Train Epoch: 897 [23040/54000 (43%)] Loss: -521893.718750\n",
      "Train Epoch: 897 [34304/54000 (64%)] Loss: -405459.843750\n",
      "Train Epoch: 897 [45568/54000 (84%)] Loss: -518386.375000\n",
      "    epoch          : 897\n",
      "    loss           : -480511.719375\n",
      "    val_loss       : -478451.6859375\n",
      "Train Epoch: 898 [512/54000 (1%)] Loss: -570391.437500\n",
      "Train Epoch: 898 [11776/54000 (22%)] Loss: -410327.750000\n",
      "Train Epoch: 898 [23040/54000 (43%)] Loss: -444638.687500\n",
      "Train Epoch: 898 [34304/54000 (64%)] Loss: -531081.125000\n",
      "Train Epoch: 898 [45568/54000 (84%)] Loss: -520220.250000\n",
      "    epoch          : 898\n",
      "    loss           : -480326.1721875\n",
      "    val_loss       : -480804.0421875\n",
      "Train Epoch: 899 [512/54000 (1%)] Loss: -406318.437500\n",
      "Train Epoch: 899 [11776/54000 (22%)] Loss: -409714.093750\n",
      "Train Epoch: 899 [23040/54000 (43%)] Loss: -404532.812500\n",
      "Train Epoch: 899 [34304/54000 (64%)] Loss: -505428.250000\n",
      "Train Epoch: 899 [45568/54000 (84%)] Loss: -521239.281250\n",
      "    epoch          : 899\n",
      "    loss           : -480683.3884375\n",
      "    val_loss       : -481527.61328125\n",
      "Train Epoch: 900 [512/54000 (1%)] Loss: -524219.562500\n",
      "Train Epoch: 900 [11776/54000 (22%)] Loss: -424039.062500\n",
      "Train Epoch: 900 [23040/54000 (43%)] Loss: -410298.656250\n",
      "Train Epoch: 900 [34304/54000 (64%)] Loss: -404008.093750\n",
      "Train Epoch: 900 [45568/54000 (84%)] Loss: -520869.562500\n",
      "    epoch          : 900\n",
      "    loss           : -481237.8503125\n",
      "    val_loss       : -481859.646875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch900.pth ...\n",
      "Train Epoch: 901 [512/54000 (1%)] Loss: -455031.125000\n",
      "Train Epoch: 901 [11776/54000 (22%)] Loss: -424413.718750\n",
      "Train Epoch: 901 [23040/54000 (43%)] Loss: -451437.093750\n",
      "Train Epoch: 901 [34304/54000 (64%)] Loss: -529927.562500\n",
      "Train Epoch: 901 [45568/54000 (84%)] Loss: -414373.406250\n",
      "    epoch          : 901\n",
      "    loss           : -481037.58375\n",
      "    val_loss       : -481419.3984375\n",
      "Train Epoch: 902 [512/54000 (1%)] Loss: -452599.625000\n",
      "Train Epoch: 902 [11776/54000 (22%)] Loss: -521259.562500\n",
      "Train Epoch: 902 [23040/54000 (43%)] Loss: -510413.593750\n",
      "Train Epoch: 902 [34304/54000 (64%)] Loss: -519149.843750\n",
      "Train Epoch: 902 [45568/54000 (84%)] Loss: -527043.250000\n",
      "    epoch          : 902\n",
      "    loss           : -480732.5878125\n",
      "    val_loss       : -480444.1546875\n",
      "Train Epoch: 903 [512/54000 (1%)] Loss: -546956.187500\n",
      "Train Epoch: 903 [11776/54000 (22%)] Loss: -566762.625000\n",
      "Train Epoch: 903 [23040/54000 (43%)] Loss: -444434.437500\n",
      "Train Epoch: 903 [34304/54000 (64%)] Loss: -525360.375000\n",
      "Train Epoch: 903 [45568/54000 (84%)] Loss: -531251.312500\n",
      "    epoch          : 903\n",
      "    loss           : -478437.894375\n",
      "    val_loss       : -480850.50859375\n",
      "Train Epoch: 904 [512/54000 (1%)] Loss: -441669.500000\n",
      "Train Epoch: 904 [11776/54000 (22%)] Loss: -544842.500000\n",
      "Train Epoch: 904 [23040/54000 (43%)] Loss: -547769.250000\n",
      "Train Epoch: 904 [34304/54000 (64%)] Loss: -452400.187500\n",
      "Train Epoch: 904 [45568/54000 (84%)] Loss: -534312.875000\n",
      "    epoch          : 904\n",
      "    loss           : -479428.3584375\n",
      "    val_loss       : -481155.96640625\n",
      "Train Epoch: 905 [512/54000 (1%)] Loss: -517892.687500\n",
      "Train Epoch: 905 [11776/54000 (22%)] Loss: -526284.187500\n",
      "Train Epoch: 905 [23040/54000 (43%)] Loss: -417899.562500\n",
      "Train Epoch: 905 [34304/54000 (64%)] Loss: -523729.843750\n",
      "Train Epoch: 905 [45568/54000 (84%)] Loss: -457348.187500\n",
      "    epoch          : 905\n",
      "    loss           : -480060.6515625\n",
      "    val_loss       : -480846.4875\n",
      "Train Epoch: 906 [512/54000 (1%)] Loss: -530734.437500\n",
      "Train Epoch: 906 [11776/54000 (22%)] Loss: -530732.375000\n",
      "Train Epoch: 906 [23040/54000 (43%)] Loss: -451768.375000\n",
      "Train Epoch: 906 [34304/54000 (64%)] Loss: -491641.437500\n",
      "Train Epoch: 906 [45568/54000 (84%)] Loss: -401385.656250\n",
      "    epoch          : 906\n",
      "    loss           : -479432.8075\n",
      "    val_loss       : -479171.09375\n",
      "Train Epoch: 907 [512/54000 (1%)] Loss: -405802.187500\n",
      "Train Epoch: 907 [11776/54000 (22%)] Loss: -568074.250000\n",
      "Train Epoch: 907 [23040/54000 (43%)] Loss: -545826.375000\n",
      "Train Epoch: 907 [34304/54000 (64%)] Loss: -440830.500000\n",
      "Train Epoch: 907 [45568/54000 (84%)] Loss: -520440.187500\n",
      "    epoch          : 907\n",
      "    loss           : -480545.61625\n",
      "    val_loss       : -482567.8390625\n",
      "Train Epoch: 908 [512/54000 (1%)] Loss: -567771.250000\n",
      "Train Epoch: 908 [11776/54000 (22%)] Loss: -522964.093750\n",
      "Train Epoch: 908 [23040/54000 (43%)] Loss: -521876.656250\n",
      "Train Epoch: 908 [34304/54000 (64%)] Loss: -443503.343750\n",
      "Train Epoch: 908 [45568/54000 (84%)] Loss: -415288.875000\n",
      "    epoch          : 908\n",
      "    loss           : -482194.66125\n",
      "    val_loss       : -482877.696875\n",
      "Train Epoch: 909 [512/54000 (1%)] Loss: -456487.687500\n",
      "Train Epoch: 909 [11776/54000 (22%)] Loss: -413771.187500\n",
      "Train Epoch: 909 [23040/54000 (43%)] Loss: -520506.593750\n",
      "Train Epoch: 909 [34304/54000 (64%)] Loss: -455700.812500\n",
      "Train Epoch: 909 [45568/54000 (84%)] Loss: -447236.906250\n",
      "    epoch          : 909\n",
      "    loss           : -482302.1509375\n",
      "    val_loss       : -483037.32578125\n",
      "Train Epoch: 910 [512/54000 (1%)] Loss: -546798.312500\n",
      "Train Epoch: 910 [11776/54000 (22%)] Loss: -572967.937500\n",
      "Train Epoch: 910 [23040/54000 (43%)] Loss: -411731.125000\n",
      "Train Epoch: 910 [34304/54000 (64%)] Loss: -413671.781250\n",
      "Train Epoch: 910 [45568/54000 (84%)] Loss: -523169.281250\n",
      "    epoch          : 910\n",
      "    loss           : -482638.981875\n",
      "    val_loss       : -483453.284375\n",
      "Train Epoch: 911 [512/54000 (1%)] Loss: -416793.437500\n",
      "Train Epoch: 911 [11776/54000 (22%)] Loss: -410975.000000\n",
      "Train Epoch: 911 [23040/54000 (43%)] Loss: -526435.375000\n",
      "Train Epoch: 911 [34304/54000 (64%)] Loss: -527596.000000\n",
      "Train Epoch: 911 [45568/54000 (84%)] Loss: -488343.437500\n",
      "    epoch          : 911\n",
      "    loss           : -479417.53\n",
      "    val_loss       : -478805.034375\n",
      "Train Epoch: 912 [512/54000 (1%)] Loss: -404483.187500\n",
      "Train Epoch: 912 [11776/54000 (22%)] Loss: -532601.125000\n",
      "Train Epoch: 912 [23040/54000 (43%)] Loss: -410273.437500\n",
      "Train Epoch: 912 [34304/54000 (64%)] Loss: -521366.437500\n",
      "Train Epoch: 912 [45568/54000 (84%)] Loss: -508745.062500\n",
      "    epoch          : 912\n",
      "    loss           : -480600.4003125\n",
      "    val_loss       : -482130.78984375\n",
      "Train Epoch: 913 [512/54000 (1%)] Loss: -412397.875000\n",
      "Train Epoch: 913 [11776/54000 (22%)] Loss: -410141.562500\n",
      "Train Epoch: 913 [23040/54000 (43%)] Loss: -513377.218750\n",
      "Train Epoch: 913 [34304/54000 (64%)] Loss: -512471.812500\n",
      "Train Epoch: 913 [45568/54000 (84%)] Loss: -573206.875000\n",
      "    epoch          : 913\n",
      "    loss           : -482636.4078125\n",
      "    val_loss       : -483429.36015625\n",
      "Train Epoch: 914 [512/54000 (1%)] Loss: -453575.625000\n",
      "Train Epoch: 914 [11776/54000 (22%)] Loss: -455088.562500\n",
      "Train Epoch: 914 [23040/54000 (43%)] Loss: -414769.750000\n",
      "Train Epoch: 914 [34304/54000 (64%)] Loss: -447263.656250\n",
      "Train Epoch: 914 [45568/54000 (84%)] Loss: -491222.750000\n",
      "    epoch          : 914\n",
      "    loss           : -482775.9034375\n",
      "    val_loss       : -484020.6921875\n",
      "Train Epoch: 915 [512/54000 (1%)] Loss: -416989.656250\n",
      "Train Epoch: 915 [11776/54000 (22%)] Loss: -429533.375000\n",
      "Train Epoch: 915 [23040/54000 (43%)] Loss: -441711.187500\n",
      "Train Epoch: 915 [34304/54000 (64%)] Loss: -426786.375000\n",
      "Train Epoch: 915 [45568/54000 (84%)] Loss: -454369.000000\n",
      "    epoch          : 915\n",
      "    loss           : -482611.8559375\n",
      "    val_loss       : -482599.240625\n",
      "Train Epoch: 916 [512/54000 (1%)] Loss: -535084.937500\n",
      "Train Epoch: 916 [11776/54000 (22%)] Loss: -534096.625000\n",
      "Train Epoch: 916 [23040/54000 (43%)] Loss: -548159.562500\n",
      "Train Epoch: 916 [34304/54000 (64%)] Loss: -531269.250000\n",
      "Train Epoch: 916 [45568/54000 (84%)] Loss: -506437.062500\n",
      "    epoch          : 916\n",
      "    loss           : -481645.874375\n",
      "    val_loss       : -481031.66171875\n",
      "Train Epoch: 917 [512/54000 (1%)] Loss: -506586.687500\n",
      "Train Epoch: 917 [11776/54000 (22%)] Loss: -570173.375000\n",
      "Train Epoch: 917 [23040/54000 (43%)] Loss: -408171.031250\n",
      "Train Epoch: 917 [34304/54000 (64%)] Loss: -411736.437500\n",
      "Train Epoch: 917 [45568/54000 (84%)] Loss: -508419.531250\n",
      "    epoch          : 917\n",
      "    loss           : -482967.97\n",
      "    val_loss       : -484097.5484375\n",
      "Train Epoch: 918 [512/54000 (1%)] Loss: -448928.062500\n",
      "Train Epoch: 918 [11776/54000 (22%)] Loss: -550140.625000\n",
      "Train Epoch: 918 [23040/54000 (43%)] Loss: -454845.218750\n",
      "Train Epoch: 918 [34304/54000 (64%)] Loss: -534325.625000\n",
      "Train Epoch: 918 [45568/54000 (84%)] Loss: -426970.062500\n",
      "    epoch          : 918\n",
      "    loss           : -483403.548125\n",
      "    val_loss       : -484490.64609375\n",
      "Train Epoch: 919 [512/54000 (1%)] Loss: -451393.781250\n",
      "Train Epoch: 919 [11776/54000 (22%)] Loss: -509904.281250\n",
      "Train Epoch: 919 [23040/54000 (43%)] Loss: -410156.218750\n",
      "Train Epoch: 919 [34304/54000 (64%)] Loss: -503812.187500\n",
      "Train Epoch: 919 [45568/54000 (84%)] Loss: -521095.531250\n",
      "    epoch          : 919\n",
      "    loss           : -483698.56125\n",
      "    val_loss       : -483373.8796875\n",
      "Train Epoch: 920 [512/54000 (1%)] Loss: -524032.218750\n",
      "Train Epoch: 920 [11776/54000 (22%)] Loss: -548754.437500\n",
      "Train Epoch: 920 [23040/54000 (43%)] Loss: -532661.125000\n",
      "Train Epoch: 920 [34304/54000 (64%)] Loss: -377351.718750\n",
      "Train Epoch: 920 [45568/54000 (84%)] Loss: -419372.437500\n",
      "    epoch          : 920\n",
      "    loss           : -481194.935625\n",
      "    val_loss       : -482163.334375\n",
      "Train Epoch: 921 [512/54000 (1%)] Loss: -410401.343750\n",
      "Train Epoch: 921 [11776/54000 (22%)] Loss: -548662.000000\n",
      "Train Epoch: 921 [23040/54000 (43%)] Loss: -417662.125000\n",
      "Train Epoch: 921 [34304/54000 (64%)] Loss: -529691.937500\n",
      "Train Epoch: 921 [45568/54000 (84%)] Loss: -528771.125000\n",
      "    epoch          : 921\n",
      "    loss           : -482734.14125\n",
      "    val_loss       : -485150.96484375\n",
      "Train Epoch: 922 [512/54000 (1%)] Loss: -524860.625000\n",
      "Train Epoch: 922 [11776/54000 (22%)] Loss: -414952.406250\n",
      "Train Epoch: 922 [23040/54000 (43%)] Loss: -575228.187500\n",
      "Train Epoch: 922 [34304/54000 (64%)] Loss: -425454.000000\n",
      "Train Epoch: 922 [45568/54000 (84%)] Loss: -418534.468750\n",
      "    epoch          : 922\n",
      "    loss           : -484540.8340625\n",
      "    val_loss       : -484815.0546875\n",
      "Train Epoch: 923 [512/54000 (1%)] Loss: -448268.687500\n",
      "Train Epoch: 923 [11776/54000 (22%)] Loss: -412351.625000\n",
      "Train Epoch: 923 [23040/54000 (43%)] Loss: -455271.750000\n",
      "Train Epoch: 923 [34304/54000 (64%)] Loss: -532412.312500\n",
      "Train Epoch: 923 [45568/54000 (84%)] Loss: -530933.375000\n",
      "    epoch          : 923\n",
      "    loss           : -484080.8359375\n",
      "    val_loss       : -484971.76171875\n",
      "Train Epoch: 924 [512/54000 (1%)] Loss: -413399.437500\n",
      "Train Epoch: 924 [11776/54000 (22%)] Loss: -550312.312500\n",
      "Train Epoch: 924 [23040/54000 (43%)] Loss: -550040.000000\n",
      "Train Epoch: 924 [34304/54000 (64%)] Loss: -458601.781250\n",
      "Train Epoch: 924 [45568/54000 (84%)] Loss: -521686.250000\n",
      "    epoch          : 924\n",
      "    loss           : -484792.184375\n",
      "    val_loss       : -485538.265625\n",
      "Train Epoch: 925 [512/54000 (1%)] Loss: -461726.593750\n",
      "Train Epoch: 925 [11776/54000 (22%)] Loss: -458801.031250\n",
      "Train Epoch: 925 [23040/54000 (43%)] Loss: -571773.750000\n",
      "Train Epoch: 925 [34304/54000 (64%)] Loss: -531289.375000\n",
      "Train Epoch: 925 [45568/54000 (84%)] Loss: -534157.125000\n",
      "    epoch          : 925\n",
      "    loss           : -484632.269375\n",
      "    val_loss       : -485970.45390625\n",
      "Train Epoch: 926 [512/54000 (1%)] Loss: -451401.093750\n",
      "Train Epoch: 926 [11776/54000 (22%)] Loss: -577332.250000\n",
      "Train Epoch: 926 [23040/54000 (43%)] Loss: -436577.593750\n",
      "Train Epoch: 926 [34304/54000 (64%)] Loss: -523848.687500\n",
      "Train Epoch: 926 [45568/54000 (84%)] Loss: -411435.250000\n",
      "    epoch          : 926\n",
      "    loss           : -484629.3134375\n",
      "    val_loss       : -484455.99140625\n",
      "Train Epoch: 927 [512/54000 (1%)] Loss: -510499.843750\n",
      "Train Epoch: 927 [11776/54000 (22%)] Loss: -572392.687500\n",
      "Train Epoch: 927 [23040/54000 (43%)] Loss: -521916.656250\n",
      "Train Epoch: 927 [34304/54000 (64%)] Loss: -448424.750000\n",
      "Train Epoch: 927 [45568/54000 (84%)] Loss: -457528.625000\n",
      "    epoch          : 927\n",
      "    loss           : -482050.805625\n",
      "    val_loss       : -484652.546875\n",
      "Train Epoch: 928 [512/54000 (1%)] Loss: -535125.750000\n",
      "Train Epoch: 928 [11776/54000 (22%)] Loss: -415192.250000\n",
      "Train Epoch: 928 [23040/54000 (43%)] Loss: -421139.375000\n",
      "Train Epoch: 928 [34304/54000 (64%)] Loss: -549631.250000\n",
      "Train Epoch: 928 [45568/54000 (84%)] Loss: -412190.375000\n",
      "    epoch          : 928\n",
      "    loss           : -484873.2215625\n",
      "    val_loss       : -485387.9390625\n",
      "Train Epoch: 929 [512/54000 (1%)] Loss: -550962.875000\n",
      "Train Epoch: 929 [11776/54000 (22%)] Loss: -526039.250000\n",
      "Train Epoch: 929 [23040/54000 (43%)] Loss: -547003.812500\n",
      "Train Epoch: 929 [34304/54000 (64%)] Loss: -415254.281250\n",
      "Train Epoch: 929 [45568/54000 (84%)] Loss: -527125.750000\n",
      "    epoch          : 929\n",
      "    loss           : -485656.808125\n",
      "    val_loss       : -485485.17421875\n",
      "Train Epoch: 930 [512/54000 (1%)] Loss: -576374.375000\n",
      "Train Epoch: 930 [11776/54000 (22%)] Loss: -542052.375000\n",
      "Train Epoch: 930 [23040/54000 (43%)] Loss: -524909.500000\n",
      "Train Epoch: 930 [34304/54000 (64%)] Loss: -526468.312500\n",
      "Train Epoch: 930 [45568/54000 (84%)] Loss: -513835.000000\n",
      "    epoch          : 930\n",
      "    loss           : -485171.705625\n",
      "    val_loss       : -486362.40859375\n",
      "Train Epoch: 931 [512/54000 (1%)] Loss: -550408.375000\n",
      "Train Epoch: 931 [11776/54000 (22%)] Loss: -415633.906250\n",
      "Train Epoch: 931 [23040/54000 (43%)] Loss: -459399.625000\n",
      "Train Epoch: 931 [34304/54000 (64%)] Loss: -529274.750000\n",
      "Train Epoch: 931 [45568/54000 (84%)] Loss: -508657.218750\n",
      "    epoch          : 931\n",
      "    loss           : -483896.3665625\n",
      "    val_loss       : -483527.18203125\n",
      "Train Epoch: 932 [512/54000 (1%)] Loss: -511871.437500\n",
      "Train Epoch: 932 [11776/54000 (22%)] Loss: -534491.937500\n",
      "Train Epoch: 932 [23040/54000 (43%)] Loss: -550467.937500\n",
      "Train Epoch: 932 [34304/54000 (64%)] Loss: -572901.375000\n",
      "Train Epoch: 932 [45568/54000 (84%)] Loss: -532918.375000\n",
      "    epoch          : 932\n",
      "    loss           : -483347.6284375\n",
      "    val_loss       : -485081.0640625\n",
      "Train Epoch: 933 [512/54000 (1%)] Loss: -410114.937500\n",
      "Train Epoch: 933 [11776/54000 (22%)] Loss: -427921.281250\n",
      "Train Epoch: 933 [23040/54000 (43%)] Loss: -413564.656250\n",
      "Train Epoch: 933 [34304/54000 (64%)] Loss: -451468.656250\n",
      "Train Epoch: 933 [45568/54000 (84%)] Loss: -525906.500000\n",
      "    epoch          : 933\n",
      "    loss           : -484224.113125\n",
      "    val_loss       : -484824.44140625\n",
      "Train Epoch: 934 [512/54000 (1%)] Loss: -552282.250000\n",
      "Train Epoch: 934 [11776/54000 (22%)] Loss: -528805.625000\n",
      "Train Epoch: 934 [23040/54000 (43%)] Loss: -555486.062500\n",
      "Train Epoch: 934 [34304/54000 (64%)] Loss: -417245.218750\n",
      "Train Epoch: 934 [45568/54000 (84%)] Loss: -511438.625000\n",
      "    epoch          : 934\n",
      "    loss           : -485511.354375\n",
      "    val_loss       : -486851.77890625\n",
      "Train Epoch: 935 [512/54000 (1%)] Loss: -448258.000000\n",
      "Train Epoch: 935 [11776/54000 (22%)] Loss: -433157.656250\n",
      "Train Epoch: 935 [23040/54000 (43%)] Loss: -449066.187500\n",
      "Train Epoch: 935 [34304/54000 (64%)] Loss: -511887.562500\n",
      "Train Epoch: 935 [45568/54000 (84%)] Loss: -413164.500000\n",
      "    epoch          : 935\n",
      "    loss           : -486165.211875\n",
      "    val_loss       : -486521.9984375\n",
      "Train Epoch: 936 [512/54000 (1%)] Loss: -577332.937500\n",
      "Train Epoch: 936 [11776/54000 (22%)] Loss: -415510.062500\n",
      "Train Epoch: 936 [23040/54000 (43%)] Loss: -450327.593750\n",
      "Train Epoch: 936 [34304/54000 (64%)] Loss: -415549.781250\n",
      "Train Epoch: 936 [45568/54000 (84%)] Loss: -530307.187500\n",
      "    epoch          : 936\n",
      "    loss           : -485674.313125\n",
      "    val_loss       : -485746.97265625\n",
      "Train Epoch: 937 [512/54000 (1%)] Loss: -530911.375000\n",
      "Train Epoch: 937 [11776/54000 (22%)] Loss: -533843.062500\n",
      "Train Epoch: 937 [23040/54000 (43%)] Loss: -421774.125000\n",
      "Train Epoch: 937 [34304/54000 (64%)] Loss: -510935.812500\n",
      "Train Epoch: 937 [45568/54000 (84%)] Loss: -511629.406250\n",
      "    epoch          : 937\n",
      "    loss           : -485940.9959375\n",
      "    val_loss       : -486776.89140625\n",
      "Train Epoch: 938 [512/54000 (1%)] Loss: -511519.593750\n",
      "Train Epoch: 938 [11776/54000 (22%)] Loss: -463054.625000\n",
      "Train Epoch: 938 [23040/54000 (43%)] Loss: -536383.750000\n",
      "Train Epoch: 938 [34304/54000 (64%)] Loss: -534313.125000\n",
      "Train Epoch: 938 [45568/54000 (84%)] Loss: -414304.875000\n",
      "    epoch          : 938\n",
      "    loss           : -486078.32375\n",
      "    val_loss       : -486946.89453125\n",
      "Train Epoch: 939 [512/54000 (1%)] Loss: -536145.750000\n",
      "Train Epoch: 939 [11776/54000 (22%)] Loss: -414876.906250\n",
      "Train Epoch: 939 [23040/54000 (43%)] Loss: -417768.031250\n",
      "Train Epoch: 939 [34304/54000 (64%)] Loss: -577786.750000\n",
      "Train Epoch: 939 [45568/54000 (84%)] Loss: -405789.187500\n",
      "    epoch          : 939\n",
      "    loss           : -484164.1528125\n",
      "    val_loss       : -485100.67578125\n",
      "Train Epoch: 940 [512/54000 (1%)] Loss: -416229.750000\n",
      "Train Epoch: 940 [11776/54000 (22%)] Loss: -421361.000000\n",
      "Train Epoch: 940 [23040/54000 (43%)] Loss: -423976.218750\n",
      "Train Epoch: 940 [34304/54000 (64%)] Loss: -527555.000000\n",
      "Train Epoch: 940 [45568/54000 (84%)] Loss: -516333.812500\n",
      "    epoch          : 940\n",
      "    loss           : -485764.0246875\n",
      "    val_loss       : -487326.1109375\n",
      "Train Epoch: 941 [512/54000 (1%)] Loss: -527577.062500\n",
      "Train Epoch: 941 [11776/54000 (22%)] Loss: -449760.437500\n",
      "Train Epoch: 941 [23040/54000 (43%)] Loss: -514511.250000\n",
      "Train Epoch: 941 [34304/54000 (64%)] Loss: -511917.125000\n",
      "Train Epoch: 941 [45568/54000 (84%)] Loss: -512568.531250\n",
      "    epoch          : 941\n",
      "    loss           : -486671.9246875\n",
      "    val_loss       : -487791.371875\n",
      "Train Epoch: 942 [512/54000 (1%)] Loss: -413459.218750\n",
      "Train Epoch: 942 [11776/54000 (22%)] Loss: -459136.343750\n",
      "Train Epoch: 942 [23040/54000 (43%)] Loss: -541337.812500\n",
      "Train Epoch: 942 [34304/54000 (64%)] Loss: -527378.562500\n",
      "Train Epoch: 942 [45568/54000 (84%)] Loss: -529677.750000\n",
      "    epoch          : 942\n",
      "    loss           : -487543.2890625\n",
      "    val_loss       : -487274.26328125\n",
      "Train Epoch: 943 [512/54000 (1%)] Loss: -458703.875000\n",
      "Train Epoch: 943 [11776/54000 (22%)] Loss: -552814.125000\n",
      "Train Epoch: 943 [23040/54000 (43%)] Loss: -428250.281250\n",
      "Train Epoch: 943 [34304/54000 (64%)] Loss: -416319.062500\n",
      "Train Epoch: 943 [45568/54000 (84%)] Loss: -529496.625000\n",
      "    epoch          : 943\n",
      "    loss           : -486684.9134375\n",
      "    val_loss       : -487277.66640625\n",
      "Train Epoch: 944 [512/54000 (1%)] Loss: -526929.562500\n",
      "Train Epoch: 944 [11776/54000 (22%)] Loss: -532562.875000\n",
      "Train Epoch: 944 [23040/54000 (43%)] Loss: -544124.375000\n",
      "Train Epoch: 944 [34304/54000 (64%)] Loss: -529952.000000\n",
      "Train Epoch: 944 [45568/54000 (84%)] Loss: -526116.437500\n",
      "    epoch          : 944\n",
      "    loss           : -486854.8684375\n",
      "    val_loss       : -487929.07109375\n",
      "Train Epoch: 945 [512/54000 (1%)] Loss: -460932.187500\n",
      "Train Epoch: 945 [11776/54000 (22%)] Loss: -422968.468750\n",
      "Train Epoch: 945 [23040/54000 (43%)] Loss: -452410.125000\n",
      "Train Epoch: 945 [34304/54000 (64%)] Loss: -531587.875000\n",
      "Train Epoch: 945 [45568/54000 (84%)] Loss: -542700.312500\n",
      "    epoch          : 945\n",
      "    loss           : -487489.0946875\n",
      "    val_loss       : -488125.84375\n",
      "Train Epoch: 946 [512/54000 (1%)] Loss: -533737.125000\n",
      "Train Epoch: 946 [11776/54000 (22%)] Loss: -555242.062500\n",
      "Train Epoch: 946 [23040/54000 (43%)] Loss: -412400.562500\n",
      "Train Epoch: 946 [34304/54000 (64%)] Loss: -412862.750000\n",
      "Train Epoch: 946 [45568/54000 (84%)] Loss: -577615.250000\n",
      "    epoch          : 946\n",
      "    loss           : -486552.6609375\n",
      "    val_loss       : -487050.8578125\n",
      "Train Epoch: 947 [512/54000 (1%)] Loss: -412957.562500\n",
      "Train Epoch: 947 [11776/54000 (22%)] Loss: -411100.000000\n",
      "Train Epoch: 947 [23040/54000 (43%)] Loss: -534525.375000\n",
      "Train Epoch: 947 [34304/54000 (64%)] Loss: -536297.812500\n",
      "Train Epoch: 947 [45568/54000 (84%)] Loss: -528061.375000\n",
      "    epoch          : 947\n",
      "    loss           : -485576.9090625\n",
      "    val_loss       : -484859.9546875\n",
      "Train Epoch: 948 [512/54000 (1%)] Loss: -522233.375000\n",
      "Train Epoch: 948 [11776/54000 (22%)] Loss: -521108.000000\n",
      "Train Epoch: 948 [23040/54000 (43%)] Loss: -407960.562500\n",
      "Train Epoch: 948 [34304/54000 (64%)] Loss: -552507.312500\n",
      "Train Epoch: 948 [45568/54000 (84%)] Loss: -533618.000000\n",
      "    epoch          : 948\n",
      "    loss           : -482655.2878125\n",
      "    val_loss       : -485899.56875\n",
      "Train Epoch: 949 [512/54000 (1%)] Loss: -573190.250000\n",
      "Train Epoch: 949 [11776/54000 (22%)] Loss: -424919.062500\n",
      "Train Epoch: 949 [23040/54000 (43%)] Loss: -455407.218750\n",
      "Train Epoch: 949 [34304/54000 (64%)] Loss: -553284.875000\n",
      "Train Epoch: 949 [45568/54000 (84%)] Loss: -413307.406250\n",
      "    epoch          : 949\n",
      "    loss           : -487330.2734375\n",
      "    val_loss       : -488521.5015625\n",
      "Train Epoch: 950 [512/54000 (1%)] Loss: -530480.437500\n",
      "Train Epoch: 950 [11776/54000 (22%)] Loss: -530724.187500\n",
      "Train Epoch: 950 [23040/54000 (43%)] Loss: -458245.500000\n",
      "Train Epoch: 950 [34304/54000 (64%)] Loss: -412962.437500\n",
      "Train Epoch: 950 [45568/54000 (84%)] Loss: -414274.875000\n",
      "    epoch          : 950\n",
      "    loss           : -487496.336875\n",
      "    val_loss       : -488090.3671875\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch950.pth ...\n",
      "Train Epoch: 951 [512/54000 (1%)] Loss: -556353.375000\n",
      "Train Epoch: 951 [11776/54000 (22%)] Loss: -582836.250000\n",
      "Train Epoch: 951 [23040/54000 (43%)] Loss: -419017.593750\n",
      "Train Epoch: 951 [34304/54000 (64%)] Loss: -455236.031250\n",
      "Train Epoch: 951 [45568/54000 (84%)] Loss: -584239.125000\n",
      "    epoch          : 951\n",
      "    loss           : -487950.4225\n",
      "    val_loss       : -489076.10625\n",
      "Train Epoch: 952 [512/54000 (1%)] Loss: -409930.593750\n",
      "Train Epoch: 952 [11776/54000 (22%)] Loss: -431979.750000\n",
      "Train Epoch: 952 [23040/54000 (43%)] Loss: -530379.500000\n",
      "Train Epoch: 952 [34304/54000 (64%)] Loss: -532100.812500\n",
      "Train Epoch: 952 [45568/54000 (84%)] Loss: -527296.375000\n",
      "    epoch          : 952\n",
      "    loss           : -487475.1703125\n",
      "    val_loss       : -487282.95625\n",
      "Train Epoch: 953 [512/54000 (1%)] Loss: -516928.437500\n",
      "Train Epoch: 953 [11776/54000 (22%)] Loss: -412012.375000\n",
      "Train Epoch: 953 [23040/54000 (43%)] Loss: -451867.062500\n",
      "Train Epoch: 953 [34304/54000 (64%)] Loss: -555716.000000\n",
      "Train Epoch: 953 [45568/54000 (84%)] Loss: -462667.718750\n",
      "    epoch          : 953\n",
      "    loss           : -487337.7965625\n",
      "    val_loss       : -489097.940625\n",
      "Train Epoch: 954 [512/54000 (1%)] Loss: -415968.687500\n",
      "Train Epoch: 954 [11776/54000 (22%)] Loss: -515006.812500\n",
      "Train Epoch: 954 [23040/54000 (43%)] Loss: -449064.906250\n",
      "Train Epoch: 954 [34304/54000 (64%)] Loss: -426684.843750\n",
      "Train Epoch: 954 [45568/54000 (84%)] Loss: -536992.000000\n",
      "    epoch          : 954\n",
      "    loss           : -489035.386875\n",
      "    val_loss       : -489061.8328125\n",
      "Train Epoch: 955 [512/54000 (1%)] Loss: -449888.187500\n",
      "Train Epoch: 955 [11776/54000 (22%)] Loss: -420248.937500\n",
      "Train Epoch: 955 [23040/54000 (43%)] Loss: -530349.937500\n",
      "Train Epoch: 955 [34304/54000 (64%)] Loss: -458919.312500\n",
      "Train Epoch: 955 [45568/54000 (84%)] Loss: -539827.625000\n",
      "    epoch          : 955\n",
      "    loss           : -488659.990625\n",
      "    val_loss       : -489888.29921875\n",
      "Train Epoch: 956 [512/54000 (1%)] Loss: -555502.750000\n",
      "Train Epoch: 956 [11776/54000 (22%)] Loss: -460776.375000\n",
      "Train Epoch: 956 [23040/54000 (43%)] Loss: -462117.250000\n",
      "Train Epoch: 956 [34304/54000 (64%)] Loss: -578581.750000\n",
      "Train Epoch: 956 [45568/54000 (84%)] Loss: -513684.437500\n",
      "    epoch          : 956\n",
      "    loss           : -488860.4715625\n",
      "    val_loss       : -490299.76640625\n",
      "Train Epoch: 957 [512/54000 (1%)] Loss: -426842.125000\n",
      "Train Epoch: 957 [11776/54000 (22%)] Loss: -420889.750000\n",
      "Train Epoch: 957 [23040/54000 (43%)] Loss: -575286.125000\n",
      "Train Epoch: 957 [34304/54000 (64%)] Loss: -579628.750000\n",
      "Train Epoch: 957 [45568/54000 (84%)] Loss: -409135.187500\n",
      "    epoch          : 957\n",
      "    loss           : -485971.0765625\n",
      "    val_loss       : -486591.68515625\n",
      "Train Epoch: 958 [512/54000 (1%)] Loss: -549183.937500\n",
      "Train Epoch: 958 [11776/54000 (22%)] Loss: -533357.187500\n",
      "Train Epoch: 958 [23040/54000 (43%)] Loss: -455430.125000\n",
      "Train Epoch: 958 [34304/54000 (64%)] Loss: -463437.937500\n",
      "Train Epoch: 958 [45568/54000 (84%)] Loss: -519557.375000\n",
      "    epoch          : 958\n",
      "    loss           : -487378.23875\n",
      "    val_loss       : -489832.87421875\n",
      "Train Epoch: 959 [512/54000 (1%)] Loss: -541494.937500\n",
      "Train Epoch: 959 [11776/54000 (22%)] Loss: -427780.750000\n",
      "Train Epoch: 959 [23040/54000 (43%)] Loss: -520946.031250\n",
      "Train Epoch: 959 [34304/54000 (64%)] Loss: -427857.500000\n",
      "Train Epoch: 959 [45568/54000 (84%)] Loss: -415160.312500\n",
      "    epoch          : 959\n",
      "    loss           : -489260.9821875\n",
      "    val_loss       : -490137.515625\n",
      "Train Epoch: 960 [512/54000 (1%)] Loss: -453155.125000\n",
      "Train Epoch: 960 [11776/54000 (22%)] Loss: -419651.312500\n",
      "Train Epoch: 960 [23040/54000 (43%)] Loss: -516550.093750\n",
      "Train Epoch: 960 [34304/54000 (64%)] Loss: -422081.531250\n",
      "Train Epoch: 960 [45568/54000 (84%)] Loss: -516876.593750\n",
      "    epoch          : 960\n",
      "    loss           : -489516.3796875\n",
      "    val_loss       : -489993.025\n",
      "Train Epoch: 961 [512/54000 (1%)] Loss: -557750.562500\n",
      "Train Epoch: 961 [11776/54000 (22%)] Loss: -419769.875000\n",
      "Train Epoch: 961 [23040/54000 (43%)] Loss: -430512.937500\n",
      "Train Epoch: 961 [34304/54000 (64%)] Loss: -511104.812500\n",
      "Train Epoch: 961 [45568/54000 (84%)] Loss: -533574.437500\n",
      "    epoch          : 961\n",
      "    loss           : -488197.0696875\n",
      "    val_loss       : -488675.15546875\n",
      "Train Epoch: 962 [512/54000 (1%)] Loss: -531827.375000\n",
      "Train Epoch: 962 [11776/54000 (22%)] Loss: -423728.125000\n",
      "Train Epoch: 962 [23040/54000 (43%)] Loss: -452827.468750\n",
      "Train Epoch: 962 [34304/54000 (64%)] Loss: -528988.375000\n",
      "Train Epoch: 962 [45568/54000 (84%)] Loss: -527516.875000\n",
      "    epoch          : 962\n",
      "    loss           : -486377.040625\n",
      "    val_loss       : -489501.63046875\n",
      "Train Epoch: 963 [512/54000 (1%)] Loss: -580357.125000\n",
      "Train Epoch: 963 [11776/54000 (22%)] Loss: -417231.000000\n",
      "Train Epoch: 963 [23040/54000 (43%)] Loss: -541423.250000\n",
      "Train Epoch: 963 [34304/54000 (64%)] Loss: -540233.437500\n",
      "Train Epoch: 963 [45568/54000 (84%)] Loss: -508213.812500\n",
      "    epoch          : 963\n",
      "    loss           : -489475.595\n",
      "    val_loss       : -490150.35625\n",
      "Train Epoch: 964 [512/54000 (1%)] Loss: -416290.687500\n",
      "Train Epoch: 964 [11776/54000 (22%)] Loss: -518114.250000\n",
      "Train Epoch: 964 [23040/54000 (43%)] Loss: -532212.375000\n",
      "Train Epoch: 964 [34304/54000 (64%)] Loss: -535604.000000\n",
      "Train Epoch: 964 [45568/54000 (84%)] Loss: -543864.750000\n",
      "    epoch          : 964\n",
      "    loss           : -488646.570625\n",
      "    val_loss       : -488677.35703125\n",
      "Train Epoch: 965 [512/54000 (1%)] Loss: -532588.437500\n",
      "Train Epoch: 965 [11776/54000 (22%)] Loss: -558301.750000\n",
      "Train Epoch: 965 [23040/54000 (43%)] Loss: -461402.812500\n",
      "Train Epoch: 965 [34304/54000 (64%)] Loss: -459829.000000\n",
      "Train Epoch: 965 [45568/54000 (84%)] Loss: -422286.375000\n",
      "    epoch          : 965\n",
      "    loss           : -489596.364375\n",
      "    val_loss       : -490811.97265625\n",
      "Train Epoch: 966 [512/54000 (1%)] Loss: -517683.906250\n",
      "Train Epoch: 966 [11776/54000 (22%)] Loss: -464971.500000\n",
      "Train Epoch: 966 [23040/54000 (43%)] Loss: -463108.031250\n",
      "Train Epoch: 966 [34304/54000 (64%)] Loss: -413431.562500\n",
      "Train Epoch: 966 [45568/54000 (84%)] Loss: -398620.937500\n",
      "    epoch          : 966\n",
      "    loss           : -490139.9971875\n",
      "    val_loss       : -490708.1265625\n",
      "Train Epoch: 967 [512/54000 (1%)] Loss: -558066.625000\n",
      "Train Epoch: 967 [11776/54000 (22%)] Loss: -531060.375000\n",
      "Train Epoch: 967 [23040/54000 (43%)] Loss: -543298.125000\n",
      "Train Epoch: 967 [34304/54000 (64%)] Loss: -416887.687500\n",
      "Train Epoch: 967 [45568/54000 (84%)] Loss: -519491.031250\n",
      "    epoch          : 967\n",
      "    loss           : -490743.5046875\n",
      "    val_loss       : -491178.140625\n",
      "Train Epoch: 968 [512/54000 (1%)] Loss: -537035.875000\n",
      "Train Epoch: 968 [11776/54000 (22%)] Loss: -423165.250000\n",
      "Train Epoch: 968 [23040/54000 (43%)] Loss: -539283.625000\n",
      "Train Epoch: 968 [34304/54000 (64%)] Loss: -409244.062500\n",
      "Train Epoch: 968 [45568/54000 (84%)] Loss: -530408.250000\n",
      "    epoch          : 968\n",
      "    loss           : -490217.6746875\n",
      "    val_loss       : -490998.81953125\n",
      "Train Epoch: 969 [512/54000 (1%)] Loss: -560409.562500\n",
      "Train Epoch: 969 [11776/54000 (22%)] Loss: -557865.437500\n",
      "Train Epoch: 969 [23040/54000 (43%)] Loss: -467843.937500\n",
      "Train Epoch: 969 [34304/54000 (64%)] Loss: -534460.750000\n",
      "Train Epoch: 969 [45568/54000 (84%)] Loss: -540099.625000\n",
      "    epoch          : 969\n",
      "    loss           : -491024.2234375\n",
      "    val_loss       : -491723.546875\n",
      "Train Epoch: 970 [512/54000 (1%)] Loss: -415179.531250\n",
      "Train Epoch: 970 [11776/54000 (22%)] Loss: -456067.437500\n",
      "Train Epoch: 970 [23040/54000 (43%)] Loss: -558858.875000\n",
      "Train Epoch: 970 [34304/54000 (64%)] Loss: -514718.750000\n",
      "Train Epoch: 970 [45568/54000 (84%)] Loss: -517855.093750\n",
      "    epoch          : 970\n",
      "    loss           : -489596.9384375\n",
      "    val_loss       : -489984.56796875\n",
      "Train Epoch: 971 [512/54000 (1%)] Loss: -556280.687500\n",
      "Train Epoch: 971 [11776/54000 (22%)] Loss: -424800.156250\n",
      "Train Epoch: 971 [23040/54000 (43%)] Loss: -531089.375000\n",
      "Train Epoch: 971 [34304/54000 (64%)] Loss: -507598.875000\n",
      "Train Epoch: 971 [45568/54000 (84%)] Loss: -516237.500000\n",
      "    epoch          : 971\n",
      "    loss           : -488045.254375\n",
      "    val_loss       : -489987.9328125\n",
      "Train Epoch: 972 [512/54000 (1%)] Loss: -535412.625000\n",
      "Train Epoch: 972 [11776/54000 (22%)] Loss: -537994.250000\n",
      "Train Epoch: 972 [23040/54000 (43%)] Loss: -457525.500000\n",
      "Train Epoch: 972 [34304/54000 (64%)] Loss: -487619.406250\n",
      "Train Epoch: 972 [45568/54000 (84%)] Loss: -520366.875000\n",
      "    epoch          : 972\n",
      "    loss           : -490702.4075\n",
      "    val_loss       : -491912.2953125\n",
      "Train Epoch: 973 [512/54000 (1%)] Loss: -420851.687500\n",
      "Train Epoch: 973 [11776/54000 (22%)] Loss: -433814.968750\n",
      "Train Epoch: 973 [23040/54000 (43%)] Loss: -532975.875000\n",
      "Train Epoch: 973 [34304/54000 (64%)] Loss: -544201.437500\n",
      "Train Epoch: 973 [45568/54000 (84%)] Loss: -535403.625000\n",
      "    epoch          : 973\n",
      "    loss           : -490768.5325\n",
      "    val_loss       : -491007.9703125\n",
      "Train Epoch: 974 [512/54000 (1%)] Loss: -413008.843750\n",
      "Train Epoch: 974 [11776/54000 (22%)] Loss: -416402.875000\n",
      "Train Epoch: 974 [23040/54000 (43%)] Loss: -468333.000000\n",
      "Train Epoch: 974 [34304/54000 (64%)] Loss: -445732.218750\n",
      "Train Epoch: 974 [45568/54000 (84%)] Loss: -513777.812500\n",
      "    epoch          : 974\n",
      "    loss           : -490298.8103125\n",
      "    val_loss       : -490943.44296875\n",
      "Train Epoch: 975 [512/54000 (1%)] Loss: -560764.000000\n",
      "Train Epoch: 975 [11776/54000 (22%)] Loss: -546543.875000\n",
      "Train Epoch: 975 [23040/54000 (43%)] Loss: -541202.125000\n",
      "Train Epoch: 975 [34304/54000 (64%)] Loss: -541075.437500\n",
      "Train Epoch: 975 [45568/54000 (84%)] Loss: -538819.937500\n",
      "    epoch          : 975\n",
      "    loss           : -490822.241875\n",
      "    val_loss       : -491618.37890625\n",
      "Train Epoch: 976 [512/54000 (1%)] Loss: -585454.250000\n",
      "Train Epoch: 976 [11776/54000 (22%)] Loss: -521160.656250\n",
      "Train Epoch: 976 [23040/54000 (43%)] Loss: -464088.062500\n",
      "Train Epoch: 976 [34304/54000 (64%)] Loss: -542685.500000\n",
      "Train Epoch: 976 [45568/54000 (84%)] Loss: -535105.875000\n",
      "    epoch          : 976\n",
      "    loss           : -491636.209375\n",
      "    val_loss       : -492716.015625\n",
      "Train Epoch: 977 [512/54000 (1%)] Loss: -519431.562500\n",
      "Train Epoch: 977 [11776/54000 (22%)] Loss: -430671.625000\n",
      "Train Epoch: 977 [23040/54000 (43%)] Loss: -463544.656250\n",
      "Train Epoch: 977 [34304/54000 (64%)] Loss: -531974.562500\n",
      "Train Epoch: 977 [45568/54000 (84%)] Loss: -423771.468750\n",
      "    epoch          : 977\n",
      "    loss           : -491765.4521875\n",
      "    val_loss       : -492092.97734375\n",
      "Train Epoch: 978 [512/54000 (1%)] Loss: -424092.062500\n",
      "Train Epoch: 978 [11776/54000 (22%)] Loss: -417274.468750\n",
      "Train Epoch: 978 [23040/54000 (43%)] Loss: -395571.937500\n",
      "Train Epoch: 978 [34304/54000 (64%)] Loss: -542631.250000\n",
      "Train Epoch: 978 [45568/54000 (84%)] Loss: -549376.625000\n",
      "    epoch          : 978\n",
      "    loss           : -491918.6646875\n",
      "    val_loss       : -491802.12421875\n",
      "Train Epoch: 979 [512/54000 (1%)] Loss: -467516.312500\n",
      "Train Epoch: 979 [11776/54000 (22%)] Loss: -430135.625000\n",
      "Train Epoch: 979 [23040/54000 (43%)] Loss: -462452.031250\n",
      "Train Epoch: 979 [34304/54000 (64%)] Loss: -559465.375000\n",
      "Train Epoch: 979 [45568/54000 (84%)] Loss: -540485.937500\n",
      "    epoch          : 979\n",
      "    loss           : -491513.1846875\n",
      "    val_loss       : -490944.2671875\n",
      "Train Epoch: 980 [512/54000 (1%)] Loss: -558645.125000\n",
      "Train Epoch: 980 [11776/54000 (22%)] Loss: -543682.937500\n",
      "Train Epoch: 980 [23040/54000 (43%)] Loss: -579042.375000\n",
      "Train Epoch: 980 [34304/54000 (64%)] Loss: -547584.437500\n",
      "Train Epoch: 980 [45568/54000 (84%)] Loss: -535549.062500\n",
      "    epoch          : 980\n",
      "    loss           : -490386.8759375\n",
      "    val_loss       : -491283.94296875\n",
      "Train Epoch: 981 [512/54000 (1%)] Loss: -415999.812500\n",
      "Train Epoch: 981 [11776/54000 (22%)] Loss: -519528.062500\n",
      "Train Epoch: 981 [23040/54000 (43%)] Loss: -483470.375000\n",
      "Train Epoch: 981 [34304/54000 (64%)] Loss: -417153.062500\n",
      "Train Epoch: 981 [45568/54000 (84%)] Loss: -453423.875000\n",
      "    epoch          : 981\n",
      "    loss           : -491638.8803125\n",
      "    val_loss       : -491931.4984375\n",
      "Train Epoch: 982 [512/54000 (1%)] Loss: -557456.375000\n",
      "Train Epoch: 982 [11776/54000 (22%)] Loss: -555313.500000\n",
      "Train Epoch: 982 [23040/54000 (43%)] Loss: -416901.875000\n",
      "Train Epoch: 982 [34304/54000 (64%)] Loss: -455184.281250\n",
      "Train Epoch: 982 [45568/54000 (84%)] Loss: -510432.187500\n",
      "    epoch          : 982\n",
      "    loss           : -491237.9121875\n",
      "    val_loss       : -490647.7234375\n",
      "Train Epoch: 983 [512/54000 (1%)] Loss: -556924.937500\n",
      "Train Epoch: 983 [11776/54000 (22%)] Loss: -459867.312500\n",
      "Train Epoch: 983 [23040/54000 (43%)] Loss: -585411.625000\n",
      "Train Epoch: 983 [34304/54000 (64%)] Loss: -536123.375000\n",
      "Train Epoch: 983 [45568/54000 (84%)] Loss: -462620.625000\n",
      "    epoch          : 983\n",
      "    loss           : -491798.51875\n",
      "    val_loss       : -492885.48203125\n",
      "Train Epoch: 984 [512/54000 (1%)] Loss: -541059.500000\n",
      "Train Epoch: 984 [11776/54000 (22%)] Loss: -583826.625000\n",
      "Train Epoch: 984 [23040/54000 (43%)] Loss: -537305.250000\n",
      "Train Epoch: 984 [34304/54000 (64%)] Loss: -544870.625000\n",
      "Train Epoch: 984 [45568/54000 (84%)] Loss: -532977.312500\n",
      "    epoch          : 984\n",
      "    loss           : -492385.276875\n",
      "    val_loss       : -493937.259375\n",
      "Train Epoch: 985 [512/54000 (1%)] Loss: -468230.125000\n",
      "Train Epoch: 985 [11776/54000 (22%)] Loss: -455629.250000\n",
      "Train Epoch: 985 [23040/54000 (43%)] Loss: -395052.593750\n",
      "Train Epoch: 985 [34304/54000 (64%)] Loss: -537819.000000\n",
      "Train Epoch: 985 [45568/54000 (84%)] Loss: -466236.718750\n",
      "    epoch          : 985\n",
      "    loss           : -492643.9909375\n",
      "    val_loss       : -493565.49296875\n",
      "Train Epoch: 986 [512/54000 (1%)] Loss: -456175.000000\n",
      "Train Epoch: 986 [11776/54000 (22%)] Loss: -509391.375000\n",
      "Train Epoch: 986 [23040/54000 (43%)] Loss: -542801.312500\n",
      "Train Epoch: 986 [34304/54000 (64%)] Loss: -518668.375000\n",
      "Train Epoch: 986 [45568/54000 (84%)] Loss: -524071.750000\n",
      "    epoch          : 986\n",
      "    loss           : -492819.718125\n",
      "    val_loss       : -493316.81015625\n",
      "Train Epoch: 987 [512/54000 (1%)] Loss: -421178.875000\n",
      "Train Epoch: 987 [11776/54000 (22%)] Loss: -401016.187500\n",
      "Train Epoch: 987 [23040/54000 (43%)] Loss: -519256.468750\n",
      "Train Epoch: 987 [34304/54000 (64%)] Loss: -418663.531250\n",
      "Train Epoch: 987 [45568/54000 (84%)] Loss: -422266.562500\n",
      "    epoch          : 987\n",
      "    loss           : -493083.2015625\n",
      "    val_loss       : -493874.7015625\n",
      "Train Epoch: 988 [512/54000 (1%)] Loss: -439673.718750\n",
      "Train Epoch: 988 [11776/54000 (22%)] Loss: -561193.125000\n",
      "Train Epoch: 988 [23040/54000 (43%)] Loss: -415857.625000\n",
      "Train Epoch: 988 [34304/54000 (64%)] Loss: -464358.531250\n",
      "Train Epoch: 988 [45568/54000 (84%)] Loss: -583039.187500\n",
      "    epoch          : 988\n",
      "    loss           : -492363.6653125\n",
      "    val_loss       : -492694.70859375\n",
      "Train Epoch: 989 [512/54000 (1%)] Loss: -522898.000000\n",
      "Train Epoch: 989 [11776/54000 (22%)] Loss: -560409.500000\n",
      "Train Epoch: 989 [23040/54000 (43%)] Loss: -516343.250000\n",
      "Train Epoch: 989 [34304/54000 (64%)] Loss: -520064.468750\n",
      "Train Epoch: 989 [45568/54000 (84%)] Loss: -520770.125000\n",
      "    epoch          : 989\n",
      "    loss           : -493366.8396875\n",
      "    val_loss       : -493666.3125\n",
      "Train Epoch: 990 [512/54000 (1%)] Loss: -423279.406250\n",
      "Train Epoch: 990 [11776/54000 (22%)] Loss: -425202.156250\n",
      "Train Epoch: 990 [23040/54000 (43%)] Loss: -461174.906250\n",
      "Train Epoch: 990 [34304/54000 (64%)] Loss: -588126.125000\n",
      "Train Epoch: 990 [45568/54000 (84%)] Loss: -541274.750000\n",
      "    epoch          : 990\n",
      "    loss           : -493277.05875\n",
      "    val_loss       : -493859.7171875\n",
      "Train Epoch: 991 [512/54000 (1%)] Loss: -562280.750000\n",
      "Train Epoch: 991 [11776/54000 (22%)] Loss: -416622.250000\n",
      "Train Epoch: 991 [23040/54000 (43%)] Loss: -455572.250000\n",
      "Train Epoch: 991 [34304/54000 (64%)] Loss: -419036.375000\n",
      "Train Epoch: 991 [45568/54000 (84%)] Loss: -414604.156250\n",
      "    epoch          : 991\n",
      "    loss           : -493394.8503125\n",
      "    val_loss       : -492914.52265625\n",
      "Train Epoch: 992 [512/54000 (1%)] Loss: -561953.875000\n",
      "Train Epoch: 992 [11776/54000 (22%)] Loss: -465763.218750\n",
      "Train Epoch: 992 [23040/54000 (43%)] Loss: -414713.375000\n",
      "Train Epoch: 992 [34304/54000 (64%)] Loss: -539225.875000\n",
      "Train Epoch: 992 [45568/54000 (84%)] Loss: -516321.812500\n",
      "    epoch          : 992\n",
      "    loss           : -491866.1603125\n",
      "    val_loss       : -493813.63984375\n",
      "Train Epoch: 993 [512/54000 (1%)] Loss: -532748.875000\n",
      "Train Epoch: 993 [11776/54000 (22%)] Loss: -429845.937500\n",
      "Train Epoch: 993 [23040/54000 (43%)] Loss: -537048.750000\n",
      "Train Epoch: 993 [34304/54000 (64%)] Loss: -520349.375000\n",
      "Train Epoch: 993 [45568/54000 (84%)] Loss: -537464.437500\n",
      "    epoch          : 993\n",
      "    loss           : -492321.9621875\n",
      "    val_loss       : -494209.10625\n",
      "Train Epoch: 994 [512/54000 (1%)] Loss: -545852.750000\n",
      "Train Epoch: 994 [11776/54000 (22%)] Loss: -520372.281250\n",
      "Train Epoch: 994 [23040/54000 (43%)] Loss: -460460.562500\n",
      "Train Epoch: 994 [34304/54000 (64%)] Loss: -548633.250000\n",
      "Train Epoch: 994 [45568/54000 (84%)] Loss: -535500.500000\n",
      "    epoch          : 994\n",
      "    loss           : -493872.7096875\n",
      "    val_loss       : -494647.41484375\n",
      "Train Epoch: 995 [512/54000 (1%)] Loss: -467360.031250\n",
      "Train Epoch: 995 [11776/54000 (22%)] Loss: -427809.125000\n",
      "Train Epoch: 995 [23040/54000 (43%)] Loss: -431391.000000\n",
      "Train Epoch: 995 [34304/54000 (64%)] Loss: -534739.625000\n",
      "Train Epoch: 995 [45568/54000 (84%)] Loss: -538285.062500\n",
      "    epoch          : 995\n",
      "    loss           : -494126.4371875\n",
      "    val_loss       : -494795.275\n",
      "Train Epoch: 996 [512/54000 (1%)] Loss: -431233.843750\n",
      "Train Epoch: 996 [11776/54000 (22%)] Loss: -587517.375000\n",
      "Train Epoch: 996 [23040/54000 (43%)] Loss: -520428.875000\n",
      "Train Epoch: 996 [34304/54000 (64%)] Loss: -416720.937500\n",
      "Train Epoch: 996 [45568/54000 (84%)] Loss: -546927.875000\n",
      "    epoch          : 996\n",
      "    loss           : -493775.265625\n",
      "    val_loss       : -494522.44921875\n",
      "Train Epoch: 997 [512/54000 (1%)] Loss: -521884.437500\n",
      "Train Epoch: 997 [11776/54000 (22%)] Loss: -586953.062500\n",
      "Train Epoch: 997 [23040/54000 (43%)] Loss: -459648.187500\n",
      "Train Epoch: 997 [34304/54000 (64%)] Loss: -548694.687500\n",
      "Train Epoch: 997 [45568/54000 (84%)] Loss: -534337.812500\n",
      "    epoch          : 997\n",
      "    loss           : -493700.44625\n",
      "    val_loss       : -494380.42578125\n",
      "Train Epoch: 998 [512/54000 (1%)] Loss: -452214.281250\n",
      "Train Epoch: 998 [11776/54000 (22%)] Loss: -562693.625000\n",
      "Train Epoch: 998 [23040/54000 (43%)] Loss: -427501.843750\n",
      "Train Epoch: 998 [34304/54000 (64%)] Loss: -517697.250000\n",
      "Train Epoch: 998 [45568/54000 (84%)] Loss: -583465.937500\n",
      "    epoch          : 998\n",
      "    loss           : -492946.114375\n",
      "    val_loss       : -493955.01875\n",
      "Train Epoch: 999 [512/54000 (1%)] Loss: -454205.187500\n",
      "Train Epoch: 999 [11776/54000 (22%)] Loss: -589397.125000\n",
      "Train Epoch: 999 [23040/54000 (43%)] Loss: -564190.437500\n",
      "Train Epoch: 999 [34304/54000 (64%)] Loss: -522408.687500\n",
      "Train Epoch: 999 [45568/54000 (84%)] Loss: -419166.875000\n",
      "    epoch          : 999\n",
      "    loss           : -494389.941875\n",
      "    val_loss       : -495287.42734375\n",
      "Train Epoch: 1000 [512/54000 (1%)] Loss: -555390.500000\n",
      "Train Epoch: 1000 [11776/54000 (22%)] Loss: -420731.000000\n",
      "Train Epoch: 1000 [23040/54000 (43%)] Loss: -419144.562500\n",
      "Train Epoch: 1000 [34304/54000 (64%)] Loss: -537422.875000\n",
      "Train Epoch: 1000 [45568/54000 (84%)] Loss: -422612.750000\n",
      "    epoch          : 1000\n",
      "    loss           : -494148.7415625\n",
      "    val_loss       : -495939.43515625\n",
      "Saving checkpoint: saved/models/FashionMnist_VaeCategory/0713_124420/checkpoint-epoch1000.pth ...\n",
      "Saving current best: model_best.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAECategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=136, bias=True)\n",
       "        (1): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=136, out_features=136, bias=True)\n",
       "        (4): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=136, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=136, bias=True)\n",
       "        (1): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=136, out_features=136, bias=True)\n",
       "        (4): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=136, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=264, bias=True)\n",
       "        (1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=264, out_features=264, bias=True)\n",
       "        (4): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=264, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=264, bias=True)\n",
       "        (1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=264, out_features=264, bias=True)\n",
       "        (4): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=264, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=400, bias=True)\n",
       "        (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (4): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=400, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_5_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "        (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (4): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=400, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=144, bias=True)\n",
       "        (1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=144, out_features=144, bias=True)\n",
       "        (4): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=144, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=144, bias=True)\n",
       "        (1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=144, out_features=144, bias=True)\n",
       "        (4): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=144, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=272, bias=True)\n",
       "        (1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=272, out_features=272, bias=True)\n",
       "        (4): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=272, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=272, bias=True)\n",
       "        (1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=272, out_features=272, bias=True)\n",
       "        (4): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=272, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_10): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=408, bias=True)\n",
       "        (1): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=408, out_features=408, bias=True)\n",
       "        (4): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=408, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_10_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=408, bias=True)\n",
       "        (1): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=408, out_features=408, bias=True)\n",
       "        (4): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=408, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_11): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_11_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_12): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_12_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_13): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_13_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_14): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_14_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_15): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_15_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_16): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_16_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_17): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_17_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_18): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_18_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_19): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_19_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_20): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_20_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "    (global_element_6): StandardNormal()\n",
       "  )\n",
       "  (guide_embedding): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=49, bias=True)\n",
       "    (1): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=49, out_features=49, bias=True)\n",
       "    (4): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (guide_confidences): Sequential(\n",
       "    (0): Linear(in_features=49, out_features=2, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAXvElEQVR4nO3de5BcZZkG8OeZmZ5MMpkwmVwmVxIuwRABAxUDCmuFZUVgawUtcUmtVnCDwV3xUlKuFrsWwdJNihKR3VXWKCwJQZTlYlBRYcNCdCkSBgwhIZGEMLlO7swkmcx93v2jT7QZ+rxnMt0z3TPf86vqmu5+++vz9pl++5zu73zfoZlBRIa+kkInICIDQ8UuEggVu0ggVOwigVCxiwRCxS4SCBX7EENyMcmVfWz7HpJ/IHmM5BfznVu+kTyd5HGSpYXOZTBQsecJyctIvkCyieQRkv9H8v2FzusU/ROA58ysysz+rdDJJDGznWY20sy6Cp3LYKBizwOSowD8EsC/A6gBMBnAHQDaCplXH0wDsCkuWExbUJJlhWw/GKnY8+McADCzh82sy8xazOxpM9sAACTPIvksycMkD5F8iGT1ycYk60l+leQGks0k7yNZS/LX0S71/5AcHT12OkkjuYjkXpINJG+NS4zkJdEeRyPJV0nOi3ncswAuB/Af0a7xOSQfIHkvyadINgO4nORpJFeQPEhyB8l/IVkSPceN0R7N3dHytpP8YHT/LpIHSC5wcn2O5BKS66I9pFUka3q87oUkdwJ4NuO+sugxk0g+Ge1ZbSP52YznXkzyUZIrSR4FcGOv/rNDiZnpkuMFwCgAhwEsB3A1gNE94mcD+DCAYQDGAVgD4HsZ8XoALwKoRXqv4ACAVwBcGLV5FsDt0WOnAzAADwOoBHA+gIMA/iqKLwawMro+OcrrGqQ/2D8c3R4X8zqeA3BTxu0HADQBuDRqXwFgBYBVAKqiXN4AsDB6/I0AOgF8BkApgG8B2Ang+9HruBLAMQAjneXvAXBe9Noey3gtJ1/3iig2POO+sugxzwP4QZTn7Gi9XJGxXjoAXBe9luGFft8M+Pu00AkMlQuAc6Pi2B294Z8EUBvz2OsA/CHjdj2Av8u4/RiAezNufwHAz6PrJ9/gMzPidwK4L7qeWexfA/Bgj2X/FsCCmLyyFfuKjNulSH81mZVx381If88/WexbM2LnR7nWZtx3GMBsZ/lLM27PAtAeLffk6z4zI/6nYgcwFUAXgKqM+BIAD2SslzWFfp8U8qLd+Dwxs81mdqOZTUF6yzQJwPcAgOR4kj8luSfahVwJYGyPp9ifcb0ly+2RPR6/K+P6jmh5PU0DcH20S91IshHAZQAmnsJLy1zOWADl0fIylz0543bPvGFmSa8lbnk7AKTwznW1C9lNAnDEzI45ucW1DYKKvR+Y2Rakt4rnRXctQXoLdIGZjQLwKQDMcTFTM66fDmBvlsfsQnrLXp1xqTSzpaewnMxhkYeQ3hWe1mPZe07h+ZL0fF0d0XKz5ZNpL4AaklVObkEP8VSx5wHJmSRvJTkluj0VwHykv4cD6e+3xwE0kpwM4Kt5WOw3SI4g+V6kvyP/LMtjVgL4G5IfIVlKsoLkvJN5nipLd3E9AuDbJKtITgPwlWg5+fIpkrNIjgDwTQCPWi+61sxsF4AXACyJXucFABYCeCiPuQ1qKvb8OAbgYgBro1+tXwSwEcDJX8nvAHAR0j92/QrA43lY5vMAtgFYDeA7ZvZ0zwdEBXAtgNuQ/rFqF9IfNLn8378AoBnAdgC/B/ATAPfn8Hw9PYj0XtE+pH9oO5WDe+Yj/T1+L4AnkP5R85k85jaoMfrxQgYJktMBvAUgZWadhc0mv0g+h/SPiz8udC5DkbbsIoFQsYsEQrvxIoHQll0kEAM6GKCcw6wClQO5yOCxLGHsSmlCvDthz6/EP1yguzz+LcbjJ/zn1k7nKWtFM9qtLes/JdeRQ1cBuAfpwxl/nHSwRgUqcTGvyGWRkg3jC660erTftqbaDbPFH7hnFeVuvG1aTWws9fyr/nN3JXSv6yvou6y11bGxPu/GR8Mdv4/0wI9ZAOaTnNXX5xOR/pXLd/a5ALaZ2XYzawfwU6QP4BCRIpRLsU/GOwcW7MY7Bx0AAKJx13Uk6zoG3VwOIkNHLsWe7Yviu75EmdkyM5tjZnNSGJbD4kQkF7kU+268c4TSFGQfeSUiRSCXYn8JwAySZ5AsB3AD0hM2iEgR6nPXm5l1krwF6ZlPSgHcb2axkxVK35VUVPjxMfHdW69/M9ucFn/21tX+mJO3Oo678bWtU934B4fHzxdxxU/8kb4zlr7uxruPN7txt+suwG67nPrZzewpAE/lKRcR6Uc6XFYkECp2kUCo2EUCoWIXCYSKXSQQKnaRQAzoTDWjWGNBDnF1hqACQOmsc9z4kdn+MNVhTd3Ost2m2HupP559xMxGN97c7B8DgL3x8dTxhLHwKf+9edo2f9Gjtzj98C9u8BsPUmttNY7akawrVlt2kUCo2EUCoWIXCYSKXSQQKnaRQKjYRQIxoFNJhyqpa23LP/gzvFa96X8mt9bEx0vb/e6rzmp/BtdjjSPceGl5wgywTq9gVb2fW0Wj/9z7LvG7DT/6lXWxsd/d9H63Lda95scHIW3ZRQKhYhcJhIpdJBAqdpFAqNhFAqFiFwmEil0kEOpnz4cSv7+3bcJIv73TFw0AzVP8Bww7FP+ZfTx+lmkAQGmz/3lfUt3qP0GCVGP8MNakIazDG1rceO06/xiAB6ZdEp/XVX7b0+O76ActbdlFAqFiFwmEil0kECp2kUCo2EUCoWIXCYSKXSQQ6mfPB/P7wbvL/CmTyyeccONtb/vTNY9cH//8w952m8JK/c/7ptRwN17S5rfvHNH3qcqPnl3pxrvK/fXa1VQeG0slTLGddOwEuhPG8RehnIqdZD2AYwC6AHSa2Zx8JCUi+ZePLfvlZnYoD88jIv1I39lFApFrsRuAp0m+THJRtgeQXESyjmRdB9pyXJyI9FWuu/GXmtlekuMBPENyi5mtyXyAmS0DsAxIn+stx+WJSB/ltGU3s73R3wMAngAwNx9JiUj+9bnYSVaSrDp5HcCVADbmKzERya9cduNrATzB9OmIywD8xMx+k5esBpuE0153VSSMGS/x23OY34/f4XRHW0Iff2lrwjer6g433NXm90d3lcY/f/vb8f3gAHD0TDeMzlH+eqmsjT9lc3PC8QNM+aVhbQH1s5vZdgDvy2MuItKP1PUmEggVu0ggVOwigVCxiwRCxS4SCA1xzQf63Vt75vmfqfPPftWN//evLnPj7dVJ4zXjpY4lxHcNc+PdKb99Z3VnbKxtjN/tN+MHO91414TRbvzEt+OHDo8Y1u62ZcL/dDAeCqotu0ggVOwigVCxiwRCxS4SCBW7SCBU7CKBULGLBEL97ANg5BlNbvwDI7e68cdb/sKNj9gX3+vbWuP3F3f7o0xR0u63L/G7q9HpTE501n/t8xuX+5347PSHuF48rj429thrF7ptx5QMve3g0HtFIpKVil0kECp2kUCo2EUCoWIXCYSKXSQQKnaRQKiffQDcNOMFN15d4p+yefzL/nTOR2bF90eP2Of3RZclTCVd+8sdbnzLnRPd+My74k/51VUz0m1bdsA/PoHNrW78+tHrYmO/GH6e/9xlQ680tGUXCYSKXSQQKnaRQKjYRQKhYhcJhIpdJBAqdpFADL3OxAIoGebPrf6xqk1uvC1hEvIR9Y1u/PD54+Kfu9r/PK95xB9Lb6NHufG/POcNN37BQ7tjY6tuucJtWzrcX6/oiJ+THgCmlsb38Z857rD/3LVj/fjRo368CCVu2UneT/IAyY0Z99WQfIbk1uivP1u/iBRcb3bjHwBwVY/7vg5gtZnNALA6ui0iRSyx2M1sDYAjPe6+FsDy6PpyANflOS8RybO+/kBXa2YNABD9HR/3QJKLSNaRrOtA/HcoEelf/f5rvJktM7M5ZjYnhYQfXESk3/S12PeTnAgA0d8D+UtJRPpDX4v9SQALousLAKzKTzoi0l8S+9lJPgxgHoCxJHcDuB3AUgCPkFwIYCeA6/szyWLHCv/ryZQyf9z2HQdn+c9/vMWNt4yPH7Ne3uh/nnNEhRvvTpi7fUrF227cYyUJ50BPlbpxHvKPPzjhHL9wfvVet+3GrulufDBKLHYzmx8T8o+IEJGiosNlRQKhYhcJhIpdJBAqdpFAqNhFAqEhrvlA/zOzqdvvOntw01w3fs6Jnf7iJ8VPqdya0C1oqYS3QKnfPVbfMsaNV42Mz62jyu9aKz/ohsFSf7170V0n/IGaljS8dhDSll0kECp2kUCo2EUCoWIXCYSKXSQQKnaRQKjYRQKhfvY8SBriWkF/NXcfSugLnxw76xcAoKsz/jN75r3+MFCUJHzed/nzXL+59Fw3fvodPacv/LPOCn/ZXSP99dJ5d7kb9w4RmFfzR7ftzy1+eu7BSlt2kUCo2EUCoWIXCYSKXSQQKnaRQKjYRQKhYhcJhPrZ86DjjFo3/vEPftyNv2f/q2684bMXufGzlp2IjXUP96eCZsJ0zUZ/PHtJh98PP7I0fjx7qjl+CuzeSN3iT4Od+m18bG7FW27bR0df6cb9tVKctGUXCYSKXSQQKnaRQKjYRQKhYhcJhIpdJBAqdpFAqJ89D5JOPYz2jpye/5y/9cdeH940PTZmZf7neVlTmxtnR5cbb0+Y+72qJL6f/fgkv21Zs//23L/Ej7/ecVpsrLrEn8u/pNX/n/lHFxSnxC07yftJHiC5MeO+xST3kFwfXa7p3zRFJFe92Y1/AMBVWe6/28xmR5en8puWiORbYrGb2RoA8XMLicigkMsPdLeQ3BDt5seeOIvkIpJ1JOs64H8/FJH+09divxfAWQBmA2gAcFfcA81smZnNMbM5KQy9k+WJDBZ9KnYz229mXWbWDeBHAPzTkIpIwfWp2ElOzLj5MQAb4x4rIsUhsZ+d5MMA5gEYS3I3gNsBzCM5G+nuxnoAN/djjsXBGdfdNtafvzy1JaGfvdTvbz72j/4c5jc/+mhs7Ief+0TCsv1jBG76md/RcnHFXjd++95sHTlpv/vGPW7bjR1+bjes8d92q4++Nzb2uTEvuG2ZMF9+0jh/WPH1xCcWu5nNz3L3ff2Qi4j0Ix0uKxIIFbtIIFTsIoFQsYsEQsUuEggNcc2DjuH+Z6a1xA/z7I3uCn866AllTfFtU34XUXe53+3XlTBp8oLPfMmN//0Pfh4b+8gXv+C2HbE7fopsADh3zz43vuXh+Cm+T68d6bZtGzvcjfv/keKkLbtIIFTsIoFQsYsEQsUuEggVu0ggVOwigVCxiwRC/ey9xfjPxdQJ/9TD1tnpP3XCEFd2+c//i8YLY2MX/esrbtvXF85040c6/f7or/zwITf+28bzY2M3fMsfPvvUX/unqrZW//iFlqvjp0Fbt9EfdjxsX7Mb7y7CIaxJtGUXCYSKXSQQKnaRQKjYRQKhYhcJhIpdJBAqdpFAqJ+9tyy+r7tyx3G3aXdHQj97wrTEZXf5p9r7df2s2NjE0466bf9z1TI3vrVjjBuvbx/rxjst/hiCtzsr3bZW7o8aZ8qPdx+P7yv/5o6Pum1LDjf6z+1Gi5O27CKBULGLBELFLhIIFbtIIFTsIoFQsYsEQsUuEojenLJ5KoAVACYg3b24zMzuIVkD4GcApiN92uZPmtnb/ZdqgTnjl7mjIaGt3yvbOi9+zDcAjCvb7sabD42IjZVW+/3FKxvnuvGjnRVuvKXbP131xiMTY2PDx7a7bRvu9N+ekz7nH59gzvENB++b7rataXrVjQ9GvdmydwK41czOBXAJgM+TnAXg6wBWm9kMAKuj2yJSpBKL3cwazOyV6PoxAJsBTAZwLYDl0cOWA7iuv5IUkdyd0nd2ktMBXAhgLYBaM2sA0h8IAMbnOzkRyZ9eFzvJkQAeA/BlM/MPuH5nu0Uk60jWdSB+TjAR6V+9KnaSKaQL/SEzezy6ez/JiVF8IoAD2dqa2TIzm2Nmc1IYlo+cRaQPEoud6SFZ9wHYbGbfzQg9CWBBdH0BgFX5T09E8qU3Q1wvBfBpAK+RXB/ddxuApQAeIbkQwE4A1/dPisWvqynhW40zDTUAVDT4Q2QPnKhy45Vvxg/1fKN1itu2aYbftTZzdNYdtj+pP1bjxndviT9t8oGJ/jTV7fviuxQBoGmJP0R25pL45x+z7qDbtqt16H3lTCx2M/s9EHuS7ivym46I9BcdQScSCBW7SCBU7CKBULGLBELFLhIIFbtIIDSVdD50d+XUvKTR72ffsT1+mCgATHNOP3wgYbrlfeX+VNEfmvCmG39x13Q3bpXxw0zPHn/IbbupeZIbnzQhYUS1s14t6diIHP+nxUhbdpFAqNhFAqFiFwmEil0kECp2kUCo2EUCoWIXCYT62YtAd5U/LnvcVL8/uemMcbGxkoua3La1Ff647TOG+eO+PzRtmxtv6Yrv5//E2Dq37cqyD7jx01KtbnzPaRNiY90HD7tthyJt2UUCoWIXCYSKXSQQKnaRQKjYRQKhYhcJhIpdJBDqZy8CnWOGu/E54ze78eer4vvZqxL60Zvb/FMuN3RUu/Erqze58T+cmObGPWdX+n38jZ3+vPIo1bYsk9aGSCBU7CKBULGLBELFLhIIFbtIIFTsIoFQsYsEIrGfneRUACsATADQDWCZmd1DcjGAzwI42Rl6m5k91V+JDmXlO/yx1b9Z9z43PuGt7tjYwdHxffAAUDrdn7N+6/HxOcWPdsSf//1Ihz+O/8UGv4++tMTc+LguZ+73ITgvfJLeHFTTCeBWM3uFZBWAl0k+E8XuNrPv9F96IpIvicVuZg0AGqLrx0huBjC5vxMTkfw6pe/sJKcDuBDA2uiuW0huIHk/ydExbRaRrCNZ1wH/0E0R6T+9LnaSIwE8BuDLZnYUwL0AzgIwG+kt/13Z2pnZMjObY2ZzUhiWh5RFpC96VewkU0gX+kNm9jgAmNl+M+sys24APwIwt//SFJFcJRY7SQK4D8BmM/tuxv2Zpxb9GICN+U9PRPKlN7/GXwrg0wBeI7k+uu82APNJzgZgAOoB3NwvGQbAWvwpkS+Z/YYbf6l5Zmxs/AX73bZfO+s3bnx66oi/7JbpbryqtCU2Vl1ywm17SZU/TfUrzf6yt56odeOh6c2v8b8HwCwh9amLDCI6gk4kECp2kUCo2EUCoWIXCYSKXSQQKnaRQNDMHyaYT6NYYxfzigFb3lDBMr+HlOXx00FzhD9Ndfe0+NMaA0B3ub/skvZON27OdM5l+xv9ZSecVrm7NWGsRYDDWNfaahy1I9m6yrVlFwmFil0kECp2kUCo2EUCoWIXCYSKXSQQKnaRQAxoPzvJgwB2ZNw1FsChAUvg1BRrbsWaF6Dc+iqfuU0zs6zzhw9osb9r4WSdmc0pWAKOYs2tWPMClFtfDVRu2o0XCYSKXSQQhS72ZQVevqdYcyvWvADl1lcDkltBv7OLyMAp9JZdRAaIil0kEAUpdpJXkfwjyW0kv16IHOKQrCf5Gsn1JOsKnMv9JA+Q3JhxXw3JZ0hujf5mPcdegXJbTHJPtO7Wk7ymQLlNJfm/JDeT3ETyS9H9BV13Tl4Dst4G/Ds7yVIAbwD4MIDdAF4CMN/MXh/QRGKQrAcwx8wKfgAGyQ8BOA5ghZmdF913J4AjZrY0+qAcbWZfK5LcFgM4XujTeEdnK5qYeZpxANcBuBEFXHdOXp/EAKy3QmzZ5wLYZmbbzawdwE8BXFuAPIqema0B0POULNcCWB5dX470m2XAxeRWFMyswcxeia4fA3DyNOMFXXdOXgOiEMU+GcCujNu7UVznezcAT5N8meSiQieTRa2ZNQDpNw+A8QXOp6fE03gPpB6nGS+addeX05/nqhDFnm1+rGLq/7vUzC4CcDWAz0e7q9I7vTqN90DJcprxotDX05/nqhDFvhvA1IzbUwDsLUAeWZnZ3ujvAQBPoPhORb3/5Bl0o78HCpzPnxTTabyznWYcRbDuCnn680IU+0sAZpA8g2Q5gBsAPFmAPN6FZGX0wwlIVgK4EsV3KuonASyIri8AsKqAubxDsZzGO+404yjwuiv46c/NbMAvAK5B+hf5NwH8cyFyiMnrTACvRpdNhc4NwMNI79Z1IL1HtBDAGACrAWyN/tYUUW4PAngNwAakC2tigXK7DOmvhhsArI8u1xR63Tl5Dch60+GyIoHQEXQigVCxiwRCxS4SCBW7SCBU7CKBULGLBELFLhKI/weWShAdEkn9xQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
