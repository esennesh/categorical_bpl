{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eli/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"cooldown\": 25,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 1870.558350\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: 1031.881958\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: 965.557007\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: 931.124023\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: 879.254761\n",
      "    epoch          : 1\n",
      "    loss           : 1025.1949676513673\n",
      "    val_loss       : 840.2924772822298\n",
      "    val_log_likelihood: -759.9784545898438\n",
      "    val_log_marginal: -795.1304565034807\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: 817.648621\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: 883.740540\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: 793.493408\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: 789.622375\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: 809.896301\n",
      "    epoch          : 2\n",
      "    loss           : 788.8219555664062\n",
      "    val_loss       : 739.8439344140701\n",
      "    val_log_likelihood: -702.5928344726562\n",
      "    val_log_marginal: -723.133229284361\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: 736.270325\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: 733.374390\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: 702.955261\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: 693.838989\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: 706.921692\n",
      "    epoch          : 3\n",
      "    loss           : 715.5354327392578\n",
      "    val_loss       : 686.1886941322125\n",
      "    val_log_likelihood: -657.8493835449219\n",
      "    val_log_marginal: -675.0362231228501\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: 684.394836\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: 682.004517\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: 671.098511\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: 659.449402\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: 657.068176\n",
      "    epoch          : 4\n",
      "    loss           : 669.1597796630859\n",
      "    val_loss       : 647.9435092160478\n",
      "    val_log_likelihood: -623.2659790039063\n",
      "    val_log_marginal: -641.4321017142386\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: 672.028198\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: 650.197021\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: 623.201111\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: 626.983643\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: 599.742371\n",
      "    epoch          : 5\n",
      "    loss           : 627.8631884765625\n",
      "    val_loss       : 616.0459400649182\n",
      "    val_log_likelihood: -586.3824829101562\n",
      "    val_log_marginal: -610.5717468611896\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: 637.868896\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: 594.188721\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: 593.125366\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: 575.286377\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: 585.194458\n",
      "    epoch          : 6\n",
      "    loss           : 588.5598297119141\n",
      "    val_loss       : 569.3017827850766\n",
      "    val_log_likelihood: -551.5662963867187\n",
      "    val_log_marginal: -563.3026788099771\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: 559.257202\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: 560.485107\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: 549.792725\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: 565.103455\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: 550.353027\n",
      "    epoch          : 7\n",
      "    loss           : 554.4952502441406\n",
      "    val_loss       : 534.2871974912472\n",
      "    val_log_likelihood: -513.1716094970703\n",
      "    val_log_marginal: -528.8912607368081\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: 514.855713\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: 518.604980\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: 537.680908\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: 537.403198\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: 492.576538\n",
      "    epoch          : 8\n",
      "    loss           : 520.240361328125\n",
      "    val_loss       : 501.6580222935416\n",
      "    val_log_likelihood: -482.1544494628906\n",
      "    val_log_marginal: -497.8915805948494\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: 511.205811\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: 509.674255\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: 512.198059\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: 461.586365\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: 482.058960\n",
      "    epoch          : 9\n",
      "    loss           : 487.2336065673828\n",
      "    val_loss       : 472.06313980547714\n",
      "    val_log_likelihood: -449.6492858886719\n",
      "    val_log_marginal: -469.5567791305069\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: 470.785095\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: 493.491486\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: 454.471771\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: 440.998199\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: 462.249390\n",
      "    epoch          : 10\n",
      "    loss           : 466.3151678466797\n",
      "    val_loss       : 454.47914866572245\n",
      "    val_log_likelihood: -417.15306091308594\n",
      "    val_log_marginal: -449.9730206519365\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: 483.610168\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: 471.674255\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: 429.407166\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: 437.514343\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: 456.737579\n",
      "    epoch          : 11\n",
      "    loss           : 448.6128549194336\n",
      "    val_loss       : 429.86143162474036\n",
      "    val_log_likelihood: -396.7626037597656\n",
      "    val_log_marginal: -421.98602020032706\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: 415.582916\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: 408.769196\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: 447.184143\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: 434.096832\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: 435.253418\n",
      "    epoch          : 12\n",
      "    loss           : 419.31267913818357\n",
      "    val_loss       : 393.2985585156828\n",
      "    val_log_likelihood: -363.4633728027344\n",
      "    val_log_marginal: -389.8304154865444\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: 386.918030\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: 370.506104\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: 401.701141\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: 386.589874\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: 395.033142\n",
      "    epoch          : 13\n",
      "    loss           : 395.88210327148437\n",
      "    val_loss       : 367.06191898239774\n",
      "    val_log_likelihood: -334.9874755859375\n",
      "    val_log_marginal: -361.0248816110194\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: 408.440277\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: 384.067566\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: 350.192169\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: 354.449951\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: 329.784515\n",
      "    epoch          : 14\n",
      "    loss           : 362.2188790893555\n",
      "    val_loss       : 329.2329724385403\n",
      "    val_log_likelihood: -300.3721435546875\n",
      "    val_log_marginal: -324.56342613280196\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: 295.476868\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: 322.567871\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: 376.504730\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: 306.266632\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: 314.735474\n",
      "    epoch          : 15\n",
      "    loss           : 316.3963473510742\n",
      "    val_loss       : 309.02245524963365\n",
      "    val_log_likelihood: -268.30511779785155\n",
      "    val_log_marginal: -300.31211774943534\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: 294.398560\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: 329.604126\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: 279.376129\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: 285.689880\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: 339.669312\n",
      "    epoch          : 16\n",
      "    loss           : 305.9365557861328\n",
      "    val_loss       : 310.5689624829218\n",
      "    val_log_likelihood: -261.1436996459961\n",
      "    val_log_marginal: -304.06776800528166\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: 297.627319\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: 298.320435\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: 269.286652\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: 282.894501\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: 321.472839\n",
      "    epoch          : 17\n",
      "    loss           : 295.1818516540527\n",
      "    val_loss       : 270.3313857037574\n",
      "    val_log_likelihood: -231.55787811279296\n",
      "    val_log_marginal: -264.72255138643084\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: 264.454987\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: 262.421387\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: 316.838684\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: 280.835388\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: 235.998520\n",
      "    epoch          : 18\n",
      "    loss           : 263.0401387023926\n",
      "    val_loss       : 267.00408973079175\n",
      "    val_log_likelihood: -215.4194366455078\n",
      "    val_log_marginal: -259.8249838151038\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: 242.126312\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: 247.243652\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: 281.313599\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: 212.118423\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: 313.949280\n",
      "    epoch          : 19\n",
      "    loss           : 245.10423706054686\n",
      "    val_loss       : 222.92903782725335\n",
      "    val_log_likelihood: -182.92425079345702\n",
      "    val_log_marginal: -217.31569541811842\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: 237.185303\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: 214.990829\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: 228.086990\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: 201.083374\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: 210.915497\n",
      "    epoch          : 20\n",
      "    loss           : 224.10134384155273\n",
      "    val_loss       : 218.38475588299335\n",
      "    val_log_likelihood: -161.83264236450196\n",
      "    val_log_marginal: -210.30926587544167\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch20.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: 214.165619\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: 278.164764\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: 207.154785\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: 208.716309\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: 246.857178\n",
      "    epoch          : 21\n",
      "    loss           : 232.4023239135742\n",
      "    val_loss       : 194.0308837787132\n",
      "    val_log_likelihood: -146.84494857788087\n",
      "    val_log_marginal: -188.31859733918674\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: 234.095490\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: 296.625031\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: 140.413666\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: 200.340759\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: 189.140411\n",
      "    epoch          : 22\n",
      "    loss           : 213.5809423828125\n",
      "    val_loss       : 188.92726866770536\n",
      "    val_log_likelihood: -121.03682556152344\n",
      "    val_log_marginal: -177.11586771905422\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: 163.496063\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: 118.742233\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: 169.628113\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: 311.024017\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: 191.971771\n",
      "    epoch          : 23\n",
      "    loss           : 190.92035827636718\n",
      "    val_loss       : 182.68920701714234\n",
      "    val_log_likelihood: -114.11225662231445\n",
      "    val_log_marginal: -167.3403266267132\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: 120.258888\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: 114.718399\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: 158.127441\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: 94.692093\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: 160.823273\n",
      "    epoch          : 24\n",
      "    loss           : 173.74683990478516\n",
      "    val_loss       : 228.47194417351858\n",
      "    val_log_likelihood: -102.65135841369629\n",
      "    val_log_marginal: -207.75389740243554\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: 227.312820\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: 272.615173\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: 124.888420\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: 84.107819\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: 243.263641\n",
      "    epoch          : 25\n",
      "    loss           : 160.96487949371337\n",
      "    val_loss       : 146.2389437938109\n",
      "    val_log_likelihood: -74.99269180297851\n",
      "    val_log_marginal: -139.82752835415303\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: 183.191803\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: 94.532913\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: 122.292221\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: 56.361023\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: 124.270790\n",
      "    epoch          : 26\n",
      "    loss           : 118.0175241470337\n",
      "    val_loss       : 113.80274017630146\n",
      "    val_log_likelihood: -50.47929085493088\n",
      "    val_log_marginal: -104.49853568143233\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: 226.930420\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: 285.442963\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: 218.597153\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: 120.681664\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: 143.195038\n",
      "    epoch          : 27\n",
      "    loss           : 129.12343231201172\n",
      "    val_loss       : 130.41721382327378\n",
      "    val_log_likelihood: -46.51894005537033\n",
      "    val_log_marginal: -120.20677362829447\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: 125.957802\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: 70.813301\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: 127.926147\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: 95.460091\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: 105.976044\n",
      "    epoch          : 28\n",
      "    loss           : 116.89166168212891\n",
      "    val_loss       : 169.17178027899936\n",
      "    val_log_likelihood: -36.60419609546661\n",
      "    val_log_marginal: -165.39970607720315\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: 187.858505\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: 106.643967\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: 133.779663\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: 73.555428\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: 109.103363\n",
      "    epoch          : 29\n",
      "    loss           : 116.55618770599365\n",
      "    val_loss       : 86.11099704597146\n",
      "    val_log_likelihood: -23.28897306919098\n",
      "    val_log_marginal: -74.76194745264948\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: 137.543564\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: 88.869797\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: 229.399796\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: 232.990662\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: 184.528275\n",
      "    epoch          : 30\n",
      "    loss           : 146.93125465393067\n",
      "    val_loss       : 108.85400036433711\n",
      "    val_log_likelihood: -35.52183649539948\n",
      "    val_log_marginal: -103.95649679265914\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: 129.974792\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: 74.676071\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: 43.277046\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: 181.695312\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: 129.156006\n",
      "    epoch          : 31\n",
      "    loss           : 77.73120268821717\n",
      "    val_loss       : 70.06835340308025\n",
      "    val_log_likelihood: 4.453063678741455\n",
      "    val_log_marginal: -61.09406648477504\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: 79.681282\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: 66.729301\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: 107.003677\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: 132.968826\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: 108.955093\n",
      "    epoch          : 32\n",
      "    loss           : 74.2285713148117\n",
      "    val_loss       : 98.08444858361035\n",
      "    val_log_likelihood: -3.885809803009033\n",
      "    val_log_marginal: -82.22783795781433\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: 87.625595\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: 126.717728\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -3.273424\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: 17.244366\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: 84.375259\n",
      "    epoch          : 33\n",
      "    loss           : 76.96063943862914\n",
      "    val_loss       : 109.90840672319754\n",
      "    val_log_likelihood: 16.32495822906494\n",
      "    val_log_marginal: -97.46261623911559\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: 258.961700\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: 141.477997\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: 154.819092\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: 109.837227\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: 184.834305\n",
      "    epoch          : 34\n",
      "    loss           : 181.38754398345947\n",
      "    val_loss       : 336.6122034100816\n",
      "    val_log_likelihood: -37.149314522743225\n",
      "    val_log_marginal: -319.65048829838634\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: 284.495148\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: 62.987690\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: 6.280287\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: 20.815647\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -36.174191\n",
      "    epoch          : 35\n",
      "    loss           : 69.73545034408569\n",
      "    val_loss       : 31.05787661522627\n",
      "    val_log_likelihood: 22.898424291610716\n",
      "    val_log_marginal: -25.995847065374242\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -47.059746\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: 29.741074\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: 25.852762\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: 71.231461\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: 63.592030\n",
      "    epoch          : 36\n",
      "    loss           : 32.69141455888748\n",
      "    val_loss       : 16.97684517018497\n",
      "    val_log_likelihood: 35.12513198852539\n",
      "    val_log_marginal: -12.89650414697812\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: 159.804321\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -42.857788\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: 176.894424\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: 64.055389\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: 87.342087\n",
      "    epoch          : 37\n",
      "    loss           : 35.13688132286072\n",
      "    val_loss       : 70.29720469070598\n",
      "    val_log_likelihood: 31.278536224365233\n",
      "    val_log_marginal: -60.48520162804768\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: 195.987274\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -10.815214\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -3.687552\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -24.682114\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: 207.100540\n",
      "    epoch          : 38\n",
      "    loss           : 66.68482267856598\n",
      "    val_loss       : 17.458200684003536\n",
      "    val_log_likelihood: 46.38704466819763\n",
      "    val_log_marginal: -12.133583456277865\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -4.584934\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: 0.555303\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: 56.009083\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: 56.504356\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: 119.209213\n",
      "    epoch          : 39\n",
      "    loss           : 31.383625667095185\n",
      "    val_loss       : 86.16128917830065\n",
      "    val_log_likelihood: 46.76945562362671\n",
      "    val_log_marginal: -64.11104209274053\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -5.977969\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: 51.743317\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: 55.049084\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: 45.764359\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -30.182800\n",
      "    epoch          : 40\n",
      "    loss           : 29.08788233280182\n",
      "    val_loss       : -6.475152665004143\n",
      "    val_log_likelihood: 70.20205230712891\n",
      "    val_log_marginal: 11.991964345425382\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch40.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -21.843481\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: 141.103775\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: 57.732075\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: 49.483479\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: 55.836807\n",
      "    epoch          : 41\n",
      "    loss           : 45.42845258235931\n",
      "    val_loss       : 25.063041033782063\n",
      "    val_log_likelihood: 46.69508457183838\n",
      "    val_log_marginal: -12.44424109049142\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -56.352680\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: 71.864243\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: 61.338100\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: 19.584818\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: 52.567013\n",
      "    epoch          : 42\n",
      "    loss           : 37.51758978366852\n",
      "    val_loss       : -2.606624819058928\n",
      "    val_log_likelihood: 69.0518519282341\n",
      "    val_log_marginal: 8.326907511800533\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -10.446251\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -79.920830\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -6.868619\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -64.022194\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: 159.039566\n",
      "    epoch          : 43\n",
      "    loss           : 8.529805526733398\n",
      "    val_loss       : 4.167292392533271\n",
      "    val_log_likelihood: 76.96736698150634\n",
      "    val_log_marginal: 6.157349162551182\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: 19.658813\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -6.599819\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -20.951174\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: 25.654119\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: 31.646145\n",
      "    epoch          : 44\n",
      "    loss           : -5.04611982345581\n",
      "    val_loss       : 2.2481468641199114\n",
      "    val_log_likelihood: 77.49586610794067\n",
      "    val_log_marginal: 4.842020992562186\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -23.160940\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: 33.868217\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -34.680237\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: 29.184658\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -41.570175\n",
      "    epoch          : 45\n",
      "    loss           : 1.0064162874221803\n",
      "    val_loss       : -8.735690915770832\n",
      "    val_log_likelihood: 89.46503715515136\n",
      "    val_log_marginal: 18.61044101353125\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -27.036676\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -7.112968\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: 9.050303\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: 64.216759\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -20.401154\n",
      "    epoch          : 46\n",
      "    loss           : 22.66266789674759\n",
      "    val_loss       : -8.857535639312118\n",
      "    val_log_likelihood: 88.5573163986206\n",
      "    val_log_marginal: 20.25743453837931\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: 117.816315\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -49.207222\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -50.969749\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -63.337734\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -32.721798\n",
      "    epoch          : 47\n",
      "    loss           : -7.868150191307068\n",
      "    val_loss       : -6.340291128866367\n",
      "    val_log_likelihood: 96.15352749824524\n",
      "    val_log_marginal: 17.01301643922926\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -38.910511\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: 112.408554\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: 125.184097\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: 45.385666\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -29.638845\n",
      "    epoch          : 48\n",
      "    loss           : 48.859476087093356\n",
      "    val_loss       : 12.535389895550912\n",
      "    val_log_likelihood: 90.37493305206299\n",
      "    val_log_marginal: 2.5026726108044444\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: 45.346210\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: 20.253687\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -97.884827\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -44.559296\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -0.049153\n",
      "    epoch          : 49\n",
      "    loss           : -23.51676488876343\n",
      "    val_loss       : -42.439742383360866\n",
      "    val_log_likelihood: 116.07321329116822\n",
      "    val_log_marginal: 49.71513693034649\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -63.469906\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -45.502678\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: 297.218262\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: 24.001753\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: 2.996337\n",
      "    epoch          : 50\n",
      "    loss           : 1.422625684738159\n",
      "    val_loss       : -35.97869298998266\n",
      "    val_log_likelihood: 110.75247211456299\n",
      "    val_log_marginal: 42.87070889903701\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -56.349243\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: 114.294434\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -26.003132\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: 11.134521\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -111.371460\n",
      "    epoch          : 51\n",
      "    loss           : -37.785754837989806\n",
      "    val_loss       : -54.98807584382594\n",
      "    val_log_likelihood: 127.05541725158692\n",
      "    val_log_marginal: 64.2545238558203\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -72.136063\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -64.898476\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: 53.269218\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: 16.048882\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: 100.909782\n",
      "    epoch          : 52\n",
      "    loss           : -8.203402395248412\n",
      "    val_loss       : 4.8358847909606935\n",
      "    val_log_likelihood: 101.46500244140626\n",
      "    val_log_marginal: 3.702899672091013\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: 31.860041\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -95.844711\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -32.843201\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: 86.346436\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -67.668076\n",
      "    epoch          : 53\n",
      "    loss           : -28.528826270103455\n",
      "    val_loss       : -34.0613839146681\n",
      "    val_log_likelihood: 126.51652069091797\n",
      "    val_log_marginal: 48.31891882624182\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -37.620773\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -31.724751\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -34.677696\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -73.658066\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -70.291924\n",
      "    epoch          : 54\n",
      "    loss           : -42.24241510629654\n",
      "    val_loss       : -44.161073656007645\n",
      "    val_log_likelihood: 133.8962345123291\n",
      "    val_log_marginal: 47.04263666458427\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -33.981445\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: 64.353340\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -12.374226\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: 21.972157\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -39.916466\n",
      "    epoch          : 55\n",
      "    loss           : 3.8064025831222534\n",
      "    val_loss       : -40.343889203853905\n",
      "    val_log_likelihood: 122.86802291870117\n",
      "    val_log_marginal: 47.07079414986077\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -126.442230\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -128.655136\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -53.680500\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -88.940514\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: 49.905731\n",
      "    epoch          : 56\n",
      "    loss           : -43.93286752939224\n",
      "    val_loss       : -50.95329726850614\n",
      "    val_log_likelihood: 142.24674949645996\n",
      "    val_log_marginal: 59.31353050097823\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -117.758919\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -96.006256\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -99.695953\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: 79.589523\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: 64.978081\n",
      "    epoch          : 57\n",
      "    loss           : -60.549725761413576\n",
      "    val_loss       : -68.1717760364525\n",
      "    val_log_likelihood: 146.70462379455566\n",
      "    val_log_marginal: 75.87199754879616\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: 62.098366\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -81.196091\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -51.020481\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -106.936325\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -14.036578\n",
      "    epoch          : 58\n",
      "    loss           : -39.44550235271454\n",
      "    val_loss       : -41.827643931005156\n",
      "    val_log_likelihood: 145.34429244995118\n",
      "    val_log_marginal: 54.1848456569016\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -133.931000\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -33.477150\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -46.813000\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -21.968126\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -86.859161\n",
      "    epoch          : 59\n",
      "    loss           : -45.02501515388489\n",
      "    val_loss       : 6.836581883672619\n",
      "    val_log_likelihood: 152.18117599487306\n",
      "    val_log_marginal: 26.05170419700442\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -38.896011\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: 11.502699\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -100.043190\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -156.613342\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -63.156532\n",
      "    epoch          : 60\n",
      "    loss           : -57.25694446086884\n",
      "    val_loss       : -80.84116782862694\n",
      "    val_log_likelihood: 161.33637237548828\n",
      "    val_log_marginal: 87.28944451212882\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch60.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -86.687286\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -142.581970\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -6.563494\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -56.517513\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: 35.672169\n",
      "    epoch          : 61\n",
      "    loss           : -49.13430109977722\n",
      "    val_loss       : -47.01984152859076\n",
      "    val_log_likelihood: 138.51674613952636\n",
      "    val_log_marginal: 54.133019373565915\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -60.173100\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: 41.370117\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -4.661993\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: 64.190750\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: 45.413742\n",
      "    epoch          : 62\n",
      "    loss           : -17.900129928588868\n",
      "    val_loss       : -42.34258485967294\n",
      "    val_log_likelihood: 137.04082527160645\n",
      "    val_log_marginal: 58.89332416020333\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -42.696308\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -141.867188\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: 31.178307\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -42.850494\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -26.181202\n",
      "    epoch          : 63\n",
      "    loss           : -81.49007685184479\n",
      "    val_loss       : -100.07752538761125\n",
      "    val_log_likelihood: 175.43763809204103\n",
      "    val_log_marginal: 108.00184025391937\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -180.249451\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -111.061432\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -136.734390\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -116.259163\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -178.710709\n",
      "    epoch          : 64\n",
      "    loss           : -102.29495545864106\n",
      "    val_loss       : -102.49071664530784\n",
      "    val_log_likelihood: 175.48304595947266\n",
      "    val_log_marginal: 106.87269186601043\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -105.946259\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -186.055038\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -140.115112\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -32.162582\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -54.014629\n",
      "    epoch          : 65\n",
      "    loss           : -97.38557163238525\n",
      "    val_loss       : -57.93739690333605\n",
      "    val_log_likelihood: 161.96544227600097\n",
      "    val_log_marginal: 68.50765164196488\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -60.494682\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -61.804707\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: 90.832870\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: 133.575226\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: 105.057373\n",
      "    epoch          : 66\n",
      "    loss           : 29.2417902302742\n",
      "    val_loss       : -77.99645365644247\n",
      "    val_log_likelihood: 159.2286304473877\n",
      "    val_log_marginal: 87.50157767795025\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: 31.040083\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -108.061813\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -84.393776\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -82.504272\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -12.629152\n",
      "    epoch          : 67\n",
      "    loss           : -85.05552561283112\n",
      "    val_loss       : -95.67168564777822\n",
      "    val_log_likelihood: 174.23896484375\n",
      "    val_log_marginal: 102.34078442379834\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -155.515991\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -105.053040\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -19.247547\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -121.149757\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -52.410030\n",
      "    epoch          : 68\n",
      "    loss           : -118.56757699966431\n",
      "    val_loss       : -116.83511694073677\n",
      "    val_log_likelihood: 191.81009368896486\n",
      "    val_log_marginal: 122.80258349925279\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -196.265747\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -99.500511\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -140.985641\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -127.124611\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: 9.685215\n",
      "    epoch          : 69\n",
      "    loss           : -95.38804630756378\n",
      "    val_loss       : -46.38363544605672\n",
      "    val_log_likelihood: 180.02616271972656\n",
      "    val_log_marginal: 62.22302195094526\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -168.339600\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -0.947332\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -101.807816\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -146.188782\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: 28.393856\n",
      "    epoch          : 70\n",
      "    loss           : -70.0420763850212\n",
      "    val_loss       : -49.8105153576471\n",
      "    val_log_likelihood: 174.89276504516602\n",
      "    val_log_marginal: 59.13593846000731\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -59.576000\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -157.634125\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -43.010525\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -0.806340\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -116.553055\n",
      "    epoch          : 71\n",
      "    loss           : -90.2837031507492\n",
      "    val_loss       : -126.45840777298436\n",
      "    val_log_likelihood: 193.83452377319335\n",
      "    val_log_marginal: 130.57905650362372\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -142.020645\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -162.434357\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -154.480042\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -5.729122\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -69.957359\n",
      "    epoch          : 72\n",
      "    loss           : -130.45772716999053\n",
      "    val_loss       : -116.11539357434958\n",
      "    val_log_likelihood: 202.0837188720703\n",
      "    val_log_marginal: 123.25242475792766\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -46.970757\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -186.802490\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -84.056976\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -0.093946\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: 68.804504\n",
      "    epoch          : 73\n",
      "    loss           : -44.31707568645477\n",
      "    val_loss       : -56.49913761354983\n",
      "    val_log_likelihood: 160.31537780761718\n",
      "    val_log_marginal: 70.76350630857051\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -129.521362\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -91.746094\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -20.145884\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -127.394928\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -122.533157\n",
      "    epoch          : 74\n",
      "    loss           : -90.09864368438721\n",
      "    val_loss       : -121.3441122050397\n",
      "    val_log_likelihood: 200.52534103393555\n",
      "    val_log_marginal: 128.16075781546547\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -105.505585\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -155.834106\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -163.415497\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -85.916962\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -146.844742\n",
      "    epoch          : 75\n",
      "    loss           : -83.21927145957947\n",
      "    val_loss       : -49.60711966818197\n",
      "    val_log_likelihood: 170.07732620239258\n",
      "    val_log_marginal: 57.67756075279486\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -151.345551\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: 60.099258\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: 44.876343\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -114.722969\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -54.904278\n",
      "    epoch          : 76\n",
      "    loss           : -86.45728653907776\n",
      "    val_loss       : -98.85109956292436\n",
      "    val_log_likelihood: 186.85004425048828\n",
      "    val_log_marginal: 104.21129697859116\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -53.577419\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -158.916809\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -4.167453\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -99.056625\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -32.199055\n",
      "    epoch          : 77\n",
      "    loss           : -100.79878897666931\n",
      "    val_loss       : -104.73466737763957\n",
      "    val_log_likelihood: 200.4696487426758\n",
      "    val_log_marginal: 113.76808106265962\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -107.046227\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -126.828598\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -28.998989\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -114.631721\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -147.215637\n",
      "    epoch          : 78\n",
      "    loss           : -107.05997983932495\n",
      "    val_loss       : -90.69739135950803\n",
      "    val_log_likelihood: 200.61438369750977\n",
      "    val_log_marginal: 98.24868279658259\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: 27.262518\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -0.482664\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -50.576809\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -101.917603\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -146.139420\n",
      "    epoch          : 79\n",
      "    loss           : -119.44271112442017\n",
      "    val_loss       : -119.68722388893366\n",
      "    val_log_likelihood: 208.32946166992187\n",
      "    val_log_marginal: 127.97751930207014\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -153.985046\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -138.517319\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -148.975418\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -113.787277\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -16.549297\n",
      "    epoch          : 80\n",
      "    loss           : -114.94300231456756\n",
      "    val_loss       : -132.22810795158148\n",
      "    val_log_likelihood: 208.7288558959961\n",
      "    val_log_marginal: 137.27455379478633\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch80.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -213.438873\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -105.396347\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -142.092728\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -65.502121\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -73.819458\n",
      "    epoch          : 81\n",
      "    loss           : -118.50104196548462\n",
      "    val_loss       : -21.369236578419805\n",
      "    val_log_likelihood: 194.3203971862793\n",
      "    val_log_marginal: 43.90929978638887\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: 40.834656\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: 15.725775\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -97.387283\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -133.656311\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -152.093842\n",
      "    epoch          : 82\n",
      "    loss           : -83.31191210746765\n",
      "    val_loss       : -132.84385857712476\n",
      "    val_log_likelihood: 208.8922218322754\n",
      "    val_log_marginal: 138.28995679095388\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -220.337128\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -161.866974\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -52.972916\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -175.599838\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -206.987228\n",
      "    epoch          : 83\n",
      "    loss           : -154.01531867980958\n",
      "    val_loss       : -152.05957028716801\n",
      "    val_log_likelihood: 226.87710876464843\n",
      "    val_log_marginal: 157.25413479928162\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -152.447418\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -210.506149\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -133.906662\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -92.118385\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -171.224106\n",
      "    epoch          : 84\n",
      "    loss           : -120.39010475158692\n",
      "    val_loss       : -117.35906229009852\n",
      "    val_log_likelihood: 205.76032104492188\n",
      "    val_log_marginal: 121.90825671777131\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -6.453092\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -104.389755\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -83.303642\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -223.353882\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -109.608932\n",
      "    epoch          : 85\n",
      "    loss           : -121.97569118976593\n",
      "    val_loss       : -19.111280677095056\n",
      "    val_log_likelihood: 209.8945686340332\n",
      "    val_log_marginal: 30.926811157539493\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -60.789223\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -104.268166\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -166.575302\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -105.399452\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -62.977673\n",
      "    epoch          : 86\n",
      "    loss           : -73.84736102581024\n",
      "    val_loss       : -90.18500900724902\n",
      "    val_log_likelihood: 200.58628311157227\n",
      "    val_log_marginal: 96.57620335444808\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -82.064117\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -103.378433\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -119.561615\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -58.568264\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -67.856094\n",
      "    epoch          : 87\n",
      "    loss           : -124.92800642490387\n",
      "    val_loss       : -152.89243755480274\n",
      "    val_log_likelihood: 229.22160110473632\n",
      "    val_log_marginal: 159.2787678834051\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -240.429306\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -193.768097\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -114.692703\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -111.436249\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -130.527054\n",
      "    epoch          : 88\n",
      "    loss           : -140.73500407218933\n",
      "    val_loss       : -128.58162521542982\n",
      "    val_log_likelihood: 228.39887390136718\n",
      "    val_log_marginal: 134.56428812928496\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -147.322784\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -132.126404\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -88.975105\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -200.637863\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -154.525208\n",
      "    epoch          : 89\n",
      "    loss           : -132.6200279712677\n",
      "    val_loss       : -139.48997062360868\n",
      "    val_log_likelihood: 230.46733016967772\n",
      "    val_log_marginal: 147.9619109030813\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -42.130714\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -237.311401\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -124.905128\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -81.811592\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -3.158840\n",
      "    epoch          : 90\n",
      "    loss           : -109.61491427898407\n",
      "    val_loss       : -78.6451681674458\n",
      "    val_log_likelihood: 216.84113845825195\n",
      "    val_log_marginal: 86.84027135111391\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -7.233416\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: 31.917505\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -52.433067\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -170.664948\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: 137.027740\n",
      "    epoch          : 91\n",
      "    loss           : -38.39687833786011\n",
      "    val_loss       : -98.65363132068887\n",
      "    val_log_likelihood: 202.94085388183595\n",
      "    val_log_marginal: 107.38073189891875\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -196.014038\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -135.125214\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -60.356728\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -25.416588\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -161.085999\n",
      "    epoch          : 92\n",
      "    loss           : -122.15378145694733\n",
      "    val_loss       : -150.23910626750438\n",
      "    val_log_likelihood: 231.87078857421875\n",
      "    val_log_marginal: 156.67909153662623\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -170.634796\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -168.174133\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -171.226700\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: 66.986160\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -13.278833\n",
      "    epoch          : 93\n",
      "    loss           : -129.1442467880249\n",
      "    val_loss       : -121.6895403534174\n",
      "    val_log_likelihood: 234.90898590087892\n",
      "    val_log_marginal: 127.01019763350487\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: 11.405871\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -226.829468\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -179.556152\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -117.118851\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -175.994110\n",
      "    epoch          : 94\n",
      "    loss           : -144.36792428016662\n",
      "    val_loss       : -164.93001773795112\n",
      "    val_log_likelihood: 241.42810974121093\n",
      "    val_log_marginal: 172.0270716696977\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -58.694290\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -200.956299\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -200.185623\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -156.543564\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -198.150269\n",
      "    epoch          : 95\n",
      "    loss           : -181.5628205871582\n",
      "    val_loss       : -174.46470444835722\n",
      "    val_log_likelihood: 253.49081726074218\n",
      "    val_log_marginal: 180.6006026763704\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -190.938354\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -245.623703\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -100.335526\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -270.138672\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -207.073380\n",
      "    epoch          : 96\n",
      "    loss           : -183.49838859558105\n",
      "    val_loss       : -170.04832369955255\n",
      "    val_log_likelihood: 250.24620819091797\n",
      "    val_log_marginal: 175.25394300855697\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -186.163925\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -87.142090\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -186.273285\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -209.034821\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -161.780212\n",
      "    epoch          : 97\n",
      "    loss           : -170.83879684448243\n",
      "    val_loss       : -169.78893453236668\n",
      "    val_log_likelihood: 257.38513336181643\n",
      "    val_log_marginal: 179.32669659666718\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -205.204391\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -197.848358\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -263.687042\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: 287.975464\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -31.512474\n",
      "    epoch          : 98\n",
      "    loss           : -81.7247725391388\n",
      "    val_loss       : -93.90370922135187\n",
      "    val_log_likelihood: 217.54834060668946\n",
      "    val_log_marginal: 108.13613158235687\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -121.871155\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -242.722717\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -87.230232\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -209.869751\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: 3.239191\n",
      "    epoch          : 99\n",
      "    loss           : -112.86592996120453\n",
      "    val_loss       : -133.23328987462446\n",
      "    val_log_likelihood: 231.26485290527344\n",
      "    val_log_marginal: 140.84348279833793\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -177.628815\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -178.522858\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -171.119141\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -211.531952\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -135.076080\n",
      "    epoch          : 100\n",
      "    loss           : -167.02884702682496\n",
      "    val_loss       : -159.5686170413159\n",
      "    val_log_likelihood: 253.1408905029297\n",
      "    val_log_marginal: 163.41190036199987\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -108.064339\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -235.070175\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -122.246216\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -133.311981\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -185.525818\n",
      "    epoch          : 101\n",
      "    loss           : -150.3659346961975\n",
      "    val_loss       : -151.03527172748\n",
      "    val_log_likelihood: 251.00484313964844\n",
      "    val_log_marginal: 158.3124857157469\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -181.583557\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -181.869446\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -91.397202\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -173.538849\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -51.459255\n",
      "    epoch          : 102\n",
      "    loss           : -163.13184442520142\n",
      "    val_loss       : -158.55396595848725\n",
      "    val_log_likelihood: 253.27111282348633\n",
      "    val_log_marginal: 165.21239156238735\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -197.730621\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -265.647766\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -120.000366\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -219.906036\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -145.778259\n",
      "    epoch          : 103\n",
      "    loss           : -175.2505978679657\n",
      "    val_loss       : -174.80564451888205\n",
      "    val_log_likelihood: 261.37230377197267\n",
      "    val_log_marginal: 180.4473255608231\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -98.877724\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -84.552002\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -133.240585\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -138.752518\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -170.186905\n",
      "    epoch          : 104\n",
      "    loss           : -118.03192162036896\n",
      "    val_loss       : -155.4489150688052\n",
      "    val_log_likelihood: 253.40702362060546\n",
      "    val_log_marginal: 163.30101250933433\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -232.964630\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -161.030273\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -115.513229\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: 11.947850\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -140.028839\n",
      "    epoch          : 105\n",
      "    loss           : -136.73874153137206\n",
      "    val_loss       : -141.02317465217783\n",
      "    val_log_likelihood: 243.76272201538086\n",
      "    val_log_marginal: 146.44415083937346\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -105.633621\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -59.547768\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -49.411823\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -101.986374\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -93.853912\n",
      "    epoch          : 106\n",
      "    loss           : -101.3989824104309\n",
      "    val_loss       : -146.89044977324085\n",
      "    val_log_likelihood: 249.2788314819336\n",
      "    val_log_marginal: 157.85299064293505\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -84.840462\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -65.548576\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -67.466110\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -200.494461\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -189.869812\n",
      "    epoch          : 107\n",
      "    loss           : -179.84682666778565\n",
      "    val_loss       : -176.241842541378\n",
      "    val_log_likelihood: 263.5470718383789\n",
      "    val_log_marginal: 182.08331241026877\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -211.820129\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -166.293976\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -275.672546\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -201.154236\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -194.361053\n",
      "    epoch          : 108\n",
      "    loss           : -188.1425372028351\n",
      "    val_loss       : -193.9784796588123\n",
      "    val_log_likelihood: 271.5522705078125\n",
      "    val_log_marginal: 197.45872880332172\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -233.776901\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -223.600159\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -175.563004\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -171.650818\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -176.570709\n",
      "    epoch          : 109\n",
      "    loss           : -199.46813028335572\n",
      "    val_loss       : -191.0591795564629\n",
      "    val_log_likelihood: 274.135237121582\n",
      "    val_log_marginal: 199.17504191026092\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -85.343872\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -190.562149\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -229.591034\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -192.040680\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: 42.160431\n",
      "    epoch          : 110\n",
      "    loss           : -155.660512714386\n",
      "    val_loss       : -117.95842511160299\n",
      "    val_log_likelihood: 250.9327819824219\n",
      "    val_log_marginal: 124.60611205214714\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -141.311218\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -90.181831\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -221.357849\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -224.455109\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -225.688446\n",
      "    epoch          : 111\n",
      "    loss           : -143.47205215930938\n",
      "    val_loss       : -177.94447026625275\n",
      "    val_log_likelihood: 271.76678466796875\n",
      "    val_log_marginal: 196.97742065710943\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -221.366211\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -221.674820\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -226.013153\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -182.938034\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -86.522850\n",
      "    epoch          : 112\n",
      "    loss           : -208.31283073425294\n",
      "    val_loss       : -197.4506440327503\n",
      "    val_log_likelihood: 281.0317092895508\n",
      "    val_log_marginal: 203.85732853524388\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -226.936066\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -201.568146\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -235.293945\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -58.683052\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -231.313385\n",
      "    epoch          : 113\n",
      "    loss           : -200.5790015411377\n",
      "    val_loss       : -202.93442790200933\n",
      "    val_log_likelihood: 284.6406967163086\n",
      "    val_log_marginal: 212.1443218421191\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -239.854248\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -313.053589\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -298.646606\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: 96.389359\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -73.778618\n",
      "    epoch          : 114\n",
      "    loss           : -161.70366609573364\n",
      "    val_loss       : -119.06227236008272\n",
      "    val_log_likelihood: 262.78399047851565\n",
      "    val_log_marginal: 129.45423127114773\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -128.566406\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -34.126366\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -279.401428\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -250.064590\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -164.759064\n",
      "    epoch          : 115\n",
      "    loss           : -172.65482670783996\n",
      "    val_loss       : -164.40234216805547\n",
      "    val_log_likelihood: 270.10618286132814\n",
      "    val_log_marginal: 175.61256865002215\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -212.427734\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -197.957825\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -18.060438\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -234.681183\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: 433.713379\n",
      "    epoch          : 116\n",
      "    loss           : -115.66146253108978\n",
      "    val_loss       : -78.99303404036911\n",
      "    val_log_likelihood: 248.05030975341796\n",
      "    val_log_marginal: 106.09320910200476\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: 62.595184\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -114.184769\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -118.632065\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -121.306190\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -209.149017\n",
      "    epoch          : 117\n",
      "    loss           : -68.77227568626404\n",
      "    val_loss       : -169.01475459719077\n",
      "    val_log_likelihood: 272.9729934692383\n",
      "    val_log_marginal: 180.36026614313408\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -44.415794\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -226.736069\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -297.454987\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -94.127274\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -215.897980\n",
      "    epoch          : 118\n",
      "    loss           : -200.65160388946532\n",
      "    val_loss       : -199.17455036500468\n",
      "    val_log_likelihood: 281.1669464111328\n",
      "    val_log_marginal: 202.18308803997934\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -225.078110\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -267.845154\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -190.873962\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -149.699524\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -197.329208\n",
      "    epoch          : 119\n",
      "    loss           : -182.6163613986969\n",
      "    val_loss       : -193.8404882467352\n",
      "    val_log_likelihood: 280.30979309082034\n",
      "    val_log_marginal: 200.77921039015055\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -184.484360\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -244.035263\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -291.835754\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -182.947906\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -218.097321\n",
      "    epoch          : 120\n",
      "    loss           : -204.66230152130126\n",
      "    val_loss       : -212.08509716708213\n",
      "    val_log_likelihood: 290.6431518554688\n",
      "    val_log_marginal: 220.19235883019866\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch120.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -314.261810\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -124.739517\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -258.349457\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -255.122009\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -127.940964\n",
      "    epoch          : 121\n",
      "    loss           : -218.88953308105468\n",
      "    val_loss       : -208.494240935985\n",
      "    val_log_likelihood: 293.6265045166016\n",
      "    val_log_marginal: 215.2823059745133\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -304.744415\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -239.003220\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -239.951859\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -150.763870\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -172.246277\n",
      "    epoch          : 122\n",
      "    loss           : -188.8558692932129\n",
      "    val_loss       : -172.78405781835318\n",
      "    val_log_likelihood: 285.8291763305664\n",
      "    val_log_marginal: 183.0964945346117\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: 0.043440\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -193.331161\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -55.632568\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -152.714066\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: 22.509659\n",
      "    epoch          : 123\n",
      "    loss           : -152.142880191803\n",
      "    val_loss       : -112.58612253786995\n",
      "    val_log_likelihood: 258.1944007873535\n",
      "    val_log_marginal: 121.63413449611342\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: 30.900003\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -106.504349\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -227.266235\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -89.172775\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -129.716522\n",
      "    epoch          : 124\n",
      "    loss           : -139.08861011505127\n",
      "    val_loss       : -186.92143746251242\n",
      "    val_log_likelihood: 281.1541549682617\n",
      "    val_log_marginal: 195.0442997895181\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -190.608871\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -126.779045\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -299.152313\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -231.730072\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -212.364548\n",
      "    epoch          : 125\n",
      "    loss           : -223.4931346130371\n",
      "    val_loss       : -217.16572642605752\n",
      "    val_log_likelihood: 297.3352386474609\n",
      "    val_log_marginal: 226.19530804678806\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -236.095306\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -251.773697\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -243.719788\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -109.740471\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -215.368759\n",
      "    epoch          : 126\n",
      "    loss           : -226.7620962524414\n",
      "    val_loss       : -215.18184758378192\n",
      "    val_log_likelihood: 299.17676239013673\n",
      "    val_log_marginal: 220.0766065236181\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -250.211227\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -250.316360\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -246.842682\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -243.014511\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -199.047501\n",
      "    epoch          : 127\n",
      "    loss           : -199.9486937046051\n",
      "    val_loss       : -146.71677947305142\n",
      "    val_log_likelihood: 295.0842681884766\n",
      "    val_log_marginal: 153.61030480638146\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -226.296631\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: 29.852219\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -22.166477\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -195.160187\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -218.703644\n",
      "    epoch          : 128\n",
      "    loss           : -158.84933280944824\n",
      "    val_loss       : -170.42285138424486\n",
      "    val_log_likelihood: 290.1972381591797\n",
      "    val_log_marginal: 178.76108944155277\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -256.230255\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -270.933228\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -219.810822\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -20.795753\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -197.507050\n",
      "    epoch          : 129\n",
      "    loss           : -191.65111658096313\n",
      "    val_loss       : -201.4524865735322\n",
      "    val_log_likelihood: 292.5761444091797\n",
      "    val_log_marginal: 208.59502470903098\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -212.669403\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -202.680405\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -306.706879\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -332.380127\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -211.112762\n",
      "    epoch          : 130\n",
      "    loss           : -217.94953594207763\n",
      "    val_loss       : -188.34354387121274\n",
      "    val_log_likelihood: 281.7645751953125\n",
      "    val_log_marginal: 194.94894826859704\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -219.977463\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -69.165672\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -231.062317\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -334.456482\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -108.883392\n",
      "    epoch          : 131\n",
      "    loss           : -212.43281925201416\n",
      "    val_loss       : -223.3579528497532\n",
      "    val_log_likelihood: 305.8395294189453\n",
      "    val_log_marginal: 231.43974680200054\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -144.860321\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -293.684021\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -294.904419\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -103.845398\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: 61.114670\n",
      "    epoch          : 132\n",
      "    loss           : -155.60174185752868\n",
      "    val_loss       : -140.29874658836053\n",
      "    val_log_likelihood: 283.21173706054685\n",
      "    val_log_marginal: 145.7335812512785\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -53.428284\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -236.767151\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -23.825821\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -138.406250\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -97.022804\n",
      "    epoch          : 133\n",
      "    loss           : -183.9011185836792\n",
      "    val_loss       : -208.70447348440067\n",
      "    val_log_likelihood: 301.2699340820312\n",
      "    val_log_marginal: 214.82078888928172\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -238.241196\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -123.224678\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -111.418922\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -301.364624\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -258.530457\n",
      "    epoch          : 134\n",
      "    loss           : -233.99296630859374\n",
      "    val_loss       : -225.00594557281585\n",
      "    val_log_likelihood: 307.0273956298828\n",
      "    val_log_marginal: 233.18618200869793\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -313.113129\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -254.745514\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -261.116821\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -202.786682\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -203.092346\n",
      "    epoch          : 135\n",
      "    loss           : -230.41088886260985\n",
      "    val_loss       : -221.08063852293418\n",
      "    val_log_likelihood: 312.1912368774414\n",
      "    val_log_marginal: 227.85641526840627\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -239.197021\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -249.159637\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -125.510849\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -256.345154\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -245.858475\n",
      "    epoch          : 136\n",
      "    loss           : -226.24765720367432\n",
      "    val_loss       : -208.54868526132776\n",
      "    val_log_likelihood: 304.71172790527345\n",
      "    val_log_marginal: 215.4828325811774\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -323.155151\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -97.510490\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -209.624756\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -228.225830\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -199.973541\n",
      "    epoch          : 137\n",
      "    loss           : -198.60433010101318\n",
      "    val_loss       : -178.64157434087247\n",
      "    val_log_likelihood: 297.7534576416016\n",
      "    val_log_marginal: 185.96246757470072\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -219.106995\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -182.201614\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -222.127106\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -244.092834\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -71.281708\n",
      "    epoch          : 138\n",
      "    loss           : -183.7262574672699\n",
      "    val_loss       : -167.46578200180085\n",
      "    val_log_likelihood: 306.46484527587893\n",
      "    val_log_marginal: 175.7662834312767\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -322.854065\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: 20.893192\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -55.344391\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -81.013290\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -156.845825\n",
      "    epoch          : 139\n",
      "    loss           : -114.7425838303566\n",
      "    val_loss       : -196.11274986369534\n",
      "    val_log_likelihood: 299.3548980712891\n",
      "    val_log_marginal: 204.07462551044867\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -216.287689\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -279.197876\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -142.344025\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -90.296783\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -232.949417\n",
      "    epoch          : 140\n",
      "    loss           : -209.6548979949951\n",
      "    val_loss       : -208.06154723502695\n",
      "    val_log_likelihood: 297.91048431396484\n",
      "    val_log_marginal: 219.69179378971458\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch140.pth ...\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -258.819885\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -233.546051\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -236.089737\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -230.208878\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -97.369301\n",
      "    epoch          : 141\n",
      "    loss           : -203.44503015518188\n",
      "    val_loss       : -214.13773287152873\n",
      "    val_log_likelihood: 310.10843353271486\n",
      "    val_log_marginal: 223.35312582701445\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -262.479858\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -283.897034\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -121.362732\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -282.487488\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -334.118134\n",
      "    epoch          : 142\n",
      "    loss           : -235.4451596069336\n",
      "    val_loss       : -177.26181426839904\n",
      "    val_log_likelihood: 307.11410064697264\n",
      "    val_log_marginal: 184.90331839807328\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -261.982147\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -19.358101\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -236.953156\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -39.152637\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -211.709366\n",
      "    epoch          : 143\n",
      "    loss           : -186.18671540260314\n",
      "    val_loss       : -167.22443400714548\n",
      "    val_log_likelihood: 303.033935546875\n",
      "    val_log_marginal: 171.78933309614658\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -211.603851\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -245.359573\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -253.938385\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -323.390381\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -253.125549\n",
      "    epoch          : 144\n",
      "    loss           : -214.49546371459962\n",
      "    val_loss       : -228.00052667101846\n",
      "    val_log_likelihood: 315.9420135498047\n",
      "    val_log_marginal: 237.12832461497064\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -203.346954\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -265.598999\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -278.271301\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -314.762054\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -250.238586\n",
      "    epoch          : 145\n",
      "    loss           : -235.435034866333\n",
      "    val_loss       : -205.9969984099269\n",
      "    val_log_likelihood: 317.5535949707031\n",
      "    val_log_marginal: 218.68586949631572\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -270.748962\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -253.603546\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -233.484634\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -259.767670\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -260.928436\n",
      "    epoch          : 146\n",
      "    loss           : -219.45534355163574\n",
      "    val_loss       : -228.98679741984233\n",
      "    val_log_likelihood: 311.4330230712891\n",
      "    val_log_marginal: 237.0899229945632\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -225.333496\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -261.443970\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -358.459503\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -108.804199\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -263.486328\n",
      "    epoch          : 147\n",
      "    loss           : -246.96761810302735\n",
      "    val_loss       : -229.71632036855445\n",
      "    val_log_likelihood: 320.4678924560547\n",
      "    val_log_marginal: 236.45784095264972\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -311.853394\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -289.525879\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -235.866135\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -153.403381\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -207.217728\n",
      "    epoch          : 148\n",
      "    loss           : -214.90217234134673\n",
      "    val_loss       : -189.8879217458889\n",
      "    val_log_likelihood: 313.77513732910154\n",
      "    val_log_marginal: 210.36208673770807\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: 57.974583\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -233.815674\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -266.620758\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -242.147476\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -147.837570\n",
      "    epoch          : 149\n",
      "    loss           : -213.8558306312561\n",
      "    val_loss       : -209.8166530514136\n",
      "    val_log_likelihood: 312.27508239746095\n",
      "    val_log_marginal: 215.9476994752884\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -303.776154\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -311.972351\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -240.721680\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -92.903580\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -94.112640\n",
      "    epoch          : 150\n",
      "    loss           : -178.84934040546418\n",
      "    val_loss       : -118.12168073803187\n",
      "    val_log_likelihood: 279.63950500488284\n",
      "    val_log_marginal: 123.77389011792839\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -153.994446\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -262.885895\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -236.616211\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -225.669739\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -145.449402\n",
      "    epoch          : 151\n",
      "    loss           : -197.3020136642456\n",
      "    val_loss       : -224.53678508233278\n",
      "    val_log_likelihood: 309.07033386230466\n",
      "    val_log_marginal: 229.5400026702364\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -357.077393\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -194.725708\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -213.043762\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -127.773239\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -231.956543\n",
      "    epoch          : 152\n",
      "    loss           : -217.08015190124513\n",
      "    val_loss       : -219.39376075295732\n",
      "    val_log_likelihood: 307.17512969970704\n",
      "    val_log_marginal: 227.18337406031787\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -255.348083\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -323.861633\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -227.266296\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -258.914459\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -226.467743\n",
      "    epoch          : 153\n",
      "    loss           : -233.8249683380127\n",
      "    val_loss       : -243.28521328996868\n",
      "    val_log_likelihood: 330.84378509521486\n",
      "    val_log_marginal: 251.65227936282753\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -274.916138\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -266.632111\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -357.208008\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -301.395630\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -220.696960\n",
      "    epoch          : 154\n",
      "    loss           : -240.0931484222412\n",
      "    val_loss       : -228.82440635226666\n",
      "    val_log_likelihood: 322.39679565429685\n",
      "    val_log_marginal: 233.75570855028928\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -246.517883\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -121.454582\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -243.885361\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -280.489655\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -211.895477\n",
      "    epoch          : 155\n",
      "    loss           : -252.1791700744629\n",
      "    val_loss       : -221.10675366185606\n",
      "    val_log_likelihood: 326.5669448852539\n",
      "    val_log_marginal: 227.07827101834118\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -264.727905\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -244.652130\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -95.369339\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -108.266945\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -207.286713\n",
      "    epoch          : 156\n",
      "    loss           : -129.87221803426743\n",
      "    val_loss       : -208.4030697831884\n",
      "    val_log_likelihood: 315.8454055786133\n",
      "    val_log_marginal: 218.2926525201739\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -232.926483\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -239.023895\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -236.814651\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -120.879051\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -122.455910\n",
      "    epoch          : 157\n",
      "    loss           : -227.78710463523865\n",
      "    val_loss       : -240.0346582410857\n",
      "    val_log_likelihood: 329.55833740234374\n",
      "    val_log_marginal: 246.92789031527937\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -323.649780\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -272.648651\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -302.025208\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -307.859894\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -305.704651\n",
      "    epoch          : 158\n",
      "    loss           : -257.46767333984377\n",
      "    val_loss       : -248.9872482445091\n",
      "    val_log_likelihood: 331.6077117919922\n",
      "    val_log_marginal: 253.15601205155252\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -276.577240\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -293.818848\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -342.413513\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -265.823090\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -230.698334\n",
      "    epoch          : 159\n",
      "    loss           : -230.32794567108155\n",
      "    val_loss       : -125.3749326813966\n",
      "    val_log_likelihood: 305.20237884521487\n",
      "    val_log_marginal: 135.21158909387887\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -152.445816\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -71.428368\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -117.973236\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -200.636536\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -123.094025\n",
      "    epoch          : 160\n",
      "    loss           : -122.26920161724091\n",
      "    val_loss       : -128.37518784468995\n",
      "    val_log_likelihood: 298.45535430908205\n",
      "    val_log_marginal: 147.29133146256208\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch160.pth ...\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -202.098846\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -226.546219\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -81.250923\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -152.194611\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -183.587296\n",
      "    epoch          : 161\n",
      "    loss           : -83.47723294258118\n",
      "    val_loss       : -213.4471010994166\n",
      "    val_log_likelihood: 309.0036163330078\n",
      "    val_log_marginal: 222.09196825214673\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -259.696533\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -267.642395\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -258.383789\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -240.023621\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -241.240845\n",
      "    epoch          : 162\n",
      "    loss           : -249.92352378845214\n",
      "    val_loss       : -249.8347718927078\n",
      "    val_log_likelihood: 330.31412811279296\n",
      "    val_log_marginal: 254.9287705950439\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -280.783203\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -124.705956\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -121.136139\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -282.505981\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -147.239731\n",
      "    epoch          : 163\n",
      "    loss           : -250.33200492858887\n",
      "    val_loss       : -213.28856167970224\n",
      "    val_log_likelihood: 324.5904022216797\n",
      "    val_log_marginal: 219.77592842280865\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -132.416107\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -320.408569\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -283.358215\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -279.458160\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -158.239380\n",
      "    epoch          : 164\n",
      "    loss           : -252.31537670135498\n",
      "    val_loss       : -261.1788664266467\n",
      "    val_log_likelihood: 337.3261093139648\n",
      "    val_log_marginal: 265.35538355089693\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -258.395844\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -379.132996\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -303.399292\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -382.996643\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -307.330231\n",
      "    epoch          : 165\n",
      "    loss           : -277.9948588562012\n",
      "    val_loss       : -263.92525719609114\n",
      "    val_log_likelihood: 344.6918090820312\n",
      "    val_log_marginal: 269.72936277166013\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -265.832825\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -291.486572\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -285.361023\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -219.205383\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -297.051392\n",
      "    epoch          : 166\n",
      "    loss           : -263.02746383666994\n",
      "    val_loss       : -239.81925721671433\n",
      "    val_log_likelihood: 340.1392196655273\n",
      "    val_log_marginal: 251.9817227497697\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -284.785553\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -287.467957\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -370.395782\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -255.766907\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -138.435089\n",
      "    epoch          : 167\n",
      "    loss           : -242.3977389907837\n",
      "    val_loss       : -216.9925320690498\n",
      "    val_log_likelihood: 331.76536254882814\n",
      "    val_log_marginal: 233.47062375769093\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -95.651909\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -330.116882\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -134.381561\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -355.860352\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -155.287170\n",
      "    epoch          : 168\n",
      "    loss           : -249.55507041931153\n",
      "    val_loss       : -252.97664394797758\n",
      "    val_log_likelihood: 345.64746856689453\n",
      "    val_log_marginal: 263.2545173574239\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -268.547058\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -280.459351\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -128.220001\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -229.200104\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -92.683075\n",
      "    epoch          : 169\n",
      "    loss           : -237.22442198753356\n",
      "    val_loss       : 23.208191466890284\n",
      "    val_log_likelihood: 304.48680877685547\n",
      "    val_log_marginal: -3.9484983175992965\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: 83.210884\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: 589.326538\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -115.254318\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -244.059479\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -245.479935\n",
      "    epoch          : 170\n",
      "    loss           : -117.77232170581817\n",
      "    val_loss       : -235.32839571246876\n",
      "    val_log_likelihood: 331.3863159179688\n",
      "    val_log_marginal: 242.3839541465044\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -288.875580\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -291.807312\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -220.829773\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -250.996735\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -133.988739\n",
      "    epoch          : 171\n",
      "    loss           : -255.7730741882324\n",
      "    val_loss       : -260.27748212302106\n",
      "    val_log_likelihood: 340.7773712158203\n",
      "    val_log_marginal: 265.98035822138246\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -121.336945\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -294.114929\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -294.535278\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -306.254028\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -306.343628\n",
      "    epoch          : 172\n",
      "    loss           : -278.8506753540039\n",
      "    val_loss       : -268.8259340408258\n",
      "    val_log_likelihood: 348.68079986572263\n",
      "    val_log_marginal: 275.0156904194504\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -395.949829\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -300.307953\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -286.755157\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -292.430267\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -272.803101\n",
      "    epoch          : 173\n",
      "    loss           : -272.76730880737307\n",
      "    val_loss       : -251.8001971779391\n",
      "    val_log_likelihood: 345.73732604980466\n",
      "    val_log_marginal: 257.4389177249619\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -278.497498\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -298.105499\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -262.741699\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -232.101730\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -130.563385\n",
      "    epoch          : 174\n",
      "    loss           : -215.9759325838089\n",
      "    val_loss       : -164.55914277052506\n",
      "    val_log_likelihood: 317.6409286499023\n",
      "    val_log_marginal: 181.74073021896183\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -219.871643\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -251.267059\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -105.716721\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: 28.704165\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -90.132538\n",
      "    epoch          : 175\n",
      "    loss           : -161.31018855571747\n",
      "    val_loss       : -206.01122629642487\n",
      "    val_log_likelihood: 314.6725158691406\n",
      "    val_log_marginal: 208.9477709982544\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -242.852707\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -251.172073\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -115.262070\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -156.749146\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -146.788269\n",
      "    epoch          : 176\n",
      "    loss           : -247.63353218078612\n",
      "    val_loss       : -231.56234401669354\n",
      "    val_log_likelihood: 334.0845062255859\n",
      "    val_log_marginal: 240.73769318088893\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -273.125641\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -377.581604\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -271.660706\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -296.364258\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -236.804565\n",
      "    epoch          : 177\n",
      "    loss           : -256.64597175598146\n",
      "    val_loss       : -231.03833271935582\n",
      "    val_log_likelihood: 341.75603942871095\n",
      "    val_log_marginal: 238.3052323166281\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -94.150787\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -334.063354\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -152.362518\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -311.589844\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -222.447815\n",
      "    epoch          : 178\n",
      "    loss           : -233.68606079101562\n",
      "    val_loss       : -197.83356583686546\n",
      "    val_log_likelihood: 327.3577163696289\n",
      "    val_log_marginal: 215.08676437065006\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -203.745941\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -70.261070\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -132.490738\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -163.815414\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -302.219299\n",
      "    epoch          : 179\n",
      "    loss           : -231.40003941059112\n",
      "    val_loss       : -249.89225706579163\n",
      "    val_log_likelihood: 342.756120300293\n",
      "    val_log_marginal: 260.01351272687316\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -378.499268\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -361.190063\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -261.640442\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -246.899704\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -141.467773\n",
      "    epoch          : 180\n",
      "    loss           : -265.37729049682616\n",
      "    val_loss       : -241.00014613922684\n",
      "    val_log_likelihood: 347.88904724121096\n",
      "    val_log_marginal: 250.67943548293528\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch180.pth ...\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -292.456848\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -121.596535\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -294.507874\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -287.703674\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -151.668045\n",
      "    epoch          : 181\n",
      "    loss           : -269.86016845703125\n",
      "    val_loss       : -214.99239340722562\n",
      "    val_log_likelihood: 350.3978958129883\n",
      "    val_log_marginal: 220.6517088573426\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -266.951508\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -311.930023\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -294.132965\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -271.638794\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -38.380722\n",
      "    epoch          : 182\n",
      "    loss           : -232.74256885528564\n",
      "    val_loss       : -147.15998224541545\n",
      "    val_log_likelihood: 334.9859649658203\n",
      "    val_log_marginal: 154.81473367363213\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -115.263031\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -260.331818\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -104.912552\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -365.486877\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -300.783264\n",
      "    epoch          : 183\n",
      "    loss           : -245.59744506835938\n",
      "    val_loss       : -228.3126013064757\n",
      "    val_log_likelihood: 342.7972946166992\n",
      "    val_log_marginal: 236.39452534615992\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -274.829041\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -323.156464\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -348.206421\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -340.974884\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -306.750366\n",
      "    epoch          : 184\n",
      "    loss           : -271.8407221221924\n",
      "    val_loss       : -260.28285036059094\n",
      "    val_log_likelihood: 351.0997619628906\n",
      "    val_log_marginal: 264.56798645146193\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -315.739868\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -293.397339\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -287.646271\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -307.843750\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -155.609619\n",
      "    epoch          : 185\n",
      "    loss           : -275.6882521057129\n",
      "    val_loss       : -247.57103592250496\n",
      "    val_log_likelihood: 348.69091644287107\n",
      "    val_log_marginal: 256.8081106878817\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -329.638367\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -311.915161\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -258.354858\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -191.583252\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: 253.699982\n",
      "    epoch          : 186\n",
      "    loss           : -129.80734169483185\n",
      "    val_loss       : -117.23506485959516\n",
      "    val_log_likelihood: 285.7624847412109\n",
      "    val_log_marginal: 131.19525965563952\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -149.398346\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -81.875000\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -359.026703\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -87.150223\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -258.476105\n",
      "    epoch          : 187\n",
      "    loss           : -216.27019231796265\n",
      "    val_loss       : -239.15347850350662\n",
      "    val_log_likelihood: 342.94847869873047\n",
      "    val_log_marginal: 245.99410938955845\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -266.492432\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -283.005951\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -250.122894\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -216.908112\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -235.381897\n",
      "    epoch          : 188\n",
      "    loss           : -219.73324462890625\n",
      "    val_loss       : -217.66318232845515\n",
      "    val_log_likelihood: 335.5112594604492\n",
      "    val_log_marginal: 229.5979896120727\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -88.118988\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -233.278519\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -304.871002\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -303.407867\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -303.387909\n",
      "    epoch          : 189\n",
      "    loss           : -257.0081746673584\n",
      "    val_loss       : -271.7045168128796\n",
      "    val_log_likelihood: 353.3028869628906\n",
      "    val_log_marginal: 275.46103940829636\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -259.435547\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -304.737854\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -318.582153\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -296.802155\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -320.788086\n",
      "    epoch          : 190\n",
      "    loss           : -287.3624041748047\n",
      "    val_loss       : -275.1005640656687\n",
      "    val_log_likelihood: 360.4428771972656\n",
      "    val_log_marginal: 283.73289902582763\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -155.994873\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -331.443787\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -326.260986\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -328.098724\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -315.763611\n",
      "    epoch          : 191\n",
      "    loss           : -296.47774520874026\n",
      "    val_loss       : -282.3375565253198\n",
      "    val_log_likelihood: 364.21100158691405\n",
      "    val_log_marginal: 288.28502522855996\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -311.949036\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -414.129456\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -361.296082\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -300.923706\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -166.998810\n",
      "    epoch          : 192\n",
      "    loss           : -297.41685844421386\n",
      "    val_loss       : -279.32825101725757\n",
      "    val_log_likelihood: 363.58397369384767\n",
      "    val_log_marginal: 283.8508035264171\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -352.831024\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -314.745758\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -295.005280\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -315.741180\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -287.421295\n",
      "    epoch          : 193\n",
      "    loss           : -288.064224319458\n",
      "    val_loss       : -260.42085535293444\n",
      "    val_log_likelihood: 357.56753997802736\n",
      "    val_log_marginal: 262.5721604639658\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -137.264618\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -389.433075\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -143.621948\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -111.592743\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -313.646790\n",
      "    epoch          : 194\n",
      "    loss           : -276.2944432067871\n",
      "    val_loss       : -266.77509510088714\n",
      "    val_log_likelihood: 362.3068878173828\n",
      "    val_log_marginal: 271.75911019183695\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -322.157959\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -377.912476\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -327.392059\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -354.911133\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -288.178162\n",
      "    epoch          : 195\n",
      "    loss           : -260.5599969100952\n",
      "    val_loss       : -238.8796156701632\n",
      "    val_log_likelihood: 360.1449142456055\n",
      "    val_log_marginal: 245.89449426718062\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -343.697784\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -172.645081\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -291.808533\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -128.512665\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -239.556427\n",
      "    epoch          : 196\n",
      "    loss           : -220.37810747146605\n",
      "    val_loss       : -253.91872110208496\n",
      "    val_log_likelihood: 354.16148071289064\n",
      "    val_log_marginal: 259.8655324176673\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -161.183319\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -135.320663\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -124.521271\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -137.776733\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -326.576569\n",
      "    epoch          : 197\n",
      "    loss           : -284.0915414428711\n",
      "    val_loss       : -281.0910972067155\n",
      "    val_log_likelihood: 365.07925872802736\n",
      "    val_log_marginal: 285.9617398235947\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -320.231628\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -315.510559\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -333.277679\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -312.253418\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -135.652618\n",
      "    epoch          : 198\n",
      "    loss           : -253.6319535446167\n",
      "    val_loss       : -197.60983820594848\n",
      "    val_log_likelihood: 333.96227569580077\n",
      "    val_log_marginal: 214.23721908330918\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -234.587982\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -61.577183\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -157.329803\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -46.644913\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -79.235664\n",
      "    epoch          : 199\n",
      "    loss           : -186.01259541511536\n",
      "    val_loss       : -241.6908768597059\n",
      "    val_log_likelihood: 357.7165191650391\n",
      "    val_log_marginal: 249.56512792669236\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -258.708099\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -285.087036\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -250.331268\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -306.946289\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -315.641083\n",
      "    epoch          : 200\n",
      "    loss           : -276.74404350280764\n",
      "    val_loss       : -274.09967908253896\n",
      "    val_log_likelihood: 364.49390563964846\n",
      "    val_log_marginal: 278.9983781270683\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -393.456848\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -138.312668\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -313.100220\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -151.442154\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -293.764954\n",
      "    epoch          : 201\n",
      "    loss           : -293.0010018157959\n",
      "    val_loss       : -278.1584269073792\n",
      "    val_log_likelihood: 369.3760192871094\n",
      "    val_log_marginal: 286.9576833665371\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -317.716980\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -331.911499\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -125.393997\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -346.299683\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -146.773041\n",
      "    epoch          : 202\n",
      "    loss           : -287.0993807220459\n",
      "    val_loss       : -215.9093136759475\n",
      "    val_log_likelihood: 359.7232360839844\n",
      "    val_log_marginal: 228.84522609710694\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -264.236572\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -289.745819\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -278.835083\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -61.994034\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -281.646637\n",
      "    epoch          : 203\n",
      "    loss           : -239.7020955657959\n",
      "    val_loss       : -245.0952002543956\n",
      "    val_log_likelihood: 362.2281723022461\n",
      "    val_log_marginal: 251.56410527490078\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -309.072540\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -112.719971\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -266.165283\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -199.063614\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: 181.619446\n",
      "    epoch          : 204\n",
      "    loss           : -140.82898948669433\n",
      "    val_loss       : -142.00496630901472\n",
      "    val_log_likelihood: 325.6551742553711\n",
      "    val_log_marginal: 153.0121566079557\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -259.144989\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -150.690338\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -243.342651\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: 99.338402\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -264.582733\n",
      "    epoch          : 205\n",
      "    loss           : -148.19671698570252\n",
      "    val_loss       : -184.3266483644955\n",
      "    val_log_likelihood: 331.8275177001953\n",
      "    val_log_marginal: 204.28351332731546\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -206.327209\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -267.590240\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -284.702576\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -240.138779\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -163.370697\n",
      "    epoch          : 206\n",
      "    loss           : -252.13631038665773\n",
      "    val_loss       : -254.6291926195845\n",
      "    val_log_likelihood: 356.35399627685547\n",
      "    val_log_marginal: 257.90634021274747\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -348.904022\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -312.429993\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -325.297546\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -173.382843\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -152.416199\n",
      "    epoch          : 207\n",
      "    loss           : -296.1433320617676\n",
      "    val_loss       : -280.3728229400702\n",
      "    val_log_likelihood: 365.872785949707\n",
      "    val_log_marginal: 284.97330072522163\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -298.917175\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -190.139740\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -177.554367\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -281.948425\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -159.146469\n",
      "    epoch          : 208\n",
      "    loss           : -287.04767890930174\n",
      "    val_loss       : -252.98716965354978\n",
      "    val_log_likelihood: 357.92956695556643\n",
      "    val_log_marginal: 260.2440856244415\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -239.514465\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -287.672363\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -252.720001\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -321.611511\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -257.792175\n",
      "    epoch          : 209\n",
      "    loss           : -269.43927391052245\n",
      "    val_loss       : -277.06504600662737\n",
      "    val_log_likelihood: 368.9100051879883\n",
      "    val_log_marginal: 282.05266643166544\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -153.324661\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -317.662292\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -168.533569\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -313.279633\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -193.097687\n",
      "    epoch          : 210\n",
      "    loss           : -244.4122359085083\n",
      "    val_loss       : -237.21491797976196\n",
      "    val_log_likelihood: 357.0261520385742\n",
      "    val_log_marginal: 245.31929878331766\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -238.610245\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -262.413330\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -135.562958\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -305.492889\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -316.359833\n",
      "    epoch          : 211\n",
      "    loss           : -278.2749104309082\n",
      "    val_loss       : -287.75053741410375\n",
      "    val_log_likelihood: 371.1373947143555\n",
      "    val_log_marginal: 293.77479201407004\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -366.887573\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -330.349396\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -420.531799\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -342.529358\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -337.014130\n",
      "    epoch          : 212\n",
      "    loss           : -307.41016708374025\n",
      "    val_loss       : -291.55028339652347\n",
      "    val_log_likelihood: 377.5424835205078\n",
      "    val_log_marginal: 298.0735326819122\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -336.254669\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -156.009796\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -430.221069\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -345.664093\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -290.490967\n",
      "    epoch          : 213\n",
      "    loss           : -310.8772294616699\n",
      "    val_loss       : -291.59494740767406\n",
      "    val_log_likelihood: 374.4277038574219\n",
      "    val_log_marginal: 294.5697062876076\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -342.821594\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -165.704437\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -321.410950\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -318.718536\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -253.586884\n",
      "    epoch          : 214\n",
      "    loss           : -298.36504470825196\n",
      "    val_loss       : -267.81008196622133\n",
      "    val_log_likelihood: 372.8372512817383\n",
      "    val_log_marginal: 275.74780532121656\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -330.925903\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -136.068359\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -267.768127\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -307.754578\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -272.363098\n",
      "    epoch          : 215\n",
      "    loss           : -289.46431854248044\n",
      "    val_loss       : -274.3716948285699\n",
      "    val_log_likelihood: 373.8666168212891\n",
      "    val_log_marginal: 279.78573259525\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -307.527527\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -191.252808\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -307.350281\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -320.266846\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -298.723236\n",
      "    epoch          : 216\n",
      "    loss           : -293.74555549621584\n",
      "    val_loss       : -248.36859970502556\n",
      "    val_log_likelihood: 377.24823303222655\n",
      "    val_log_marginal: 255.8938755106181\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -356.111694\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -248.517822\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -27.949003\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -276.556000\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -250.046036\n",
      "    epoch          : 217\n",
      "    loss           : -238.26591467380524\n",
      "    val_loss       : -244.8683108064346\n",
      "    val_log_likelihood: 364.5384521484375\n",
      "    val_log_marginal: 249.6217346545156\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -271.523987\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -108.342667\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -386.402466\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -246.955597\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -297.826416\n",
      "    epoch          : 218\n",
      "    loss           : -259.4838502883911\n",
      "    val_loss       : -243.59447812801227\n",
      "    val_log_likelihood: 362.2603790283203\n",
      "    val_log_marginal: 248.40215396471322\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -322.839966\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -170.490646\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -340.138245\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -132.539886\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -148.369919\n",
      "    epoch          : 219\n",
      "    loss           : -276.6531907653809\n",
      "    val_loss       : -279.14627112299206\n",
      "    val_log_likelihood: 372.27693939208984\n",
      "    val_log_marginal: 285.22033361159265\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -367.802063\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -276.100494\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -305.418671\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -137.630020\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -207.251114\n",
      "    epoch          : 220\n",
      "    loss           : -259.97850587844846\n",
      "    val_loss       : -151.56710131280124\n",
      "    val_log_likelihood: 290.87221279144285\n",
      "    val_log_marginal: 169.1859380088747\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch220.pth ...\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -222.578339\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -232.239594\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -277.924255\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -310.043640\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -316.980103\n",
      "    epoch          : 221\n",
      "    loss           : -249.8912180042267\n",
      "    val_loss       : -280.581805799529\n",
      "    val_log_likelihood: 379.1167053222656\n",
      "    val_log_marginal: 288.92092926147404\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -337.548553\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -174.752045\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -311.403412\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -431.516266\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -292.127747\n",
      "    epoch          : 222\n",
      "    loss           : -310.35740615844725\n",
      "    val_loss       : -289.1413422903046\n",
      "    val_log_likelihood: 381.84424285888673\n",
      "    val_log_marginal: 298.0342620297877\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -334.787292\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -361.538452\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -313.625244\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -274.681458\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -133.946228\n",
      "    epoch          : 223\n",
      "    loss           : -262.41890043258667\n",
      "    val_loss       : -156.4513414765708\n",
      "    val_log_likelihood: 354.13892974853513\n",
      "    val_log_marginal: 171.02584339964832\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -106.268127\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -305.181519\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -277.927155\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -310.949341\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -246.746933\n",
      "    epoch          : 224\n",
      "    loss           : -231.40185990333558\n",
      "    val_loss       : -264.99752586549147\n",
      "    val_log_likelihood: 367.7965057373047\n",
      "    val_log_marginal: 271.8040994688869\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -326.718170\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -315.866577\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -172.104218\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -306.004150\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -185.090149\n",
      "    epoch          : 225\n",
      "    loss           : -271.2868198013306\n",
      "    val_loss       : -216.86470189644024\n",
      "    val_log_likelihood: 374.5860061645508\n",
      "    val_log_marginal: 222.54481514319778\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -355.440979\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -133.420197\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -289.291412\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -339.631378\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -291.477600\n",
      "    epoch          : 226\n",
      "    loss           : -200.5558431148529\n",
      "    val_loss       : -272.68750832546505\n",
      "    val_log_likelihood: 372.5779327392578\n",
      "    val_log_marginal: 283.73327307999136\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -363.966309\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -312.661804\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -136.310165\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -325.971649\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -153.704926\n",
      "    epoch          : 227\n",
      "    loss           : -289.2304396820068\n",
      "    val_loss       : -272.6076472437009\n",
      "    val_log_likelihood: 373.1647232055664\n",
      "    val_log_marginal: 279.55015894509853\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -335.293793\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -326.315369\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -285.638763\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -288.244934\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -338.145081\n",
      "    epoch          : 228\n",
      "    loss           : -303.92845611572267\n",
      "    val_loss       : -288.1791050201282\n",
      "    val_log_likelihood: 380.72839508056643\n",
      "    val_log_marginal: 292.93696604885497\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -148.407928\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -357.353943\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -332.464752\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -175.298508\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -291.142639\n",
      "    epoch          : 229\n",
      "    loss           : -302.4265933990479\n",
      "    val_loss       : -288.17265648003666\n",
      "    val_log_likelihood: 383.68334808349607\n",
      "    val_log_marginal: 294.51275045387445\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -320.631897\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -273.895172\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -337.480347\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -364.924255\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -159.014099\n",
      "    epoch          : 230\n",
      "    loss           : -297.6248760986328\n",
      "    val_loss       : -264.4947671984322\n",
      "    val_log_likelihood: 374.6630142211914\n",
      "    val_log_marginal: 268.584074145928\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -368.804626\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -258.489594\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -287.129181\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -284.839050\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -281.449677\n",
      "    epoch          : 231\n",
      "    loss           : -249.9898136806488\n",
      "    val_loss       : -197.28589878110216\n",
      "    val_log_likelihood: 365.75834197998046\n",
      "    val_log_marginal: 213.53305877670647\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -243.517273\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -103.996033\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -313.139404\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -326.765625\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -266.105774\n",
      "    epoch          : 232\n",
      "    loss           : -213.9727197790146\n",
      "    val_loss       : -251.55398924052716\n",
      "    val_log_likelihood: 366.3301742553711\n",
      "    val_log_marginal: 255.87654277177845\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -295.283447\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -38.018810\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -421.625275\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -282.175598\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -176.153809\n",
      "    epoch          : 233\n",
      "    loss           : -284.64951610565186\n",
      "    val_loss       : -292.01108924439177\n",
      "    val_log_likelihood: 382.65733337402344\n",
      "    val_log_marginal: 297.0090660978109\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -322.605072\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -333.093628\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -325.790039\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -186.708328\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -165.448685\n",
      "    epoch          : 234\n",
      "    loss           : -304.84103240966795\n",
      "    val_loss       : -283.70266287829725\n",
      "    val_log_likelihood: 385.3991424560547\n",
      "    val_log_marginal: 291.74282825104893\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -313.495117\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -288.881653\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -371.368561\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -233.319916\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -254.149963\n",
      "    epoch          : 235\n",
      "    loss           : -267.1595382499695\n",
      "    val_loss       : -183.44661248484627\n",
      "    val_log_likelihood: 365.72599029541016\n",
      "    val_log_marginal: 193.927799083665\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -109.827469\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -269.064636\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -201.754974\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -221.946411\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -160.750122\n",
      "    epoch          : 236\n",
      "    loss           : -164.56873777866363\n",
      "    val_loss       : -220.38877034811304\n",
      "    val_log_likelihood: 362.19910888671876\n",
      "    val_log_marginal: 232.00914872927896\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -313.348267\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -54.453461\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -290.510071\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -179.795959\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -314.620911\n",
      "    epoch          : 237\n",
      "    loss           : -283.79913291931155\n",
      "    val_loss       : -284.3326357328333\n",
      "    val_log_likelihood: 379.0733673095703\n",
      "    val_log_marginal: 289.49394283182914\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -403.479065\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -164.297058\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -167.159698\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -375.803040\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -203.119843\n",
      "    epoch          : 238\n",
      "    loss           : -310.5369413757324\n",
      "    val_loss       : -301.63121170233933\n",
      "    val_log_likelihood: 387.15576934814453\n",
      "    val_log_marginal: 306.4602564800531\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -391.814148\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -344.641846\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -338.241943\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -157.955475\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -306.816833\n",
      "    epoch          : 239\n",
      "    loss           : -319.14306884765625\n",
      "    val_loss       : -300.2109099457972\n",
      "    val_log_likelihood: 387.5330612182617\n",
      "    val_log_marginal: 305.6263755397982\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -340.439392\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -346.751007\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -158.156158\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -358.698120\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -339.681335\n",
      "    epoch          : 240\n",
      "    loss           : -319.6155778503418\n",
      "    val_loss       : -308.11907820198684\n",
      "    val_log_likelihood: 390.6332717895508\n",
      "    val_log_marginal: 310.0089517489076\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch240.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -361.960266\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -301.395905\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -295.857117\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -302.119324\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -180.364944\n",
      "    epoch          : 241\n",
      "    loss           : -310.81771560668943\n",
      "    val_loss       : -252.4886060926132\n",
      "    val_log_likelihood: 379.6092987060547\n",
      "    val_log_marginal: 258.3192124761641\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -245.823105\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -344.646912\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -321.824463\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -151.687714\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -329.681885\n",
      "    epoch          : 242\n",
      "    loss           : -293.90313735961917\n",
      "    val_loss       : -274.6487479040399\n",
      "    val_log_likelihood: 386.1029541015625\n",
      "    val_log_marginal: 280.11663440801203\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -311.271851\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -341.530884\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -255.783539\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -339.960876\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -316.862610\n",
      "    epoch          : 243\n",
      "    loss           : -288.9201099395752\n",
      "    val_loss       : -263.07896915432065\n",
      "    val_log_likelihood: 380.16272735595703\n",
      "    val_log_marginal: 267.34803041480484\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -119.442741\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -288.291687\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -334.547516\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -305.567932\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -321.451721\n",
      "    epoch          : 244\n",
      "    loss           : -282.518330078125\n",
      "    val_loss       : -291.8352003880776\n",
      "    val_log_likelihood: 390.0744003295898\n",
      "    val_log_marginal: 302.07216101512313\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -374.120209\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -136.643127\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -340.747498\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -359.444458\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -319.215454\n",
      "    epoch          : 245\n",
      "    loss           : -290.90505516052247\n",
      "    val_loss       : -217.27509189965204\n",
      "    val_log_likelihood: 363.2887237548828\n",
      "    val_log_marginal: 223.59100254029036\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -94.592499\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -138.096848\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -328.031830\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -146.748871\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -103.634277\n",
      "    epoch          : 246\n",
      "    loss           : -207.8922656440735\n",
      "    val_loss       : -197.0343973898329\n",
      "    val_log_likelihood: 372.88915863037107\n",
      "    val_log_marginal: 210.62221672534943\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -226.463501\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -237.784454\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -315.937805\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -350.538361\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -258.267517\n",
      "    epoch          : 247\n",
      "    loss           : -251.63718288421632\n",
      "    val_loss       : -280.1336957996711\n",
      "    val_log_likelihood: 380.7087875366211\n",
      "    val_log_marginal: 288.1762251686305\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -334.259399\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -326.780334\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -304.133392\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -443.793304\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -336.775360\n",
      "    epoch          : 248\n",
      "    loss           : -308.97233871459963\n",
      "    val_loss       : -295.0526040424593\n",
      "    val_log_likelihood: 389.7002670288086\n",
      "    val_log_marginal: 303.048401169762\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -184.854599\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -304.199463\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -302.205841\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -332.401550\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -343.153473\n",
      "    epoch          : 249\n",
      "    loss           : -321.0530244445801\n",
      "    val_loss       : -302.835155711323\n",
      "    val_log_likelihood: 392.9425216674805\n",
      "    val_log_marginal: 309.292274306342\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -350.996521\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -330.545929\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -345.504944\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -279.894836\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -273.579254\n",
      "    epoch          : 250\n",
      "    loss           : -290.69370918273927\n",
      "    val_loss       : -274.92983880536633\n",
      "    val_log_likelihood: 383.5011520385742\n",
      "    val_log_marginal: 281.1736217770725\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -355.706543\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -241.303375\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -245.921341\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -249.761078\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -290.542877\n",
      "    epoch          : 251\n",
      "    loss           : -237.2046521091461\n",
      "    val_loss       : -215.44075988633557\n",
      "    val_log_likelihood: 372.35719909667966\n",
      "    val_log_marginal: 232.48227151967586\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -256.593201\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -305.301819\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -375.297546\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -354.493652\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -158.128723\n",
      "    epoch          : 252\n",
      "    loss           : -300.2904507827759\n",
      "    val_loss       : -298.70128277009354\n",
      "    val_log_likelihood: 392.2735931396484\n",
      "    val_log_marginal: 304.80653581864124\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -182.137054\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -156.297592\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -390.458038\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -366.904999\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -302.731628\n",
      "    epoch          : 253\n",
      "    loss           : -321.194376373291\n",
      "    val_loss       : -311.2090968504548\n",
      "    val_log_likelihood: 396.16272430419923\n",
      "    val_log_marginal: 315.89693613387647\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -209.343201\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -369.152466\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -340.450745\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -350.318115\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -342.669312\n",
      "    epoch          : 254\n",
      "    loss           : -314.21847885131837\n",
      "    val_loss       : -237.438994257804\n",
      "    val_log_likelihood: 389.626481628418\n",
      "    val_log_marginal: 247.83820831785928\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -259.695374\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -306.050110\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -350.873169\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -333.163208\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -297.254150\n",
      "    epoch          : 255\n",
      "    loss           : -269.9594500732422\n",
      "    val_loss       : -221.04856546111404\n",
      "    val_log_likelihood: 378.35260314941405\n",
      "    val_log_marginal: 236.5366146807036\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -49.780411\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -238.874588\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: 116.128441\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -269.679779\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -78.425461\n",
      "    epoch          : 256\n",
      "    loss           : -158.07367958545686\n",
      "    val_loss       : -141.37092181323095\n",
      "    val_log_likelihood: 365.3154098510742\n",
      "    val_log_marginal: 154.13084297217426\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -205.113937\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -294.277344\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -137.249100\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -247.510147\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -160.871735\n",
      "    epoch          : 257\n",
      "    loss           : -271.40726375579834\n",
      "    val_loss       : -248.23205326227472\n",
      "    val_log_likelihood: 380.3896774291992\n",
      "    val_log_marginal: 256.1945438977856\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -269.242645\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -365.076172\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -297.938416\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -189.074402\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -124.075012\n",
      "    epoch          : 258\n",
      "    loss           : -270.0892770385742\n",
      "    val_loss       : -207.8254086812958\n",
      "    val_log_likelihood: 355.152326965332\n",
      "    val_log_marginal: 214.62184225466544\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -276.684265\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -343.518738\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -364.113464\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -338.322388\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -305.568909\n",
      "    epoch          : 259\n",
      "    loss           : -303.01255264282224\n",
      "    val_loss       : -306.60744676673784\n",
      "    val_log_likelihood: 396.3824966430664\n",
      "    val_log_marginal: 310.9571195867428\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -190.807327\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -348.923492\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -172.944138\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -344.130249\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -299.912537\n",
      "    epoch          : 260\n",
      "    loss           : -330.68720626831055\n",
      "    val_loss       : -310.8525156415068\n",
      "    val_log_likelihood: 396.20909271240237\n",
      "    val_log_marginal: 316.45652995742887\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch260.pth ...\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -452.114746\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -307.768524\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -310.026245\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -204.468124\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -204.110275\n",
      "    epoch          : 261\n",
      "    loss           : -333.5157698059082\n",
      "    val_loss       : -315.72119864095004\n",
      "    val_log_likelihood: 401.56595611572266\n",
      "    val_log_marginal: 321.8759459290653\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -217.494568\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -338.129456\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -296.168030\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -189.778931\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -345.701416\n",
      "    epoch          : 262\n",
      "    loss           : -318.1232733154297\n",
      "    val_loss       : -274.8568008005619\n",
      "    val_log_likelihood: 392.80064086914064\n",
      "    val_log_marginal: 280.68965283297\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -426.169556\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -407.020081\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -331.301453\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -340.437836\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -189.415970\n",
      "    epoch          : 263\n",
      "    loss           : -296.1586327362061\n",
      "    val_loss       : -303.4233802543022\n",
      "    val_log_likelihood: 400.0674575805664\n",
      "    val_log_marginal: 309.2146826360375\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -448.356232\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -346.877258\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -349.366699\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -429.112396\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -445.371490\n",
      "    epoch          : 264\n",
      "    loss           : -318.9576821136475\n",
      "    val_loss       : -293.8961880110204\n",
      "    val_log_likelihood: 400.322802734375\n",
      "    val_log_marginal: 300.1108123715967\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -364.141449\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -363.095032\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -97.027206\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -397.353271\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -297.920135\n",
      "    epoch          : 265\n",
      "    loss           : -292.01510025024413\n",
      "    val_loss       : -285.77728685019537\n",
      "    val_log_likelihood: 393.8150390625\n",
      "    val_log_marginal: 295.8742275368422\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -163.565277\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -300.417206\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -339.364502\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -425.582581\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -252.978348\n",
      "    epoch          : 266\n",
      "    loss           : -290.4672268676758\n",
      "    val_loss       : -289.6030280640349\n",
      "    val_log_likelihood: 397.7020782470703\n",
      "    val_log_marginal: 299.3537272125483\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -326.092560\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -113.564110\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -370.551117\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -278.752869\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -274.188477\n",
      "    epoch          : 267\n",
      "    loss           : -298.76929138183596\n",
      "    val_loss       : -219.54684560801834\n",
      "    val_log_likelihood: 395.12684326171876\n",
      "    val_log_marginal: 232.11142263524235\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -261.290375\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -224.499420\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: 171.550430\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -238.456039\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -329.152649\n",
      "    epoch          : 268\n",
      "    loss           : -194.63763182640076\n",
      "    val_loss       : -288.0705647020601\n",
      "    val_log_likelihood: 391.907177734375\n",
      "    val_log_marginal: 302.4582515625866\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -336.577576\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -326.078918\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -300.072479\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -351.094757\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -381.175171\n",
      "    epoch          : 269\n",
      "    loss           : -323.53289039611815\n",
      "    val_loss       : -304.964170481544\n",
      "    val_log_likelihood: 399.97050323486326\n",
      "    val_log_marginal: 313.1729141931981\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -151.653870\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -396.589722\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -408.494446\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -372.174500\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -362.926880\n",
      "    epoch          : 270\n",
      "    loss           : -334.7208612060547\n",
      "    val_loss       : -321.62272875728087\n",
      "    val_log_likelihood: 405.40196838378904\n",
      "    val_log_marginal: 326.36577625255507\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -340.891449\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -469.792603\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -469.176788\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -203.186707\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -225.275238\n",
      "    epoch          : 271\n",
      "    loss           : -344.1575746154785\n",
      "    val_loss       : -324.6075708137825\n",
      "    val_log_likelihood: 409.1538116455078\n",
      "    val_log_marginal: 328.9240870572627\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -332.386963\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -323.023193\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -349.200958\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -357.257599\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -202.947052\n",
      "    epoch          : 272\n",
      "    loss           : -338.3663493347168\n",
      "    val_loss       : -310.6607704071328\n",
      "    val_log_likelihood: 406.4489318847656\n",
      "    val_log_marginal: 317.1122093845159\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -345.239441\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -181.473724\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -332.683807\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -316.876343\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -114.657860\n",
      "    epoch          : 273\n",
      "    loss           : -283.92834148406985\n",
      "    val_loss       : -183.27923645181582\n",
      "    val_log_likelihood: 375.5966430664063\n",
      "    val_log_marginal: 202.5251772198826\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -210.178452\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -334.958344\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -283.119751\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -152.871124\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -316.197571\n",
      "    epoch          : 274\n",
      "    loss           : -205.13047284126282\n",
      "    val_loss       : -207.3003361504525\n",
      "    val_log_likelihood: 382.0268783569336\n",
      "    val_log_marginal: 225.39164190192614\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -248.036896\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -294.785736\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -126.248283\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -188.596924\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -326.989624\n",
      "    epoch          : 275\n",
      "    loss           : -229.23640900611878\n",
      "    val_loss       : -284.4319180017337\n",
      "    val_log_likelihood: 387.09131774902346\n",
      "    val_log_marginal: 292.807055317238\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -173.901978\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -390.263672\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -307.895355\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -178.910858\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -259.151367\n",
      "    epoch          : 276\n",
      "    loss           : -282.38274209976197\n",
      "    val_loss       : -225.5925929836929\n",
      "    val_log_likelihood: 363.9667404174805\n",
      "    val_log_marginal: 234.40023360587656\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -265.427490\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -140.863510\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -230.482819\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -284.058533\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -334.779846\n",
      "    epoch          : 277\n",
      "    loss           : -245.01065570831298\n",
      "    val_loss       : -231.94001343222334\n",
      "    val_log_likelihood: 366.5846206665039\n",
      "    val_log_marginal: 239.4615063790232\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -230.458130\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -174.660553\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -162.633163\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -360.737244\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -323.182312\n",
      "    epoch          : 278\n",
      "    loss           : -306.39011001586914\n",
      "    val_loss       : -312.2896079868078\n",
      "    val_log_likelihood: 403.16529083251953\n",
      "    val_log_marginal: 320.15725790262223\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -372.349365\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -188.552628\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -369.257080\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -370.441742\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -375.492249\n",
      "    epoch          : 279\n",
      "    loss           : -342.04467483520506\n",
      "    val_loss       : -322.48166371481494\n",
      "    val_log_likelihood: 407.35106353759767\n",
      "    val_log_marginal: 327.96105029061437\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -411.082886\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -360.927368\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -201.250702\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -388.273621\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -362.939880\n",
      "    epoch          : 280\n",
      "    loss           : -341.2022006225586\n",
      "    val_loss       : -308.7110962868668\n",
      "    val_log_likelihood: 405.46336364746094\n",
      "    val_log_marginal: 312.23342281505467\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch280.pth ...\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -373.187866\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -357.448425\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -365.308716\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -337.586914\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -311.772247\n",
      "    epoch          : 281\n",
      "    loss           : -311.84860961914063\n",
      "    val_loss       : -293.0791114983149\n",
      "    val_log_likelihood: 401.7110305786133\n",
      "    val_log_marginal: 297.6858552195132\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -339.929565\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -359.980530\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -54.324337\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: 93.970032\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -94.512604\n",
      "    epoch          : 282\n",
      "    loss           : -169.189751329422\n",
      "    val_loss       : -211.75020791823044\n",
      "    val_log_likelihood: 362.81328125\n",
      "    val_log_marginal: 231.08336940072476\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -296.609680\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -252.896362\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -264.943207\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -39.559635\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -204.198303\n",
      "    epoch          : 283\n",
      "    loss           : -217.724360370636\n",
      "    val_loss       : -246.31659268662332\n",
      "    val_log_likelihood: 391.55830078125\n",
      "    val_log_marginal: 264.18761693127453\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -265.244934\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -433.411011\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -170.172394\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -357.292664\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -373.502228\n",
      "    epoch          : 284\n",
      "    loss           : -309.8866567993164\n",
      "    val_loss       : -311.3335209462792\n",
      "    val_log_likelihood: 404.57901153564455\n",
      "    val_log_marginal: 315.9765781085938\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -357.037994\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -372.704529\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -346.352417\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -364.528534\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -330.359192\n",
      "    epoch          : 285\n",
      "    loss           : -337.1112515258789\n",
      "    val_loss       : -321.70280463173987\n",
      "    val_log_likelihood: 407.66529693603513\n",
      "    val_log_marginal: 327.299387139827\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -364.534607\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -211.290787\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -404.200317\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -198.367493\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -203.210541\n",
      "    epoch          : 286\n",
      "    loss           : -342.3455937194824\n",
      "    val_loss       : -320.14765169303865\n",
      "    val_log_likelihood: 408.150325012207\n",
      "    val_log_marginal: 325.77326225743917\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -397.445526\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -458.135254\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -351.430634\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -193.287964\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -367.749054\n",
      "    epoch          : 287\n",
      "    loss           : -343.43795501708985\n",
      "    val_loss       : -325.30527467066423\n",
      "    val_log_likelihood: 413.35299530029295\n",
      "    val_log_marginal: 329.9219716418934\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -229.496216\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -212.851440\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -468.643036\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -323.787231\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -352.471222\n",
      "    epoch          : 288\n",
      "    loss           : -336.4379063415527\n",
      "    val_loss       : -307.4162173310295\n",
      "    val_log_likelihood: 407.10997772216797\n",
      "    val_log_marginal: 311.3014630336445\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -355.366394\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -361.505463\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -333.178589\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -119.449341\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -385.996033\n",
      "    epoch          : 289\n",
      "    loss           : -320.38870986938474\n",
      "    val_loss       : -318.8430197838694\n",
      "    val_log_likelihood: 411.69358825683594\n",
      "    val_log_marginal: 323.8634974371642\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -370.164917\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -453.260101\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -455.307220\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -202.348450\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -265.864075\n",
      "    epoch          : 290\n",
      "    loss           : -300.8718046569824\n",
      "    val_loss       : -281.75655403565617\n",
      "    val_log_likelihood: 404.14779510498045\n",
      "    val_log_marginal: 289.95060251019896\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -354.111389\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -211.102570\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -75.050186\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -345.827271\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -268.869873\n",
      "    epoch          : 291\n",
      "    loss           : -212.44349565505982\n",
      "    val_loss       : -247.35376457571982\n",
      "    val_log_likelihood: 397.67790069580076\n",
      "    val_log_marginal: 255.30720789022743\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -353.198883\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -351.485840\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -278.326050\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -306.398132\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -356.991913\n",
      "    epoch          : 292\n",
      "    loss           : -296.42049682617187\n",
      "    val_loss       : -288.1582722634077\n",
      "    val_log_likelihood: 404.5361083984375\n",
      "    val_log_marginal: 297.46504829488697\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -446.759125\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -342.686768\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -432.077606\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -170.510956\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -371.838440\n",
      "    epoch          : 293\n",
      "    loss           : -321.5388412475586\n",
      "    val_loss       : -315.61564947972073\n",
      "    val_log_likelihood: 407.48290252685547\n",
      "    val_log_marginal: 320.4072198961042\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -405.763336\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -405.105835\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -364.491577\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -210.561127\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -380.621307\n",
      "    epoch          : 294\n",
      "    loss           : -335.9178059387207\n",
      "    val_loss       : -306.43683760249985\n",
      "    val_log_likelihood: 407.3448852539062\n",
      "    val_log_marginal: 311.532977251336\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -201.988022\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -355.491760\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -383.400146\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -436.953674\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -318.808044\n",
      "    epoch          : 295\n",
      "    loss           : -309.8197281646728\n",
      "    val_loss       : -297.33442033343016\n",
      "    val_log_likelihood: 400.59637451171875\n",
      "    val_log_marginal: 300.76605421938\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -201.812622\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -467.591003\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -211.657532\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -357.450470\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -372.878540\n",
      "    epoch          : 296\n",
      "    loss           : -337.7522853088379\n",
      "    val_loss       : -327.2833110138774\n",
      "    val_log_likelihood: 414.9280517578125\n",
      "    val_log_marginal: 333.18310285025274\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -411.938965\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -383.687714\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -319.811249\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -360.272675\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -370.707764\n",
      "    epoch          : 297\n",
      "    loss           : -340.40594650268554\n",
      "    val_loss       : -303.5301840969361\n",
      "    val_log_likelihood: 411.06767578125\n",
      "    val_log_marginal: 312.2715667497367\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -359.892273\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -342.894409\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -420.444183\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -359.793762\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -135.904404\n",
      "    epoch          : 298\n",
      "    loss           : -311.7161777496338\n",
      "    val_loss       : -279.8849405472167\n",
      "    val_log_likelihood: 403.10887298583987\n",
      "    val_log_marginal: 285.8743138187716\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -308.593384\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -115.630600\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -221.118240\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -304.245880\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -164.722733\n",
      "    epoch          : 299\n",
      "    loss           : -281.9186193084717\n",
      "    val_loss       : -289.5824410436675\n",
      "    val_log_likelihood: 404.3946792602539\n",
      "    val_log_marginal: 300.78659065105023\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -297.869904\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -347.064819\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -221.741013\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -248.744690\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -116.492538\n",
      "    epoch          : 300\n",
      "    loss           : -204.27703819274902\n",
      "    val_loss       : -215.7576292362064\n",
      "    val_log_likelihood: 374.73329162597656\n",
      "    val_log_marginal: 223.36928403675557\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -351.152802\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -326.583130\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -169.640411\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -318.043121\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -331.872070\n",
      "    epoch          : 301\n",
      "    loss           : -295.21654945373535\n",
      "    val_loss       : -304.36432014172897\n",
      "    val_log_likelihood: 405.7699569702148\n",
      "    val_log_marginal: 307.4793566029519\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -210.448242\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -362.347412\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -353.365173\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -355.147217\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -176.334229\n",
      "    epoch          : 302\n",
      "    loss           : -332.07114608764647\n",
      "    val_loss       : -312.40205246498806\n",
      "    val_log_likelihood: 409.5869506835937\n",
      "    val_log_marginal: 317.5117426626384\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -180.467209\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -312.386047\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -197.040405\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -391.012756\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -353.544037\n",
      "    epoch          : 303\n",
      "    loss           : -346.1273831176758\n",
      "    val_loss       : -328.491253271047\n",
      "    val_log_likelihood: 416.5167663574219\n",
      "    val_log_marginal: 335.2990221713373\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -364.334961\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -415.451477\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -381.377808\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -202.432983\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -379.816711\n",
      "    epoch          : 304\n",
      "    loss           : -350.0476893615723\n",
      "    val_loss       : -327.65813793633134\n",
      "    val_log_likelihood: 414.55777435302736\n",
      "    val_log_marginal: 331.3174970988184\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -411.923584\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -232.080414\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -414.833740\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -324.045868\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -194.094543\n",
      "    epoch          : 305\n",
      "    loss           : -340.3232209777832\n",
      "    val_loss       : -315.87257724953815\n",
      "    val_log_likelihood: 418.73312225341795\n",
      "    val_log_marginal: 323.1092098478228\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -361.283661\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -473.727539\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -349.450409\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -122.995361\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -94.937500\n",
      "    epoch          : 306\n",
      "    loss           : -301.88064035415647\n",
      "    val_loss       : -230.7902139778249\n",
      "    val_log_likelihood: 394.65970611572266\n",
      "    val_log_marginal: 242.36818692348896\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -281.598724\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: 5.059238\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -270.312073\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -347.539734\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -108.713669\n",
      "    epoch          : 307\n",
      "    loss           : -229.36403251171112\n",
      "    val_loss       : -251.6228743438609\n",
      "    val_log_likelihood: 394.8172805786133\n",
      "    val_log_marginal: 263.2012795913964\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -271.553162\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -111.473969\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -332.925171\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -366.056976\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -358.008240\n",
      "    epoch          : 308\n",
      "    loss           : -299.378491973877\n",
      "    val_loss       : -318.54696953259406\n",
      "    val_log_likelihood: 412.38798828125\n",
      "    val_log_marginal: 323.38292150981727\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -419.662262\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -390.270081\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -324.049377\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -463.729736\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -215.918457\n",
      "    epoch          : 309\n",
      "    loss           : -341.520749206543\n",
      "    val_loss       : -328.9089494490996\n",
      "    val_log_likelihood: 419.8140502929688\n",
      "    val_log_marginal: 334.06062477789817\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -386.567780\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -418.455475\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -366.372833\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -202.403076\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -356.515778\n",
      "    epoch          : 310\n",
      "    loss           : -353.3573574829102\n",
      "    val_loss       : -333.9011861748062\n",
      "    val_log_likelihood: 419.76478271484376\n",
      "    val_log_marginal: 336.8889612492174\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -376.890411\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -379.455200\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -303.649475\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -475.586395\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -209.368408\n",
      "    epoch          : 311\n",
      "    loss           : -349.10518310546877\n",
      "    val_loss       : -318.6155779296532\n",
      "    val_log_likelihood: 418.0478820800781\n",
      "    val_log_marginal: 323.7137850567709\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -380.402802\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -409.542480\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -375.239868\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -320.466003\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -373.565338\n",
      "    epoch          : 312\n",
      "    loss           : -351.198470916748\n",
      "    val_loss       : -328.0235098483041\n",
      "    val_log_likelihood: 423.6039321899414\n",
      "    val_log_marginal: 334.1742061238736\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -387.177795\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -377.363556\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -363.862244\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -206.062958\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: 88.197701\n",
      "    epoch          : 313\n",
      "    loss           : -283.8816402816772\n",
      "    val_loss       : -125.07213692171499\n",
      "    val_log_likelihood: 404.0905197143555\n",
      "    val_log_marginal: 138.51848689801992\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: 240.997803\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -52.523544\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -165.171478\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -369.257965\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -219.441986\n",
      "    epoch          : 314\n",
      "    loss           : -109.52726081848145\n",
      "    val_loss       : -293.62499756570907\n",
      "    val_log_likelihood: 399.14517364501955\n",
      "    val_log_marginal: 302.27759138048225\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -326.875977\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -346.924500\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -346.884094\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -407.133179\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -369.396820\n",
      "    epoch          : 315\n",
      "    loss           : -334.7669845581055\n",
      "    val_loss       : -323.4070455303416\n",
      "    val_log_likelihood: 416.7470397949219\n",
      "    val_log_marginal: 329.36873966641724\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -362.606323\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -376.343079\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -350.468109\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -375.970947\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -332.391083\n",
      "    epoch          : 316\n",
      "    loss           : -346.4582083129883\n",
      "    val_loss       : -333.1936334040016\n",
      "    val_log_likelihood: 423.8603546142578\n",
      "    val_log_marginal: 338.6793044753373\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -341.301636\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -414.495148\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -425.057861\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -491.384888\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -379.720490\n",
      "    epoch          : 317\n",
      "    loss           : -358.219859161377\n",
      "    val_loss       : -334.1532833931036\n",
      "    val_log_likelihood: 425.82719268798826\n",
      "    val_log_marginal: 339.56375604519957\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -476.286591\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -203.747574\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -384.525940\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -394.639343\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -368.390686\n",
      "    epoch          : 318\n",
      "    loss           : -358.67186721801755\n",
      "    val_loss       : -327.71192864300684\n",
      "    val_log_likelihood: 425.08290100097656\n",
      "    val_log_marginal: 333.1264797832817\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -349.388550\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -356.013977\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -329.420959\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -308.124878\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -315.233276\n",
      "    epoch          : 319\n",
      "    loss           : -350.11869522094725\n",
      "    val_loss       : -324.58688556738196\n",
      "    val_log_likelihood: 424.92884826660156\n",
      "    val_log_marginal: 331.8114817233416\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -327.922333\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -386.062073\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -326.911682\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -387.336670\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -176.643677\n",
      "    epoch          : 320\n",
      "    loss           : -340.1891387939453\n",
      "    val_loss       : -301.0420166197233\n",
      "    val_log_likelihood: 417.2026077270508\n",
      "    val_log_marginal: 306.70137101225555\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch320.pth ...\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -338.273743\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -359.848969\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -370.183380\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -324.721924\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -294.901245\n",
      "    epoch          : 321\n",
      "    loss           : -317.94382148742676\n",
      "    val_loss       : -275.07472761776296\n",
      "    val_log_likelihood: 408.66956481933596\n",
      "    val_log_marginal: 294.90009969584645\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -246.583817\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -361.315857\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -360.065338\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -298.463287\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -346.803406\n",
      "    epoch          : 322\n",
      "    loss           : -299.5265460205078\n",
      "    val_loss       : -293.3076456823386\n",
      "    val_log_likelihood: 411.29834747314453\n",
      "    val_log_marginal: 297.65134767778216\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -300.391357\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -305.660889\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -333.646118\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -372.495178\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -214.811829\n",
      "    epoch          : 323\n",
      "    loss           : -326.5166703796387\n",
      "    val_loss       : -311.3685410834849\n",
      "    val_log_likelihood: 420.5405807495117\n",
      "    val_log_marginal: 316.5029832033318\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -332.442444\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -162.288239\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -452.069519\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -365.994629\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -179.552032\n",
      "    epoch          : 324\n",
      "    loss           : -319.4698419189453\n",
      "    val_loss       : -306.2213471800089\n",
      "    val_log_likelihood: 409.1717269897461\n",
      "    val_log_marginal: 308.91843804605304\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -198.482666\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -40.110291\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -343.690430\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -149.336823\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -169.075165\n",
      "    epoch          : 325\n",
      "    loss           : -304.20257095336916\n",
      "    val_loss       : -311.8404165493324\n",
      "    val_log_likelihood: 423.93653869628906\n",
      "    val_log_marginal: 322.18121417164804\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -190.169632\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -336.186493\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -396.647705\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -277.167938\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -236.991119\n",
      "    epoch          : 326\n",
      "    loss           : -289.19459831237793\n",
      "    val_loss       : -254.14824793199077\n",
      "    val_log_likelihood: 391.6968048095703\n",
      "    val_log_marginal: 263.26104949600995\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -329.159302\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -345.165466\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -380.249939\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -398.773499\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -205.449341\n",
      "    epoch          : 327\n",
      "    loss           : -322.9617925262451\n",
      "    val_loss       : -321.23929596953093\n",
      "    val_log_likelihood: 423.4698196411133\n",
      "    val_log_marginal: 327.81279602684083\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -368.042480\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -360.192078\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -489.602356\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -467.390656\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -191.418381\n",
      "    epoch          : 328\n",
      "    loss           : -340.96549438476563\n",
      "    val_loss       : -308.92987988302485\n",
      "    val_log_likelihood: 423.7099365234375\n",
      "    val_log_marginal: 315.1567189623628\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -361.333130\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -164.569687\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -181.038406\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -383.785919\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -214.450363\n",
      "    epoch          : 329\n",
      "    loss           : -341.33536560058593\n",
      "    val_loss       : -330.980625582207\n",
      "    val_log_likelihood: 423.14036560058594\n",
      "    val_log_marginal: 336.14254692681135\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -396.762848\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -188.147125\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -406.688324\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -458.883026\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -286.078278\n",
      "    epoch          : 330\n",
      "    loss           : -342.8793183898926\n",
      "    val_loss       : -315.75616090511903\n",
      "    val_log_likelihood: 421.2916061401367\n",
      "    val_log_marginal: 321.814169718749\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -391.377014\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -365.406281\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -373.938293\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -323.478882\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -310.761017\n",
      "    epoch          : 331\n",
      "    loss           : -317.5089385223389\n",
      "    val_loss       : -301.66606884840877\n",
      "    val_log_likelihood: 418.11746673583986\n",
      "    val_log_marginal: 313.755565426093\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -342.662476\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -291.254272\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -376.381897\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -376.901428\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -394.468384\n",
      "    epoch          : 332\n",
      "    loss           : -334.672219543457\n",
      "    val_loss       : -325.71692784707994\n",
      "    val_log_likelihood: 425.1398208618164\n",
      "    val_log_marginal: 331.78015997558833\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -335.760895\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -323.144409\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -348.740265\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -378.988953\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -196.741608\n",
      "    epoch          : 333\n",
      "    loss           : -337.8099850463867\n",
      "    val_loss       : -306.14280316373333\n",
      "    val_log_likelihood: 429.2616897583008\n",
      "    val_log_marginal: 314.6691153999418\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -306.102448\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -474.258606\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -169.910004\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -358.283142\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -329.213074\n",
      "    epoch          : 334\n",
      "    loss           : -345.4803726196289\n",
      "    val_loss       : -331.8590480899438\n",
      "    val_log_likelihood: 426.08445434570314\n",
      "    val_log_marginal: 334.5138618875295\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -404.647308\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -495.220703\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -183.511322\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -314.954041\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -369.052032\n",
      "    epoch          : 335\n",
      "    loss           : -355.1263980102539\n",
      "    val_loss       : -325.8529438901693\n",
      "    val_log_likelihood: 429.4688690185547\n",
      "    val_log_marginal: 333.17561751790345\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -475.224945\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -356.242462\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -314.202759\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -330.479797\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -330.632416\n",
      "    epoch          : 336\n",
      "    loss           : -305.89007080078125\n",
      "    val_loss       : -298.3137536027469\n",
      "    val_log_likelihood: 422.2722366333008\n",
      "    val_log_marginal: 308.9740985237062\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -363.695251\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -346.027039\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -474.803711\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -352.459503\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -290.984192\n",
      "    epoch          : 337\n",
      "    loss           : -317.4943716430664\n",
      "    val_loss       : -303.05962242614476\n",
      "    val_log_likelihood: 418.89034881591795\n",
      "    val_log_marginal: 310.3195506032556\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -355.998077\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -440.590271\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -373.511169\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -142.677292\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -371.807068\n",
      "    epoch          : 338\n",
      "    loss           : -323.50165939331055\n",
      "    val_loss       : -300.81105011785405\n",
      "    val_log_likelihood: 421.42191314697266\n",
      "    val_log_marginal: 306.44639581032095\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -452.134918\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -337.944702\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -217.348358\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -140.565903\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -105.471336\n",
      "    epoch          : 339\n",
      "    loss           : -264.4707326602936\n",
      "    val_loss       : -258.3988465787843\n",
      "    val_log_likelihood: 409.4690765380859\n",
      "    val_log_marginal: 271.4047749441117\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -293.483032\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -337.387756\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -332.733459\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -264.701324\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -302.133423\n",
      "    epoch          : 340\n",
      "    loss           : -281.43541474342345\n",
      "    val_loss       : -276.18134564664217\n",
      "    val_log_likelihood: 408.01805267333987\n",
      "    val_log_marginal: 284.473027715832\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch340.pth ...\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -353.454926\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -280.002472\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -471.453644\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -200.047821\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -222.924454\n",
      "    epoch          : 341\n",
      "    loss           : -310.18324974060056\n",
      "    val_loss       : -330.2828771303408\n",
      "    val_log_likelihood: 427.1306686401367\n",
      "    val_log_marginal: 340.2129637335987\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -231.882980\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -376.330231\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -307.295380\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -360.191589\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -208.092636\n",
      "    epoch          : 342\n",
      "    loss           : -337.88002487182615\n",
      "    val_loss       : -311.8123226051219\n",
      "    val_log_likelihood: 426.43724365234374\n",
      "    val_log_marginal: 319.77382142581047\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -157.642670\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -337.071930\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -368.568115\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -389.831665\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -230.985275\n",
      "    epoch          : 343\n",
      "    loss           : -351.0978692626953\n",
      "    val_loss       : -334.10920652737843\n",
      "    val_log_likelihood: 432.0783325195313\n",
      "    val_log_marginal: 344.6765001263469\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -387.631775\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -350.121979\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -482.143524\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -336.127380\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -386.329132\n",
      "    epoch          : 344\n",
      "    loss           : -363.75890686035154\n",
      "    val_loss       : -344.5258353662677\n",
      "    val_log_likelihood: 434.2019805908203\n",
      "    val_log_marginal: 348.2155908394605\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -238.245941\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -496.801025\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -390.599792\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -505.403259\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -392.743744\n",
      "    epoch          : 345\n",
      "    loss           : -369.321921081543\n",
      "    val_loss       : -339.28373101968316\n",
      "    val_log_likelihood: 434.8269500732422\n",
      "    val_log_marginal: 344.8283686269075\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -412.240112\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -391.136902\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -196.499161\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -211.325012\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -181.896881\n",
      "    epoch          : 346\n",
      "    loss           : -359.34905868530274\n",
      "    val_loss       : -321.1577550415881\n",
      "    val_log_likelihood: 433.25197143554686\n",
      "    val_log_marginal: 329.0976707372814\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -184.850586\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -222.590332\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -339.693726\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -281.097687\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -232.808075\n",
      "    epoch          : 347\n",
      "    loss           : -307.94527214050294\n",
      "    val_loss       : -220.88970423052086\n",
      "    val_log_likelihood: 408.6957504272461\n",
      "    val_log_marginal: 226.62294131331146\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -211.572372\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -154.147034\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -419.203522\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -411.363464\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -97.224823\n",
      "    epoch          : 348\n",
      "    loss           : -242.55853804588318\n",
      "    val_loss       : -209.86020683255046\n",
      "    val_log_likelihood: 392.18127593994143\n",
      "    val_log_marginal: 220.3472285028547\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -309.012299\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -425.336182\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -351.849060\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -343.554779\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -358.086975\n",
      "    epoch          : 349\n",
      "    loss           : -302.7589625930786\n",
      "    val_loss       : -274.2673641812988\n",
      "    val_log_likelihood: 418.666276550293\n",
      "    val_log_marginal: 280.07921499460934\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -321.289917\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -353.811615\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -374.789307\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -400.155060\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -200.876984\n",
      "    epoch          : 350\n",
      "    loss           : -308.7089751052856\n",
      "    val_loss       : -322.6855366889387\n",
      "    val_log_likelihood: 428.4065795898438\n",
      "    val_log_marginal: 331.7124613378197\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -376.559631\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -363.509155\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -481.395691\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -219.649384\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -282.186035\n",
      "    epoch          : 351\n",
      "    loss           : -345.2123207092285\n",
      "    val_loss       : -330.19478808995336\n",
      "    val_log_likelihood: 431.4066589355469\n",
      "    val_log_marginal: 338.81571034602837\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -369.666718\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -480.883392\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -395.554932\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -225.210724\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -343.367981\n",
      "    epoch          : 352\n",
      "    loss           : -353.8406620788574\n",
      "    val_loss       : -326.71065052077176\n",
      "    val_log_likelihood: 432.83725891113284\n",
      "    val_log_marginal: 335.3196744004987\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -490.663635\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -487.311707\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -380.988892\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -380.467651\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -327.951294\n",
      "    epoch          : 353\n",
      "    loss           : -356.7514860534668\n",
      "    val_loss       : -331.9946011645719\n",
      "    val_log_likelihood: 433.9592346191406\n",
      "    val_log_marginal: 334.3769648265094\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -423.633392\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -370.281586\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -485.976257\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -333.250732\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -334.244629\n",
      "    epoch          : 354\n",
      "    loss           : -349.0391831970215\n",
      "    val_loss       : -320.349736917112\n",
      "    val_log_likelihood: 433.09983520507814\n",
      "    val_log_marginal: 324.5100557297468\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -182.538254\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -489.879547\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -363.895386\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -294.568909\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -369.783081\n",
      "    epoch          : 355\n",
      "    loss           : -349.81502044677734\n",
      "    val_loss       : -323.3306205548346\n",
      "    val_log_likelihood: 426.28425750732424\n",
      "    val_log_marginal: 329.6841171897948\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -383.344604\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -390.289032\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -489.206299\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -370.724548\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -396.348358\n",
      "    epoch          : 356\n",
      "    loss           : -350.255817565918\n",
      "    val_loss       : -323.0649178214371\n",
      "    val_log_likelihood: 434.4582855224609\n",
      "    val_log_marginal: 333.8706450019045\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -383.326965\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -396.988708\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -370.621735\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -390.657928\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -333.683350\n",
      "    epoch          : 357\n",
      "    loss           : -345.3775401306152\n",
      "    val_loss       : -320.10940890349445\n",
      "    val_log_likelihood: 432.1599914550781\n",
      "    val_log_marginal: 324.0817507926375\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -375.783173\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -360.738831\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -395.803223\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -365.358398\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -321.703186\n",
      "    epoch          : 358\n",
      "    loss           : -337.1929270172119\n",
      "    val_loss       : -252.67916457299143\n",
      "    val_log_likelihood: 414.56622924804685\n",
      "    val_log_marginal: 264.35745035223664\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -245.735748\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -297.466492\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -143.681381\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -208.893646\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -308.072418\n",
      "    epoch          : 359\n",
      "    loss           : -291.8292554664612\n",
      "    val_loss       : -297.2196762220934\n",
      "    val_log_likelihood: 420.0505966186523\n",
      "    val_log_marginal: 301.66286803632977\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -362.993561\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -316.911133\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -377.620178\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -332.050079\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -314.077820\n",
      "    epoch          : 360\n",
      "    loss           : -333.2863793182373\n",
      "    val_loss       : -322.1849313846789\n",
      "    val_log_likelihood: 430.2571624755859\n",
      "    val_log_marginal: 327.93409351669254\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch360.pth ...\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -480.065002\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -378.261658\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -188.990112\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -144.619293\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -346.792358\n",
      "    epoch          : 361\n",
      "    loss           : -328.70415115356445\n",
      "    val_loss       : -165.92768366187812\n",
      "    val_log_likelihood: 422.5440872192383\n",
      "    val_log_marginal: 172.42163165622324\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -287.421173\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -10.767357\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: 76.144089\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -247.677444\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -271.431641\n",
      "    epoch          : 362\n",
      "    loss           : -159.14101379394532\n",
      "    val_loss       : -183.64778174944223\n",
      "    val_log_likelihood: 396.92693786621095\n",
      "    val_log_marginal: 195.9213849257679\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -255.654755\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -374.200287\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -376.043030\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -197.136627\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -482.646271\n",
      "    epoch          : 363\n",
      "    loss           : -317.9555425262451\n",
      "    val_loss       : -311.27817877195776\n",
      "    val_log_likelihood: 429.2168914794922\n",
      "    val_log_marginal: 317.74844895713034\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -148.051361\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -314.423340\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -397.376160\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -335.509369\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -378.653015\n",
      "    epoch          : 364\n",
      "    loss           : -356.20824645996095\n",
      "    val_loss       : -329.0702395064756\n",
      "    val_log_likelihood: 430.2458862304687\n",
      "    val_log_marginal: 332.69593034721913\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -439.814697\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -489.204041\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -429.621552\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -403.453064\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -231.465881\n",
      "    epoch          : 365\n",
      "    loss           : -359.12134033203125\n",
      "    val_loss       : -337.3132654404268\n",
      "    val_log_likelihood: 435.05234680175784\n",
      "    val_log_marginal: 344.7838378475081\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -382.473938\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -473.116241\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -361.968079\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -190.432297\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -206.769592\n",
      "    epoch          : 366\n",
      "    loss           : -354.88385528564453\n",
      "    val_loss       : -336.1884882572107\n",
      "    val_log_likelihood: 432.3822708129883\n",
      "    val_log_marginal: 338.4486942805424\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -394.995789\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -196.963196\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -206.369415\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -394.297974\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -381.171112\n",
      "    epoch          : 367\n",
      "    loss           : -356.84581756591797\n",
      "    val_loss       : -330.5495617969893\n",
      "    val_log_likelihood: 435.3512878417969\n",
      "    val_log_marginal: 336.6385038319975\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -184.166580\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -385.628540\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -231.816864\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -339.046875\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -416.542297\n",
      "    epoch          : 368\n",
      "    loss           : -361.25588150024413\n",
      "    val_loss       : -340.0021445644088\n",
      "    val_log_likelihood: 437.2062561035156\n",
      "    val_log_marginal: 347.2811866920441\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -391.642822\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -379.942017\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -193.872833\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -342.141296\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -341.628418\n",
      "    epoch          : 369\n",
      "    loss           : -365.0584007263184\n",
      "    val_loss       : -336.14755285009744\n",
      "    val_log_likelihood: 437.7421356201172\n",
      "    val_log_marginal: 345.00885602329555\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -429.644775\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -236.437958\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -409.289978\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -339.743713\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -205.586029\n",
      "    epoch          : 370\n",
      "    loss           : -358.26396026611326\n",
      "    val_loss       : -335.44346273755656\n",
      "    val_log_likelihood: 438.20783081054685\n",
      "    val_log_marginal: 341.56909034363923\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -398.293274\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -392.082916\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -192.478180\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -222.009048\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -387.917145\n",
      "    epoch          : 371\n",
      "    loss           : -363.0165676879883\n",
      "    val_loss       : -327.09022181537\n",
      "    val_log_likelihood: 440.5956176757812\n",
      "    val_log_marginal: 334.2223831336945\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -229.279785\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -180.474350\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -365.938965\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -386.973511\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: 342.228943\n",
      "    epoch          : 372\n",
      "    loss           : -228.987271194458\n",
      "    val_loss       : 356.0168902967125\n",
      "    val_log_likelihood: 351.16441650390624\n",
      "    val_log_marginal: -326.4698186825961\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: 107.339607\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: 264.917938\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: 17.998812\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -324.581543\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -187.970917\n",
      "    epoch          : 373\n",
      "    loss           : -74.62560098648072\n",
      "    val_loss       : -317.20303654037417\n",
      "    val_log_likelihood: 424.65533447265625\n",
      "    val_log_marginal: 328.4258784327656\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -376.144836\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -416.760315\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -365.768555\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -192.741257\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -405.142578\n",
      "    epoch          : 374\n",
      "    loss           : -352.42784088134766\n",
      "    val_loss       : -339.04453129339964\n",
      "    val_log_likelihood: 434.33153381347654\n",
      "    val_log_marginal: 345.5464340486589\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -339.886871\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -214.900528\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -395.599854\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -408.541016\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -403.891785\n",
      "    epoch          : 375\n",
      "    loss           : -370.1653303527832\n",
      "    val_loss       : -351.94532319046556\n",
      "    val_log_likelihood: 440.26295471191406\n",
      "    val_log_marginal: 357.4912538792938\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -401.532745\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -407.087677\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -502.105042\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -391.844666\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -413.098511\n",
      "    epoch          : 376\n",
      "    loss           : -375.1743406677246\n",
      "    val_loss       : -351.94040795024483\n",
      "    val_log_likelihood: 439.10142822265624\n",
      "    val_log_marginal: 355.3455496420475\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -394.749390\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -369.821594\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -413.920288\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -192.298187\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -183.795746\n",
      "    epoch          : 377\n",
      "    loss           : -368.3301496887207\n",
      "    val_loss       : -331.9441859741695\n",
      "    val_log_likelihood: 438.7007141113281\n",
      "    val_log_marginal: 336.19074438549575\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -220.564392\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -388.892578\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -389.640076\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -407.011475\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -341.785187\n",
      "    epoch          : 378\n",
      "    loss           : -346.3489434814453\n",
      "    val_loss       : -331.14867078661916\n",
      "    val_log_likelihood: 436.4861633300781\n",
      "    val_log_marginal: 338.6434064209461\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -365.130402\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -204.759491\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -485.296570\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -406.685333\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -231.319244\n",
      "    epoch          : 379\n",
      "    loss           : -364.4310205078125\n",
      "    val_loss       : -346.5822560966015\n",
      "    val_log_likelihood: 440.24151306152345\n",
      "    val_log_marginal: 350.945996100083\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -406.122314\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -440.657349\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -354.114197\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -408.474701\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -405.365387\n",
      "    epoch          : 380\n",
      "    loss           : -378.29614944458007\n",
      "    val_loss       : -354.82693262342366\n",
      "    val_log_likelihood: 445.8382598876953\n",
      "    val_log_marginal: 360.69676480609064\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch380.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -417.237427\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -217.288635\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -395.800842\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -194.833252\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -371.804871\n",
      "    epoch          : 381\n",
      "    loss           : -360.5998403930664\n",
      "    val_loss       : -308.13245881376787\n",
      "    val_log_likelihood: 435.375260925293\n",
      "    val_log_marginal: 314.0999447721988\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -376.460327\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -323.842407\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -383.795959\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -322.225159\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -309.272705\n",
      "    epoch          : 382\n",
      "    loss           : -343.79858283996583\n",
      "    val_loss       : -340.4905498119071\n",
      "    val_log_likelihood: 438.0060119628906\n",
      "    val_log_marginal: 343.58350724015963\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -406.508087\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -194.832977\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -345.402008\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -403.840973\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -324.300079\n",
      "    epoch          : 383\n",
      "    loss           : -333.31689880371096\n",
      "    val_loss       : -318.73323870729655\n",
      "    val_log_likelihood: 438.8446502685547\n",
      "    val_log_marginal: 329.0476011414081\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -134.676071\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -491.646240\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -413.365662\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -330.543945\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -363.263550\n",
      "    epoch          : 384\n",
      "    loss           : -348.5541925048828\n",
      "    val_loss       : -332.40254051545634\n",
      "    val_log_likelihood: 435.99769287109376\n",
      "    val_log_marginal: 336.8382878150449\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -341.202972\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -232.801971\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -364.421082\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -474.698181\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -401.724854\n",
      "    epoch          : 385\n",
      "    loss           : -356.75124267578127\n",
      "    val_loss       : -341.2965417491272\n",
      "    val_log_likelihood: 443.0080535888672\n",
      "    val_log_marginal: 346.73534530399803\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -419.564789\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -394.608765\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -444.569641\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -467.018280\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -390.024658\n",
      "    epoch          : 386\n",
      "    loss           : -351.3047206115723\n",
      "    val_loss       : -331.03640946960076\n",
      "    val_log_likelihood: 441.2700759887695\n",
      "    val_log_marginal: 338.3647292163223\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -401.921539\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -414.314697\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -413.385101\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -386.791901\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -366.415802\n",
      "    epoch          : 387\n",
      "    loss           : -350.6374841308594\n",
      "    val_loss       : -293.84818403515965\n",
      "    val_log_likelihood: 439.3700805664063\n",
      "    val_log_marginal: 301.42461032602205\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -206.715851\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -134.745041\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -291.384033\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -263.145935\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -316.432983\n",
      "    epoch          : 388\n",
      "    loss           : -289.48138080596925\n",
      "    val_loss       : -273.0822091618553\n",
      "    val_log_likelihood: 426.87072296142577\n",
      "    val_log_marginal: 280.1627002868801\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -280.735901\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -157.460144\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -382.280090\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -487.209961\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -312.495422\n",
      "    epoch          : 389\n",
      "    loss           : -334.00016815185546\n",
      "    val_loss       : -319.80568772312256\n",
      "    val_log_likelihood: 437.4842864990234\n",
      "    val_log_marginal: 330.7559169497341\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -335.750488\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -372.728607\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -366.079773\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -393.971375\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -325.139191\n",
      "    epoch          : 390\n",
      "    loss           : -344.0954662322998\n",
      "    val_loss       : -309.7489181998186\n",
      "    val_log_likelihood: 434.8586730957031\n",
      "    val_log_marginal: 313.10452361516656\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -337.596252\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -237.729568\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -336.965759\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -205.811172\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -406.743530\n",
      "    epoch          : 391\n",
      "    loss           : -351.11480102539065\n",
      "    val_loss       : -340.28694999013095\n",
      "    val_log_likelihood: 439.9208953857422\n",
      "    val_log_marginal: 346.6137919802219\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -512.175903\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -408.095337\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -387.571136\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -188.944778\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -386.085358\n",
      "    epoch          : 392\n",
      "    loss           : -357.8740023803711\n",
      "    val_loss       : -343.81865865178406\n",
      "    val_log_likelihood: 438.82571563720705\n",
      "    val_log_marginal: 348.8341889668256\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -382.188812\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -208.805298\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -382.374023\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -420.887115\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -389.702606\n",
      "    epoch          : 393\n",
      "    loss           : -361.5611460876465\n",
      "    val_loss       : -336.1554899447598\n",
      "    val_log_likelihood: 441.1301025390625\n",
      "    val_log_marginal: 344.42099379040303\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -406.851196\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -222.098175\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -374.152161\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -490.319489\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -320.412476\n",
      "    epoch          : 394\n",
      "    loss           : -356.5921211242676\n",
      "    val_loss       : -341.17091781599447\n",
      "    val_log_likelihood: 445.42828063964845\n",
      "    val_log_marginal: 345.81779844500124\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -500.082764\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -380.194366\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -433.909454\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -395.511139\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -217.314789\n",
      "    epoch          : 395\n",
      "    loss           : -369.52093017578125\n",
      "    val_loss       : -332.7001691176556\n",
      "    val_log_likelihood: 438.58811950683594\n",
      "    val_log_marginal: 336.01995522081853\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -359.604767\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -424.457977\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -193.704865\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -322.297241\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -145.910980\n",
      "    epoch          : 396\n",
      "    loss           : -337.82820587158204\n",
      "    val_loss       : -289.61666014296935\n",
      "    val_log_likelihood: 432.13045501708984\n",
      "    val_log_marginal: 305.14891862720253\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -361.953735\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -392.045074\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -477.958740\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -104.438339\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -302.524506\n",
      "    epoch          : 397\n",
      "    loss           : -332.094970703125\n",
      "    val_loss       : -315.8764344974421\n",
      "    val_log_likelihood: 435.1497543334961\n",
      "    val_log_marginal: 324.2256006460637\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -425.652893\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -429.327026\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -342.494019\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -368.541656\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -224.867462\n",
      "    epoch          : 398\n",
      "    loss           : -329.0960961151123\n",
      "    val_loss       : -329.7534952622838\n",
      "    val_log_likelihood: 440.23390197753906\n",
      "    val_log_marginal: 335.09071705155077\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -441.961731\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -405.709106\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -341.909332\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -195.995697\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -352.020020\n",
      "    epoch          : 399\n",
      "    loss           : -331.86935501098634\n",
      "    val_loss       : -272.51230733394624\n",
      "    val_log_likelihood: 431.13266906738284\n",
      "    val_log_marginal: 288.6638593900949\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -297.688232\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -256.470184\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -299.646667\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -196.123169\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -326.603119\n",
      "    epoch          : 400\n",
      "    loss           : -327.70649574279787\n",
      "    val_loss       : -314.5835271866061\n",
      "    val_log_likelihood: 436.90131225585935\n",
      "    val_log_marginal: 320.24766472764315\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -392.050598\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -405.695374\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -231.298630\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -378.390198\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -399.570770\n",
      "    epoch          : 401\n",
      "    loss           : -355.47723388671875\n",
      "    val_loss       : -347.62593925260006\n",
      "    val_log_likelihood: 446.9097534179688\n",
      "    val_log_marginal: 352.48660764358937\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -398.130493\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -361.891144\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -422.271332\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -406.756775\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -237.054886\n",
      "    epoch          : 402\n",
      "    loss           : -378.44403610229494\n",
      "    val_loss       : -351.3948300858028\n",
      "    val_log_likelihood: 447.96079711914064\n",
      "    val_log_marginal: 360.2444149259478\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -389.585938\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -428.766541\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -351.265350\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -458.192932\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -375.148651\n",
      "    epoch          : 403\n",
      "    loss           : -354.35043731689456\n",
      "    val_loss       : -323.75575320702046\n",
      "    val_log_likelihood: 435.5619384765625\n",
      "    val_log_marginal: 328.2558713067323\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -411.685791\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -201.940430\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -212.948257\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -354.503601\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -372.469177\n",
      "    epoch          : 404\n",
      "    loss           : -331.4110433959961\n",
      "    val_loss       : -321.13363163676115\n",
      "    val_log_likelihood: 436.57555999755857\n",
      "    val_log_marginal: 326.73138217963276\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -407.459869\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -386.534424\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -362.401154\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -469.694580\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -322.883362\n",
      "    epoch          : 405\n",
      "    loss           : -354.08639877319337\n",
      "    val_loss       : -336.217554314062\n",
      "    val_log_likelihood: 440.2624237060547\n",
      "    val_log_marginal: 341.27277375645934\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -427.848633\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -227.200516\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -362.265991\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -244.659409\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -420.693542\n",
      "    epoch          : 406\n",
      "    loss           : -375.41765167236326\n",
      "    val_loss       : -354.95255742147566\n",
      "    val_log_likelihood: 449.1034423828125\n",
      "    val_log_marginal: 358.5523058028715\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -396.300293\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -358.944244\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -355.909637\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -422.493561\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -423.958832\n",
      "    epoch          : 407\n",
      "    loss           : -382.4150192260742\n",
      "    val_loss       : -359.8903969719075\n",
      "    val_log_likelihood: 451.59319458007815\n",
      "    val_log_marginal: 365.0281783404882\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -231.705063\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -439.295563\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -443.654724\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -256.933899\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -233.277512\n",
      "    epoch          : 408\n",
      "    loss           : -382.0536149597168\n",
      "    val_loss       : -351.7789051673375\n",
      "    val_log_likelihood: 449.4660705566406\n",
      "    val_log_marginal: 356.5559692658484\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -205.468246\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -449.460358\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -433.223602\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -484.599304\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -408.943359\n",
      "    epoch          : 409\n",
      "    loss           : -362.02238174438475\n",
      "    val_loss       : -299.2248930873349\n",
      "    val_log_likelihood: 429.42696685791014\n",
      "    val_log_marginal: 309.12730720499025\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -314.819794\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -113.088173\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -294.883667\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -11.149407\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: 205.515442\n",
      "    epoch          : 410\n",
      "    loss           : -248.5934907245636\n",
      "    val_loss       : -259.09495033575223\n",
      "    val_log_likelihood: 427.25494689941405\n",
      "    val_log_marginal: 274.16180883869526\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -491.584595\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -285.699738\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -343.570099\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -164.262390\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -260.957764\n",
      "    epoch          : 411\n",
      "    loss           : -199.30823815345764\n",
      "    val_loss       : -280.9057976877317\n",
      "    val_log_likelihood: 420.0122406005859\n",
      "    val_log_marginal: 292.87105605266987\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -311.868469\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -83.923538\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -211.536346\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -426.653748\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -232.937881\n",
      "    epoch          : 412\n",
      "    loss           : -343.83096809387206\n",
      "    val_loss       : -351.89343522200363\n",
      "    val_log_likelihood: 446.5411376953125\n",
      "    val_log_marginal: 356.6903361425192\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -518.291382\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -409.638367\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -202.589432\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -423.361877\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -412.587891\n",
      "    epoch          : 413\n",
      "    loss           : -378.9849589538574\n",
      "    val_loss       : -357.9926872111857\n",
      "    val_log_likelihood: 447.2950927734375\n",
      "    val_log_marginal: 362.54609053144713\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -409.398010\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -397.658691\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -441.822021\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -427.920898\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -527.781311\n",
      "    epoch          : 414\n",
      "    loss           : -385.3027607727051\n",
      "    val_loss       : -356.89191193319857\n",
      "    val_log_likelihood: 452.6902557373047\n",
      "    val_log_marginal: 363.43844130448997\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -517.416504\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -452.591400\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -398.142181\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -241.321503\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -233.563507\n",
      "    epoch          : 415\n",
      "    loss           : -381.6622666931152\n",
      "    val_loss       : -347.6838184435852\n",
      "    val_log_likelihood: 451.55431213378904\n",
      "    val_log_marginal: 353.8341306839138\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -384.982239\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -527.180054\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -513.091797\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -401.336212\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -248.274796\n",
      "    epoch          : 416\n",
      "    loss           : -373.61289108276367\n",
      "    val_loss       : -347.12236080104486\n",
      "    val_log_likelihood: 450.4369415283203\n",
      "    val_log_marginal: 352.96278278157115\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -208.584213\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -397.574982\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -516.617920\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -356.765259\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -14.802713\n",
      "    epoch          : 417\n",
      "    loss           : -334.42279306411746\n",
      "    val_loss       : -232.1063761562109\n",
      "    val_log_likelihood: 423.349186706543\n",
      "    val_log_marginal: 244.36387064568697\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -252.917221\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -322.428711\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -307.645020\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -68.427795\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -293.676086\n",
      "    epoch          : 418\n",
      "    loss           : -251.25745630264282\n",
      "    val_loss       : -174.2561134632677\n",
      "    val_log_likelihood: 424.7969451904297\n",
      "    val_log_marginal: 187.48703138791024\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -273.619202\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -168.079422\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -338.372955\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -485.763977\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -307.796326\n",
      "    epoch          : 419\n",
      "    loss           : -304.10640102386475\n",
      "    val_loss       : -320.1758898695931\n",
      "    val_log_likelihood: 438.9169952392578\n",
      "    val_log_marginal: 329.7757119186223\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -363.450684\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -394.518066\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -411.412354\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -219.432617\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -350.511627\n",
      "    epoch          : 420\n",
      "    loss           : -367.0415378570557\n",
      "    val_loss       : -354.9055890896358\n",
      "    val_log_likelihood: 449.2717620849609\n",
      "    val_log_marginal: 359.43468711860476\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch420.pth ...\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -422.912811\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -381.719788\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -510.986572\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -401.251892\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -235.780716\n",
      "    epoch          : 421\n",
      "    loss           : -375.37186248779295\n",
      "    val_loss       : -356.7250542723574\n",
      "    val_log_likelihood: 447.9781005859375\n",
      "    val_log_marginal: 362.4106076680124\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -367.596191\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -397.174713\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -409.460083\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -375.444000\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -420.880737\n",
      "    epoch          : 422\n",
      "    loss           : -379.35961715698244\n",
      "    val_loss       : -357.36465870616956\n",
      "    val_log_likelihood: 450.6861907958984\n",
      "    val_log_marginal: 364.1707561130049\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -230.339981\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -243.182327\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -523.666809\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -398.771545\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -406.030029\n",
      "    epoch          : 423\n",
      "    loss           : -389.7039099121094\n",
      "    val_loss       : -361.3492834519595\n",
      "    val_log_likelihood: 454.31305541992185\n",
      "    val_log_marginal: 367.2735191848129\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -242.118774\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -217.881195\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -452.447571\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -521.480347\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -236.788071\n",
      "    epoch          : 424\n",
      "    loss           : -386.6653277587891\n",
      "    val_loss       : -359.01335652777925\n",
      "    val_log_likelihood: 452.81236877441404\n",
      "    val_log_marginal: 363.34896476529536\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -453.685913\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -412.829742\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -522.608826\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -182.744110\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -353.874939\n",
      "    epoch          : 425\n",
      "    loss           : -383.1063082885742\n",
      "    val_loss       : -354.7593645500019\n",
      "    val_log_likelihood: 454.5752777099609\n",
      "    val_log_marginal: 360.2288583692163\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -401.141357\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -427.186188\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -480.415192\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -402.071045\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -391.823853\n",
      "    epoch          : 426\n",
      "    loss           : -363.80299240112305\n",
      "    val_loss       : -333.561054089386\n",
      "    val_log_likelihood: 451.995751953125\n",
      "    val_log_marginal: 339.51471452018455\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -464.519775\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -389.101288\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -431.120605\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -431.540375\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -521.385254\n",
      "    epoch          : 427\n",
      "    loss           : -383.36372085571287\n",
      "    val_loss       : -361.9877745671198\n",
      "    val_log_likelihood: 455.4135345458984\n",
      "    val_log_marginal: 365.3591902133078\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -223.766632\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -411.297516\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -417.274689\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -532.223145\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -503.467163\n",
      "    epoch          : 428\n",
      "    loss           : -385.5631292724609\n",
      "    val_loss       : -359.7141141007654\n",
      "    val_log_likelihood: 457.6663848876953\n",
      "    val_log_marginal: 365.92600392140446\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -434.130859\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -381.867004\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -413.000763\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -373.859131\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -231.860016\n",
      "    epoch          : 429\n",
      "    loss           : -386.95587203979494\n",
      "    val_loss       : -353.64582482362164\n",
      "    val_log_likelihood: 456.19956665039064\n",
      "    val_log_marginal: 359.03334553949924\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -388.562347\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -387.927856\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -524.532166\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -404.612518\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -393.245514\n",
      "    epoch          : 430\n",
      "    loss           : -375.0818412780762\n",
      "    val_loss       : -350.6930638876744\n",
      "    val_log_likelihood: 453.08038330078125\n",
      "    val_log_marginal: 353.89615668992855\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -441.988037\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -516.697754\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -410.360413\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -418.953064\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -423.437836\n",
      "    epoch          : 431\n",
      "    loss           : -375.1976107788086\n",
      "    val_loss       : -348.33743040272964\n",
      "    val_log_likelihood: 452.80210876464844\n",
      "    val_log_marginal: 355.7873274015667\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -424.011414\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -413.580078\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -126.654938\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -116.351639\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -162.964584\n",
      "    epoch          : 432\n",
      "    loss           : -299.7961050605774\n",
      "    val_loss       : -218.49369637798517\n",
      "    val_log_likelihood: 425.082048034668\n",
      "    val_log_marginal: 241.3824159491807\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -171.069977\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -385.355957\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -378.220093\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -410.903320\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -148.789062\n",
      "    epoch          : 433\n",
      "    loss           : -312.7001292419434\n",
      "    val_loss       : -292.0819323003292\n",
      "    val_log_likelihood: 425.2159027099609\n",
      "    val_log_marginal: 297.9258388940245\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -462.269531\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -403.161743\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -214.063202\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -396.016602\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -205.997879\n",
      "    epoch          : 434\n",
      "    loss           : -355.8028847503662\n",
      "    val_loss       : -311.5546821936965\n",
      "    val_log_likelihood: 444.37727508544924\n",
      "    val_log_marginal: 320.16396239958704\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -396.887756\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -351.096252\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -311.837769\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -386.632599\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -151.348648\n",
      "    epoch          : 435\n",
      "    loss           : -338.23834983825685\n",
      "    val_loss       : -287.6869003118016\n",
      "    val_log_likelihood: 445.3166076660156\n",
      "    val_log_marginal: 295.41321224831046\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -310.403656\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -290.139709\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -25.155397\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -325.012512\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -319.223450\n",
      "    epoch          : 436\n",
      "    loss           : -333.21410135269167\n",
      "    val_loss       : -326.6975630430505\n",
      "    val_log_likelihood: 452.2521087646484\n",
      "    val_log_marginal: 333.53180272169413\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -416.242737\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -522.666260\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -432.214172\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -396.235321\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -270.821045\n",
      "    epoch          : 437\n",
      "    loss           : -364.4278340148926\n",
      "    val_loss       : -332.1629483343102\n",
      "    val_log_likelihood: 450.7528045654297\n",
      "    val_log_marginal: 334.25822670610535\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -472.730469\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -403.609192\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -246.649673\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -522.931396\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -419.462250\n",
      "    epoch          : 438\n",
      "    loss           : -370.59376434326174\n",
      "    val_loss       : -345.9886073444039\n",
      "    val_log_likelihood: 452.9776947021484\n",
      "    val_log_marginal: 351.38062674092777\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -403.774933\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -444.977356\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -364.173279\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -228.002625\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -344.474243\n",
      "    epoch          : 439\n",
      "    loss           : -308.92312971115115\n",
      "    val_loss       : -283.99159203805027\n",
      "    val_log_likelihood: 436.0785171508789\n",
      "    val_log_marginal: 300.71401111446323\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -396.933075\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -49.136429\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -172.699387\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: 301.040405\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -307.831696\n",
      "    epoch          : 440\n",
      "    loss           : -176.59239659786223\n",
      "    val_loss       : -318.0050169765018\n",
      "    val_log_likelihood: 431.1409454345703\n",
      "    val_log_marginal: 323.768722614646\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch440.pth ...\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -142.505295\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -472.111481\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -425.166321\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -369.134460\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -395.027313\n",
      "    epoch          : 441\n",
      "    loss           : -346.8919366455078\n",
      "    val_loss       : -345.949130459968\n",
      "    val_log_likelihood: 447.17291259765625\n",
      "    val_log_marginal: 351.2902440112854\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -383.410767\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -493.301422\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -406.141998\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -453.255707\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -357.252625\n",
      "    epoch          : 442\n",
      "    loss           : -382.27186233520507\n",
      "    val_loss       : -358.26805102471263\n",
      "    val_log_likelihood: 448.40541687011716\n",
      "    val_log_marginal: 363.1422757875174\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -249.636200\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -362.598389\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -521.674988\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -415.377686\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -212.230820\n",
      "    epoch          : 443\n",
      "    loss           : -389.87694808959964\n",
      "    val_loss       : -366.94704071879386\n",
      "    val_log_likelihood: 456.67662353515624\n",
      "    val_log_marginal: 371.4233448293443\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -460.753479\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -419.986725\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -425.311890\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -460.713806\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -239.991333\n",
      "    epoch          : 444\n",
      "    loss           : -393.9770323181152\n",
      "    val_loss       : -365.61725901495663\n",
      "    val_log_likelihood: 455.52716369628905\n",
      "    val_log_marginal: 370.92923285476866\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -448.182465\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -416.926880\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -426.915192\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -414.757172\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -412.895264\n",
      "    epoch          : 445\n",
      "    loss           : -393.8620402526856\n",
      "    val_loss       : -358.1937795104459\n",
      "    val_log_likelihood: 456.4336334228516\n",
      "    val_log_marginal: 364.96440908834336\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -451.650848\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -524.202942\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -189.466980\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -388.607605\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -407.978363\n",
      "    epoch          : 446\n",
      "    loss           : -375.3865689086914\n",
      "    val_loss       : -355.5353121100925\n",
      "    val_log_likelihood: 453.09946899414064\n",
      "    val_log_marginal: 360.7061087090522\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -234.584778\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -396.151184\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -447.449768\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -355.577545\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -425.783936\n",
      "    epoch          : 447\n",
      "    loss           : -379.4948928833008\n",
      "    val_loss       : -356.80207906542347\n",
      "    val_log_likelihood: 455.6489990234375\n",
      "    val_log_marginal: 362.008867322281\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -421.549927\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -380.588989\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -413.963562\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -391.750244\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -359.225647\n",
      "    epoch          : 448\n",
      "    loss           : -360.8162554931641\n",
      "    val_loss       : -327.5410122845322\n",
      "    val_log_likelihood: 452.67529296875\n",
      "    val_log_marginal: 333.5847507152706\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -361.344635\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -180.789185\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -168.254761\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -393.734497\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -385.486298\n",
      "    epoch          : 449\n",
      "    loss           : -340.44722160339353\n",
      "    val_loss       : -330.91389872077855\n",
      "    val_log_likelihood: 445.645947265625\n",
      "    val_log_marginal: 335.98676264174287\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -461.435486\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -385.388885\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -387.417755\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -382.278961\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -360.568054\n",
      "    epoch          : 450\n",
      "    loss           : -329.3872189331055\n",
      "    val_loss       : -273.5650870825164\n",
      "    val_log_likelihood: 412.77420196533205\n",
      "    val_log_marginal: 280.4720120128244\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -121.684509\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -313.031036\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -392.358154\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -404.298248\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -190.560944\n",
      "    epoch          : 451\n",
      "    loss           : -334.5556380462647\n",
      "    val_loss       : -304.5155585197732\n",
      "    val_log_likelihood: 446.90639343261716\n",
      "    val_log_marginal: 315.3467359431088\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -216.503174\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -378.980865\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -352.771423\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -420.624786\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -218.394531\n",
      "    epoch          : 452\n",
      "    loss           : -349.7037473297119\n",
      "    val_loss       : -351.96139101795853\n",
      "    val_log_likelihood: 456.2575622558594\n",
      "    val_log_marginal: 357.08065008036795\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -427.878235\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -345.701965\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -507.045654\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -107.068954\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -384.687378\n",
      "    epoch          : 453\n",
      "    loss           : -361.92873947143556\n",
      "    val_loss       : -330.6320726213977\n",
      "    val_log_likelihood: 452.83815002441406\n",
      "    val_log_marginal: 337.6976608578116\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -396.238647\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -196.346680\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -407.939941\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -419.026306\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -403.366974\n",
      "    epoch          : 454\n",
      "    loss           : -379.79746520996093\n",
      "    val_loss       : -327.0622378911823\n",
      "    val_log_likelihood: 453.2930206298828\n",
      "    val_log_marginal: 330.9276228431614\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -344.252930\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -449.121277\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -203.943237\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -395.413452\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -351.573425\n",
      "    epoch          : 455\n",
      "    loss           : -373.40102096557615\n",
      "    val_loss       : -339.5645602433011\n",
      "    val_log_likelihood: 453.7187957763672\n",
      "    val_log_marginal: 344.03226030357183\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -370.329315\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -347.696411\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -440.687561\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -417.326752\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -203.704224\n",
      "    epoch          : 456\n",
      "    loss           : -372.241100769043\n",
      "    val_loss       : -339.50095100589095\n",
      "    val_log_likelihood: 453.8274658203125\n",
      "    val_log_marginal: 344.0899712827057\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -386.842468\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -453.350525\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -401.301422\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -337.521698\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -118.877304\n",
      "    epoch          : 457\n",
      "    loss           : -343.03091804504396\n",
      "    val_loss       : -302.59737623110414\n",
      "    val_log_likelihood: 441.76476135253904\n",
      "    val_log_marginal: 318.8793519835919\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -305.731537\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -323.735382\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -513.758179\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -348.150055\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -423.801086\n",
      "    epoch          : 458\n",
      "    loss           : -345.20615760803224\n",
      "    val_loss       : -345.5864459735341\n",
      "    val_log_likelihood: 446.5870162963867\n",
      "    val_log_marginal: 353.01668511772516\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -184.582443\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -173.078308\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -354.808807\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -405.970398\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -416.617889\n",
      "    epoch          : 459\n",
      "    loss           : -375.49160339355467\n",
      "    val_loss       : -358.14208952421325\n",
      "    val_log_likelihood: 456.528173828125\n",
      "    val_log_marginal: 362.75058620082984\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -367.690979\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -424.231415\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -399.841003\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -231.930481\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -240.956604\n",
      "    epoch          : 460\n",
      "    loss           : -382.75244140625\n",
      "    val_loss       : -351.79820639640093\n",
      "    val_log_likelihood: 456.4823486328125\n",
      "    val_log_marginal: 358.0477137427777\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch460.pth ...\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -375.304596\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -146.041107\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -133.376129\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -346.152771\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -347.965820\n",
      "    epoch          : 461\n",
      "    loss           : -321.74053337097166\n",
      "    val_loss       : -316.7379611644894\n",
      "    val_log_likelihood: 449.78920593261716\n",
      "    val_log_marginal: 327.2535292226821\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -369.751465\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -383.741699\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -390.670197\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -490.174438\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -412.053589\n",
      "    epoch          : 462\n",
      "    loss           : -341.7090774536133\n",
      "    val_loss       : -337.2438936223276\n",
      "    val_log_likelihood: 455.88771667480466\n",
      "    val_log_marginal: 346.2588687751442\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -374.438477\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -395.414490\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -212.251312\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -362.644836\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -371.979614\n",
      "    epoch          : 463\n",
      "    loss           : -383.8548773193359\n",
      "    val_loss       : -361.064923582226\n",
      "    val_log_likelihood: 460.8248229980469\n",
      "    val_log_marginal: 367.6740067217499\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -252.997803\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -419.363007\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -405.795258\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -410.213867\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -373.851654\n",
      "    epoch          : 464\n",
      "    loss           : -394.30449813842773\n",
      "    val_loss       : -361.54619100131094\n",
      "    val_log_likelihood: 459.7774688720703\n",
      "    val_log_marginal: 367.0464383906926\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -440.805023\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -413.422302\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -415.440216\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -352.444153\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -375.383789\n",
      "    epoch          : 465\n",
      "    loss           : -391.9051182556152\n",
      "    val_loss       : -358.23646819926796\n",
      "    val_log_likelihood: 458.1192687988281\n",
      "    val_log_marginal: 362.4445487725495\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -417.816071\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -461.322510\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -370.892242\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -377.106750\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -408.869568\n",
      "    epoch          : 466\n",
      "    loss           : -381.7922587585449\n",
      "    val_loss       : -348.7230495552532\n",
      "    val_log_likelihood: 454.9113067626953\n",
      "    val_log_marginal: 353.4601420681925\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -533.948730\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -368.770813\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -402.598297\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -436.118103\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: 81.387688\n",
      "    epoch          : 467\n",
      "    loss           : -353.5626747894287\n",
      "    val_loss       : -226.0089545656927\n",
      "    val_log_likelihood: 437.19922790527346\n",
      "    val_log_marginal: 239.75387706421324\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -103.955658\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -426.618591\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -242.942688\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -322.166809\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: 14.072040\n",
      "    epoch          : 468\n",
      "    loss           : -257.30615282058716\n",
      "    val_loss       : -303.2978913065046\n",
      "    val_log_likelihood: 444.58926849365236\n",
      "    val_log_marginal: 310.4250830169767\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -443.648315\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -149.783997\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -374.033020\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -206.302612\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -173.929199\n",
      "    epoch          : 469\n",
      "    loss           : -294.52381437301636\n",
      "    val_loss       : -328.9220855323598\n",
      "    val_log_likelihood: 442.77928924560547\n",
      "    val_log_marginal: 334.40655357949436\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -378.995483\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -150.399460\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -485.139771\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -413.236694\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -229.180939\n",
      "    epoch          : 470\n",
      "    loss           : -359.29375091552737\n",
      "    val_loss       : -356.3178054783493\n",
      "    val_log_likelihood: 461.7765167236328\n",
      "    val_log_marginal: 361.3571156222373\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -424.231689\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -420.614014\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -412.883545\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -424.459595\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -356.441589\n",
      "    epoch          : 471\n",
      "    loss           : -390.25764068603513\n",
      "    val_loss       : -368.23947982899847\n",
      "    val_log_likelihood: 465.8046417236328\n",
      "    val_log_marginal: 374.48897362835703\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -416.298157\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -356.348480\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -335.750275\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -356.916107\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -428.328735\n",
      "    epoch          : 472\n",
      "    loss           : -378.4285452270508\n",
      "    val_loss       : -355.22818421479315\n",
      "    val_log_likelihood: 461.95018005371094\n",
      "    val_log_marginal: 360.2487014915794\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -447.222687\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -414.034668\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -428.843933\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -209.361725\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -233.884155\n",
      "    epoch          : 473\n",
      "    loss           : -371.49556427001954\n",
      "    val_loss       : -356.8209168292582\n",
      "    val_log_likelihood: 457.5288848876953\n",
      "    val_log_marginal: 361.3852524567395\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -433.365967\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -400.415955\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -442.914215\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -421.448975\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -409.838379\n",
      "    epoch          : 474\n",
      "    loss           : -384.15143493652346\n",
      "    val_loss       : -339.9607105087489\n",
      "    val_log_likelihood: 458.92719116210935\n",
      "    val_log_marginal: 345.0069465760141\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -372.465851\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -372.031006\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -456.413544\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -401.754883\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -375.950439\n",
      "    epoch          : 475\n",
      "    loss           : -376.1852626037598\n",
      "    val_loss       : -325.582118452806\n",
      "    val_log_likelihood: 463.3630767822266\n",
      "    val_log_marginal: 331.12585960738363\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -349.711853\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -364.546448\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -508.939392\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -335.135071\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -407.569366\n",
      "    epoch          : 476\n",
      "    loss           : -365.8499116516113\n",
      "    val_loss       : -349.78181784069164\n",
      "    val_log_likelihood: 456.98742828369143\n",
      "    val_log_marginal: 354.4628315032856\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -412.396759\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -334.443481\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -422.533875\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -195.292847\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -231.206039\n",
      "    epoch          : 477\n",
      "    loss           : -366.8229032897949\n",
      "    val_loss       : -350.79848488820716\n",
      "    val_log_likelihood: 458.5997253417969\n",
      "    val_log_marginal: 357.2850239794701\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -403.662384\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -536.431274\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -371.165649\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -433.887848\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -383.347137\n",
      "    epoch          : 478\n",
      "    loss           : -381.8292109680176\n",
      "    val_loss       : -347.07431557783855\n",
      "    val_log_likelihood: 465.37677001953125\n",
      "    val_log_marginal: 351.24428750537334\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -229.901657\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -228.588760\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -116.732635\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -321.148682\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -301.068024\n",
      "    epoch          : 479\n",
      "    loss           : -298.7043547821045\n",
      "    val_loss       : -293.33847606843335\n",
      "    val_log_likelihood: 440.4030242919922\n",
      "    val_log_marginal: 302.80907059647143\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -274.511688\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -405.531311\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -515.530273\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -393.574097\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -427.695831\n",
      "    epoch          : 480\n",
      "    loss           : -374.6535105895996\n",
      "    val_loss       : -365.49999833861364\n",
      "    val_log_likelihood: 464.1803344726562\n",
      "    val_log_marginal: 370.3861593547326\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch480.pth ...\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -372.670776\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -373.917419\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -376.144165\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -408.091919\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -436.016449\n",
      "    epoch          : 481\n",
      "    loss           : -396.21775833129885\n",
      "    val_loss       : -364.40826496565717\n",
      "    val_log_likelihood: 465.307421875\n",
      "    val_log_marginal: 369.29181327186524\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -235.834961\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -379.010223\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -209.071457\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -455.857422\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -353.028290\n",
      "    epoch          : 482\n",
      "    loss           : -388.85065795898436\n",
      "    val_loss       : -360.3469718500972\n",
      "    val_log_likelihood: 461.2316467285156\n",
      "    val_log_marginal: 365.1740439836003\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -530.412109\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -433.362793\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -409.161377\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -449.398132\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -253.520477\n",
      "    epoch          : 483\n",
      "    loss           : -380.6979107666016\n",
      "    val_loss       : -351.7030717788264\n",
      "    val_log_likelihood: 457.8162780761719\n",
      "    val_log_marginal: 357.3079515065998\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -445.551392\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -454.759918\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -372.739136\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -343.906799\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -244.550751\n",
      "    epoch          : 484\n",
      "    loss           : -325.4805174064636\n",
      "    val_loss       : -248.08741777930408\n",
      "    val_log_likelihood: 447.39574127197267\n",
      "    val_log_marginal: 259.0117309998721\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -182.614899\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -404.424561\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -407.244690\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -371.555908\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -181.387009\n",
      "    epoch          : 485\n",
      "    loss           : -338.58764934539795\n",
      "    val_loss       : -338.6701256081462\n",
      "    val_log_likelihood: 450.2534744262695\n",
      "    val_log_marginal: 345.0371635463089\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -389.341461\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -425.837433\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -523.784180\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -337.723022\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -208.282623\n",
      "    epoch          : 486\n",
      "    loss           : -366.1952229309082\n",
      "    val_loss       : -342.4697757768445\n",
      "    val_log_likelihood: 455.1420379638672\n",
      "    val_log_marginal: 345.09310399517415\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -185.839600\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -401.895935\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -251.066132\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -411.715668\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -224.800873\n",
      "    epoch          : 487\n",
      "    loss           : -381.5174621582031\n",
      "    val_loss       : -354.41717637572435\n",
      "    val_log_likelihood: 464.5545928955078\n",
      "    val_log_marginal: 363.36202316097916\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -412.325928\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -380.066162\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -359.328156\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -227.362930\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -433.651672\n",
      "    epoch          : 488\n",
      "    loss           : -395.9740606689453\n",
      "    val_loss       : -367.20749308308586\n",
      "    val_log_likelihood: 466.36757202148436\n",
      "    val_log_marginal: 373.41168159432704\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -443.694794\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -423.381104\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -193.946198\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -228.949799\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -228.232269\n",
      "    epoch          : 489\n",
      "    loss           : -379.1695759582519\n",
      "    val_loss       : -347.00452062292027\n",
      "    val_log_likelihood: 464.89736328125\n",
      "    val_log_marginal: 354.5142080474645\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -405.272583\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -409.661865\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -478.076752\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -419.775696\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -210.959793\n",
      "    epoch          : 490\n",
      "    loss           : -365.374344329834\n",
      "    val_loss       : -347.27352514639495\n",
      "    val_log_likelihood: 460.0644561767578\n",
      "    val_log_marginal: 351.7148640330881\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -508.529205\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -202.883942\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -483.818573\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -227.603561\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -412.801544\n",
      "    epoch          : 491\n",
      "    loss           : -369.29493713378906\n",
      "    val_loss       : -353.4117493056692\n",
      "    val_log_likelihood: 467.0364471435547\n",
      "    val_log_marginal: 358.8778287095144\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -375.499756\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -214.246582\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -527.764954\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -189.584137\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -430.199921\n",
      "    epoch          : 492\n",
      "    loss           : -386.141886138916\n",
      "    val_loss       : -361.6682571968995\n",
      "    val_log_likelihood: 466.488720703125\n",
      "    val_log_marginal: 369.37531610541043\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -431.682861\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -185.129364\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -419.793396\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -420.158630\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -388.425140\n",
      "    epoch          : 493\n",
      "    loss           : -388.16919723510745\n",
      "    val_loss       : -364.10207155039535\n",
      "    val_log_likelihood: 466.85398559570314\n",
      "    val_log_marginal: 369.98796772224534\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -257.224030\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -375.731384\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -141.280121\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: 152.404541\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -25.142887\n",
      "    epoch          : 494\n",
      "    loss           : -257.5770527076721\n",
      "    val_loss       : -232.21834469363094\n",
      "    val_log_likelihood: 420.2610824584961\n",
      "    val_log_marginal: 241.46078219152986\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -105.151657\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -379.759064\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -370.585846\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -246.070236\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -360.280701\n",
      "    epoch          : 495\n",
      "    loss           : -350.89376152038574\n",
      "    val_loss       : -353.46711173383517\n",
      "    val_log_likelihood: 465.4238739013672\n",
      "    val_log_marginal: 358.80287541411815\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -419.051117\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -453.376190\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -207.349472\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -534.394592\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -356.511414\n",
      "    epoch          : 496\n",
      "    loss           : -381.21391632080076\n",
      "    val_loss       : -349.9607605278492\n",
      "    val_log_likelihood: 462.8571746826172\n",
      "    val_log_marginal: 356.9820953545033\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -195.455658\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -350.460419\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -349.659058\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -324.091492\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -367.997040\n",
      "    epoch          : 497\n",
      "    loss           : -282.8899925684929\n",
      "    val_loss       : -275.5820044271648\n",
      "    val_log_likelihood: 431.0972732543945\n",
      "    val_log_marginal: 285.63533740592436\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -304.887207\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -408.901459\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -412.952087\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -395.433136\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -192.744019\n",
      "    epoch          : 498\n",
      "    loss           : -347.7328955841065\n",
      "    val_loss       : -327.93571235369893\n",
      "    val_log_likelihood: 449.10466918945315\n",
      "    val_log_marginal: 340.2243745326996\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -170.079361\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -344.136078\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -261.910248\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -316.002014\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -187.413116\n",
      "    epoch          : 499\n",
      "    loss           : -331.1580307197571\n",
      "    val_loss       : -343.0075842601247\n",
      "    val_log_likelihood: 457.18421325683596\n",
      "    val_log_marginal: 353.39274715445936\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -523.128723\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -261.584778\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -397.404327\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -215.492111\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -371.412628\n",
      "    epoch          : 500\n",
      "    loss           : -381.2647807312012\n",
      "    val_loss       : -366.9909061432816\n",
      "    val_log_likelihood: 464.41258544921874\n",
      "    val_log_marginal: 370.90741795562207\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch500.pth ...\n",
      "Train Epoch: 501 [512/54000 (1%)] Loss: -435.015594\n",
      "Train Epoch: 501 [11776/54000 (22%)] Loss: -537.641785\n",
      "Train Epoch: 501 [23040/54000 (43%)] Loss: -250.721649\n",
      "Train Epoch: 501 [34304/54000 (64%)] Loss: -469.402893\n",
      "Train Epoch: 501 [45568/54000 (84%)] Loss: -234.718002\n",
      "    epoch          : 501\n",
      "    loss           : -397.58391098022463\n",
      "    val_loss       : -364.13096685260535\n",
      "    val_log_likelihood: 464.6815490722656\n",
      "    val_log_marginal: 368.9110826980322\n",
      "Train Epoch: 502 [512/54000 (1%)] Loss: -404.723724\n",
      "Train Epoch: 502 [11776/54000 (22%)] Loss: -430.335297\n",
      "Train Epoch: 502 [23040/54000 (43%)] Loss: -458.451843\n",
      "Train Epoch: 502 [34304/54000 (64%)] Loss: -444.336853\n",
      "Train Epoch: 502 [45568/54000 (84%)] Loss: -428.573914\n",
      "    epoch          : 502\n",
      "    loss           : -400.34706802368163\n",
      "    val_loss       : -369.3974194317125\n",
      "    val_log_likelihood: 468.9302032470703\n",
      "    val_log_marginal: 375.59719772972164\n",
      "Train Epoch: 503 [512/54000 (1%)] Loss: -239.378571\n",
      "Train Epoch: 503 [11776/54000 (22%)] Loss: -415.935211\n",
      "Train Epoch: 503 [23040/54000 (43%)] Loss: -451.589050\n",
      "Train Epoch: 503 [34304/54000 (64%)] Loss: -234.508881\n",
      "Train Epoch: 503 [45568/54000 (84%)] Loss: -434.595886\n",
      "    epoch          : 503\n",
      "    loss           : -395.8963624572754\n",
      "    val_loss       : -368.90742177348585\n",
      "    val_log_likelihood: 468.7618041992188\n",
      "    val_log_marginal: 372.3215625461191\n",
      "Train Epoch: 504 [512/54000 (1%)] Loss: -451.835236\n",
      "Train Epoch: 504 [11776/54000 (22%)] Loss: -411.186707\n",
      "Train Epoch: 504 [23040/54000 (43%)] Loss: -409.224792\n",
      "Train Epoch: 504 [34304/54000 (64%)] Loss: -406.753784\n",
      "Train Epoch: 504 [45568/54000 (84%)] Loss: -362.600616\n",
      "    epoch          : 504\n",
      "    loss           : -391.0191537475586\n",
      "    val_loss       : -358.56801927071064\n",
      "    val_log_likelihood: 468.7899627685547\n",
      "    val_log_marginal: 363.46189218633174\n",
      "Train Epoch: 505 [512/54000 (1%)] Loss: -385.275696\n",
      "Train Epoch: 505 [11776/54000 (22%)] Loss: -521.616943\n",
      "Train Epoch: 505 [23040/54000 (43%)] Loss: -394.233582\n",
      "Train Epoch: 505 [34304/54000 (64%)] Loss: -188.043625\n",
      "Train Epoch: 505 [45568/54000 (84%)] Loss: -425.014313\n",
      "    epoch          : 505\n",
      "    loss           : -365.3206967163086\n",
      "    val_loss       : -343.2039727703668\n",
      "    val_log_likelihood: 463.6835510253906\n",
      "    val_log_marginal: 351.34401214979584\n",
      "Train Epoch: 506 [512/54000 (1%)] Loss: -379.143616\n",
      "Train Epoch: 506 [11776/54000 (22%)] Loss: -389.088959\n",
      "Train Epoch: 506 [23040/54000 (43%)] Loss: -343.420349\n",
      "Train Epoch: 506 [34304/54000 (64%)] Loss: -244.314209\n",
      "Train Epoch: 506 [45568/54000 (84%)] Loss: -218.751617\n",
      "    epoch          : 506\n",
      "    loss           : -373.0846855163574\n",
      "    val_loss       : -362.296675208956\n",
      "    val_log_likelihood: 468.5179046630859\n",
      "    val_log_marginal: 367.9768988687545\n",
      "Train Epoch: 507 [512/54000 (1%)] Loss: -250.019196\n",
      "Train Epoch: 507 [11776/54000 (22%)] Loss: -374.208832\n",
      "Train Epoch: 507 [23040/54000 (43%)] Loss: -423.572968\n",
      "Train Epoch: 507 [34304/54000 (64%)] Loss: -231.780823\n",
      "Train Epoch: 507 [45568/54000 (84%)] Loss: -432.523346\n",
      "    epoch          : 507\n",
      "    loss           : -402.7968553161621\n",
      "    val_loss       : -375.8008368799463\n",
      "    val_log_likelihood: 469.6162841796875\n",
      "    val_log_marginal: 380.1571386944506\n",
      "Train Epoch: 508 [512/54000 (1%)] Loss: -548.869385\n",
      "Train Epoch: 508 [11776/54000 (22%)] Loss: -384.042358\n",
      "Train Epoch: 508 [23040/54000 (43%)] Loss: -423.665985\n",
      "Train Epoch: 508 [34304/54000 (64%)] Loss: -433.613037\n",
      "Train Epoch: 508 [45568/54000 (84%)] Loss: -418.322357\n",
      "    epoch          : 508\n",
      "    loss           : -398.32775512695315\n",
      "    val_loss       : -357.57128056455406\n",
      "    val_log_likelihood: 469.52109985351564\n",
      "    val_log_marginal: 361.8588741861284\n",
      "Train Epoch: 509 [512/54000 (1%)] Loss: -420.862732\n",
      "Train Epoch: 509 [11776/54000 (22%)] Loss: -251.441940\n",
      "Train Epoch: 509 [23040/54000 (43%)] Loss: -170.954453\n",
      "Train Epoch: 509 [34304/54000 (64%)] Loss: -397.150696\n",
      "Train Epoch: 509 [45568/54000 (84%)] Loss: -418.600342\n",
      "    epoch          : 509\n",
      "    loss           : -375.8361851501465\n",
      "    val_loss       : -352.766979244072\n",
      "    val_log_likelihood: 465.41453552246094\n",
      "    val_log_marginal: 359.1018319342285\n",
      "Train Epoch: 510 [512/54000 (1%)] Loss: -435.723328\n",
      "Train Epoch: 510 [11776/54000 (22%)] Loss: -221.882141\n",
      "Train Epoch: 510 [23040/54000 (43%)] Loss: -443.742950\n",
      "Train Epoch: 510 [34304/54000 (64%)] Loss: -420.778503\n",
      "Train Epoch: 510 [45568/54000 (84%)] Loss: -441.915344\n",
      "    epoch          : 510\n",
      "    loss           : -385.8139389038086\n",
      "    val_loss       : -365.43273997046055\n",
      "    val_log_likelihood: 468.5085174560547\n",
      "    val_log_marginal: 370.2822425108403\n",
      "Train Epoch: 511 [512/54000 (1%)] Loss: -416.170288\n",
      "Train Epoch: 511 [11776/54000 (22%)] Loss: -189.004120\n",
      "Train Epoch: 511 [23040/54000 (43%)] Loss: -384.900543\n",
      "Train Epoch: 511 [34304/54000 (64%)] Loss: -369.401978\n",
      "Train Epoch: 511 [45568/54000 (84%)] Loss: -376.103638\n",
      "    epoch          : 511\n",
      "    loss           : -313.5590161037445\n",
      "    val_loss       : -277.4910133957863\n",
      "    val_log_likelihood: 438.37679290771484\n",
      "    val_log_marginal: 279.3810110800282\n",
      "Train Epoch: 512 [512/54000 (1%)] Loss: -365.265869\n",
      "Train Epoch: 512 [11776/54000 (22%)] Loss: -492.824463\n",
      "Train Epoch: 512 [23040/54000 (43%)] Loss: -429.068451\n",
      "Train Epoch: 512 [34304/54000 (64%)] Loss: -392.444031\n",
      "Train Epoch: 512 [45568/54000 (84%)] Loss: -374.446228\n",
      "    epoch          : 512\n",
      "    loss           : -349.25704734802247\n",
      "    val_loss       : -333.11210437854754\n",
      "    val_log_likelihood: 460.84038696289065\n",
      "    val_log_marginal: 353.291377947107\n",
      "Train Epoch: 513 [512/54000 (1%)] Loss: -364.151947\n",
      "Train Epoch: 513 [11776/54000 (22%)] Loss: -381.950928\n",
      "Train Epoch: 513 [23040/54000 (43%)] Loss: -355.796936\n",
      "Train Epoch: 513 [34304/54000 (64%)] Loss: -379.710907\n",
      "Train Epoch: 513 [45568/54000 (84%)] Loss: -209.041504\n",
      "    epoch          : 513\n",
      "    loss           : -350.5804567718506\n",
      "    val_loss       : -345.9409431481734\n",
      "    val_log_likelihood: 456.305810546875\n",
      "    val_log_marginal: 352.15016675330696\n",
      "Train Epoch: 514 [512/54000 (1%)] Loss: -411.067291\n",
      "Train Epoch: 514 [11776/54000 (22%)] Loss: -354.887299\n",
      "Train Epoch: 514 [23040/54000 (43%)] Loss: -425.518372\n",
      "Train Epoch: 514 [34304/54000 (64%)] Loss: -365.274719\n",
      "Train Epoch: 514 [45568/54000 (84%)] Loss: -414.350525\n",
      "    epoch          : 514\n",
      "    loss           : -373.6764585876465\n",
      "    val_loss       : -360.5342634140514\n",
      "    val_log_likelihood: 468.2120880126953\n",
      "    val_log_marginal: 368.6363799688606\n",
      "Train Epoch: 515 [512/54000 (1%)] Loss: -533.669800\n",
      "Train Epoch: 515 [11776/54000 (22%)] Loss: -460.573059\n",
      "Train Epoch: 515 [23040/54000 (43%)] Loss: -214.537643\n",
      "Train Epoch: 515 [34304/54000 (64%)] Loss: -448.271606\n",
      "Train Epoch: 515 [45568/54000 (84%)] Loss: -439.691528\n",
      "    epoch          : 515\n",
      "    loss           : -392.4147247314453\n",
      "    val_loss       : -366.666148532182\n",
      "    val_log_likelihood: 467.9839752197266\n",
      "    val_log_marginal: 374.8788513116539\n",
      "Train Epoch: 516 [512/54000 (1%)] Loss: -268.280334\n",
      "Train Epoch: 516 [11776/54000 (22%)] Loss: -224.306534\n",
      "Train Epoch: 516 [23040/54000 (43%)] Loss: -361.711670\n",
      "Train Epoch: 516 [34304/54000 (64%)] Loss: -412.670410\n",
      "Train Epoch: 516 [45568/54000 (84%)] Loss: -388.701630\n",
      "    epoch          : 516\n",
      "    loss           : -400.1404447937012\n",
      "    val_loss       : -374.124198124744\n",
      "    val_log_likelihood: 474.0034454345703\n",
      "    val_log_marginal: 378.55691267587247\n",
      "Train Epoch: 517 [512/54000 (1%)] Loss: -446.377930\n",
      "Train Epoch: 517 [11776/54000 (22%)] Loss: -391.919250\n",
      "Train Epoch: 517 [23040/54000 (43%)] Loss: -379.512024\n",
      "Train Epoch: 517 [34304/54000 (64%)] Loss: -373.113281\n",
      "Train Epoch: 517 [45568/54000 (84%)] Loss: -248.501312\n",
      "    epoch          : 517\n",
      "    loss           : -399.1484718322754\n",
      "    val_loss       : -368.0343302812427\n",
      "    val_log_likelihood: 473.12013854980466\n",
      "    val_log_marginal: 373.56550822108983\n",
      "Train Epoch: 518 [512/54000 (1%)] Loss: -218.432587\n",
      "Train Epoch: 518 [11776/54000 (22%)] Loss: -413.972107\n",
      "Train Epoch: 518 [23040/54000 (43%)] Loss: -452.183319\n",
      "Train Epoch: 518 [34304/54000 (64%)] Loss: -428.459534\n",
      "Train Epoch: 518 [45568/54000 (84%)] Loss: -381.025208\n",
      "    epoch          : 518\n",
      "    loss           : -385.2791444396973\n",
      "    val_loss       : -336.6192567558028\n",
      "    val_log_likelihood: 466.12896118164065\n",
      "    val_log_marginal: 339.7923146950217\n",
      "Train Epoch: 519 [512/54000 (1%)] Loss: -239.149445\n",
      "Train Epoch: 519 [11776/54000 (22%)] Loss: -420.667847\n",
      "Train Epoch: 519 [23040/54000 (43%)] Loss: -254.757431\n",
      "Train Epoch: 519 [34304/54000 (64%)] Loss: -530.132080\n",
      "Train Epoch: 519 [45568/54000 (84%)] Loss: 33.566532\n",
      "    epoch          : 519\n",
      "    loss           : -347.83206384658814\n",
      "    val_loss       : -71.82381418794394\n",
      "    val_log_likelihood: 456.3231887817383\n",
      "    val_log_marginal: 100.85967771671713\n",
      "Train Epoch: 520 [512/54000 (1%)] Loss: 458.472931\n",
      "Train Epoch: 520 [11776/54000 (22%)] Loss: -392.068909\n",
      "Train Epoch: 520 [23040/54000 (43%)] Loss: -192.878571\n",
      "Train Epoch: 520 [34304/54000 (64%)] Loss: -179.281998\n",
      "Train Epoch: 520 [45568/54000 (84%)] Loss: -416.660095\n",
      "    epoch          : 520\n",
      "    loss           : -321.9172759628296\n",
      "    val_loss       : -342.0107793598436\n",
      "    val_log_likelihood: 465.7739532470703\n",
      "    val_log_marginal: 348.5675749707967\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch520.pth ...\n",
      "Train Epoch: 521 [512/54000 (1%)] Loss: -405.754120\n",
      "Train Epoch: 521 [11776/54000 (22%)] Loss: -394.948700\n",
      "Train Epoch: 521 [23040/54000 (43%)] Loss: -362.383667\n",
      "Train Epoch: 521 [34304/54000 (64%)] Loss: -257.799713\n",
      "Train Epoch: 521 [45568/54000 (84%)] Loss: -413.484283\n",
      "    epoch          : 521\n",
      "    loss           : -387.6178190612793\n",
      "    val_loss       : -366.359657779336\n",
      "    val_log_likelihood: 469.9578338623047\n",
      "    val_log_marginal: 371.6463140286505\n",
      "Train Epoch: 522 [512/54000 (1%)] Loss: -435.388977\n",
      "Train Epoch: 522 [11776/54000 (22%)] Loss: -406.377136\n",
      "Train Epoch: 522 [23040/54000 (43%)] Loss: -401.224884\n",
      "Train Epoch: 522 [34304/54000 (64%)] Loss: -241.985229\n",
      "Train Epoch: 522 [45568/54000 (84%)] Loss: -484.128632\n",
      "    epoch          : 522\n",
      "    loss           : -386.55275299072264\n",
      "    val_loss       : -366.8633049042895\n",
      "    val_log_likelihood: 470.16029357910156\n",
      "    val_log_marginal: 372.46989780977106\n",
      "Train Epoch: 523 [512/54000 (1%)] Loss: -419.044678\n",
      "Train Epoch: 523 [11776/54000 (22%)] Loss: -218.971024\n",
      "Train Epoch: 523 [23040/54000 (43%)] Loss: -398.999695\n",
      "Train Epoch: 523 [34304/54000 (64%)] Loss: -547.809937\n",
      "Train Epoch: 523 [45568/54000 (84%)] Loss: -225.915436\n",
      "    epoch          : 523\n",
      "    loss           : -402.0117269897461\n",
      "    val_loss       : -376.4874190410599\n",
      "    val_log_likelihood: 476.4698974609375\n",
      "    val_log_marginal: 382.6451634729087\n",
      "Train Epoch: 524 [512/54000 (1%)] Loss: -424.254425\n",
      "Train Epoch: 524 [11776/54000 (22%)] Loss: -436.552002\n",
      "Train Epoch: 524 [23040/54000 (43%)] Loss: -442.608521\n",
      "Train Epoch: 524 [34304/54000 (64%)] Loss: -471.365784\n",
      "Train Epoch: 524 [45568/54000 (84%)] Loss: -446.309479\n",
      "    epoch          : 524\n",
      "    loss           : -407.9547006225586\n",
      "    val_loss       : -374.5933676529676\n",
      "    val_log_likelihood: 476.52869873046876\n",
      "    val_log_marginal: 379.62263994775714\n",
      "Train Epoch: 525 [512/54000 (1%)] Loss: -247.258224\n",
      "Train Epoch: 525 [11776/54000 (22%)] Loss: -424.600952\n",
      "Train Epoch: 525 [23040/54000 (43%)] Loss: -440.594635\n",
      "Train Epoch: 525 [34304/54000 (64%)] Loss: -418.916809\n",
      "Train Epoch: 525 [45568/54000 (84%)] Loss: -404.867188\n",
      "    epoch          : 525\n",
      "    loss           : -405.8347673034668\n",
      "    val_loss       : -367.8199876557104\n",
      "    val_log_likelihood: 474.4038421630859\n",
      "    val_log_marginal: 373.5586674258113\n",
      "Train Epoch: 526 [512/54000 (1%)] Loss: -408.113342\n",
      "Train Epoch: 526 [11776/54000 (22%)] Loss: -457.735260\n",
      "Train Epoch: 526 [23040/54000 (43%)] Loss: -440.725311\n",
      "Train Epoch: 526 [34304/54000 (64%)] Loss: -541.165161\n",
      "Train Epoch: 526 [45568/54000 (84%)] Loss: -380.862030\n",
      "    epoch          : 526\n",
      "    loss           : -394.81018478393554\n",
      "    val_loss       : -369.7945978604257\n",
      "    val_log_likelihood: 474.4791198730469\n",
      "    val_log_marginal: 375.01055942960085\n",
      "Train Epoch: 527 [512/54000 (1%)] Loss: -425.135437\n",
      "Train Epoch: 527 [11776/54000 (22%)] Loss: -404.406433\n",
      "Train Epoch: 527 [23040/54000 (43%)] Loss: -428.871216\n",
      "Train Epoch: 527 [34304/54000 (64%)] Loss: -338.754761\n",
      "Train Epoch: 527 [45568/54000 (84%)] Loss: -274.166565\n",
      "    epoch          : 527\n",
      "    loss           : -311.5777941131592\n",
      "    val_loss       : -257.48013423010707\n",
      "    val_log_likelihood: 428.07059783935546\n",
      "    val_log_marginal: 281.8761856433004\n",
      "Train Epoch: 528 [512/54000 (1%)] Loss: -360.166962\n",
      "Train Epoch: 528 [11776/54000 (22%)] Loss: -327.552307\n",
      "Train Epoch: 528 [23040/54000 (43%)] Loss: -332.056854\n",
      "Train Epoch: 528 [34304/54000 (64%)] Loss: -52.765427\n",
      "Train Epoch: 528 [45568/54000 (84%)] Loss: -302.536194\n",
      "    epoch          : 528\n",
      "    loss           : -205.06179240226746\n",
      "    val_loss       : -155.71637096935882\n",
      "    val_log_likelihood: 431.11328277587893\n",
      "    val_log_marginal: 173.13656576387584\n",
      "Train Epoch: 529 [512/54000 (1%)] Loss: -229.867233\n",
      "Train Epoch: 529 [11776/54000 (22%)] Loss: -347.227570\n",
      "Train Epoch: 529 [23040/54000 (43%)] Loss: -389.271484\n",
      "Train Epoch: 529 [34304/54000 (64%)] Loss: -494.108490\n",
      "Train Epoch: 529 [45568/54000 (84%)] Loss: -410.353455\n",
      "    epoch          : 529\n",
      "    loss           : -334.97751113891604\n",
      "    val_loss       : -359.50934907523913\n",
      "    val_log_likelihood: 469.8345550537109\n",
      "    val_log_marginal: 366.8679685551673\n",
      "Train Epoch: 530 [512/54000 (1%)] Loss: -266.654083\n",
      "Train Epoch: 530 [11776/54000 (22%)] Loss: -194.636703\n",
      "Train Epoch: 530 [23040/54000 (43%)] Loss: -257.609344\n",
      "Train Epoch: 530 [34304/54000 (64%)] Loss: -230.439621\n",
      "Train Epoch: 530 [45568/54000 (84%)] Loss: -379.656738\n",
      "    epoch          : 530\n",
      "    loss           : -385.50378036499023\n",
      "    val_loss       : -369.6237920244224\n",
      "    val_log_likelihood: 470.61916198730466\n",
      "    val_log_marginal: 376.6103249069303\n",
      "Train Epoch: 531 [512/54000 (1%)] Loss: -441.834412\n",
      "Train Epoch: 531 [11776/54000 (22%)] Loss: -423.482483\n",
      "Train Epoch: 531 [23040/54000 (43%)] Loss: -400.957489\n",
      "Train Epoch: 531 [34304/54000 (64%)] Loss: -440.638184\n",
      "Train Epoch: 531 [45568/54000 (84%)] Loss: -95.982025\n",
      "    epoch          : 531\n",
      "    loss           : -355.97952012062075\n",
      "    val_loss       : -267.2661614398472\n",
      "    val_log_likelihood: 447.707373046875\n",
      "    val_log_marginal: 278.0563242219389\n",
      "Train Epoch: 532 [512/54000 (1%)] Loss: -318.511719\n",
      "Train Epoch: 532 [11776/54000 (22%)] Loss: -408.265137\n",
      "Train Epoch: 532 [23040/54000 (43%)] Loss: -408.711609\n",
      "Train Epoch: 532 [34304/54000 (64%)] Loss: -532.443848\n",
      "Train Epoch: 532 [45568/54000 (84%)] Loss: -232.257782\n",
      "    epoch          : 532\n",
      "    loss           : -359.94428245544435\n",
      "    val_loss       : -365.8056461967528\n",
      "    val_log_likelihood: 469.6781921386719\n",
      "    val_log_marginal: 369.4179117982316\n",
      "Train Epoch: 533 [512/54000 (1%)] Loss: -399.867249\n",
      "Train Epoch: 533 [11776/54000 (22%)] Loss: -442.081055\n",
      "Train Epoch: 533 [23040/54000 (43%)] Loss: -189.536514\n",
      "Train Epoch: 533 [34304/54000 (64%)] Loss: -435.158508\n",
      "Train Epoch: 533 [45568/54000 (84%)] Loss: -330.439880\n",
      "    epoch          : 533\n",
      "    loss           : -375.28415756225587\n",
      "    val_loss       : -317.73284196769816\n",
      "    val_log_likelihood: 450.75458374023435\n",
      "    val_log_marginal: 327.8432562816888\n",
      "Train Epoch: 534 [512/54000 (1%)] Loss: -326.644287\n",
      "Train Epoch: 534 [11776/54000 (22%)] Loss: -353.829834\n",
      "Train Epoch: 534 [23040/54000 (43%)] Loss: -294.772278\n",
      "Train Epoch: 534 [34304/54000 (64%)] Loss: -393.445831\n",
      "Train Epoch: 534 [45568/54000 (84%)] Loss: -145.289368\n",
      "    epoch          : 534\n",
      "    loss           : -340.97187072753906\n",
      "    val_loss       : -338.21460201442244\n",
      "    val_log_likelihood: 461.2259552001953\n",
      "    val_log_marginal: 343.44071548239634\n",
      "Train Epoch: 535 [512/54000 (1%)] Loss: -363.799561\n",
      "Train Epoch: 535 [11776/54000 (22%)] Loss: -425.490295\n",
      "Train Epoch: 535 [23040/54000 (43%)] Loss: -390.377655\n",
      "Train Epoch: 535 [34304/54000 (64%)] Loss: -434.789673\n",
      "Train Epoch: 535 [45568/54000 (84%)] Loss: -260.661926\n",
      "    epoch          : 535\n",
      "    loss           : -393.93439422607423\n",
      "    val_loss       : -370.6998755096458\n",
      "    val_log_likelihood: 472.7245147705078\n",
      "    val_log_marginal: 376.4693575043231\n",
      "Train Epoch: 536 [512/54000 (1%)] Loss: -556.649719\n",
      "Train Epoch: 536 [11776/54000 (22%)] Loss: -363.516968\n",
      "Train Epoch: 536 [23040/54000 (43%)] Loss: -547.930359\n",
      "Train Epoch: 536 [34304/54000 (64%)] Loss: -427.135651\n",
      "Train Epoch: 536 [45568/54000 (84%)] Loss: -235.661072\n",
      "    epoch          : 536\n",
      "    loss           : -396.20567504882814\n",
      "    val_loss       : -371.8978761302307\n",
      "    val_log_likelihood: 472.6376159667969\n",
      "    val_log_marginal: 376.1728578496724\n",
      "Train Epoch: 537 [512/54000 (1%)] Loss: -262.462219\n",
      "Train Epoch: 537 [11776/54000 (22%)] Loss: -227.794037\n",
      "Train Epoch: 537 [23040/54000 (43%)] Loss: -217.640930\n",
      "Train Epoch: 537 [34304/54000 (64%)] Loss: -0.464716\n",
      "Train Epoch: 537 [45568/54000 (84%)] Loss: -128.893631\n",
      "    epoch          : 537\n",
      "    loss           : -331.37390657424925\n",
      "    val_loss       : -305.7426977142692\n",
      "    val_log_likelihood: 450.10434875488284\n",
      "    val_log_marginal: 317.0052330877632\n",
      "Train Epoch: 538 [512/54000 (1%)] Loss: -125.314789\n",
      "Train Epoch: 538 [11776/54000 (22%)] Loss: -384.334473\n",
      "Train Epoch: 538 [23040/54000 (43%)] Loss: -152.390244\n",
      "Train Epoch: 538 [34304/54000 (64%)] Loss: -418.151733\n",
      "Train Epoch: 538 [45568/54000 (84%)] Loss: -408.852692\n",
      "    epoch          : 538\n",
      "    loss           : -367.8476232147217\n",
      "    val_loss       : -346.7118059094995\n",
      "    val_log_likelihood: 470.7693145751953\n",
      "    val_log_marginal: 355.79234732352205\n",
      "Train Epoch: 539 [512/54000 (1%)] Loss: -388.560364\n",
      "Train Epoch: 539 [11776/54000 (22%)] Loss: -356.086121\n",
      "Train Epoch: 539 [23040/54000 (43%)] Loss: -382.986908\n",
      "Train Epoch: 539 [34304/54000 (64%)] Loss: -198.202087\n",
      "Train Epoch: 539 [45568/54000 (84%)] Loss: -406.315735\n",
      "    epoch          : 539\n",
      "    loss           : -365.5623956298828\n",
      "    val_loss       : -365.96968468790874\n",
      "    val_log_likelihood: 469.9962158203125\n",
      "    val_log_marginal: 372.27869924642266\n",
      "Train Epoch: 540 [512/54000 (1%)] Loss: -536.396851\n",
      "Train Epoch: 540 [11776/54000 (22%)] Loss: -416.497559\n",
      "Train Epoch: 540 [23040/54000 (43%)] Loss: -425.148315\n",
      "Train Epoch: 540 [34304/54000 (64%)] Loss: -434.182098\n",
      "Train Epoch: 540 [45568/54000 (84%)] Loss: -367.595154\n",
      "    epoch          : 540\n",
      "    loss           : -380.5579702758789\n",
      "    val_loss       : -353.5597606537864\n",
      "    val_log_likelihood: 468.4643096923828\n",
      "    val_log_marginal: 359.89185134880245\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch540.pth ...\n",
      "Train Epoch: 541 [512/54000 (1%)] Loss: -240.174057\n",
      "Train Epoch: 541 [11776/54000 (22%)] Loss: -269.621399\n",
      "Train Epoch: 541 [23040/54000 (43%)] Loss: -431.495117\n",
      "Train Epoch: 541 [34304/54000 (64%)] Loss: -246.717560\n",
      "Train Epoch: 541 [45568/54000 (84%)] Loss: -208.599564\n",
      "    epoch          : 541\n",
      "    loss           : -395.5090557861328\n",
      "    val_loss       : -355.9561179068871\n",
      "    val_log_likelihood: 472.69169921875\n",
      "    val_log_marginal: 361.1433129088258\n",
      "Train Epoch: 542 [512/54000 (1%)] Loss: -525.298096\n",
      "Train Epoch: 542 [11776/54000 (22%)] Loss: -442.070526\n",
      "Train Epoch: 542 [23040/54000 (43%)] Loss: -202.433746\n",
      "Train Epoch: 542 [34304/54000 (64%)] Loss: -247.284424\n",
      "Train Epoch: 542 [45568/54000 (84%)] Loss: -245.234436\n",
      "    epoch          : 542\n",
      "    loss           : -382.46006515502927\n",
      "    val_loss       : -356.0932198899798\n",
      "    val_log_likelihood: 470.1310577392578\n",
      "    val_log_marginal: 360.9322112790279\n",
      "Train Epoch: 543 [512/54000 (1%)] Loss: -433.589294\n",
      "Train Epoch: 543 [11776/54000 (22%)] Loss: -415.714813\n",
      "Train Epoch: 543 [23040/54000 (43%)] Loss: -507.096161\n",
      "Train Epoch: 543 [34304/54000 (64%)] Loss: -206.338577\n",
      "Train Epoch: 543 [45568/54000 (84%)] Loss: -402.162415\n",
      "    epoch          : 543\n",
      "    loss           : -387.85173812866213\n",
      "    val_loss       : -374.5787299444899\n",
      "    val_log_likelihood: 473.9991851806641\n",
      "    val_log_marginal: 377.6808176604542\n",
      "Train Epoch: 544 [512/54000 (1%)] Loss: -544.945068\n",
      "Train Epoch: 544 [11776/54000 (22%)] Loss: -216.050842\n",
      "Train Epoch: 544 [23040/54000 (43%)] Loss: -452.716309\n",
      "Train Epoch: 544 [34304/54000 (64%)] Loss: -392.679321\n",
      "Train Epoch: 544 [45568/54000 (84%)] Loss: -391.067627\n",
      "    epoch          : 544\n",
      "    loss           : -397.8155487060547\n",
      "    val_loss       : -341.5986575106159\n",
      "    val_log_likelihood: 469.9265930175781\n",
      "    val_log_marginal: 347.4065469082445\n",
      "Train Epoch: 545 [512/54000 (1%)] Loss: -387.513916\n",
      "Train Epoch: 545 [11776/54000 (22%)] Loss: -439.909210\n",
      "Train Epoch: 545 [23040/54000 (43%)] Loss: -224.654678\n",
      "Train Epoch: 545 [34304/54000 (64%)] Loss: -464.863342\n",
      "Train Epoch: 545 [45568/54000 (84%)] Loss: -252.570038\n",
      "    epoch          : 545\n",
      "    loss           : -389.6472785949707\n",
      "    val_loss       : -362.9273921361193\n",
      "    val_log_likelihood: 471.2000305175781\n",
      "    val_log_marginal: 368.59577425606557\n",
      "Train Epoch: 546 [512/54000 (1%)] Loss: -439.601440\n",
      "Train Epoch: 546 [11776/54000 (22%)] Loss: -422.810913\n",
      "Train Epoch: 546 [23040/54000 (43%)] Loss: -256.332764\n",
      "Train Epoch: 546 [34304/54000 (64%)] Loss: -427.106812\n",
      "Train Epoch: 546 [45568/54000 (84%)] Loss: -350.796234\n",
      "    epoch          : 546\n",
      "    loss           : -389.29053955078126\n",
      "    val_loss       : -356.49647208759563\n",
      "    val_log_likelihood: 468.05738830566406\n",
      "    val_log_marginal: 360.77695236392316\n",
      "Train Epoch: 547 [512/54000 (1%)] Loss: -411.491974\n",
      "Train Epoch: 547 [11776/54000 (22%)] Loss: -446.156738\n",
      "Train Epoch: 547 [23040/54000 (43%)] Loss: -424.225891\n",
      "Train Epoch: 547 [34304/54000 (64%)] Loss: -370.257080\n",
      "Train Epoch: 547 [45568/54000 (84%)] Loss: -340.086914\n",
      "    epoch          : 547\n",
      "    loss           : -353.9174323272705\n",
      "    val_loss       : -307.89594846814873\n",
      "    val_log_likelihood: 451.71295166015625\n",
      "    val_log_marginal: 315.8882462281734\n",
      "Train Epoch: 548 [512/54000 (1%)] Loss: -332.196320\n",
      "Train Epoch: 548 [11776/54000 (22%)] Loss: -408.801453\n",
      "Train Epoch: 548 [23040/54000 (43%)] Loss: -395.188599\n",
      "Train Epoch: 548 [34304/54000 (64%)] Loss: -203.523743\n",
      "Train Epoch: 548 [45568/54000 (84%)] Loss: -223.737061\n",
      "    epoch          : 548\n",
      "    loss           : -374.4306033325195\n",
      "    val_loss       : -366.0515619234182\n",
      "    val_log_likelihood: 472.778173828125\n",
      "    val_log_marginal: 371.099860439077\n",
      "Train Epoch: 549 [512/54000 (1%)] Loss: -435.877563\n",
      "Train Epoch: 549 [11776/54000 (22%)] Loss: -404.611542\n",
      "Train Epoch: 549 [23040/54000 (43%)] Loss: -416.266296\n",
      "Train Epoch: 549 [34304/54000 (64%)] Loss: -470.197693\n",
      "Train Epoch: 549 [45568/54000 (84%)] Loss: -280.560303\n",
      "    epoch          : 549\n",
      "    loss           : -383.90858154296876\n",
      "    val_loss       : -316.86313685281203\n",
      "    val_log_likelihood: 468.65882873535156\n",
      "    val_log_marginal: 325.7525997620076\n",
      "Train Epoch: 550 [512/54000 (1%)] Loss: -336.222198\n",
      "Train Epoch: 550 [11776/54000 (22%)] Loss: -420.919067\n",
      "Train Epoch: 550 [23040/54000 (43%)] Loss: -465.456238\n",
      "Train Epoch: 550 [34304/54000 (64%)] Loss: -431.891907\n",
      "Train Epoch: 550 [45568/54000 (84%)] Loss: -418.473328\n",
      "    epoch          : 550\n",
      "    loss           : -372.7668615722656\n",
      "    val_loss       : -350.3409544968978\n",
      "    val_log_likelihood: 468.4749725341797\n",
      "    val_log_marginal: 358.6139676835395\n",
      "Train Epoch: 551 [512/54000 (1%)] Loss: -518.686401\n",
      "Train Epoch: 551 [11776/54000 (22%)] Loss: -419.435547\n",
      "Train Epoch: 551 [23040/54000 (43%)] Loss: -566.632751\n",
      "Train Epoch: 551 [34304/54000 (64%)] Loss: -231.862747\n",
      "Train Epoch: 551 [45568/54000 (84%)] Loss: -467.975098\n",
      "    epoch          : 551\n",
      "    loss           : -403.05431747436523\n",
      "    val_loss       : -377.7487065754831\n",
      "    val_log_likelihood: 476.9415618896484\n",
      "    val_log_marginal: 382.57082798853514\n",
      "Train Epoch: 552 [512/54000 (1%)] Loss: -214.048126\n",
      "Train Epoch: 552 [11776/54000 (22%)] Loss: -244.280853\n",
      "Train Epoch: 552 [23040/54000 (43%)] Loss: -420.967468\n",
      "Train Epoch: 552 [34304/54000 (64%)] Loss: -444.101379\n",
      "Train Epoch: 552 [45568/54000 (84%)] Loss: -248.639725\n",
      "    epoch          : 552\n",
      "    loss           : -401.670665435791\n",
      "    val_loss       : -356.78235365636647\n",
      "    val_log_likelihood: 474.53789978027345\n",
      "    val_log_marginal: 362.5285332988948\n",
      "Train Epoch: 553 [512/54000 (1%)] Loss: -420.506958\n",
      "Train Epoch: 553 [11776/54000 (22%)] Loss: -461.480255\n",
      "Train Epoch: 553 [23040/54000 (43%)] Loss: -387.844788\n",
      "Train Epoch: 553 [34304/54000 (64%)] Loss: -539.397461\n",
      "Train Epoch: 553 [45568/54000 (84%)] Loss: -449.228912\n",
      "    epoch          : 553\n",
      "    loss           : -401.37286819458006\n",
      "    val_loss       : -376.287087503355\n",
      "    val_log_likelihood: 480.44751892089846\n",
      "    val_log_marginal: 382.34214116819203\n",
      "Train Epoch: 554 [512/54000 (1%)] Loss: -251.618530\n",
      "Train Epoch: 554 [11776/54000 (22%)] Loss: -573.447144\n",
      "Train Epoch: 554 [23040/54000 (43%)] Loss: -398.565857\n",
      "Train Epoch: 554 [34304/54000 (64%)] Loss: -434.921265\n",
      "Train Epoch: 554 [45568/54000 (84%)] Loss: -431.134277\n",
      "    epoch          : 554\n",
      "    loss           : -410.2013607788086\n",
      "    val_loss       : -378.18025608900933\n",
      "    val_log_likelihood: 479.13963012695314\n",
      "    val_log_marginal: 383.3792592417449\n",
      "Train Epoch: 555 [512/54000 (1%)] Loss: -448.283691\n",
      "Train Epoch: 555 [11776/54000 (22%)] Loss: -445.274567\n",
      "Train Epoch: 555 [23040/54000 (43%)] Loss: -173.459702\n",
      "Train Epoch: 555 [34304/54000 (64%)] Loss: -446.816498\n",
      "Train Epoch: 555 [45568/54000 (84%)] Loss: -214.000732\n",
      "    epoch          : 555\n",
      "    loss           : -405.5814839172363\n",
      "    val_loss       : -375.2806585362181\n",
      "    val_log_likelihood: 480.7867950439453\n",
      "    val_log_marginal: 381.14015035293994\n",
      "Train Epoch: 556 [512/54000 (1%)] Loss: -438.538452\n",
      "Train Epoch: 556 [11776/54000 (22%)] Loss: -539.089905\n",
      "Train Epoch: 556 [23040/54000 (43%)] Loss: -412.827911\n",
      "Train Epoch: 556 [34304/54000 (64%)] Loss: -458.541565\n",
      "Train Epoch: 556 [45568/54000 (84%)] Loss: -445.959839\n",
      "    epoch          : 556\n",
      "    loss           : -393.1684336853027\n",
      "    val_loss       : -352.55599627997725\n",
      "    val_log_likelihood: 476.00889892578124\n",
      "    val_log_marginal: 361.30620199851694\n",
      "Train Epoch: 557 [512/54000 (1%)] Loss: -502.289490\n",
      "Train Epoch: 557 [11776/54000 (22%)] Loss: -435.456085\n",
      "Train Epoch: 557 [23040/54000 (43%)] Loss: -160.201782\n",
      "Train Epoch: 557 [34304/54000 (64%)] Loss: -409.977509\n",
      "Train Epoch: 557 [45568/54000 (84%)] Loss: -411.667480\n",
      "    epoch          : 557\n",
      "    loss           : -385.7065156555176\n",
      "    val_loss       : -371.17226239005106\n",
      "    val_log_likelihood: 479.7830413818359\n",
      "    val_log_marginal: 377.7313512545079\n",
      "Train Epoch: 558 [512/54000 (1%)] Loss: -254.957733\n",
      "Train Epoch: 558 [11776/54000 (22%)] Loss: -229.873154\n",
      "Train Epoch: 558 [23040/54000 (43%)] Loss: -557.055664\n",
      "Train Epoch: 558 [34304/54000 (64%)] Loss: -329.795471\n",
      "Train Epoch: 558 [45568/54000 (84%)] Loss: -315.802246\n",
      "    epoch          : 558\n",
      "    loss           : -391.67528121948243\n",
      "    val_loss       : -357.4219312835485\n",
      "    val_log_likelihood: 472.084619140625\n",
      "    val_log_marginal: 361.1460578273982\n",
      "Train Epoch: 559 [512/54000 (1%)] Loss: -415.893799\n",
      "Train Epoch: 559 [11776/54000 (22%)] Loss: -387.647339\n",
      "Train Epoch: 559 [23040/54000 (43%)] Loss: -350.502289\n",
      "Train Epoch: 559 [34304/54000 (64%)] Loss: -383.808289\n",
      "Train Epoch: 559 [45568/54000 (84%)] Loss: -14.622715\n",
      "    epoch          : 559\n",
      "    loss           : -303.725582652092\n",
      "    val_loss       : -151.65654726848007\n",
      "    val_log_likelihood: 456.1681304931641\n",
      "    val_log_marginal: 175.10783147774637\n",
      "Train Epoch: 560 [512/54000 (1%)] Loss: -324.213806\n",
      "Train Epoch: 560 [11776/54000 (22%)] Loss: -106.533829\n",
      "Train Epoch: 560 [23040/54000 (43%)] Loss: -485.182770\n",
      "Train Epoch: 560 [34304/54000 (64%)] Loss: -359.201935\n",
      "Train Epoch: 560 [45568/54000 (84%)] Loss: -310.898926\n",
      "    epoch          : 560\n",
      "    loss           : -224.25063868522645\n",
      "    val_loss       : -262.43558044899254\n",
      "    val_log_likelihood: 436.1664154052734\n",
      "    val_log_marginal: 272.3430178705603\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch560.pth ...\n",
      "Train Epoch: 561 [512/54000 (1%)] Loss: -315.424316\n",
      "Train Epoch: 561 [11776/54000 (22%)] Loss: -422.536682\n",
      "Train Epoch: 561 [23040/54000 (43%)] Loss: -517.371765\n",
      "Train Epoch: 561 [34304/54000 (64%)] Loss: -356.673492\n",
      "Train Epoch: 561 [45568/54000 (84%)] Loss: -433.452820\n",
      "    epoch          : 561\n",
      "    loss           : -333.7974964904785\n",
      "    val_loss       : -343.9302936175838\n",
      "    val_log_likelihood: 468.8115539550781\n",
      "    val_log_marginal: 353.6022307667881\n",
      "Train Epoch: 562 [512/54000 (1%)] Loss: -393.205078\n",
      "Train Epoch: 562 [11776/54000 (22%)] Loss: -442.983124\n",
      "Train Epoch: 562 [23040/54000 (43%)] Loss: -432.416260\n",
      "Train Epoch: 562 [34304/54000 (64%)] Loss: -424.919281\n",
      "Train Epoch: 562 [45568/54000 (84%)] Loss: -357.467102\n",
      "    epoch          : 562\n",
      "    loss           : -380.0608332824707\n",
      "    val_loss       : -352.0314032847062\n",
      "    val_log_likelihood: 471.04047241210935\n",
      "    val_log_marginal: 357.72391930408776\n",
      "Train Epoch: 563 [512/54000 (1%)] Loss: -407.414886\n",
      "Train Epoch: 563 [11776/54000 (22%)] Loss: -250.827835\n",
      "Train Epoch: 563 [23040/54000 (43%)] Loss: -211.427002\n",
      "Train Epoch: 563 [34304/54000 (64%)] Loss: -447.784393\n",
      "Train Epoch: 563 [45568/54000 (84%)] Loss: -210.965942\n",
      "    epoch          : 563\n",
      "    loss           : -390.34105895996095\n",
      "    val_loss       : -367.423949254863\n",
      "    val_log_likelihood: 471.08118896484376\n",
      "    val_log_marginal: 373.4052201155573\n",
      "Train Epoch: 564 [512/54000 (1%)] Loss: -254.109528\n",
      "Train Epoch: 564 [11776/54000 (22%)] Loss: -479.293091\n",
      "Train Epoch: 564 [23040/54000 (43%)] Loss: -459.017487\n",
      "Train Epoch: 564 [34304/54000 (64%)] Loss: -215.417755\n",
      "Train Epoch: 564 [45568/54000 (84%)] Loss: -384.748474\n",
      "    epoch          : 564\n",
      "    loss           : -402.8908773803711\n",
      "    val_loss       : -375.07314523439857\n",
      "    val_log_likelihood: 478.7033203125\n",
      "    val_log_marginal: 379.79970364011825\n",
      "Train Epoch: 565 [512/54000 (1%)] Loss: -478.686218\n",
      "Train Epoch: 565 [11776/54000 (22%)] Loss: -430.495728\n",
      "Train Epoch: 565 [23040/54000 (43%)] Loss: -551.584290\n",
      "Train Epoch: 565 [34304/54000 (64%)] Loss: -455.559540\n",
      "Train Epoch: 565 [45568/54000 (84%)] Loss: -338.408356\n",
      "    epoch          : 565\n",
      "    loss           : -401.9634065246582\n",
      "    val_loss       : -338.97600776581095\n",
      "    val_log_likelihood: 464.13453369140626\n",
      "    val_log_marginal: 345.9840808428824\n",
      "Train Epoch: 566 [512/54000 (1%)] Loss: -360.122437\n",
      "Train Epoch: 566 [11776/54000 (22%)] Loss: -394.837433\n",
      "Train Epoch: 566 [23040/54000 (43%)] Loss: -360.394470\n",
      "Train Epoch: 566 [34304/54000 (64%)] Loss: -436.004120\n",
      "Train Epoch: 566 [45568/54000 (84%)] Loss: -425.702606\n",
      "    epoch          : 566\n",
      "    loss           : -387.7088883972168\n",
      "    val_loss       : -368.5289585395716\n",
      "    val_log_likelihood: 480.3879089355469\n",
      "    val_log_marginal: 371.4533558409661\n",
      "Train Epoch: 567 [512/54000 (1%)] Loss: -396.662781\n",
      "Train Epoch: 567 [11776/54000 (22%)] Loss: -442.201477\n",
      "Train Epoch: 567 [23040/54000 (43%)] Loss: -471.626343\n",
      "Train Epoch: 567 [34304/54000 (64%)] Loss: -420.401001\n",
      "Train Epoch: 567 [45568/54000 (84%)] Loss: -234.400116\n",
      "    epoch          : 567\n",
      "    loss           : -399.4865982055664\n",
      "    val_loss       : -368.65291097806767\n",
      "    val_log_likelihood: 473.8312744140625\n",
      "    val_log_marginal: 373.79700678251686\n",
      "Train Epoch: 568 [512/54000 (1%)] Loss: -448.699738\n",
      "Train Epoch: 568 [11776/54000 (22%)] Loss: -432.569580\n",
      "Train Epoch: 568 [23040/54000 (43%)] Loss: -442.607605\n",
      "Train Epoch: 568 [34304/54000 (64%)] Loss: -420.797119\n",
      "Train Epoch: 568 [45568/54000 (84%)] Loss: -408.926788\n",
      "    epoch          : 568\n",
      "    loss           : -377.1328953552246\n",
      "    val_loss       : -357.49879444008695\n",
      "    val_log_likelihood: 470.5124755859375\n",
      "    val_log_marginal: 359.58778792843225\n",
      "Train Epoch: 569 [512/54000 (1%)] Loss: -203.057892\n",
      "Train Epoch: 569 [11776/54000 (22%)] Loss: -411.114258\n",
      "Train Epoch: 569 [23040/54000 (43%)] Loss: -476.582184\n",
      "Train Epoch: 569 [34304/54000 (64%)] Loss: -436.633575\n",
      "Train Epoch: 569 [45568/54000 (84%)] Loss: -427.985016\n",
      "    epoch          : 569\n",
      "    loss           : -402.9411473083496\n",
      "    val_loss       : -370.3434300103225\n",
      "    val_log_likelihood: 480.1840484619141\n",
      "    val_log_marginal: 375.86427435986695\n",
      "Train Epoch: 570 [512/54000 (1%)] Loss: -479.172852\n",
      "Train Epoch: 570 [11776/54000 (22%)] Loss: -435.440582\n",
      "Train Epoch: 570 [23040/54000 (43%)] Loss: -527.758667\n",
      "Train Epoch: 570 [34304/54000 (64%)] Loss: -389.174866\n",
      "Train Epoch: 570 [45568/54000 (84%)] Loss: -107.916153\n",
      "    epoch          : 570\n",
      "    loss           : -357.0139685058594\n",
      "    val_loss       : -303.7096298900433\n",
      "    val_log_likelihood: 460.3132049560547\n",
      "    val_log_marginal: 309.01431870646775\n",
      "Train Epoch: 571 [512/54000 (1%)] Loss: -349.333649\n",
      "Train Epoch: 571 [11776/54000 (22%)] Loss: -369.212738\n",
      "Train Epoch: 571 [23040/54000 (43%)] Loss: -303.198334\n",
      "Train Epoch: 571 [34304/54000 (64%)] Loss: -126.261497\n",
      "Train Epoch: 571 [45568/54000 (84%)] Loss: -313.527039\n",
      "    epoch          : 571\n",
      "    loss           : -336.3422853088379\n",
      "    val_loss       : -332.03531656842677\n",
      "    val_log_likelihood: 462.591325378418\n",
      "    val_log_marginal: 342.8586712229997\n",
      "Train Epoch: 572 [512/54000 (1%)] Loss: -159.977692\n",
      "Train Epoch: 572 [11776/54000 (22%)] Loss: -405.555542\n",
      "Train Epoch: 572 [23040/54000 (43%)] Loss: -366.908905\n",
      "Train Epoch: 572 [34304/54000 (64%)] Loss: -375.472504\n",
      "Train Epoch: 572 [45568/54000 (84%)] Loss: -381.218781\n",
      "    epoch          : 572\n",
      "    loss           : -376.84960784912107\n",
      "    val_loss       : -361.13572469726205\n",
      "    val_log_likelihood: 472.2707763671875\n",
      "    val_log_marginal: 371.4876605693251\n",
      "Train Epoch: 573 [512/54000 (1%)] Loss: -435.651703\n",
      "Train Epoch: 573 [11776/54000 (22%)] Loss: -228.174164\n",
      "Train Epoch: 573 [23040/54000 (43%)] Loss: -406.851501\n",
      "Train Epoch: 573 [34304/54000 (64%)] Loss: -405.356812\n",
      "Train Epoch: 573 [45568/54000 (84%)] Loss: -446.337463\n",
      "    epoch          : 573\n",
      "    loss           : -393.4868380737305\n",
      "    val_loss       : -375.3409306387417\n",
      "    val_log_likelihood: 478.0119934082031\n",
      "    val_log_marginal: 382.9942122968071\n",
      "Train Epoch: 574 [512/54000 (1%)] Loss: -470.767914\n",
      "Train Epoch: 574 [11776/54000 (22%)] Loss: -416.206726\n",
      "Train Epoch: 574 [23040/54000 (43%)] Loss: -392.385040\n",
      "Train Epoch: 574 [34304/54000 (64%)] Loss: -467.706024\n",
      "Train Epoch: 574 [45568/54000 (84%)] Loss: -379.923309\n",
      "    epoch          : 574\n",
      "    loss           : -413.34021148681643\n",
      "    val_loss       : -386.5686674202792\n",
      "    val_log_likelihood: 484.2171295166016\n",
      "    val_log_marginal: 390.9019772928208\n",
      "Train Epoch: 575 [512/54000 (1%)] Loss: -453.322144\n",
      "Train Epoch: 575 [11776/54000 (22%)] Loss: -416.860931\n",
      "Train Epoch: 575 [23040/54000 (43%)] Loss: -453.642456\n",
      "Train Epoch: 575 [34304/54000 (64%)] Loss: -435.006714\n",
      "Train Epoch: 575 [45568/54000 (84%)] Loss: -400.830658\n",
      "    epoch          : 575\n",
      "    loss           : -416.50106857299807\n",
      "    val_loss       : -385.03434734465554\n",
      "    val_log_likelihood: 481.62103576660155\n",
      "    val_log_marginal: 389.2899932574481\n",
      "Train Epoch: 576 [512/54000 (1%)] Loss: -471.489990\n",
      "Train Epoch: 576 [11776/54000 (22%)] Loss: -238.316406\n",
      "Train Epoch: 576 [23040/54000 (43%)] Loss: -428.854218\n",
      "Train Epoch: 576 [34304/54000 (64%)] Loss: -259.343628\n",
      "Train Epoch: 576 [45568/54000 (84%)] Loss: -264.479401\n",
      "    epoch          : 576\n",
      "    loss           : -412.9643716430664\n",
      "    val_loss       : -374.81566825956105\n",
      "    val_log_likelihood: 481.4482849121094\n",
      "    val_log_marginal: 377.86279553739485\n",
      "Train Epoch: 577 [512/54000 (1%)] Loss: -423.152863\n",
      "Train Epoch: 577 [11776/54000 (22%)] Loss: -391.981293\n",
      "Train Epoch: 577 [23040/54000 (43%)] Loss: -397.886749\n",
      "Train Epoch: 577 [34304/54000 (64%)] Loss: -255.823959\n",
      "Train Epoch: 577 [45568/54000 (84%)] Loss: -257.491852\n",
      "    epoch          : 577\n",
      "    loss           : -412.52299133300784\n",
      "    val_loss       : -369.829870583117\n",
      "    val_log_likelihood: 480.73292236328126\n",
      "    val_log_marginal: 375.6679183550179\n",
      "Train Epoch: 578 [512/54000 (1%)] Loss: -209.128952\n",
      "Train Epoch: 578 [11776/54000 (22%)] Loss: -469.475830\n",
      "Train Epoch: 578 [23040/54000 (43%)] Loss: -411.069427\n",
      "Train Epoch: 578 [34304/54000 (64%)] Loss: -252.864288\n",
      "Train Epoch: 578 [45568/54000 (84%)] Loss: -460.637848\n",
      "    epoch          : 578\n",
      "    loss           : -389.8491452026367\n",
      "    val_loss       : -354.4064429452643\n",
      "    val_log_likelihood: 477.37305908203126\n",
      "    val_log_marginal: 358.8113241203132\n",
      "Train Epoch: 579 [512/54000 (1%)] Loss: -417.942017\n",
      "Train Epoch: 579 [11776/54000 (22%)] Loss: -454.357208\n",
      "Train Epoch: 579 [23040/54000 (43%)] Loss: -445.986359\n",
      "Train Epoch: 579 [34304/54000 (64%)] Loss: -252.939819\n",
      "Train Epoch: 579 [45568/54000 (84%)] Loss: -418.768494\n",
      "    epoch          : 579\n",
      "    loss           : -394.39342880249023\n",
      "    val_loss       : -358.2948510803282\n",
      "    val_log_likelihood: 481.5013946533203\n",
      "    val_log_marginal: 363.0176341932267\n",
      "Train Epoch: 580 [512/54000 (1%)] Loss: -405.449524\n",
      "Train Epoch: 580 [11776/54000 (22%)] Loss: -394.408752\n",
      "Train Epoch: 580 [23040/54000 (43%)] Loss: -329.957031\n",
      "Train Epoch: 580 [34304/54000 (64%)] Loss: -366.766479\n",
      "Train Epoch: 580 [45568/54000 (84%)] Loss: -242.388123\n",
      "    epoch          : 580\n",
      "    loss           : -383.6464208984375\n",
      "    val_loss       : -371.1126530529931\n",
      "    val_log_likelihood: 479.3978973388672\n",
      "    val_log_marginal: 376.5490631792694\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch580.pth ...\n",
      "Train Epoch: 581 [512/54000 (1%)] Loss: -437.223236\n",
      "Train Epoch: 581 [11776/54000 (22%)] Loss: -226.672440\n",
      "Train Epoch: 581 [23040/54000 (43%)] Loss: -478.270020\n",
      "Train Epoch: 581 [34304/54000 (64%)] Loss: -439.545441\n",
      "Train Epoch: 581 [45568/54000 (84%)] Loss: -459.952026\n",
      "    epoch          : 581\n",
      "    loss           : -410.8188752746582\n",
      "    val_loss       : -377.7443409351632\n",
      "    val_log_likelihood: 484.5737640380859\n",
      "    val_log_marginal: 382.5081410359591\n",
      "Train Epoch: 582 [512/54000 (1%)] Loss: -449.490906\n",
      "Train Epoch: 582 [11776/54000 (22%)] Loss: -223.816132\n",
      "Train Epoch: 582 [23040/54000 (43%)] Loss: -567.077209\n",
      "Train Epoch: 582 [34304/54000 (64%)] Loss: -399.318237\n",
      "Train Epoch: 582 [45568/54000 (84%)] Loss: -425.543091\n",
      "    epoch          : 582\n",
      "    loss           : -382.7656290435791\n",
      "    val_loss       : -348.5461671705358\n",
      "    val_log_likelihood: 478.50962219238284\n",
      "    val_log_marginal: 357.9292607095093\n",
      "Train Epoch: 583 [512/54000 (1%)] Loss: -401.437134\n",
      "Train Epoch: 583 [11776/54000 (22%)] Loss: -365.700806\n",
      "Train Epoch: 583 [23040/54000 (43%)] Loss: -393.614197\n",
      "Train Epoch: 583 [34304/54000 (64%)] Loss: -303.926849\n",
      "Train Epoch: 583 [45568/54000 (84%)] Loss: -282.044220\n",
      "    epoch          : 583\n",
      "    loss           : -357.27064193725585\n",
      "    val_loss       : -315.02515487233177\n",
      "    val_log_likelihood: 469.2111541748047\n",
      "    val_log_marginal: 323.56109322197733\n",
      "Train Epoch: 584 [512/54000 (1%)] Loss: -386.438416\n",
      "Train Epoch: 584 [11776/54000 (22%)] Loss: -316.782837\n",
      "Train Epoch: 584 [23040/54000 (43%)] Loss: -413.187744\n",
      "Train Epoch: 584 [34304/54000 (64%)] Loss: -445.850891\n",
      "Train Epoch: 584 [45568/54000 (84%)] Loss: -438.120667\n",
      "    epoch          : 584\n",
      "    loss           : -378.1906170654297\n",
      "    val_loss       : -365.2721653578803\n",
      "    val_log_likelihood: 482.02586059570314\n",
      "    val_log_marginal: 369.12774799156983\n",
      "Train Epoch: 585 [512/54000 (1%)] Loss: -400.085876\n",
      "Train Epoch: 585 [11776/54000 (22%)] Loss: -203.393066\n",
      "Train Epoch: 585 [23040/54000 (43%)] Loss: -223.722687\n",
      "Train Epoch: 585 [34304/54000 (64%)] Loss: -384.767700\n",
      "Train Epoch: 585 [45568/54000 (84%)] Loss: -398.118561\n",
      "    epoch          : 585\n",
      "    loss           : -400.6870930480957\n",
      "    val_loss       : -371.36637308569624\n",
      "    val_log_likelihood: 483.6851867675781\n",
      "    val_log_marginal: 376.46223748736094\n",
      "Train Epoch: 586 [512/54000 (1%)] Loss: -456.074799\n",
      "Train Epoch: 586 [11776/54000 (22%)] Loss: -430.689423\n",
      "Train Epoch: 586 [23040/54000 (43%)] Loss: -198.254349\n",
      "Train Epoch: 586 [34304/54000 (64%)] Loss: -433.750244\n",
      "Train Epoch: 586 [45568/54000 (84%)] Loss: -353.619141\n",
      "    epoch          : 586\n",
      "    loss           : -378.2675965881348\n",
      "    val_loss       : -356.1887738700025\n",
      "    val_log_likelihood: 479.4012115478516\n",
      "    val_log_marginal: 365.6699826291189\n",
      "Train Epoch: 587 [512/54000 (1%)] Loss: -217.338638\n",
      "Train Epoch: 587 [11776/54000 (22%)] Loss: -216.368912\n",
      "Train Epoch: 587 [23040/54000 (43%)] Loss: -391.766327\n",
      "Train Epoch: 587 [34304/54000 (64%)] Loss: -25.515759\n",
      "Train Epoch: 587 [45568/54000 (84%)] Loss: -377.528809\n",
      "    epoch          : 587\n",
      "    loss           : -289.8879505157471\n",
      "    val_loss       : -257.1845290750265\n",
      "    val_log_likelihood: 439.279052734375\n",
      "    val_log_marginal: 272.9134554713663\n",
      "Train Epoch: 588 [512/54000 (1%)] Loss: 76.628738\n",
      "Train Epoch: 588 [11776/54000 (22%)] Loss: -325.806152\n",
      "Train Epoch: 588 [23040/54000 (43%)] Loss: -516.059753\n",
      "Train Epoch: 588 [34304/54000 (64%)] Loss: -167.213196\n",
      "Train Epoch: 588 [45568/54000 (84%)] Loss: -449.887177\n",
      "    epoch          : 588\n",
      "    loss           : -345.6150382232666\n",
      "    val_loss       : -360.3011615297757\n",
      "    val_log_likelihood: 472.525439453125\n",
      "    val_log_marginal: 370.0554085134915\n",
      "Train Epoch: 589 [512/54000 (1%)] Loss: -416.061523\n",
      "Train Epoch: 589 [11776/54000 (22%)] Loss: -432.292572\n",
      "Train Epoch: 589 [23040/54000 (43%)] Loss: -424.692932\n",
      "Train Epoch: 589 [34304/54000 (64%)] Loss: -251.562683\n",
      "Train Epoch: 589 [45568/54000 (84%)] Loss: -451.120422\n",
      "    epoch          : 589\n",
      "    loss           : -403.6615295410156\n",
      "    val_loss       : -379.46169130140913\n",
      "    val_log_likelihood: 483.38005981445315\n",
      "    val_log_marginal: 385.0672767367214\n",
      "Train Epoch: 590 [512/54000 (1%)] Loss: -270.247620\n",
      "Train Epoch: 590 [11776/54000 (22%)] Loss: -449.570221\n",
      "Train Epoch: 590 [23040/54000 (43%)] Loss: -442.060486\n",
      "Train Epoch: 590 [34304/54000 (64%)] Loss: -433.114807\n",
      "Train Epoch: 590 [45568/54000 (84%)] Loss: -415.847656\n",
      "    epoch          : 590\n",
      "    loss           : -382.34766510009763\n",
      "    val_loss       : -338.4718686133623\n",
      "    val_log_likelihood: 475.1403076171875\n",
      "    val_log_marginal: 345.1824518214911\n",
      "Train Epoch: 591 [512/54000 (1%)] Loss: -175.071030\n",
      "Train Epoch: 591 [11776/54000 (22%)] Loss: -420.459259\n",
      "Train Epoch: 591 [23040/54000 (43%)] Loss: -445.280273\n",
      "Train Epoch: 591 [34304/54000 (64%)] Loss: -401.788544\n",
      "Train Epoch: 591 [45568/54000 (84%)] Loss: -268.702576\n",
      "    epoch          : 591\n",
      "    loss           : -386.0783331298828\n",
      "    val_loss       : -380.93126004040244\n",
      "    val_log_likelihood: 482.8941284179688\n",
      "    val_log_marginal: 385.70458069331943\n",
      "Train Epoch: 592 [512/54000 (1%)] Loss: -222.323669\n",
      "Train Epoch: 592 [11776/54000 (22%)] Loss: -434.710449\n",
      "Train Epoch: 592 [23040/54000 (43%)] Loss: -561.871399\n",
      "Train Epoch: 592 [34304/54000 (64%)] Loss: -463.193695\n",
      "Train Epoch: 592 [45568/54000 (84%)] Loss: -403.421783\n",
      "    epoch          : 592\n",
      "    loss           : -415.4179051208496\n",
      "    val_loss       : -387.8297450153157\n",
      "    val_log_likelihood: 485.4220672607422\n",
      "    val_log_marginal: 392.0641610044986\n",
      "Train Epoch: 593 [512/54000 (1%)] Loss: -449.880676\n",
      "Train Epoch: 593 [11776/54000 (22%)] Loss: -422.867188\n",
      "Train Epoch: 593 [23040/54000 (43%)] Loss: -245.140808\n",
      "Train Epoch: 593 [34304/54000 (64%)] Loss: -568.233032\n",
      "Train Epoch: 593 [45568/54000 (84%)] Loss: -564.023315\n",
      "    epoch          : 593\n",
      "    loss           : -418.64440139770505\n",
      "    val_loss       : -384.6966924008913\n",
      "    val_log_likelihood: 487.1790832519531\n",
      "    val_log_marginal: 390.54136440567675\n",
      "Train Epoch: 594 [512/54000 (1%)] Loss: -479.722534\n",
      "Train Epoch: 594 [11776/54000 (22%)] Loss: -417.325897\n",
      "Train Epoch: 594 [23040/54000 (43%)] Loss: -228.611603\n",
      "Train Epoch: 594 [34304/54000 (64%)] Loss: -424.258606\n",
      "Train Epoch: 594 [45568/54000 (84%)] Loss: -354.453979\n",
      "    epoch          : 594\n",
      "    loss           : -397.89413879394533\n",
      "    val_loss       : -360.5158023693599\n",
      "    val_log_likelihood: 483.6280914306641\n",
      "    val_log_marginal: 364.45161497108637\n",
      "Train Epoch: 595 [512/54000 (1%)] Loss: -365.460754\n",
      "Train Epoch: 595 [11776/54000 (22%)] Loss: -414.249084\n",
      "Train Epoch: 595 [23040/54000 (43%)] Loss: -255.375809\n",
      "Train Epoch: 595 [34304/54000 (64%)] Loss: -432.630066\n",
      "Train Epoch: 595 [45568/54000 (84%)] Loss: -404.931732\n",
      "    epoch          : 595\n",
      "    loss           : -405.2522869873047\n",
      "    val_loss       : -384.38977179303765\n",
      "    val_log_likelihood: 487.427392578125\n",
      "    val_log_marginal: 389.8176738101989\n",
      "Train Epoch: 596 [512/54000 (1%)] Loss: -427.195679\n",
      "Train Epoch: 596 [11776/54000 (22%)] Loss: -549.033447\n",
      "Train Epoch: 596 [23040/54000 (43%)] Loss: -566.283325\n",
      "Train Epoch: 596 [34304/54000 (64%)] Loss: -486.937134\n",
      "Train Epoch: 596 [45568/54000 (84%)] Loss: -456.923706\n",
      "    epoch          : 596\n",
      "    loss           : -420.21448120117185\n",
      "    val_loss       : -389.5841970018111\n",
      "    val_log_likelihood: 488.69553833007814\n",
      "    val_log_marginal: 394.1532025013122\n",
      "Train Epoch: 597 [512/54000 (1%)] Loss: -270.047485\n",
      "Train Epoch: 597 [11776/54000 (22%)] Loss: -451.403473\n",
      "Train Epoch: 597 [23040/54000 (43%)] Loss: -239.066254\n",
      "Train Epoch: 597 [34304/54000 (64%)] Loss: -218.539886\n",
      "Train Epoch: 597 [45568/54000 (84%)] Loss: -443.816467\n",
      "    epoch          : 597\n",
      "    loss           : -418.24011642456054\n",
      "    val_loss       : -380.93737255949526\n",
      "    val_log_likelihood: 486.88397216796875\n",
      "    val_log_marginal: 386.8522599268713\n",
      "Train Epoch: 598 [512/54000 (1%)] Loss: -423.717987\n",
      "Train Epoch: 598 [11776/54000 (22%)] Loss: -446.344238\n",
      "Train Epoch: 598 [23040/54000 (43%)] Loss: -220.551437\n",
      "Train Epoch: 598 [34304/54000 (64%)] Loss: -458.715240\n",
      "Train Epoch: 598 [45568/54000 (84%)] Loss: -432.641357\n",
      "    epoch          : 598\n",
      "    loss           : -398.4387889099121\n",
      "    val_loss       : -351.46388574978334\n",
      "    val_log_likelihood: 486.22887268066404\n",
      "    val_log_marginal: 361.8012984860689\n",
      "Train Epoch: 599 [512/54000 (1%)] Loss: -223.797760\n",
      "Train Epoch: 599 [11776/54000 (22%)] Loss: -548.056030\n",
      "Train Epoch: 599 [23040/54000 (43%)] Loss: -400.834778\n",
      "Train Epoch: 599 [34304/54000 (64%)] Loss: -425.212097\n",
      "Train Epoch: 599 [45568/54000 (84%)] Loss: -369.924561\n",
      "    epoch          : 599\n",
      "    loss           : -380.4149446105957\n",
      "    val_loss       : -323.75167798083277\n",
      "    val_log_likelihood: 482.0995635986328\n",
      "    val_log_marginal: 329.0312118891896\n",
      "Train Epoch: 600 [512/54000 (1%)] Loss: -448.902588\n",
      "Train Epoch: 600 [11776/54000 (22%)] Loss: -397.739105\n",
      "Train Epoch: 600 [23040/54000 (43%)] Loss: -223.816620\n",
      "Train Epoch: 600 [34304/54000 (64%)] Loss: -253.621536\n",
      "Train Epoch: 600 [45568/54000 (84%)] Loss: -444.760834\n",
      "    epoch          : 600\n",
      "    loss           : -390.0883326721191\n",
      "    val_loss       : -377.6597621803172\n",
      "    val_log_likelihood: 482.5427551269531\n",
      "    val_log_marginal: 382.07290427573287\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch600.pth ...\n",
      "Train Epoch: 601 [512/54000 (1%)] Loss: -409.361877\n",
      "Train Epoch: 601 [11776/54000 (22%)] Loss: -445.260406\n",
      "Train Epoch: 601 [23040/54000 (43%)] Loss: -456.746857\n",
      "Train Epoch: 601 [34304/54000 (64%)] Loss: -451.593445\n",
      "Train Epoch: 601 [45568/54000 (84%)] Loss: -433.945038\n",
      "    epoch          : 601\n",
      "    loss           : -388.7267219543457\n",
      "    val_loss       : -347.64872299572454\n",
      "    val_log_likelihood: 475.7519897460937\n",
      "    val_log_marginal: 352.2126575883478\n",
      "Train Epoch: 602 [512/54000 (1%)] Loss: -199.909195\n",
      "Train Epoch: 602 [11776/54000 (22%)] Loss: -394.706726\n",
      "Train Epoch: 602 [23040/54000 (43%)] Loss: -489.096741\n",
      "Train Epoch: 602 [34304/54000 (64%)] Loss: -110.119133\n",
      "Train Epoch: 602 [45568/54000 (84%)] Loss: -352.931702\n",
      "    epoch          : 602\n",
      "    loss           : -320.2886992740631\n",
      "    val_loss       : -231.62883466118947\n",
      "    val_log_likelihood: 464.46387634277346\n",
      "    val_log_marginal: 250.21996238864958\n",
      "Train Epoch: 603 [512/54000 (1%)] Loss: -92.953201\n",
      "Train Epoch: 603 [11776/54000 (22%)] Loss: -428.881104\n",
      "Train Epoch: 603 [23040/54000 (43%)] Loss: -123.774498\n",
      "Train Epoch: 603 [34304/54000 (64%)] Loss: -395.914612\n",
      "Train Epoch: 603 [45568/54000 (84%)] Loss: -345.829010\n",
      "    epoch          : 603\n",
      "    loss           : -338.3166621208191\n",
      "    val_loss       : -283.71380563182754\n",
      "    val_log_likelihood: 472.10589599609375\n",
      "    val_log_marginal: 291.8931743558496\n",
      "Train Epoch: 604 [512/54000 (1%)] Loss: -333.776489\n",
      "Train Epoch: 604 [11776/54000 (22%)] Loss: -349.390381\n",
      "Train Epoch: 604 [23040/54000 (43%)] Loss: -216.295761\n",
      "Train Epoch: 604 [34304/54000 (64%)] Loss: -257.767822\n",
      "Train Epoch: 604 [45568/54000 (84%)] Loss: -440.142273\n",
      "    epoch          : 604\n",
      "    loss           : -391.0589599609375\n",
      "    val_loss       : -383.08977951295674\n",
      "    val_log_likelihood: 486.17917785644534\n",
      "    val_log_marginal: 388.90592718947806\n",
      "Train Epoch: 605 [512/54000 (1%)] Loss: -243.568420\n",
      "Train Epoch: 605 [11776/54000 (22%)] Loss: -262.144287\n",
      "Train Epoch: 605 [23040/54000 (43%)] Loss: -483.710114\n",
      "Train Epoch: 605 [34304/54000 (64%)] Loss: -445.906372\n",
      "Train Epoch: 605 [45568/54000 (84%)] Loss: -448.860840\n",
      "    epoch          : 605\n",
      "    loss           : -421.3682571411133\n",
      "    val_loss       : -391.14155790153893\n",
      "    val_log_likelihood: 490.264013671875\n",
      "    val_log_marginal: 394.66402786150576\n",
      "Train Epoch: 606 [512/54000 (1%)] Loss: -494.740326\n",
      "Train Epoch: 606 [11776/54000 (22%)] Loss: -400.016693\n",
      "Train Epoch: 606 [23040/54000 (43%)] Loss: -234.282318\n",
      "Train Epoch: 606 [34304/54000 (64%)] Loss: -458.264404\n",
      "Train Epoch: 606 [45568/54000 (84%)] Loss: -246.216827\n",
      "    epoch          : 606\n",
      "    loss           : -415.7519612121582\n",
      "    val_loss       : -372.3722128205933\n",
      "    val_log_likelihood: 486.58656311035156\n",
      "    val_log_marginal: 379.654652807489\n",
      "Train Epoch: 607 [512/54000 (1%)] Loss: -475.506775\n",
      "Train Epoch: 607 [11776/54000 (22%)] Loss: -410.283569\n",
      "Train Epoch: 607 [23040/54000 (43%)] Loss: -487.830444\n",
      "Train Epoch: 607 [34304/54000 (64%)] Loss: -563.593872\n",
      "Train Epoch: 607 [45568/54000 (84%)] Loss: -264.876709\n",
      "    epoch          : 607\n",
      "    loss           : -410.25654541015626\n",
      "    val_loss       : -378.05589858209714\n",
      "    val_log_likelihood: 488.5160339355469\n",
      "    val_log_marginal: 383.1362837854773\n",
      "Train Epoch: 608 [512/54000 (1%)] Loss: -405.667297\n",
      "Train Epoch: 608 [11776/54000 (22%)] Loss: -448.654541\n",
      "Train Epoch: 608 [23040/54000 (43%)] Loss: -534.195618\n",
      "Train Epoch: 608 [34304/54000 (64%)] Loss: -526.760742\n",
      "Train Epoch: 608 [45568/54000 (84%)] Loss: -423.936707\n",
      "    epoch          : 608\n",
      "    loss           : -396.5902081298828\n",
      "    val_loss       : -367.0036016382277\n",
      "    val_log_likelihood: 485.40593872070315\n",
      "    val_log_marginal: 372.04384747333825\n",
      "Train Epoch: 609 [512/54000 (1%)] Loss: -475.107880\n",
      "Train Epoch: 609 [11776/54000 (22%)] Loss: -398.795227\n",
      "Train Epoch: 609 [23040/54000 (43%)] Loss: -354.616913\n",
      "Train Epoch: 609 [34304/54000 (64%)] Loss: -213.557953\n",
      "Train Epoch: 609 [45568/54000 (84%)] Loss: -431.337769\n",
      "    epoch          : 609\n",
      "    loss           : -389.6267451477051\n",
      "    val_loss       : -370.22783576883376\n",
      "    val_log_likelihood: 486.37940368652346\n",
      "    val_log_marginal: 377.19234345182866\n",
      "Train Epoch: 610 [512/54000 (1%)] Loss: -233.448181\n",
      "Train Epoch: 610 [11776/54000 (22%)] Loss: -447.687012\n",
      "Train Epoch: 610 [23040/54000 (43%)] Loss: -441.370361\n",
      "Train Epoch: 610 [34304/54000 (64%)] Loss: -435.872192\n",
      "Train Epoch: 610 [45568/54000 (84%)] Loss: -420.545258\n",
      "    epoch          : 610\n",
      "    loss           : -411.0235061645508\n",
      "    val_loss       : -373.1082261594012\n",
      "    val_log_likelihood: 484.78330383300784\n",
      "    val_log_marginal: 379.79377849139274\n",
      "Train Epoch: 611 [512/54000 (1%)] Loss: -259.177765\n",
      "Train Epoch: 611 [11776/54000 (22%)] Loss: -196.988388\n",
      "Train Epoch: 611 [23040/54000 (43%)] Loss: -402.639648\n",
      "Train Epoch: 611 [34304/54000 (64%)] Loss: -404.746552\n",
      "Train Epoch: 611 [45568/54000 (84%)] Loss: -350.388550\n",
      "    epoch          : 611\n",
      "    loss           : -321.9844534111023\n",
      "    val_loss       : 182.12464686371385\n",
      "    val_log_likelihood: 470.41746826171874\n",
      "    val_log_marginal: -151.4282869197428\n",
      "Train Epoch: 612 [512/54000 (1%)] Loss: 203.363968\n",
      "Train Epoch: 612 [11776/54000 (22%)] Loss: -89.482864\n",
      "Train Epoch: 612 [23040/54000 (43%)] Loss: -13.869278\n",
      "Train Epoch: 612 [34304/54000 (64%)] Loss: -259.721375\n",
      "Train Epoch: 612 [45568/54000 (84%)] Loss: -188.585648\n",
      "    epoch          : 612\n",
      "    loss           : -132.06442991256714\n",
      "    val_loss       : -273.87549396064134\n",
      "    val_log_likelihood: 461.46878204345705\n",
      "    val_log_marginal: 289.52363655529916\n",
      "Train Epoch: 613 [512/54000 (1%)] Loss: -366.907593\n",
      "Train Epoch: 613 [11776/54000 (22%)] Loss: -279.937561\n",
      "Train Epoch: 613 [23040/54000 (43%)] Loss: -238.276047\n",
      "Train Epoch: 613 [34304/54000 (64%)] Loss: -425.349030\n",
      "Train Epoch: 613 [45568/54000 (84%)] Loss: -392.325104\n",
      "    epoch          : 613\n",
      "    loss           : -365.0909422302246\n",
      "    val_loss       : -379.2927298633382\n",
      "    val_log_likelihood: 479.36705627441404\n",
      "    val_log_marginal: 384.3882394630462\n",
      "Train Epoch: 614 [512/54000 (1%)] Loss: -252.657654\n",
      "Train Epoch: 614 [11776/54000 (22%)] Loss: -271.007721\n",
      "Train Epoch: 614 [23040/54000 (43%)] Loss: -248.669250\n",
      "Train Epoch: 614 [34304/54000 (64%)] Loss: -464.495209\n",
      "Train Epoch: 614 [45568/54000 (84%)] Loss: -446.931671\n",
      "    epoch          : 614\n",
      "    loss           : -413.0981146240234\n",
      "    val_loss       : -388.3268889664672\n",
      "    val_log_likelihood: 485.8941711425781\n",
      "    val_log_marginal: 393.3811774507165\n",
      "Train Epoch: 615 [512/54000 (1%)] Loss: -275.803406\n",
      "Train Epoch: 615 [11776/54000 (22%)] Loss: -560.805054\n",
      "Train Epoch: 615 [23040/54000 (43%)] Loss: -255.827988\n",
      "Train Epoch: 615 [34304/54000 (64%)] Loss: -441.212402\n",
      "Train Epoch: 615 [45568/54000 (84%)] Loss: -277.853943\n",
      "    epoch          : 615\n",
      "    loss           : -423.56915267944333\n",
      "    val_loss       : -391.8576081640087\n",
      "    val_log_likelihood: 489.7539520263672\n",
      "    val_log_marginal: 395.9807221326977\n",
      "Train Epoch: 616 [512/54000 (1%)] Loss: -266.843353\n",
      "Train Epoch: 616 [11776/54000 (22%)] Loss: -454.651367\n",
      "Train Epoch: 616 [23040/54000 (43%)] Loss: -419.761475\n",
      "Train Epoch: 616 [34304/54000 (64%)] Loss: -256.351135\n",
      "Train Epoch: 616 [45568/54000 (84%)] Loss: -250.193115\n",
      "    epoch          : 616\n",
      "    loss           : -421.156681060791\n",
      "    val_loss       : -377.74476171638815\n",
      "    val_log_likelihood: 486.6895690917969\n",
      "    val_log_marginal: 384.22145737968384\n",
      "Train Epoch: 617 [512/54000 (1%)] Loss: -445.931183\n",
      "Train Epoch: 617 [11776/54000 (22%)] Loss: -434.861755\n",
      "Train Epoch: 617 [23040/54000 (43%)] Loss: -448.592560\n",
      "Train Epoch: 617 [34304/54000 (64%)] Loss: -476.573792\n",
      "Train Epoch: 617 [45568/54000 (84%)] Loss: -265.212219\n",
      "    epoch          : 617\n",
      "    loss           : -412.10345458984375\n",
      "    val_loss       : -379.22593959579245\n",
      "    val_log_likelihood: 489.7269714355469\n",
      "    val_log_marginal: 386.4769836876541\n",
      "Train Epoch: 618 [512/54000 (1%)] Loss: -552.729492\n",
      "Train Epoch: 618 [11776/54000 (22%)] Loss: -450.541748\n",
      "Train Epoch: 618 [23040/54000 (43%)] Loss: -439.724854\n",
      "Train Epoch: 618 [34304/54000 (64%)] Loss: -468.938965\n",
      "Train Epoch: 618 [45568/54000 (84%)] Loss: -449.827393\n",
      "    epoch          : 618\n",
      "    loss           : -416.37686752319337\n",
      "    val_loss       : -382.6152469785884\n",
      "    val_log_likelihood: 486.9672424316406\n",
      "    val_log_marginal: 385.3372712243348\n",
      "Train Epoch: 619 [512/54000 (1%)] Loss: -436.443512\n",
      "Train Epoch: 619 [11776/54000 (22%)] Loss: -374.998474\n",
      "Train Epoch: 619 [23040/54000 (43%)] Loss: -243.005753\n",
      "Train Epoch: 619 [34304/54000 (64%)] Loss: -468.701111\n",
      "Train Epoch: 619 [45568/54000 (84%)] Loss: -377.465454\n",
      "    epoch          : 619\n",
      "    loss           : -409.90107391357424\n",
      "    val_loss       : -379.28211243171245\n",
      "    val_log_likelihood: 485.4845397949219\n",
      "    val_log_marginal: 382.50203330107036\n",
      "Train Epoch: 620 [512/54000 (1%)] Loss: -416.232635\n",
      "Train Epoch: 620 [11776/54000 (22%)] Loss: -441.699402\n",
      "Train Epoch: 620 [23040/54000 (43%)] Loss: -441.970093\n",
      "Train Epoch: 620 [34304/54000 (64%)] Loss: -242.141876\n",
      "Train Epoch: 620 [45568/54000 (84%)] Loss: -201.415298\n",
      "    epoch          : 620\n",
      "    loss           : -396.7840606689453\n",
      "    val_loss       : -334.80327875716614\n",
      "    val_log_likelihood: 477.90736389160156\n",
      "    val_log_marginal: 339.5768466170877\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch620.pth ...\n",
      "Train Epoch: 621 [512/54000 (1%)] Loss: -378.677734\n",
      "Train Epoch: 621 [11776/54000 (22%)] Loss: -99.088242\n",
      "Train Epoch: 621 [23040/54000 (43%)] Loss: -223.010559\n",
      "Train Epoch: 621 [34304/54000 (64%)] Loss: -356.458740\n",
      "Train Epoch: 621 [45568/54000 (84%)] Loss: -429.082520\n",
      "    epoch          : 621\n",
      "    loss           : -365.80530296325685\n",
      "    val_loss       : -364.9637957362458\n",
      "    val_log_likelihood: 484.96827392578126\n",
      "    val_log_marginal: 368.3721644762903\n",
      "Train Epoch: 622 [512/54000 (1%)] Loss: -396.826599\n",
      "Train Epoch: 622 [11776/54000 (22%)] Loss: -453.978241\n",
      "Train Epoch: 622 [23040/54000 (43%)] Loss: -171.561951\n",
      "Train Epoch: 622 [34304/54000 (64%)] Loss: -395.365662\n",
      "Train Epoch: 622 [45568/54000 (84%)] Loss: -158.128235\n",
      "    epoch          : 622\n",
      "    loss           : -338.7820035934448\n",
      "    val_loss       : -121.23356158351525\n",
      "    val_log_likelihood: 451.45018310546874\n",
      "    val_log_marginal: 129.63262984640895\n",
      "Train Epoch: 623 [512/54000 (1%)] Loss: -427.795471\n",
      "Train Epoch: 623 [11776/54000 (22%)] Loss: -197.555786\n",
      "Train Epoch: 623 [23040/54000 (43%)] Loss: -409.409973\n",
      "Train Epoch: 623 [34304/54000 (64%)] Loss: -405.108704\n",
      "Train Epoch: 623 [45568/54000 (84%)] Loss: -183.244293\n",
      "    epoch          : 623\n",
      "    loss           : -242.54977584838866\n",
      "    val_loss       : -294.5874522928149\n",
      "    val_log_likelihood: 463.36268615722656\n",
      "    val_log_marginal: 311.83847947381435\n",
      "Train Epoch: 624 [512/54000 (1%)] Loss: -335.999176\n",
      "Train Epoch: 624 [11776/54000 (22%)] Loss: -217.661865\n",
      "Train Epoch: 624 [23040/54000 (43%)] Loss: -414.643921\n",
      "Train Epoch: 624 [34304/54000 (64%)] Loss: -243.385040\n",
      "Train Epoch: 624 [45568/54000 (84%)] Loss: -444.619568\n",
      "    epoch          : 624\n",
      "    loss           : -384.6236360168457\n",
      "    val_loss       : -383.0253686234355\n",
      "    val_log_likelihood: 483.6770874023438\n",
      "    val_log_marginal: 387.8915655594319\n",
      "Train Epoch: 625 [512/54000 (1%)] Loss: -555.659912\n",
      "Train Epoch: 625 [11776/54000 (22%)] Loss: -475.865479\n",
      "Train Epoch: 625 [23040/54000 (43%)] Loss: -241.037231\n",
      "Train Epoch: 625 [34304/54000 (64%)] Loss: -435.363922\n",
      "Train Epoch: 625 [45568/54000 (84%)] Loss: -468.271484\n",
      "    epoch          : 625\n",
      "    loss           : -415.3213003540039\n",
      "    val_loss       : -375.1428563066758\n",
      "    val_log_likelihood: 487.7002288818359\n",
      "    val_log_marginal: 382.3915726822012\n",
      "Train Epoch: 626 [512/54000 (1%)] Loss: -561.425415\n",
      "Train Epoch: 626 [11776/54000 (22%)] Loss: -247.639618\n",
      "Train Epoch: 626 [23040/54000 (43%)] Loss: -391.250183\n",
      "Train Epoch: 626 [34304/54000 (64%)] Loss: -402.075562\n",
      "Train Epoch: 626 [45568/54000 (84%)] Loss: -435.004700\n",
      "    epoch          : 626\n",
      "    loss           : -415.2835238647461\n",
      "    val_loss       : -377.5867987796664\n",
      "    val_log_likelihood: 484.2406433105469\n",
      "    val_log_marginal: 380.65477792434393\n",
      "Train Epoch: 627 [512/54000 (1%)] Loss: -430.714355\n",
      "Train Epoch: 627 [11776/54000 (22%)] Loss: -429.606720\n",
      "Train Epoch: 627 [23040/54000 (43%)] Loss: -481.710236\n",
      "Train Epoch: 627 [34304/54000 (64%)] Loss: -474.688171\n",
      "Train Epoch: 627 [45568/54000 (84%)] Loss: -476.687927\n",
      "    epoch          : 627\n",
      "    loss           : -418.682113494873\n",
      "    val_loss       : -388.9555450396612\n",
      "    val_log_likelihood: 485.3114898681641\n",
      "    val_log_marginal: 392.22198348753153\n",
      "Train Epoch: 628 [512/54000 (1%)] Loss: -576.023560\n",
      "Train Epoch: 628 [11776/54000 (22%)] Loss: -481.162659\n",
      "Train Epoch: 628 [23040/54000 (43%)] Loss: -471.712036\n",
      "Train Epoch: 628 [34304/54000 (64%)] Loss: -462.537170\n",
      "Train Epoch: 628 [45568/54000 (84%)] Loss: -232.033310\n",
      "    epoch          : 628\n",
      "    loss           : -405.9013311767578\n",
      "    val_loss       : -383.911677342467\n",
      "    val_log_likelihood: 487.1019683837891\n",
      "    val_log_marginal: 387.23012499325097\n",
      "Train Epoch: 629 [512/54000 (1%)] Loss: -213.211426\n",
      "Train Epoch: 629 [11776/54000 (22%)] Loss: -524.096558\n",
      "Train Epoch: 629 [23040/54000 (43%)] Loss: -293.024933\n",
      "Train Epoch: 629 [34304/54000 (64%)] Loss: -373.522919\n",
      "Train Epoch: 629 [45568/54000 (84%)] Loss: -371.137756\n",
      "    epoch          : 629\n",
      "    loss           : -361.23419708251953\n",
      "    val_loss       : -302.15123458523306\n",
      "    val_log_likelihood: 474.9834503173828\n",
      "    val_log_marginal: 306.96883071774755\n",
      "Train Epoch: 630 [512/54000 (1%)] Loss: -342.755035\n",
      "Train Epoch: 630 [11776/54000 (22%)] Loss: -411.703705\n",
      "Train Epoch: 630 [23040/54000 (43%)] Loss: -266.035553\n",
      "Train Epoch: 630 [34304/54000 (64%)] Loss: -424.386475\n",
      "Train Epoch: 630 [45568/54000 (84%)] Loss: -467.865234\n",
      "    epoch          : 630\n",
      "    loss           : -399.4798641967773\n",
      "    val_loss       : -359.9961074324325\n",
      "    val_log_likelihood: 484.7702606201172\n",
      "    val_log_marginal: 365.38856482468543\n",
      "Train Epoch: 631 [512/54000 (1%)] Loss: -536.749146\n",
      "Train Epoch: 631 [11776/54000 (22%)] Loss: -231.869003\n",
      "Train Epoch: 631 [23040/54000 (43%)] Loss: -413.317749\n",
      "Train Epoch: 631 [34304/54000 (64%)] Loss: -206.224411\n",
      "Train Epoch: 631 [45568/54000 (84%)] Loss: -306.884003\n",
      "    epoch          : 631\n",
      "    loss           : -322.3450280141831\n",
      "    val_loss       : -226.8308185855858\n",
      "    val_log_likelihood: 458.9045043945313\n",
      "    val_log_marginal: 236.08971569687128\n",
      "Train Epoch: 632 [512/54000 (1%)] Loss: -420.407837\n",
      "Train Epoch: 632 [11776/54000 (22%)] Loss: -144.433456\n",
      "Train Epoch: 632 [23040/54000 (43%)] Loss: -400.678406\n",
      "Train Epoch: 632 [34304/54000 (64%)] Loss: -321.389984\n",
      "Train Epoch: 632 [45568/54000 (84%)] Loss: -554.617798\n",
      "    epoch          : 632\n",
      "    loss           : -349.1149730682373\n",
      "    val_loss       : -359.8340150491334\n",
      "    val_log_likelihood: 480.3145050048828\n",
      "    val_log_marginal: 368.95209331476525\n",
      "Train Epoch: 633 [512/54000 (1%)] Loss: -440.232300\n",
      "Train Epoch: 633 [11776/54000 (22%)] Loss: -220.892792\n",
      "Train Epoch: 633 [23040/54000 (43%)] Loss: -381.684998\n",
      "Train Epoch: 633 [34304/54000 (64%)] Loss: -463.823090\n",
      "Train Epoch: 633 [45568/54000 (84%)] Loss: -415.864807\n",
      "    epoch          : 633\n",
      "    loss           : -411.9840707397461\n",
      "    val_loss       : -385.9883874427527\n",
      "    val_log_likelihood: 488.7679168701172\n",
      "    val_log_marginal: 392.75721025876703\n",
      "Train Epoch: 634 [512/54000 (1%)] Loss: -249.451385\n",
      "Train Epoch: 634 [11776/54000 (22%)] Loss: -459.123901\n",
      "Train Epoch: 634 [23040/54000 (43%)] Loss: -445.973145\n",
      "Train Epoch: 634 [34304/54000 (64%)] Loss: -484.352661\n",
      "Train Epoch: 634 [45568/54000 (84%)] Loss: -558.143127\n",
      "    epoch          : 634\n",
      "    loss           : -421.9610829162598\n",
      "    val_loss       : -390.756517734006\n",
      "    val_log_likelihood: 491.6211700439453\n",
      "    val_log_marginal: 394.900675066188\n",
      "Train Epoch: 635 [512/54000 (1%)] Loss: -480.216614\n",
      "Train Epoch: 635 [11776/54000 (22%)] Loss: -443.194611\n",
      "Train Epoch: 635 [23040/54000 (43%)] Loss: -431.908325\n",
      "Train Epoch: 635 [34304/54000 (64%)] Loss: -424.722839\n",
      "Train Epoch: 635 [45568/54000 (84%)] Loss: -559.715515\n",
      "    epoch          : 635\n",
      "    loss           : -414.42622924804687\n",
      "    val_loss       : -386.3451467793435\n",
      "    val_log_likelihood: 490.014013671875\n",
      "    val_log_marginal: 392.42745034284894\n",
      "Train Epoch: 636 [512/54000 (1%)] Loss: -486.460388\n",
      "Train Epoch: 636 [11776/54000 (22%)] Loss: -413.762207\n",
      "Train Epoch: 636 [23040/54000 (43%)] Loss: -481.571533\n",
      "Train Epoch: 636 [34304/54000 (64%)] Loss: -466.180023\n",
      "Train Epoch: 636 [45568/54000 (84%)] Loss: -442.975830\n",
      "    epoch          : 636\n",
      "    loss           : -426.70929702758787\n",
      "    val_loss       : -392.67142369421197\n",
      "    val_log_likelihood: 492.84596862792966\n",
      "    val_log_marginal: 398.37735769934955\n",
      "Train Epoch: 637 [512/54000 (1%)] Loss: -245.940704\n",
      "Train Epoch: 637 [11776/54000 (22%)] Loss: -487.572540\n",
      "Train Epoch: 637 [23040/54000 (43%)] Loss: -249.818176\n",
      "Train Epoch: 637 [34304/54000 (64%)] Loss: -460.760254\n",
      "Train Epoch: 637 [45568/54000 (84%)] Loss: -461.493835\n",
      "    epoch          : 637\n",
      "    loss           : -430.24371292114256\n",
      "    val_loss       : -395.44399027656766\n",
      "    val_log_likelihood: 491.31793823242185\n",
      "    val_log_marginal: 398.0342154596001\n",
      "Train Epoch: 638 [512/54000 (1%)] Loss: -485.099030\n",
      "Train Epoch: 638 [11776/54000 (22%)] Loss: -456.206024\n",
      "Train Epoch: 638 [23040/54000 (43%)] Loss: -454.238800\n",
      "Train Epoch: 638 [34304/54000 (64%)] Loss: -405.281097\n",
      "Train Epoch: 638 [45568/54000 (84%)] Loss: -411.943542\n",
      "    epoch          : 638\n",
      "    loss           : -429.6134544372559\n",
      "    val_loss       : -392.3512907071039\n",
      "    val_log_likelihood: 493.57066650390624\n",
      "    val_log_marginal: 396.2051362980157\n",
      "Train Epoch: 639 [512/54000 (1%)] Loss: -484.199982\n",
      "Train Epoch: 639 [11776/54000 (22%)] Loss: -261.365234\n",
      "Train Epoch: 639 [23040/54000 (43%)] Loss: -504.729126\n",
      "Train Epoch: 639 [34304/54000 (64%)] Loss: -568.153748\n",
      "Train Epoch: 639 [45568/54000 (84%)] Loss: -370.294373\n",
      "    epoch          : 639\n",
      "    loss           : -418.946350402832\n",
      "    val_loss       : -371.1453314449638\n",
      "    val_log_likelihood: 486.0656219482422\n",
      "    val_log_marginal: 376.50920854620637\n",
      "Train Epoch: 640 [512/54000 (1%)] Loss: -410.103607\n",
      "Train Epoch: 640 [11776/54000 (22%)] Loss: -407.848389\n",
      "Train Epoch: 640 [23040/54000 (43%)] Loss: -427.331421\n",
      "Train Epoch: 640 [34304/54000 (64%)] Loss: -397.256256\n",
      "Train Epoch: 640 [45568/54000 (84%)] Loss: -409.899170\n",
      "    epoch          : 640\n",
      "    loss           : -374.1988631439209\n",
      "    val_loss       : -362.42717238664625\n",
      "    val_log_likelihood: 482.11138305664065\n",
      "    val_log_marginal: 370.8215299459234\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch640.pth ...\n",
      "Train Epoch: 641 [512/54000 (1%)] Loss: -249.530151\n",
      "Train Epoch: 641 [11776/54000 (22%)] Loss: -325.377014\n",
      "Train Epoch: 641 [23040/54000 (43%)] Loss: -318.193268\n",
      "Train Epoch: 641 [34304/54000 (64%)] Loss: -316.028625\n",
      "Train Epoch: 641 [45568/54000 (84%)] Loss: -211.028458\n",
      "    epoch          : 641\n",
      "    loss           : -343.1839754104614\n",
      "    val_loss       : -326.0386219067499\n",
      "    val_log_likelihood: 450.4190902709961\n",
      "    val_log_marginal: 338.36088873110714\n",
      "Train Epoch: 642 [512/54000 (1%)] Loss: -349.320496\n",
      "Train Epoch: 642 [11776/54000 (22%)] Loss: -185.434265\n",
      "Train Epoch: 642 [23040/54000 (43%)] Loss: -194.071136\n",
      "Train Epoch: 642 [34304/54000 (64%)] Loss: -232.310532\n",
      "Train Epoch: 642 [45568/54000 (84%)] Loss: -195.667435\n",
      "    epoch          : 642\n",
      "    loss           : -391.78322227478026\n",
      "    val_loss       : -382.05685167144986\n",
      "    val_log_likelihood: 486.1083984375\n",
      "    val_log_marginal: 387.0608454223722\n",
      "Train Epoch: 643 [512/54000 (1%)] Loss: -261.134644\n",
      "Train Epoch: 643 [11776/54000 (22%)] Loss: -471.727905\n",
      "Train Epoch: 643 [23040/54000 (43%)] Loss: -442.346130\n",
      "Train Epoch: 643 [34304/54000 (64%)] Loss: -449.666992\n",
      "Train Epoch: 643 [45568/54000 (84%)] Loss: -425.090118\n",
      "    epoch          : 643\n",
      "    loss           : -416.9947819519043\n",
      "    val_loss       : -387.4026406395249\n",
      "    val_log_likelihood: 493.9839324951172\n",
      "    val_log_marginal: 393.6564172547311\n",
      "Train Epoch: 644 [512/54000 (1%)] Loss: -274.145447\n",
      "Train Epoch: 644 [11776/54000 (22%)] Loss: -443.832550\n",
      "Train Epoch: 644 [23040/54000 (43%)] Loss: -450.765503\n",
      "Train Epoch: 644 [34304/54000 (64%)] Loss: -236.126526\n",
      "Train Epoch: 644 [45568/54000 (84%)] Loss: -238.930115\n",
      "    epoch          : 644\n",
      "    loss           : -413.1644418334961\n",
      "    val_loss       : -376.3997071705759\n",
      "    val_log_likelihood: 484.4830688476562\n",
      "    val_log_marginal: 380.23435054682193\n",
      "Train Epoch: 645 [512/54000 (1%)] Loss: -454.830475\n",
      "Train Epoch: 645 [11776/54000 (22%)] Loss: -436.411804\n",
      "Train Epoch: 645 [23040/54000 (43%)] Loss: -435.143616\n",
      "Train Epoch: 645 [34304/54000 (64%)] Loss: -429.967834\n",
      "Train Epoch: 645 [45568/54000 (84%)] Loss: -274.021759\n",
      "    epoch          : 645\n",
      "    loss           : -396.77534912109377\n",
      "    val_loss       : -374.563302624505\n",
      "    val_log_likelihood: 486.1361999511719\n",
      "    val_log_marginal: 378.1463437467813\n",
      "Train Epoch: 646 [512/54000 (1%)] Loss: -436.402069\n",
      "Train Epoch: 646 [11776/54000 (22%)] Loss: -461.101257\n",
      "Train Epoch: 646 [23040/54000 (43%)] Loss: -232.450638\n",
      "Train Epoch: 646 [34304/54000 (64%)] Loss: -456.417236\n",
      "Train Epoch: 646 [45568/54000 (84%)] Loss: -242.834442\n",
      "    epoch          : 646\n",
      "    loss           : -415.84085845947266\n",
      "    val_loss       : -382.14349198015407\n",
      "    val_log_likelihood: 492.10468139648435\n",
      "    val_log_marginal: 387.0698873218149\n",
      "Train Epoch: 647 [512/54000 (1%)] Loss: -474.002167\n",
      "Train Epoch: 647 [11776/54000 (22%)] Loss: -404.104706\n",
      "Train Epoch: 647 [23040/54000 (43%)] Loss: -541.470947\n",
      "Train Epoch: 647 [34304/54000 (64%)] Loss: -173.313446\n",
      "Train Epoch: 647 [45568/54000 (84%)] Loss: -405.509766\n",
      "    epoch          : 647\n",
      "    loss           : -337.81462913513184\n",
      "    val_loss       : -177.87181509658694\n",
      "    val_log_likelihood: 472.1465209960937\n",
      "    val_log_marginal: 184.4581600639969\n",
      "Train Epoch: 648 [512/54000 (1%)] Loss: -254.594025\n",
      "Train Epoch: 648 [11776/54000 (22%)] Loss: -289.208771\n",
      "Train Epoch: 648 [23040/54000 (43%)] Loss: 1192.663818\n",
      "Train Epoch: 648 [34304/54000 (64%)] Loss: 312.978516\n",
      "Train Epoch: 648 [45568/54000 (84%)] Loss: -219.331070\n",
      "    epoch          : 648\n",
      "    loss           : 111.53576993465424\n",
      "    val_loss       : -198.62651674887167\n",
      "    val_log_likelihood: 404.0994842529297\n",
      "    val_log_marginal: 217.95719441212714\n",
      "Train Epoch: 649 [512/54000 (1%)] Loss: -223.049591\n",
      "Train Epoch: 649 [11776/54000 (22%)] Loss: -120.994194\n",
      "Train Epoch: 649 [23040/54000 (43%)] Loss: -416.486511\n",
      "Train Epoch: 649 [34304/54000 (64%)] Loss: -202.361130\n",
      "Train Epoch: 649 [45568/54000 (84%)] Loss: -420.505005\n",
      "    epoch          : 649\n",
      "    loss           : -361.1573865509033\n",
      "    val_loss       : -375.56425956534224\n",
      "    val_log_likelihood: 476.5474365234375\n",
      "    val_log_marginal: 381.651323768124\n",
      "Train Epoch: 650 [512/54000 (1%)] Loss: -464.758728\n",
      "Train Epoch: 650 [11776/54000 (22%)] Loss: -439.974060\n",
      "Train Epoch: 650 [23040/54000 (43%)] Loss: -425.207245\n",
      "Train Epoch: 650 [34304/54000 (64%)] Loss: -282.675690\n",
      "Train Epoch: 650 [45568/54000 (84%)] Loss: -273.274231\n",
      "    epoch          : 650\n",
      "    loss           : -410.17893142700194\n",
      "    val_loss       : -388.81853399798274\n",
      "    val_log_likelihood: 484.7481689453125\n",
      "    val_log_marginal: 392.62983794286845\n",
      "Train Epoch: 651 [512/54000 (1%)] Loss: -459.238892\n",
      "Train Epoch: 651 [11776/54000 (22%)] Loss: -446.465820\n",
      "Train Epoch: 651 [23040/54000 (43%)] Loss: -468.281067\n",
      "Train Epoch: 651 [34304/54000 (64%)] Loss: -222.906174\n",
      "Train Epoch: 651 [45568/54000 (84%)] Loss: -268.216003\n",
      "    epoch          : 651\n",
      "    loss           : -419.5619107055664\n",
      "    val_loss       : -390.41028946666046\n",
      "    val_log_likelihood: 487.5008941650391\n",
      "    val_log_marginal: 395.9561405321564\n",
      "Train Epoch: 652 [512/54000 (1%)] Loss: -419.249054\n",
      "Train Epoch: 652 [11776/54000 (22%)] Loss: -268.867157\n",
      "Train Epoch: 652 [23040/54000 (43%)] Loss: -392.450684\n",
      "Train Epoch: 652 [34304/54000 (64%)] Loss: -273.566528\n",
      "Train Epoch: 652 [45568/54000 (84%)] Loss: -275.848236\n",
      "    epoch          : 652\n",
      "    loss           : -424.0805989074707\n",
      "    val_loss       : -391.7654472018592\n",
      "    val_log_likelihood: 487.6748382568359\n",
      "    val_log_marginal: 396.8769796957022\n",
      "Train Epoch: 653 [512/54000 (1%)] Loss: -244.775208\n",
      "Train Epoch: 653 [11776/54000 (22%)] Loss: -468.843872\n",
      "Train Epoch: 653 [23040/54000 (43%)] Loss: -465.480194\n",
      "Train Epoch: 653 [34304/54000 (64%)] Loss: -435.570679\n",
      "Train Epoch: 653 [45568/54000 (84%)] Loss: -449.514313\n",
      "    epoch          : 653\n",
      "    loss           : -426.44431060791015\n",
      "    val_loss       : -396.43914302028713\n",
      "    val_log_likelihood: 492.9280120849609\n",
      "    val_log_marginal: 399.69019871912894\n",
      "Train Epoch: 654 [512/54000 (1%)] Loss: -490.125916\n",
      "Train Epoch: 654 [11776/54000 (22%)] Loss: -572.901489\n",
      "Train Epoch: 654 [23040/54000 (43%)] Loss: -305.372986\n",
      "Train Epoch: 654 [34304/54000 (64%)] Loss: -290.595367\n",
      "Train Epoch: 654 [45568/54000 (84%)] Loss: -459.316986\n",
      "    epoch          : 654\n",
      "    loss           : -426.8550094604492\n",
      "    val_loss       : -393.3417096313089\n",
      "    val_log_likelihood: 493.05487060546875\n",
      "    val_log_marginal: 400.18244254402816\n",
      "Train Epoch: 655 [512/54000 (1%)] Loss: -468.611053\n",
      "Train Epoch: 655 [11776/54000 (22%)] Loss: -481.244690\n",
      "Train Epoch: 655 [23040/54000 (43%)] Loss: -436.764709\n",
      "Train Epoch: 655 [34304/54000 (64%)] Loss: -400.708130\n",
      "Train Epoch: 655 [45568/54000 (84%)] Loss: -446.098694\n",
      "    epoch          : 655\n",
      "    loss           : -423.9601153564453\n",
      "    val_loss       : -387.81514243204145\n",
      "    val_log_likelihood: 491.4345458984375\n",
      "    val_log_marginal: 393.27304171957076\n",
      "Train Epoch: 656 [512/54000 (1%)] Loss: -471.045258\n",
      "Train Epoch: 656 [11776/54000 (22%)] Loss: -488.812561\n",
      "Train Epoch: 656 [23040/54000 (43%)] Loss: -465.866150\n",
      "Train Epoch: 656 [34304/54000 (64%)] Loss: -407.904999\n",
      "Train Epoch: 656 [45568/54000 (84%)] Loss: -456.656372\n",
      "    epoch          : 656\n",
      "    loss           : -426.7345495605469\n",
      "    val_loss       : -395.61866783369334\n",
      "    val_log_likelihood: 495.74293212890626\n",
      "    val_log_marginal: 400.9616217348724\n",
      "Train Epoch: 657 [512/54000 (1%)] Loss: -489.228149\n",
      "Train Epoch: 657 [11776/54000 (22%)] Loss: -463.619232\n",
      "Train Epoch: 657 [23040/54000 (43%)] Loss: -573.632935\n",
      "Train Epoch: 657 [34304/54000 (64%)] Loss: -472.582062\n",
      "Train Epoch: 657 [45568/54000 (84%)] Loss: -439.557800\n",
      "    epoch          : 657\n",
      "    loss           : -423.2473249816895\n",
      "    val_loss       : -392.8390949006192\n",
      "    val_log_likelihood: 496.20908203125\n",
      "    val_log_marginal: 398.6522242423147\n",
      "Train Epoch: 658 [512/54000 (1%)] Loss: -472.525208\n",
      "Train Epoch: 658 [11776/54000 (22%)] Loss: -263.771515\n",
      "Train Epoch: 658 [23040/54000 (43%)] Loss: -464.595642\n",
      "Train Epoch: 658 [34304/54000 (64%)] Loss: -231.788544\n",
      "Train Epoch: 658 [45568/54000 (84%)] Loss: -469.787170\n",
      "    epoch          : 658\n",
      "    loss           : -427.14629196166993\n",
      "    val_loss       : -390.5694891226478\n",
      "    val_log_likelihood: 494.33587341308595\n",
      "    val_log_marginal: 395.8268953677267\n",
      "Train Epoch: 659 [512/54000 (1%)] Loss: -452.470734\n",
      "Train Epoch: 659 [11776/54000 (22%)] Loss: -492.604492\n",
      "Train Epoch: 659 [23040/54000 (43%)] Loss: -478.562775\n",
      "Train Epoch: 659 [34304/54000 (64%)] Loss: -410.492188\n",
      "Train Epoch: 659 [45568/54000 (84%)] Loss: -474.394348\n",
      "    epoch          : 659\n",
      "    loss           : -424.58645065307616\n",
      "    val_loss       : -385.2133445043117\n",
      "    val_log_likelihood: 493.3361328125\n",
      "    val_log_marginal: 392.182891221717\n",
      "Train Epoch: 660 [512/54000 (1%)] Loss: -410.044922\n",
      "Train Epoch: 660 [11776/54000 (22%)] Loss: -430.445618\n",
      "Train Epoch: 660 [23040/54000 (43%)] Loss: -420.491821\n",
      "Train Epoch: 660 [34304/54000 (64%)] Loss: -394.452484\n",
      "Train Epoch: 660 [45568/54000 (84%)] Loss: -263.032288\n",
      "    epoch          : 660\n",
      "    loss           : -409.84110778808594\n",
      "    val_loss       : -378.12213516589253\n",
      "    val_log_likelihood: 492.9992736816406\n",
      "    val_log_marginal: 385.43834519423547\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch660.pth ...\n",
      "Train Epoch: 661 [512/54000 (1%)] Loss: -208.829742\n",
      "Train Epoch: 661 [11776/54000 (22%)] Loss: -572.101807\n",
      "Train Epoch: 661 [23040/54000 (43%)] Loss: -429.328857\n",
      "Train Epoch: 661 [34304/54000 (64%)] Loss: -467.825012\n",
      "Train Epoch: 661 [45568/54000 (84%)] Loss: -455.800446\n",
      "    epoch          : 661\n",
      "    loss           : -424.0877610778809\n",
      "    val_loss       : -391.35313340900467\n",
      "    val_log_likelihood: 496.2879669189453\n",
      "    val_log_marginal: 395.58269259296355\n",
      "Train Epoch: 662 [512/54000 (1%)] Loss: -497.955872\n",
      "Train Epoch: 662 [11776/54000 (22%)] Loss: -476.003448\n",
      "Train Epoch: 662 [23040/54000 (43%)] Loss: -252.902954\n",
      "Train Epoch: 662 [34304/54000 (64%)] Loss: -448.101318\n",
      "Train Epoch: 662 [45568/54000 (84%)] Loss: -451.615173\n",
      "    epoch          : 662\n",
      "    loss           : -427.61079818725585\n",
      "    val_loss       : -394.13029290707783\n",
      "    val_log_likelihood: 496.9147094726562\n",
      "    val_log_marginal: 399.29981203638016\n",
      "Train Epoch: 663 [512/54000 (1%)] Loss: -460.064453\n",
      "Train Epoch: 663 [11776/54000 (22%)] Loss: -268.306519\n",
      "Train Epoch: 663 [23040/54000 (43%)] Loss: -420.699341\n",
      "Train Epoch: 663 [34304/54000 (64%)] Loss: -401.778320\n",
      "Train Epoch: 663 [45568/54000 (84%)] Loss: -444.235748\n",
      "    epoch          : 663\n",
      "    loss           : -423.75940490722655\n",
      "    val_loss       : -388.1557229405269\n",
      "    val_log_likelihood: 496.1591552734375\n",
      "    val_log_marginal: 396.83677229247996\n",
      "Train Epoch: 664 [512/54000 (1%)] Loss: -467.115173\n",
      "Train Epoch: 664 [11776/54000 (22%)] Loss: -487.819885\n",
      "Train Epoch: 664 [23040/54000 (43%)] Loss: -433.471802\n",
      "Train Epoch: 664 [34304/54000 (64%)] Loss: -479.208923\n",
      "Train Epoch: 664 [45568/54000 (84%)] Loss: -453.022552\n",
      "    epoch          : 664\n",
      "    loss           : -430.5529110717773\n",
      "    val_loss       : -391.27256240127605\n",
      "    val_log_likelihood: 497.4897521972656\n",
      "    val_log_marginal: 396.20244587771595\n",
      "Train Epoch: 665 [512/54000 (1%)] Loss: -425.696320\n",
      "Train Epoch: 665 [11776/54000 (22%)] Loss: -454.459656\n",
      "Train Epoch: 665 [23040/54000 (43%)] Loss: -276.953918\n",
      "Train Epoch: 665 [34304/54000 (64%)] Loss: -467.887329\n",
      "Train Epoch: 665 [45568/54000 (84%)] Loss: -587.508972\n",
      "    epoch          : 665\n",
      "    loss           : -425.63088775634765\n",
      "    val_loss       : -384.75765276513994\n",
      "    val_log_likelihood: 497.69208374023435\n",
      "    val_log_marginal: 389.05684642198855\n",
      "Train Epoch: 666 [512/54000 (1%)] Loss: -433.208984\n",
      "Train Epoch: 666 [11776/54000 (22%)] Loss: -427.596191\n",
      "Train Epoch: 666 [23040/54000 (43%)] Loss: -475.009552\n",
      "Train Epoch: 666 [34304/54000 (64%)] Loss: -553.118774\n",
      "Train Epoch: 666 [45568/54000 (84%)] Loss: -368.279663\n",
      "    epoch          : 666\n",
      "    loss           : -403.61721237182616\n",
      "    val_loss       : -363.8042657734826\n",
      "    val_log_likelihood: 494.4040161132813\n",
      "    val_log_marginal: 369.9513137702598\n",
      "Train Epoch: 667 [512/54000 (1%)] Loss: -482.662354\n",
      "Train Epoch: 667 [11776/54000 (22%)] Loss: -553.284058\n",
      "Train Epoch: 667 [23040/54000 (43%)] Loss: -149.486755\n",
      "Train Epoch: 667 [34304/54000 (64%)] Loss: -395.966797\n",
      "Train Epoch: 667 [45568/54000 (84%)] Loss: -344.778503\n",
      "    epoch          : 667\n",
      "    loss           : -377.3661628723145\n",
      "    val_loss       : -315.15595437493175\n",
      "    val_log_likelihood: 481.20949401855466\n",
      "    val_log_marginal: 321.41692096181214\n",
      "Train Epoch: 668 [512/54000 (1%)] Loss: -348.563629\n",
      "Train Epoch: 668 [11776/54000 (22%)] Loss: -413.101196\n",
      "Train Epoch: 668 [23040/54000 (43%)] Loss: -485.596588\n",
      "Train Epoch: 668 [34304/54000 (64%)] Loss: -237.706619\n",
      "Train Epoch: 668 [45568/54000 (84%)] Loss: -172.203827\n",
      "    epoch          : 668\n",
      "    loss           : -383.81843322753906\n",
      "    val_loss       : -360.80780265536157\n",
      "    val_log_likelihood: 487.5921569824219\n",
      "    val_log_marginal: 367.6350477579984\n",
      "Train Epoch: 669 [512/54000 (1%)] Loss: -429.211395\n",
      "Train Epoch: 669 [11776/54000 (22%)] Loss: -456.326019\n",
      "Train Epoch: 669 [23040/54000 (43%)] Loss: -232.525192\n",
      "Train Epoch: 669 [34304/54000 (64%)] Loss: -398.218658\n",
      "Train Epoch: 669 [45568/54000 (84%)] Loss: -409.465729\n",
      "    epoch          : 669\n",
      "    loss           : -388.4037319946289\n",
      "    val_loss       : -296.0424135601148\n",
      "    val_log_likelihood: 476.5813873291016\n",
      "    val_log_marginal: 301.5283126030117\n",
      "Train Epoch: 670 [512/54000 (1%)] Loss: -226.511963\n",
      "Train Epoch: 670 [11776/54000 (22%)] Loss: -552.426758\n",
      "Train Epoch: 670 [23040/54000 (43%)] Loss: -468.911194\n",
      "Train Epoch: 670 [34304/54000 (64%)] Loss: -434.896973\n",
      "Train Epoch: 670 [45568/54000 (84%)] Loss: -470.167664\n",
      "    epoch          : 670\n",
      "    loss           : -394.97075286865237\n",
      "    val_loss       : -389.0288252165541\n",
      "    val_log_likelihood: 494.9488555908203\n",
      "    val_log_marginal: 397.3414909530238\n",
      "Train Epoch: 671 [512/54000 (1%)] Loss: -264.230743\n",
      "Train Epoch: 671 [11776/54000 (22%)] Loss: -265.465607\n",
      "Train Epoch: 671 [23040/54000 (43%)] Loss: -575.387817\n",
      "Train Epoch: 671 [34304/54000 (64%)] Loss: -226.312378\n",
      "Train Epoch: 671 [45568/54000 (84%)] Loss: -435.001678\n",
      "    epoch          : 671\n",
      "    loss           : -423.61306427001955\n",
      "    val_loss       : -386.6209647041745\n",
      "    val_log_likelihood: 497.23296813964845\n",
      "    val_log_marginal: 391.91489699520173\n",
      "Train Epoch: 672 [512/54000 (1%)] Loss: -488.960754\n",
      "Train Epoch: 672 [11776/54000 (22%)] Loss: -233.004608\n",
      "Train Epoch: 672 [23040/54000 (43%)] Loss: -219.991638\n",
      "Train Epoch: 672 [34304/54000 (64%)] Loss: -458.133057\n",
      "Train Epoch: 672 [45568/54000 (84%)] Loss: -477.010986\n",
      "    epoch          : 672\n",
      "    loss           : -426.1581895446777\n",
      "    val_loss       : -392.71726182615384\n",
      "    val_log_likelihood: 495.58421936035154\n",
      "    val_log_marginal: 399.20963073037564\n",
      "Train Epoch: 673 [512/54000 (1%)] Loss: -247.735275\n",
      "Train Epoch: 673 [11776/54000 (22%)] Loss: -290.891174\n",
      "Train Epoch: 673 [23040/54000 (43%)] Loss: -471.613037\n",
      "Train Epoch: 673 [34304/54000 (64%)] Loss: -448.337769\n",
      "Train Epoch: 673 [45568/54000 (84%)] Loss: -284.380554\n",
      "    epoch          : 673\n",
      "    loss           : -430.9169538879394\n",
      "    val_loss       : -391.1158021725714\n",
      "    val_log_likelihood: 499.1246673583984\n",
      "    val_log_marginal: 397.35423964783547\n",
      "Train Epoch: 674 [512/54000 (1%)] Loss: -453.960449\n",
      "Train Epoch: 674 [11776/54000 (22%)] Loss: -454.550293\n",
      "Train Epoch: 674 [23040/54000 (43%)] Loss: -470.714233\n",
      "Train Epoch: 674 [34304/54000 (64%)] Loss: -475.894196\n",
      "Train Epoch: 674 [45568/54000 (84%)] Loss: -402.311035\n",
      "    epoch          : 674\n",
      "    loss           : -428.35791931152346\n",
      "    val_loss       : -383.3656276607886\n",
      "    val_log_likelihood: 493.78004760742186\n",
      "    val_log_marginal: 386.6297163185671\n",
      "Train Epoch: 675 [512/54000 (1%)] Loss: -421.535339\n",
      "Train Epoch: 675 [11776/54000 (22%)] Loss: -417.686676\n",
      "Train Epoch: 675 [23040/54000 (43%)] Loss: -541.849365\n",
      "Train Epoch: 675 [34304/54000 (64%)] Loss: -457.761475\n",
      "Train Epoch: 675 [45568/54000 (84%)] Loss: -445.079712\n",
      "    epoch          : 675\n",
      "    loss           : -405.1350128173828\n",
      "    val_loss       : -353.6727101240307\n",
      "    val_log_likelihood: 485.2701446533203\n",
      "    val_log_marginal: 357.06051825409145\n",
      "Train Epoch: 676 [512/54000 (1%)] Loss: -362.327057\n",
      "Train Epoch: 676 [11776/54000 (22%)] Loss: -387.475159\n",
      "Train Epoch: 676 [23040/54000 (43%)] Loss: -241.045639\n",
      "Train Epoch: 676 [34304/54000 (64%)] Loss: -459.721191\n",
      "Train Epoch: 676 [45568/54000 (84%)] Loss: -413.946014\n",
      "    epoch          : 676\n",
      "    loss           : -395.01966217041013\n",
      "    val_loss       : -386.47109342627226\n",
      "    val_log_likelihood: 495.54819641113284\n",
      "    val_log_marginal: 395.1200545940548\n",
      "Train Epoch: 677 [512/54000 (1%)] Loss: -458.542664\n",
      "Train Epoch: 677 [11776/54000 (22%)] Loss: -545.217285\n",
      "Train Epoch: 677 [23040/54000 (43%)] Loss: -428.526093\n",
      "Train Epoch: 677 [34304/54000 (64%)] Loss: -314.586823\n",
      "Train Epoch: 677 [45568/54000 (84%)] Loss: -219.893646\n",
      "    epoch          : 677\n",
      "    loss           : -340.35140327453615\n",
      "    val_loss       : -291.5150903759524\n",
      "    val_log_likelihood: 459.43157653808595\n",
      "    val_log_marginal: 308.0428272936493\n",
      "Train Epoch: 678 [512/54000 (1%)] Loss: -343.505280\n",
      "Train Epoch: 678 [11776/54000 (22%)] Loss: -524.442017\n",
      "Train Epoch: 678 [23040/54000 (43%)] Loss: -422.771942\n",
      "Train Epoch: 678 [34304/54000 (64%)] Loss: -448.045288\n",
      "Train Epoch: 678 [45568/54000 (84%)] Loss: -398.781372\n",
      "    epoch          : 678\n",
      "    loss           : -381.3447631072998\n",
      "    val_loss       : -382.3887656349689\n",
      "    val_log_likelihood: 492.78550415039064\n",
      "    val_log_marginal: 386.7176104900423\n",
      "Train Epoch: 679 [512/54000 (1%)] Loss: -405.995514\n",
      "Train Epoch: 679 [11776/54000 (22%)] Loss: -451.329346\n",
      "Train Epoch: 679 [23040/54000 (43%)] Loss: -275.066956\n",
      "Train Epoch: 679 [34304/54000 (64%)] Loss: -473.120728\n",
      "Train Epoch: 679 [45568/54000 (84%)] Loss: -266.122864\n",
      "    epoch          : 679\n",
      "    loss           : -418.5370208740234\n",
      "    val_loss       : -392.3231489005499\n",
      "    val_log_likelihood: 498.5101379394531\n",
      "    val_log_marginal: 396.76110598109665\n",
      "Train Epoch: 680 [512/54000 (1%)] Loss: -235.825760\n",
      "Train Epoch: 680 [11776/54000 (22%)] Loss: -240.595032\n",
      "Train Epoch: 680 [23040/54000 (43%)] Loss: -447.197388\n",
      "Train Epoch: 680 [34304/54000 (64%)] Loss: -397.067871\n",
      "Train Epoch: 680 [45568/54000 (84%)] Loss: -273.326050\n",
      "    epoch          : 680\n",
      "    loss           : -425.7161177062988\n",
      "    val_loss       : -394.2322289789096\n",
      "    val_log_likelihood: 497.8647399902344\n",
      "    val_log_marginal: 398.4283191334456\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch680.pth ...\n",
      "Train Epoch: 681 [512/54000 (1%)] Loss: -268.422699\n",
      "Train Epoch: 681 [11776/54000 (22%)] Loss: -486.495026\n",
      "Train Epoch: 681 [23040/54000 (43%)] Loss: -452.544739\n",
      "Train Epoch: 681 [34304/54000 (64%)] Loss: -565.551636\n",
      "Train Epoch: 681 [45568/54000 (84%)] Loss: -247.077515\n",
      "    epoch          : 681\n",
      "    loss           : -426.42755249023435\n",
      "    val_loss       : -393.82487232591956\n",
      "    val_log_likelihood: 501.5550506591797\n",
      "    val_log_marginal: 398.8970736810089\n",
      "Train Epoch: 682 [512/54000 (1%)] Loss: -484.857208\n",
      "Train Epoch: 682 [11776/54000 (22%)] Loss: -461.762970\n",
      "Train Epoch: 682 [23040/54000 (43%)] Loss: -218.929749\n",
      "Train Epoch: 682 [34304/54000 (64%)] Loss: -577.283813\n",
      "Train Epoch: 682 [45568/54000 (84%)] Loss: -587.869873\n",
      "    epoch          : 682\n",
      "    loss           : -422.80973526000975\n",
      "    val_loss       : -388.1852010152303\n",
      "    val_log_likelihood: 498.55963134765625\n",
      "    val_log_marginal: 392.1573607001031\n",
      "Train Epoch: 683 [512/54000 (1%)] Loss: -436.169525\n",
      "Train Epoch: 683 [11776/54000 (22%)] Loss: -457.062805\n",
      "Train Epoch: 683 [23040/54000 (43%)] Loss: -456.371216\n",
      "Train Epoch: 683 [34304/54000 (64%)] Loss: -195.881805\n",
      "Train Epoch: 683 [45568/54000 (84%)] Loss: -369.485046\n",
      "    epoch          : 683\n",
      "    loss           : -405.63494216918946\n",
      "    val_loss       : -364.90748779866846\n",
      "    val_log_likelihood: 493.75546875\n",
      "    val_log_marginal: 374.9284037772566\n",
      "Train Epoch: 684 [512/54000 (1%)] Loss: -477.270996\n",
      "Train Epoch: 684 [11776/54000 (22%)] Loss: -431.097870\n",
      "Train Epoch: 684 [23040/54000 (43%)] Loss: -440.800690\n",
      "Train Epoch: 684 [34304/54000 (64%)] Loss: -464.130371\n",
      "Train Epoch: 684 [45568/54000 (84%)] Loss: -399.643311\n",
      "    epoch          : 684\n",
      "    loss           : -401.7196112060547\n",
      "    val_loss       : -371.7847731024027\n",
      "    val_log_likelihood: 491.9151885986328\n",
      "    val_log_marginal: 375.1229771953076\n",
      "Train Epoch: 685 [512/54000 (1%)] Loss: -424.153503\n",
      "Train Epoch: 685 [11776/54000 (22%)] Loss: -236.640030\n",
      "Train Epoch: 685 [23040/54000 (43%)] Loss: -458.301849\n",
      "Train Epoch: 685 [34304/54000 (64%)] Loss: -276.460144\n",
      "Train Epoch: 685 [45568/54000 (84%)] Loss: -401.900391\n",
      "    epoch          : 685\n",
      "    loss           : -391.24600891113283\n",
      "    val_loss       : -369.54561379924417\n",
      "    val_log_likelihood: 495.3196990966797\n",
      "    val_log_marginal: 375.683286299184\n",
      "Train Epoch: 686 [512/54000 (1%)] Loss: -468.723236\n",
      "Train Epoch: 686 [11776/54000 (22%)] Loss: -548.325012\n",
      "Train Epoch: 686 [23040/54000 (43%)] Loss: -268.251221\n",
      "Train Epoch: 686 [34304/54000 (64%)] Loss: -361.462585\n",
      "Train Epoch: 686 [45568/54000 (84%)] Loss: -454.949402\n",
      "    epoch          : 686\n",
      "    loss           : -403.86423065185545\n",
      "    val_loss       : -375.4393225638196\n",
      "    val_log_likelihood: 494.9455902099609\n",
      "    val_log_marginal: 381.43890040554106\n",
      "Train Epoch: 687 [512/54000 (1%)] Loss: -561.480347\n",
      "Train Epoch: 687 [11776/54000 (22%)] Loss: -552.263184\n",
      "Train Epoch: 687 [23040/54000 (43%)] Loss: -443.860596\n",
      "Train Epoch: 687 [34304/54000 (64%)] Loss: -421.748199\n",
      "Train Epoch: 687 [45568/54000 (84%)] Loss: -348.886627\n",
      "    epoch          : 687\n",
      "    loss           : -387.3090766906738\n",
      "    val_loss       : -330.1415317453444\n",
      "    val_log_likelihood: 488.8051300048828\n",
      "    val_log_marginal: 338.50712877176704\n",
      "Train Epoch: 688 [512/54000 (1%)] Loss: -519.218506\n",
      "Train Epoch: 688 [11776/54000 (22%)] Loss: -246.885895\n",
      "Train Epoch: 688 [23040/54000 (43%)] Loss: -188.968643\n",
      "Train Epoch: 688 [34304/54000 (64%)] Loss: -429.371307\n",
      "Train Epoch: 688 [45568/54000 (84%)] Loss: -433.326141\n",
      "    epoch          : 688\n",
      "    loss           : -369.05188888549804\n",
      "    val_loss       : -355.6580170575529\n",
      "    val_log_likelihood: 485.2156494140625\n",
      "    val_log_marginal: 367.42565112225714\n",
      "Train Epoch: 689 [512/54000 (1%)] Loss: -445.444092\n",
      "Train Epoch: 689 [11776/54000 (22%)] Loss: -333.245728\n",
      "Train Epoch: 689 [23040/54000 (43%)] Loss: -167.504028\n",
      "Train Epoch: 689 [34304/54000 (64%)] Loss: -199.825089\n",
      "Train Epoch: 689 [45568/54000 (84%)] Loss: -408.821045\n",
      "    epoch          : 689\n",
      "    loss           : -317.90072227478026\n",
      "    val_loss       : -332.367768252641\n",
      "    val_log_likelihood: 487.1142333984375\n",
      "    val_log_marginal: 339.7700837757655\n",
      "Train Epoch: 690 [512/54000 (1%)] Loss: -377.314453\n",
      "Train Epoch: 690 [11776/54000 (22%)] Loss: -449.751282\n",
      "Train Epoch: 690 [23040/54000 (43%)] Loss: -449.411865\n",
      "Train Epoch: 690 [34304/54000 (64%)] Loss: -285.356445\n",
      "Train Epoch: 690 [45568/54000 (84%)] Loss: -246.797852\n",
      "    epoch          : 690\n",
      "    loss           : -415.4904063415527\n",
      "    val_loss       : -389.92401856547224\n",
      "    val_log_likelihood: 493.9499267578125\n",
      "    val_log_marginal: 394.6312936399132\n",
      "Train Epoch: 691 [512/54000 (1%)] Loss: -480.662415\n",
      "Train Epoch: 691 [11776/54000 (22%)] Loss: -456.226959\n",
      "Train Epoch: 691 [23040/54000 (43%)] Loss: -457.013245\n",
      "Train Epoch: 691 [34304/54000 (64%)] Loss: -459.963776\n",
      "Train Epoch: 691 [45568/54000 (84%)] Loss: -457.947968\n",
      "    epoch          : 691\n",
      "    loss           : -423.08384841918945\n",
      "    val_loss       : -398.3879393476993\n",
      "    val_log_likelihood: 497.73602905273435\n",
      "    val_log_marginal: 403.09884826354687\n",
      "Train Epoch: 692 [512/54000 (1%)] Loss: -448.392120\n",
      "Train Epoch: 692 [11776/54000 (22%)] Loss: -470.224670\n",
      "Train Epoch: 692 [23040/54000 (43%)] Loss: -450.709564\n",
      "Train Epoch: 692 [34304/54000 (64%)] Loss: -281.609375\n",
      "Train Epoch: 692 [45568/54000 (84%)] Loss: -395.643921\n",
      "    epoch          : 692\n",
      "    loss           : -425.77743515014646\n",
      "    val_loss       : -360.2970213959925\n",
      "    val_log_likelihood: 493.3717559814453\n",
      "    val_log_marginal: 366.8927039641887\n",
      "Train Epoch: 693 [512/54000 (1%)] Loss: -232.506744\n",
      "Train Epoch: 693 [11776/54000 (22%)] Loss: -420.892517\n",
      "Train Epoch: 693 [23040/54000 (43%)] Loss: -325.030579\n",
      "Train Epoch: 693 [34304/54000 (64%)] Loss: -429.801331\n",
      "Train Epoch: 693 [45568/54000 (84%)] Loss: -231.167480\n",
      "    epoch          : 693\n",
      "    loss           : -367.8671883392334\n",
      "    val_loss       : -337.2765803509392\n",
      "    val_log_likelihood: 497.00182189941404\n",
      "    val_log_marginal: 344.46090077497064\n",
      "Train Epoch: 694 [512/54000 (1%)] Loss: -133.932114\n",
      "Train Epoch: 694 [11776/54000 (22%)] Loss: -407.026794\n",
      "Train Epoch: 694 [23040/54000 (43%)] Loss: -438.393799\n",
      "Train Epoch: 694 [34304/54000 (64%)] Loss: -443.284027\n",
      "Train Epoch: 694 [45568/54000 (84%)] Loss: -267.005554\n",
      "    epoch          : 694\n",
      "    loss           : -401.7342825317383\n",
      "    val_loss       : -370.95926934015006\n",
      "    val_log_likelihood: 493.04511108398435\n",
      "    val_log_marginal: 383.6077719386667\n",
      "Train Epoch: 695 [512/54000 (1%)] Loss: -562.946106\n",
      "Train Epoch: 695 [11776/54000 (22%)] Loss: -467.110260\n",
      "Train Epoch: 695 [23040/54000 (43%)] Loss: -437.051300\n",
      "Train Epoch: 695 [34304/54000 (64%)] Loss: -435.887970\n",
      "Train Epoch: 695 [45568/54000 (84%)] Loss: -257.991730\n",
      "    epoch          : 695\n",
      "    loss           : -390.0187719726562\n",
      "    val_loss       : -381.56589331980797\n",
      "    val_log_likelihood: 492.64512023925784\n",
      "    val_log_marginal: 386.5008751797837\n",
      "Train Epoch: 696 [512/54000 (1%)] Loss: -542.875366\n",
      "Train Epoch: 696 [11776/54000 (22%)] Loss: -483.896179\n",
      "Train Epoch: 696 [23040/54000 (43%)] Loss: -446.271851\n",
      "Train Epoch: 696 [34304/54000 (64%)] Loss: -418.053680\n",
      "Train Epoch: 696 [45568/54000 (84%)] Loss: -459.565308\n",
      "    epoch          : 696\n",
      "    loss           : -411.43044036865234\n",
      "    val_loss       : -383.6506270254031\n",
      "    val_log_likelihood: 494.1328094482422\n",
      "    val_log_marginal: 387.2890583369881\n",
      "Train Epoch: 697 [512/54000 (1%)] Loss: -441.016113\n",
      "Train Epoch: 697 [11776/54000 (22%)] Loss: -493.792114\n",
      "Train Epoch: 697 [23040/54000 (43%)] Loss: -497.769287\n",
      "Train Epoch: 697 [34304/54000 (64%)] Loss: -250.918045\n",
      "Train Epoch: 697 [45568/54000 (84%)] Loss: -282.999878\n",
      "    epoch          : 697\n",
      "    loss           : -428.21558044433596\n",
      "    val_loss       : -399.7951924870722\n",
      "    val_log_likelihood: 500.50904541015626\n",
      "    val_log_marginal: 403.925881503539\n",
      "Train Epoch: 698 [512/54000 (1%)] Loss: -497.375793\n",
      "Train Epoch: 698 [11776/54000 (22%)] Loss: -396.721924\n",
      "Train Epoch: 698 [23040/54000 (43%)] Loss: -445.432281\n",
      "Train Epoch: 698 [34304/54000 (64%)] Loss: -455.800720\n",
      "Train Epoch: 698 [45568/54000 (84%)] Loss: -468.517059\n",
      "    epoch          : 698\n",
      "    loss           : -431.3874824523926\n",
      "    val_loss       : -395.01788666518405\n",
      "    val_log_likelihood: 499.0760498046875\n",
      "    val_log_marginal: 398.4923368837684\n",
      "Train Epoch: 699 [512/54000 (1%)] Loss: -459.307098\n",
      "Train Epoch: 699 [11776/54000 (22%)] Loss: -489.519409\n",
      "Train Epoch: 699 [23040/54000 (43%)] Loss: -456.227722\n",
      "Train Epoch: 699 [34304/54000 (64%)] Loss: -444.965149\n",
      "Train Epoch: 699 [45568/54000 (84%)] Loss: -261.216553\n",
      "    epoch          : 699\n",
      "    loss           : -420.708896484375\n",
      "    val_loss       : -376.1827587226406\n",
      "    val_log_likelihood: 498.45232849121095\n",
      "    val_log_marginal: 381.7770292390313\n",
      "Train Epoch: 700 [512/54000 (1%)] Loss: -427.795715\n",
      "Train Epoch: 700 [11776/54000 (22%)] Loss: -538.364136\n",
      "Train Epoch: 700 [23040/54000 (43%)] Loss: -550.474243\n",
      "Train Epoch: 700 [34304/54000 (64%)] Loss: -336.550476\n",
      "Train Epoch: 700 [45568/54000 (84%)] Loss: -204.606003\n",
      "    epoch          : 700\n",
      "    loss           : -349.2690947532654\n",
      "    val_loss       : -254.408517965395\n",
      "    val_log_likelihood: 481.50054321289065\n",
      "    val_log_marginal: 264.21717898137865\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch700.pth ...\n",
      "Train Epoch: 701 [512/54000 (1%)] Loss: -71.708931\n",
      "Train Epoch: 701 [11776/54000 (22%)] Loss: -432.618469\n",
      "Train Epoch: 701 [23040/54000 (43%)] Loss: -388.606201\n",
      "Train Epoch: 701 [34304/54000 (64%)] Loss: -537.354492\n",
      "Train Epoch: 701 [45568/54000 (84%)] Loss: -193.806213\n",
      "    epoch          : 701\n",
      "    loss           : -354.40130294799803\n",
      "    val_loss       : -364.2311350178905\n",
      "    val_log_likelihood: 496.1568542480469\n",
      "    val_log_marginal: 368.72473839707675\n",
      "Train Epoch: 702 [512/54000 (1%)] Loss: -403.010254\n",
      "Train Epoch: 702 [11776/54000 (22%)] Loss: -391.623535\n",
      "Train Epoch: 702 [23040/54000 (43%)] Loss: -282.275787\n",
      "Train Epoch: 702 [34304/54000 (64%)] Loss: -476.164215\n",
      "Train Epoch: 702 [45568/54000 (84%)] Loss: -472.384979\n",
      "    epoch          : 702\n",
      "    loss           : -423.38311584472655\n",
      "    val_loss       : -399.26476527964695\n",
      "    val_log_likelihood: 499.16241455078125\n",
      "    val_log_marginal: 403.3388814713806\n",
      "Train Epoch: 703 [512/54000 (1%)] Loss: -448.303894\n",
      "Train Epoch: 703 [11776/54000 (22%)] Loss: -467.515442\n",
      "Train Epoch: 703 [23040/54000 (43%)] Loss: -250.894058\n",
      "Train Epoch: 703 [34304/54000 (64%)] Loss: -470.428253\n",
      "Train Epoch: 703 [45568/54000 (84%)] Loss: -591.085815\n",
      "    epoch          : 703\n",
      "    loss           : -433.9305421447754\n",
      "    val_loss       : -398.20419255271554\n",
      "    val_log_likelihood: 499.9955078125\n",
      "    val_log_marginal: 403.01965359492544\n",
      "Train Epoch: 704 [512/54000 (1%)] Loss: -490.469025\n",
      "Train Epoch: 704 [11776/54000 (22%)] Loss: -465.296051\n",
      "Train Epoch: 704 [23040/54000 (43%)] Loss: -472.148285\n",
      "Train Epoch: 704 [34304/54000 (64%)] Loss: -486.109802\n",
      "Train Epoch: 704 [45568/54000 (84%)] Loss: -418.420410\n",
      "    epoch          : 704\n",
      "    loss           : -436.3322564697266\n",
      "    val_loss       : -397.70357320141045\n",
      "    val_log_likelihood: 498.07984313964846\n",
      "    val_log_marginal: 402.543540308997\n",
      "Train Epoch: 705 [512/54000 (1%)] Loss: -485.726685\n",
      "Train Epoch: 705 [11776/54000 (22%)] Loss: -268.193909\n",
      "Train Epoch: 705 [23040/54000 (43%)] Loss: -444.889526\n",
      "Train Epoch: 705 [34304/54000 (64%)] Loss: -466.628296\n",
      "Train Epoch: 705 [45568/54000 (84%)] Loss: -474.841888\n",
      "    epoch          : 705\n",
      "    loss           : -430.0416819763184\n",
      "    val_loss       : -400.2524755895138\n",
      "    val_log_likelihood: 506.67588500976564\n",
      "    val_log_marginal: 407.50680146850647\n",
      "Train Epoch: 706 [512/54000 (1%)] Loss: -436.702881\n",
      "Train Epoch: 706 [11776/54000 (22%)] Loss: -507.238464\n",
      "Train Epoch: 706 [23040/54000 (43%)] Loss: -231.750046\n",
      "Train Epoch: 706 [34304/54000 (64%)] Loss: -284.905518\n",
      "Train Epoch: 706 [45568/54000 (84%)] Loss: -386.950195\n",
      "    epoch          : 706\n",
      "    loss           : -412.6751244354248\n",
      "    val_loss       : -370.3991647182964\n",
      "    val_log_likelihood: 495.4409515380859\n",
      "    val_log_marginal: 376.7002670228253\n",
      "Train Epoch: 707 [512/54000 (1%)] Loss: -441.410095\n",
      "Train Epoch: 707 [11776/54000 (22%)] Loss: -569.036926\n",
      "Train Epoch: 707 [23040/54000 (43%)] Loss: -430.032166\n",
      "Train Epoch: 707 [34304/54000 (64%)] Loss: -555.942017\n",
      "Train Epoch: 707 [45568/54000 (84%)] Loss: -367.461060\n",
      "    epoch          : 707\n",
      "    loss           : -397.32333770751956\n",
      "    val_loss       : -390.9823000065982\n",
      "    val_log_likelihood: 501.7616424560547\n",
      "    val_log_marginal: 395.4990792506605\n",
      "Train Epoch: 708 [512/54000 (1%)] Loss: -422.674194\n",
      "Train Epoch: 708 [11776/54000 (22%)] Loss: -478.563904\n",
      "Train Epoch: 708 [23040/54000 (43%)] Loss: -240.272385\n",
      "Train Epoch: 708 [34304/54000 (64%)] Loss: -479.748383\n",
      "Train Epoch: 708 [45568/54000 (84%)] Loss: -275.554352\n",
      "    epoch          : 708\n",
      "    loss           : -433.76198776245116\n",
      "    val_loss       : -401.65907918401064\n",
      "    val_log_likelihood: 505.2757568359375\n",
      "    val_log_marginal: 408.71466230545286\n",
      "Train Epoch: 709 [512/54000 (1%)] Loss: -494.892181\n",
      "Train Epoch: 709 [11776/54000 (22%)] Loss: -273.049896\n",
      "Train Epoch: 709 [23040/54000 (43%)] Loss: -252.699707\n",
      "Train Epoch: 709 [34304/54000 (64%)] Loss: -242.692276\n",
      "Train Epoch: 709 [45568/54000 (84%)] Loss: -457.238647\n",
      "    epoch          : 709\n",
      "    loss           : -436.91758865356445\n",
      "    val_loss       : -361.1119580113329\n",
      "    val_log_likelihood: 499.33763122558594\n",
      "    val_log_marginal: 365.56847070418297\n",
      "Train Epoch: 710 [512/54000 (1%)] Loss: -388.257355\n",
      "Train Epoch: 710 [11776/54000 (22%)] Loss: -476.481781\n",
      "Train Epoch: 710 [23040/54000 (43%)] Loss: -404.332306\n",
      "Train Epoch: 710 [34304/54000 (64%)] Loss: -566.690308\n",
      "Train Epoch: 710 [45568/54000 (84%)] Loss: -193.166626\n",
      "    epoch          : 710\n",
      "    loss           : -367.57091640472413\n",
      "    val_loss       : -215.07659798730165\n",
      "    val_log_likelihood: 457.89417877197263\n",
      "    val_log_marginal: 227.63180024586617\n",
      "Train Epoch: 711 [512/54000 (1%)] Loss: -213.874802\n",
      "Train Epoch: 711 [11776/54000 (22%)] Loss: -426.815430\n",
      "Train Epoch: 711 [23040/54000 (43%)] Loss: -423.202759\n",
      "Train Epoch: 711 [34304/54000 (64%)] Loss: -234.049179\n",
      "Train Epoch: 711 [45568/54000 (84%)] Loss: -432.234497\n",
      "    epoch          : 711\n",
      "    loss           : -343.4546619796753\n",
      "    val_loss       : -372.9178380944766\n",
      "    val_log_likelihood: 489.2435821533203\n",
      "    val_log_marginal: 378.0055741176298\n",
      "Train Epoch: 712 [512/54000 (1%)] Loss: -484.388458\n",
      "Train Epoch: 712 [11776/54000 (22%)] Loss: -460.603149\n",
      "Train Epoch: 712 [23040/54000 (43%)] Loss: -395.920044\n",
      "Train Epoch: 712 [34304/54000 (64%)] Loss: -559.220215\n",
      "Train Epoch: 712 [45568/54000 (84%)] Loss: -397.944336\n",
      "    epoch          : 712\n",
      "    loss           : -405.00659332275393\n",
      "    val_loss       : -379.6624230565503\n",
      "    val_log_likelihood: 494.3650695800781\n",
      "    val_log_marginal: 387.06787704117596\n",
      "Train Epoch: 713 [512/54000 (1%)] Loss: -417.821014\n",
      "Train Epoch: 713 [11776/54000 (22%)] Loss: -416.449280\n",
      "Train Epoch: 713 [23040/54000 (43%)] Loss: -451.543335\n",
      "Train Epoch: 713 [34304/54000 (64%)] Loss: -488.666748\n",
      "Train Epoch: 713 [45568/54000 (84%)] Loss: -440.638855\n",
      "    epoch          : 713\n",
      "    loss           : -425.24609222412107\n",
      "    val_loss       : -387.68466800963506\n",
      "    val_log_likelihood: 498.9744506835938\n",
      "    val_log_marginal: 392.091697571054\n",
      "Train Epoch: 714 [512/54000 (1%)] Loss: -226.565292\n",
      "Train Epoch: 714 [11776/54000 (22%)] Loss: -463.997925\n",
      "Train Epoch: 714 [23040/54000 (43%)] Loss: -442.944092\n",
      "Train Epoch: 714 [34304/54000 (64%)] Loss: -428.107849\n",
      "Train Epoch: 714 [45568/54000 (84%)] Loss: -205.227783\n",
      "    epoch          : 714\n",
      "    loss           : -411.0801203918457\n",
      "    val_loss       : -350.17378773577514\n",
      "    val_log_likelihood: 498.5530975341797\n",
      "    val_log_marginal: 352.69969302900137\n",
      "Train Epoch: 715 [512/54000 (1%)] Loss: -421.633362\n",
      "Train Epoch: 715 [11776/54000 (22%)] Loss: -409.364807\n",
      "Train Epoch: 715 [23040/54000 (43%)] Loss: -452.219055\n",
      "Train Epoch: 715 [34304/54000 (64%)] Loss: -452.845184\n",
      "Train Epoch: 715 [45568/54000 (84%)] Loss: -266.683899\n",
      "    epoch          : 715\n",
      "    loss           : -386.43233093261716\n",
      "    val_loss       : -380.98222420057283\n",
      "    val_log_likelihood: 501.3136322021484\n",
      "    val_log_marginal: 386.30768425174256\n",
      "Train Epoch: 716 [512/54000 (1%)] Loss: -424.409515\n",
      "Train Epoch: 716 [11776/54000 (22%)] Loss: -573.688477\n",
      "Train Epoch: 716 [23040/54000 (43%)] Loss: -426.821136\n",
      "Train Epoch: 716 [34304/54000 (64%)] Loss: -135.565689\n",
      "Train Epoch: 716 [45568/54000 (84%)] Loss: -375.433594\n",
      "    epoch          : 716\n",
      "    loss           : -369.2114865112305\n",
      "    val_loss       : -357.89226904753593\n",
      "    val_log_likelihood: 491.0823699951172\n",
      "    val_log_marginal: 365.6280890140682\n",
      "Train Epoch: 717 [512/54000 (1%)] Loss: -452.889038\n",
      "Train Epoch: 717 [11776/54000 (22%)] Loss: -464.259521\n",
      "Train Epoch: 717 [23040/54000 (43%)] Loss: -380.642120\n",
      "Train Epoch: 717 [34304/54000 (64%)] Loss: -463.166595\n",
      "Train Epoch: 717 [45568/54000 (84%)] Loss: -251.703583\n",
      "    epoch          : 717\n",
      "    loss           : -402.5526285171509\n",
      "    val_loss       : -373.9708192991093\n",
      "    val_log_likelihood: 498.96300354003904\n",
      "    val_log_marginal: 380.5651130016893\n",
      "Train Epoch: 718 [512/54000 (1%)] Loss: -487.489410\n",
      "Train Epoch: 718 [11776/54000 (22%)] Loss: -484.116821\n",
      "Train Epoch: 718 [23040/54000 (43%)] Loss: -454.323944\n",
      "Train Epoch: 718 [34304/54000 (64%)] Loss: -448.004303\n",
      "Train Epoch: 718 [45568/54000 (84%)] Loss: -591.073181\n",
      "    epoch          : 718\n",
      "    loss           : -419.03003128051756\n",
      "    val_loss       : -387.7332329560071\n",
      "    val_log_likelihood: 505.2412475585937\n",
      "    val_log_marginal: 391.9526565697044\n",
      "Train Epoch: 719 [512/54000 (1%)] Loss: -588.995544\n",
      "Train Epoch: 719 [11776/54000 (22%)] Loss: -469.904510\n",
      "Train Epoch: 719 [23040/54000 (43%)] Loss: -398.603790\n",
      "Train Epoch: 719 [34304/54000 (64%)] Loss: -419.720947\n",
      "Train Epoch: 719 [45568/54000 (84%)] Loss: -407.270813\n",
      "    epoch          : 719\n",
      "    loss           : -401.8189181518555\n",
      "    val_loss       : -370.17848494630306\n",
      "    val_log_likelihood: 495.6933197021484\n",
      "    val_log_marginal: 373.62282227687535\n",
      "Train Epoch: 720 [512/54000 (1%)] Loss: -464.079468\n",
      "Train Epoch: 720 [11776/54000 (22%)] Loss: -570.713379\n",
      "Train Epoch: 720 [23040/54000 (43%)] Loss: -433.979980\n",
      "Train Epoch: 720 [34304/54000 (64%)] Loss: -261.063110\n",
      "Train Epoch: 720 [45568/54000 (84%)] Loss: -441.195984\n",
      "    epoch          : 720\n",
      "    loss           : -421.99396682739257\n",
      "    val_loss       : -393.82989230733364\n",
      "    val_log_likelihood: 504.67870178222654\n",
      "    val_log_marginal: 399.2642700199038\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch720.pth ...\n",
      "Train Epoch: 721 [512/54000 (1%)] Loss: -596.843994\n",
      "Train Epoch: 721 [11776/54000 (22%)] Loss: -264.445984\n",
      "Train Epoch: 721 [23040/54000 (43%)] Loss: -486.170593\n",
      "Train Epoch: 721 [34304/54000 (64%)] Loss: -454.718903\n",
      "Train Epoch: 721 [45568/54000 (84%)] Loss: -477.205933\n",
      "    epoch          : 721\n",
      "    loss           : -438.82688064575194\n",
      "    val_loss       : -406.91816085521134\n",
      "    val_log_likelihood: 508.9534576416016\n",
      "    val_log_marginal: 411.71508021838963\n",
      "Train Epoch: 722 [512/54000 (1%)] Loss: -273.499359\n",
      "Train Epoch: 722 [11776/54000 (22%)] Loss: -453.722076\n",
      "Train Epoch: 722 [23040/54000 (43%)] Loss: -487.482666\n",
      "Train Epoch: 722 [34304/54000 (64%)] Loss: -603.587585\n",
      "Train Epoch: 722 [45568/54000 (84%)] Loss: -283.413025\n",
      "    epoch          : 722\n",
      "    loss           : -442.81108947753904\n",
      "    val_loss       : -404.8346742251888\n",
      "    val_log_likelihood: 508.5247741699219\n",
      "    val_log_marginal: 410.6183303404599\n",
      "Train Epoch: 723 [512/54000 (1%)] Loss: -483.332397\n",
      "Train Epoch: 723 [11776/54000 (22%)] Loss: -274.905731\n",
      "Train Epoch: 723 [23040/54000 (43%)] Loss: -237.784271\n",
      "Train Epoch: 723 [34304/54000 (64%)] Loss: -434.617188\n",
      "Train Epoch: 723 [45568/54000 (84%)] Loss: -472.077911\n",
      "    epoch          : 723\n",
      "    loss           : -428.2146966552734\n",
      "    val_loss       : -383.50669022779914\n",
      "    val_log_likelihood: 501.242822265625\n",
      "    val_log_marginal: 387.4135114725679\n",
      "Train Epoch: 724 [512/54000 (1%)] Loss: -481.144592\n",
      "Train Epoch: 724 [11776/54000 (22%)] Loss: -438.891479\n",
      "Train Epoch: 724 [23040/54000 (43%)] Loss: -427.638062\n",
      "Train Epoch: 724 [34304/54000 (64%)] Loss: -423.560364\n",
      "Train Epoch: 724 [45568/54000 (84%)] Loss: -488.448334\n",
      "    epoch          : 724\n",
      "    loss           : -431.32884979248047\n",
      "    val_loss       : -400.37166548669336\n",
      "    val_log_likelihood: 504.6644012451172\n",
      "    val_log_marginal: 403.7650234778788\n",
      "Train Epoch: 725 [512/54000 (1%)] Loss: -588.600952\n",
      "Train Epoch: 725 [11776/54000 (22%)] Loss: -493.273651\n",
      "Train Epoch: 725 [23040/54000 (43%)] Loss: -478.615051\n",
      "Train Epoch: 725 [34304/54000 (64%)] Loss: -452.562988\n",
      "Train Epoch: 725 [45568/54000 (84%)] Loss: -391.060791\n",
      "    epoch          : 725\n",
      "    loss           : -425.5313888549805\n",
      "    val_loss       : -385.05024979468436\n",
      "    val_log_likelihood: 503.824267578125\n",
      "    val_log_marginal: 390.9260689906058\n",
      "Train Epoch: 726 [512/54000 (1%)] Loss: -570.070190\n",
      "Train Epoch: 726 [11776/54000 (22%)] Loss: -458.745697\n",
      "Train Epoch: 726 [23040/54000 (43%)] Loss: -221.391541\n",
      "Train Epoch: 726 [34304/54000 (64%)] Loss: -506.427643\n",
      "Train Epoch: 726 [45568/54000 (84%)] Loss: -464.803406\n",
      "    epoch          : 726\n",
      "    loss           : -426.9175555419922\n",
      "    val_loss       : -402.8196386405267\n",
      "    val_log_likelihood: 506.3696533203125\n",
      "    val_log_marginal: 406.37044259645046\n",
      "Train Epoch: 727 [512/54000 (1%)] Loss: -590.375610\n",
      "Train Epoch: 727 [11776/54000 (22%)] Loss: -254.683975\n",
      "Train Epoch: 727 [23040/54000 (43%)] Loss: -461.229248\n",
      "Train Epoch: 727 [34304/54000 (64%)] Loss: -366.850006\n",
      "Train Epoch: 727 [45568/54000 (84%)] Loss: 0.436354\n",
      "    epoch          : 727\n",
      "    loss           : -405.36074213027956\n",
      "    val_loss       : -230.42406669044868\n",
      "    val_log_likelihood: 497.0053405761719\n",
      "    val_log_marginal: 246.7856041055173\n",
      "Train Epoch: 728 [512/54000 (1%)] Loss: -378.127228\n",
      "Train Epoch: 728 [11776/54000 (22%)] Loss: 108.393585\n",
      "Train Epoch: 728 [23040/54000 (43%)] Loss: -195.730530\n",
      "Train Epoch: 728 [34304/54000 (64%)] Loss: -151.236984\n",
      "Train Epoch: 728 [45568/54000 (84%)] Loss: -127.661224\n",
      "    epoch          : 728\n",
      "    loss           : -163.60905880451202\n",
      "    val_loss       : -157.7667495947331\n",
      "    val_log_likelihood: 445.2772918701172\n",
      "    val_log_marginal: 194.46175156645478\n",
      "Train Epoch: 729 [512/54000 (1%)] Loss: -155.108109\n",
      "Train Epoch: 729 [11776/54000 (22%)] Loss: -278.432343\n",
      "Train Epoch: 729 [23040/54000 (43%)] Loss: -429.344482\n",
      "Train Epoch: 729 [34304/54000 (64%)] Loss: -387.907288\n",
      "Train Epoch: 729 [45568/54000 (84%)] Loss: -443.100403\n",
      "    epoch          : 729\n",
      "    loss           : -350.8957141494751\n",
      "    val_loss       : -376.51658326871694\n",
      "    val_log_likelihood: 497.6994567871094\n",
      "    val_log_marginal: 381.91678352728485\n",
      "Train Epoch: 730 [512/54000 (1%)] Loss: -451.903839\n",
      "Train Epoch: 730 [11776/54000 (22%)] Loss: -456.632812\n",
      "Train Epoch: 730 [23040/54000 (43%)] Loss: -471.551117\n",
      "Train Epoch: 730 [34304/54000 (64%)] Loss: -502.488953\n",
      "Train Epoch: 730 [45568/54000 (84%)] Loss: -468.175751\n",
      "    epoch          : 730\n",
      "    loss           : -424.73690124511717\n",
      "    val_loss       : -389.9239422418177\n",
      "    val_log_likelihood: 500.28936767578125\n",
      "    val_log_marginal: 396.51510696299385\n",
      "Train Epoch: 731 [512/54000 (1%)] Loss: -256.350464\n",
      "Train Epoch: 731 [11776/54000 (22%)] Loss: -453.036682\n",
      "Train Epoch: 731 [23040/54000 (43%)] Loss: -476.409607\n",
      "Train Epoch: 731 [34304/54000 (64%)] Loss: -470.078735\n",
      "Train Epoch: 731 [45568/54000 (84%)] Loss: -475.397888\n",
      "    epoch          : 731\n",
      "    loss           : -438.43110229492186\n",
      "    val_loss       : -403.1765131788328\n",
      "    val_log_likelihood: 504.6802001953125\n",
      "    val_log_marginal: 409.42767752595245\n",
      "Train Epoch: 732 [512/54000 (1%)] Loss: -499.927765\n",
      "Train Epoch: 732 [11776/54000 (22%)] Loss: -422.531494\n",
      "Train Epoch: 732 [23040/54000 (43%)] Loss: -469.588043\n",
      "Train Epoch: 732 [34304/54000 (64%)] Loss: -495.823547\n",
      "Train Epoch: 732 [45568/54000 (84%)] Loss: -470.214600\n",
      "    epoch          : 732\n",
      "    loss           : -442.2948065185547\n",
      "    val_loss       : -405.6878725149669\n",
      "    val_log_likelihood: 506.96283569335935\n",
      "    val_log_marginal: 409.3308925855795\n",
      "Train Epoch: 733 [512/54000 (1%)] Loss: -461.214813\n",
      "Train Epoch: 733 [11776/54000 (22%)] Loss: -498.995331\n",
      "Train Epoch: 733 [23040/54000 (43%)] Loss: -288.437073\n",
      "Train Epoch: 733 [34304/54000 (64%)] Loss: -499.448273\n",
      "Train Epoch: 733 [45568/54000 (84%)] Loss: -431.364594\n",
      "    epoch          : 733\n",
      "    loss           : -441.39115585327147\n",
      "    val_loss       : -405.03813000712546\n",
      "    val_log_likelihood: 502.39080505371095\n",
      "    val_log_marginal: 408.60189367027266\n",
      "Train Epoch: 734 [512/54000 (1%)] Loss: -505.528442\n",
      "Train Epoch: 734 [11776/54000 (22%)] Loss: -444.651337\n",
      "Train Epoch: 734 [23040/54000 (43%)] Loss: -460.650574\n",
      "Train Epoch: 734 [34304/54000 (64%)] Loss: -506.322723\n",
      "Train Epoch: 734 [45568/54000 (84%)] Loss: -280.655029\n",
      "    epoch          : 734\n",
      "    loss           : -438.502995300293\n",
      "    val_loss       : -398.64893297981473\n",
      "    val_log_likelihood: 505.1730499267578\n",
      "    val_log_marginal: 404.4570539671928\n",
      "Train Epoch: 735 [512/54000 (1%)] Loss: -234.376999\n",
      "Train Epoch: 735 [11776/54000 (22%)] Loss: -467.338623\n",
      "Train Epoch: 735 [23040/54000 (43%)] Loss: -440.906982\n",
      "Train Epoch: 735 [34304/54000 (64%)] Loss: -206.091522\n",
      "Train Epoch: 735 [45568/54000 (84%)] Loss: -367.166840\n",
      "    epoch          : 735\n",
      "    loss           : -401.20286140441897\n",
      "    val_loss       : -343.3133679036051\n",
      "    val_log_likelihood: 485.6074645996094\n",
      "    val_log_marginal: 350.6304768513898\n",
      "Train Epoch: 736 [512/54000 (1%)] Loss: -433.500397\n",
      "Train Epoch: 736 [11776/54000 (22%)] Loss: -573.171936\n",
      "Train Epoch: 736 [23040/54000 (43%)] Loss: -387.807129\n",
      "Train Epoch: 736 [34304/54000 (64%)] Loss: -451.281860\n",
      "Train Epoch: 736 [45568/54000 (84%)] Loss: -466.945801\n",
      "    epoch          : 736\n",
      "    loss           : -415.2723484802246\n",
      "    val_loss       : -383.0609085017815\n",
      "    val_log_likelihood: 499.894482421875\n",
      "    val_log_marginal: 388.3714164245874\n",
      "Train Epoch: 737 [512/54000 (1%)] Loss: -477.253113\n",
      "Train Epoch: 737 [11776/54000 (22%)] Loss: -242.551544\n",
      "Train Epoch: 737 [23040/54000 (43%)] Loss: -463.827240\n",
      "Train Epoch: 737 [34304/54000 (64%)] Loss: -472.265472\n",
      "Train Epoch: 737 [45568/54000 (84%)] Loss: -288.705200\n",
      "    epoch          : 737\n",
      "    loss           : -428.69956405639647\n",
      "    val_loss       : -404.1195017129183\n",
      "    val_log_likelihood: 508.19737854003904\n",
      "    val_log_marginal: 407.5314953301102\n",
      "Train Epoch: 738 [512/54000 (1%)] Loss: -469.083954\n",
      "Train Epoch: 738 [11776/54000 (22%)] Loss: -455.070953\n",
      "Train Epoch: 738 [23040/54000 (43%)] Loss: -467.559753\n",
      "Train Epoch: 738 [34304/54000 (64%)] Loss: -562.357849\n",
      "Train Epoch: 738 [45568/54000 (84%)] Loss: -468.609070\n",
      "    epoch          : 738\n",
      "    loss           : -426.05807006835937\n",
      "    val_loss       : -395.5786950102076\n",
      "    val_log_likelihood: 506.1122650146484\n",
      "    val_log_marginal: 402.24040379710493\n",
      "Train Epoch: 739 [512/54000 (1%)] Loss: -585.883606\n",
      "Train Epoch: 739 [11776/54000 (22%)] Loss: -471.977203\n",
      "Train Epoch: 739 [23040/54000 (43%)] Loss: -464.188019\n",
      "Train Epoch: 739 [34304/54000 (64%)] Loss: -458.271210\n",
      "Train Epoch: 739 [45568/54000 (84%)] Loss: -416.453400\n",
      "    epoch          : 739\n",
      "    loss           : -426.9477314758301\n",
      "    val_loss       : -394.38982959818094\n",
      "    val_log_likelihood: 503.7709930419922\n",
      "    val_log_marginal: 401.2332317788495\n",
      "Train Epoch: 740 [512/54000 (1%)] Loss: -485.325287\n",
      "Train Epoch: 740 [11776/54000 (22%)] Loss: -239.776062\n",
      "Train Epoch: 740 [23040/54000 (43%)] Loss: -483.790253\n",
      "Train Epoch: 740 [34304/54000 (64%)] Loss: -476.892303\n",
      "Train Epoch: 740 [45568/54000 (84%)] Loss: -471.705170\n",
      "    epoch          : 740\n",
      "    loss           : -437.6073147583008\n",
      "    val_loss       : -403.9719835611992\n",
      "    val_log_likelihood: 508.6619476318359\n",
      "    val_log_marginal: 407.9383408781141\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch740.pth ...\n",
      "Train Epoch: 741 [512/54000 (1%)] Loss: -600.968262\n",
      "Train Epoch: 741 [11776/54000 (22%)] Loss: -480.450775\n",
      "Train Epoch: 741 [23040/54000 (43%)] Loss: -467.831268\n",
      "Train Epoch: 741 [34304/54000 (64%)] Loss: -441.081726\n",
      "Train Epoch: 741 [45568/54000 (84%)] Loss: -240.978302\n",
      "    epoch          : 741\n",
      "    loss           : -423.2307521057129\n",
      "    val_loss       : -384.4712080093101\n",
      "    val_log_likelihood: 506.0091918945312\n",
      "    val_log_marginal: 391.4679875414819\n",
      "Train Epoch: 742 [512/54000 (1%)] Loss: -416.178345\n",
      "Train Epoch: 742 [11776/54000 (22%)] Loss: -401.383301\n",
      "Train Epoch: 742 [23040/54000 (43%)] Loss: -440.931091\n",
      "Train Epoch: 742 [34304/54000 (64%)] Loss: -455.987061\n",
      "Train Epoch: 742 [45568/54000 (84%)] Loss: -413.070068\n",
      "    epoch          : 742\n",
      "    loss           : -415.77416107177737\n",
      "    val_loss       : -401.37294037397953\n",
      "    val_log_likelihood: 505.11011657714846\n",
      "    val_log_marginal: 403.26380350776014\n",
      "Train Epoch: 743 [512/54000 (1%)] Loss: -278.486206\n",
      "Train Epoch: 743 [11776/54000 (22%)] Loss: -505.643982\n",
      "Train Epoch: 743 [23040/54000 (43%)] Loss: -594.661865\n",
      "Train Epoch: 743 [34304/54000 (64%)] Loss: -405.529266\n",
      "Train Epoch: 743 [45568/54000 (84%)] Loss: -279.868835\n",
      "    epoch          : 743\n",
      "    loss           : -427.62979278564455\n",
      "    val_loss       : -389.0482302339748\n",
      "    val_log_likelihood: 505.04469909667966\n",
      "    val_log_marginal: 395.66122918538844\n",
      "Train Epoch: 744 [512/54000 (1%)] Loss: -460.195496\n",
      "Train Epoch: 744 [11776/54000 (22%)] Loss: -478.142883\n",
      "Train Epoch: 744 [23040/54000 (43%)] Loss: -472.102295\n",
      "Train Epoch: 744 [34304/54000 (64%)] Loss: -554.712830\n",
      "Train Epoch: 744 [45568/54000 (84%)] Loss: -349.351746\n",
      "    epoch          : 744\n",
      "    loss           : -419.97369430541994\n",
      "    val_loss       : -332.49313234342264\n",
      "    val_log_likelihood: 492.6209243774414\n",
      "    val_log_marginal: 337.2018141333014\n",
      "Train Epoch: 745 [512/54000 (1%)] Loss: -198.319000\n",
      "Train Epoch: 745 [11776/54000 (22%)] Loss: -111.930466\n",
      "Train Epoch: 745 [23040/54000 (43%)] Loss: -189.642868\n",
      "Train Epoch: 745 [34304/54000 (64%)] Loss: -134.833206\n",
      "Train Epoch: 745 [45568/54000 (84%)] Loss: -14.562456\n",
      "    epoch          : 745\n",
      "    loss           : -316.4394586515427\n",
      "    val_loss       : -269.1547374300659\n",
      "    val_log_likelihood: 470.55093841552736\n",
      "    val_log_marginal: 282.7176024191458\n",
      "Train Epoch: 746 [512/54000 (1%)] Loss: -350.051758\n",
      "Train Epoch: 746 [11776/54000 (22%)] Loss: -254.626053\n",
      "Train Epoch: 746 [23040/54000 (43%)] Loss: -423.489227\n",
      "Train Epoch: 746 [34304/54000 (64%)] Loss: -430.615173\n",
      "Train Epoch: 746 [45568/54000 (84%)] Loss: -541.372437\n",
      "    epoch          : 746\n",
      "    loss           : -311.2908696317673\n",
      "    val_loss       : -352.09582732589917\n",
      "    val_log_likelihood: 491.3711791992188\n",
      "    val_log_marginal: 369.283912517503\n",
      "Train Epoch: 747 [512/54000 (1%)] Loss: -579.073364\n",
      "Train Epoch: 747 [11776/54000 (22%)] Loss: -388.007294\n",
      "Train Epoch: 747 [23040/54000 (43%)] Loss: -463.602844\n",
      "Train Epoch: 747 [34304/54000 (64%)] Loss: -243.692978\n",
      "Train Epoch: 747 [45568/54000 (84%)] Loss: -441.657043\n",
      "    epoch          : 747\n",
      "    loss           : -397.19412506103515\n",
      "    val_loss       : -354.2257121785544\n",
      "    val_log_likelihood: 498.8296752929688\n",
      "    val_log_marginal: 359.8674405384809\n",
      "Train Epoch: 748 [512/54000 (1%)] Loss: -216.541489\n",
      "Train Epoch: 748 [11776/54000 (22%)] Loss: -394.439453\n",
      "Train Epoch: 748 [23040/54000 (43%)] Loss: -400.961426\n",
      "Train Epoch: 748 [34304/54000 (64%)] Loss: -494.887390\n",
      "Train Epoch: 748 [45568/54000 (84%)] Loss: -403.718201\n",
      "    epoch          : 748\n",
      "    loss           : -344.006354598999\n",
      "    val_loss       : -329.3659194687381\n",
      "    val_log_likelihood: 489.2395721435547\n",
      "    val_log_marginal: 337.4172993663699\n",
      "Train Epoch: 749 [512/54000 (1%)] Loss: -176.949280\n",
      "Train Epoch: 749 [11776/54000 (22%)] Loss: -463.613007\n",
      "Train Epoch: 749 [23040/54000 (43%)] Loss: -399.271210\n",
      "Train Epoch: 749 [34304/54000 (64%)] Loss: -382.099487\n",
      "Train Epoch: 749 [45568/54000 (84%)] Loss: -448.942657\n",
      "    epoch          : 749\n",
      "    loss           : -407.42388671875\n",
      "    val_loss       : -371.11201038453726\n",
      "    val_log_likelihood: 503.06675109863284\n",
      "    val_log_marginal: 380.5129140283977\n",
      "Train Epoch: 750 [512/54000 (1%)] Loss: -273.522766\n",
      "Train Epoch: 750 [11776/54000 (22%)] Loss: -444.421112\n",
      "Train Epoch: 750 [23040/54000 (43%)] Loss: -261.020050\n",
      "Train Epoch: 750 [34304/54000 (64%)] Loss: -466.694672\n",
      "Train Epoch: 750 [45568/54000 (84%)] Loss: -412.635895\n",
      "    epoch          : 750\n",
      "    loss           : -417.6265124511719\n",
      "    val_loss       : -384.4956331201829\n",
      "    val_log_likelihood: 502.86134643554686\n",
      "    val_log_marginal: 388.09357000030576\n",
      "Train Epoch: 751 [512/54000 (1%)] Loss: -456.390839\n",
      "Train Epoch: 751 [11776/54000 (22%)] Loss: -243.996399\n",
      "Train Epoch: 751 [23040/54000 (43%)] Loss: -251.482681\n",
      "Train Epoch: 751 [34304/54000 (64%)] Loss: -483.973785\n",
      "Train Epoch: 751 [45568/54000 (84%)] Loss: -485.471466\n",
      "    epoch          : 751\n",
      "    loss           : -435.2439678955078\n",
      "    val_loss       : -404.00070251347495\n",
      "    val_log_likelihood: 505.17799987792966\n",
      "    val_log_marginal: 407.6064334798604\n",
      "Train Epoch: 752 [512/54000 (1%)] Loss: -499.370667\n",
      "Train Epoch: 752 [11776/54000 (22%)] Loss: -507.485229\n",
      "Train Epoch: 752 [23040/54000 (43%)] Loss: -485.397644\n",
      "Train Epoch: 752 [34304/54000 (64%)] Loss: -596.441467\n",
      "Train Epoch: 752 [45568/54000 (84%)] Loss: -470.647125\n",
      "    epoch          : 752\n",
      "    loss           : -441.9260636901856\n",
      "    val_loss       : -402.365296124015\n",
      "    val_log_likelihood: 504.2474029541016\n",
      "    val_log_marginal: 406.90026756458326\n",
      "Train Epoch: 753 [512/54000 (1%)] Loss: -465.711456\n",
      "Train Epoch: 753 [11776/54000 (22%)] Loss: -261.097107\n",
      "Train Epoch: 753 [23040/54000 (43%)] Loss: -596.821472\n",
      "Train Epoch: 753 [34304/54000 (64%)] Loss: -477.893738\n",
      "Train Epoch: 753 [45568/54000 (84%)] Loss: -403.533752\n",
      "    epoch          : 753\n",
      "    loss           : -437.5748471069336\n",
      "    val_loss       : -383.0501909369603\n",
      "    val_log_likelihood: 499.6563018798828\n",
      "    val_log_marginal: 390.8860877465457\n",
      "Train Epoch: 754 [512/54000 (1%)] Loss: -502.845245\n",
      "Train Epoch: 754 [11776/54000 (22%)] Loss: -582.784607\n",
      "Train Epoch: 754 [23040/54000 (43%)] Loss: -443.921326\n",
      "Train Epoch: 754 [34304/54000 (64%)] Loss: -467.746460\n",
      "Train Epoch: 754 [45568/54000 (84%)] Loss: -479.675171\n",
      "    epoch          : 754\n",
      "    loss           : -430.7430764770508\n",
      "    val_loss       : -401.63455212414266\n",
      "    val_log_likelihood: 506.703955078125\n",
      "    val_log_marginal: 407.56909215338527\n",
      "Train Epoch: 755 [512/54000 (1%)] Loss: -445.875183\n",
      "Train Epoch: 755 [11776/54000 (22%)] Loss: -477.204590\n",
      "Train Epoch: 755 [23040/54000 (43%)] Loss: -454.180725\n",
      "Train Epoch: 755 [34304/54000 (64%)] Loss: -467.545776\n",
      "Train Epoch: 755 [45568/54000 (84%)] Loss: -247.857574\n",
      "    epoch          : 755\n",
      "    loss           : -419.5513331604004\n",
      "    val_loss       : -362.1527314352803\n",
      "    val_log_likelihood: 496.1171936035156\n",
      "    val_log_marginal: 372.5218716614263\n",
      "Train Epoch: 756 [512/54000 (1%)] Loss: -434.867950\n",
      "Train Epoch: 756 [11776/54000 (22%)] Loss: -485.749359\n",
      "Train Epoch: 756 [23040/54000 (43%)] Loss: -468.553955\n",
      "Train Epoch: 756 [34304/54000 (64%)] Loss: -422.724823\n",
      "Train Epoch: 756 [45568/54000 (84%)] Loss: -342.443481\n",
      "    epoch          : 756\n",
      "    loss           : -387.40709465026856\n",
      "    val_loss       : -365.26305924132464\n",
      "    val_log_likelihood: 497.63697509765626\n",
      "    val_log_marginal: 372.0250862110406\n",
      "Train Epoch: 757 [512/54000 (1%)] Loss: -436.012909\n",
      "Train Epoch: 757 [11776/54000 (22%)] Loss: -315.620300\n",
      "Train Epoch: 757 [23040/54000 (43%)] Loss: -455.298615\n",
      "Train Epoch: 757 [34304/54000 (64%)] Loss: -297.056946\n",
      "Train Epoch: 757 [45568/54000 (84%)] Loss: -408.774536\n",
      "    epoch          : 757\n",
      "    loss           : -345.16642444610596\n",
      "    val_loss       : -376.00080897528676\n",
      "    val_log_likelihood: 498.8050048828125\n",
      "    val_log_marginal: 383.4404497542009\n",
      "Train Epoch: 758 [512/54000 (1%)] Loss: -467.357666\n",
      "Train Epoch: 758 [11776/54000 (22%)] Loss: -488.318054\n",
      "Train Epoch: 758 [23040/54000 (43%)] Loss: -491.411987\n",
      "Train Epoch: 758 [34304/54000 (64%)] Loss: -469.701355\n",
      "Train Epoch: 758 [45568/54000 (84%)] Loss: -455.890991\n",
      "    epoch          : 758\n",
      "    loss           : -425.53717025756833\n",
      "    val_loss       : -386.73248478686435\n",
      "    val_log_likelihood: 503.2665161132812\n",
      "    val_log_marginal: 390.5441018458456\n",
      "Train Epoch: 759 [512/54000 (1%)] Loss: -448.712555\n",
      "Train Epoch: 759 [11776/54000 (22%)] Loss: -464.060516\n",
      "Train Epoch: 759 [23040/54000 (43%)] Loss: -504.323792\n",
      "Train Epoch: 759 [34304/54000 (64%)] Loss: -429.475830\n",
      "Train Epoch: 759 [45568/54000 (84%)] Loss: -488.976532\n",
      "    epoch          : 759\n",
      "    loss           : -437.3335238647461\n",
      "    val_loss       : -398.7893458319828\n",
      "    val_log_likelihood: 507.17133178710935\n",
      "    val_log_marginal: 403.13888754776826\n",
      "Train Epoch: 760 [512/54000 (1%)] Loss: -452.403107\n",
      "Train Epoch: 760 [11776/54000 (22%)] Loss: -587.165771\n",
      "Train Epoch: 760 [23040/54000 (43%)] Loss: -463.353943\n",
      "Train Epoch: 760 [34304/54000 (64%)] Loss: -449.023987\n",
      "Train Epoch: 760 [45568/54000 (84%)] Loss: -405.233765\n",
      "    epoch          : 760\n",
      "    loss           : -431.2474932861328\n",
      "    val_loss       : -401.3207848144695\n",
      "    val_log_likelihood: 508.17210388183594\n",
      "    val_log_marginal: 407.9490263197571\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch760.pth ...\n",
      "Train Epoch: 761 [512/54000 (1%)] Loss: -273.834778\n",
      "Train Epoch: 761 [11776/54000 (22%)] Loss: -489.806946\n",
      "Train Epoch: 761 [23040/54000 (43%)] Loss: -444.710999\n",
      "Train Epoch: 761 [34304/54000 (64%)] Loss: -459.415588\n",
      "Train Epoch: 761 [45568/54000 (84%)] Loss: -471.541138\n",
      "    epoch          : 761\n",
      "    loss           : -440.5422770690918\n",
      "    val_loss       : -402.0724873702973\n",
      "    val_log_likelihood: 507.3718566894531\n",
      "    val_log_marginal: 404.63376388471835\n",
      "Train Epoch: 762 [512/54000 (1%)] Loss: -483.889587\n",
      "Train Epoch: 762 [11776/54000 (22%)] Loss: -448.173096\n",
      "Train Epoch: 762 [23040/54000 (43%)] Loss: -484.742950\n",
      "Train Epoch: 762 [34304/54000 (64%)] Loss: -565.629150\n",
      "Train Epoch: 762 [45568/54000 (84%)] Loss: -296.418121\n",
      "    epoch          : 762\n",
      "    loss           : -420.48789276123046\n",
      "    val_loss       : -368.51056155404075\n",
      "    val_log_likelihood: 503.86655578613284\n",
      "    val_log_marginal: 372.6568249076605\n",
      "Train Epoch: 763 [512/54000 (1%)] Loss: -458.152344\n",
      "Train Epoch: 763 [11776/54000 (22%)] Loss: -406.247772\n",
      "Train Epoch: 763 [23040/54000 (43%)] Loss: -481.283020\n",
      "Train Epoch: 763 [34304/54000 (64%)] Loss: -577.813232\n",
      "Train Epoch: 763 [45568/54000 (84%)] Loss: -489.808899\n",
      "    epoch          : 763\n",
      "    loss           : -419.32349807739257\n",
      "    val_loss       : -399.73862041775135\n",
      "    val_log_likelihood: 506.9326934814453\n",
      "    val_log_marginal: 403.927299907431\n",
      "Train Epoch: 764 [512/54000 (1%)] Loss: -489.822235\n",
      "Train Epoch: 764 [11776/54000 (22%)] Loss: -461.351135\n",
      "Train Epoch: 764 [23040/54000 (43%)] Loss: -484.985596\n",
      "Train Epoch: 764 [34304/54000 (64%)] Loss: -258.029419\n",
      "Train Epoch: 764 [45568/54000 (84%)] Loss: -486.353149\n",
      "    epoch          : 764\n",
      "    loss           : -441.745538482666\n",
      "    val_loss       : -401.36294211288913\n",
      "    val_log_likelihood: 508.6227630615234\n",
      "    val_log_marginal: 407.90594119615855\n",
      "Train Epoch: 765 [512/54000 (1%)] Loss: -487.312622\n",
      "Train Epoch: 765 [11776/54000 (22%)] Loss: -605.410645\n",
      "Train Epoch: 765 [23040/54000 (43%)] Loss: -469.812164\n",
      "Train Epoch: 765 [34304/54000 (64%)] Loss: -305.566406\n",
      "Train Epoch: 765 [45568/54000 (84%)] Loss: -238.143845\n",
      "    epoch          : 765\n",
      "    loss           : -441.8341633605957\n",
      "    val_loss       : -396.60233345488086\n",
      "    val_log_likelihood: 508.3448211669922\n",
      "    val_log_marginal: 402.2565271284431\n",
      "Train Epoch: 766 [512/54000 (1%)] Loss: -418.735443\n",
      "Train Epoch: 766 [11776/54000 (22%)] Loss: -402.238403\n",
      "Train Epoch: 766 [23040/54000 (43%)] Loss: -446.524902\n",
      "Train Epoch: 766 [34304/54000 (64%)] Loss: -269.205200\n",
      "Train Epoch: 766 [45568/54000 (84%)] Loss: -260.476379\n",
      "    epoch          : 766\n",
      "    loss           : -412.94989059448244\n",
      "    val_loss       : -336.3541114637628\n",
      "    val_log_likelihood: 509.1263092041016\n",
      "    val_log_marginal: 342.74730916582047\n",
      "Train Epoch: 767 [512/54000 (1%)] Loss: -350.567200\n",
      "Train Epoch: 767 [11776/54000 (22%)] Loss: -201.990326\n",
      "Train Epoch: 767 [23040/54000 (43%)] Loss: -563.746948\n",
      "Train Epoch: 767 [34304/54000 (64%)] Loss: -434.848541\n",
      "Train Epoch: 767 [45568/54000 (84%)] Loss: -433.487152\n",
      "    epoch          : 767\n",
      "    loss           : -384.9870199584961\n",
      "    val_loss       : -365.63449956513944\n",
      "    val_log_likelihood: 501.7937744140625\n",
      "    val_log_marginal: 389.33025203607974\n",
      "Train Epoch: 768 [512/54000 (1%)] Loss: -215.359680\n",
      "Train Epoch: 768 [11776/54000 (22%)] Loss: -392.036407\n",
      "Train Epoch: 768 [23040/54000 (43%)] Loss: -449.435608\n",
      "Train Epoch: 768 [34304/54000 (64%)] Loss: -376.244690\n",
      "Train Epoch: 768 [45568/54000 (84%)] Loss: -379.227722\n",
      "    epoch          : 768\n",
      "    loss           : -402.7640251159668\n",
      "    val_loss       : -386.6843864200637\n",
      "    val_log_likelihood: 505.30437622070315\n",
      "    val_log_marginal: 393.1877955315807\n",
      "Train Epoch: 769 [512/54000 (1%)] Loss: -472.740051\n",
      "Train Epoch: 769 [11776/54000 (22%)] Loss: -443.419800\n",
      "Train Epoch: 769 [23040/54000 (43%)] Loss: -499.282440\n",
      "Train Epoch: 769 [34304/54000 (64%)] Loss: -493.293274\n",
      "Train Epoch: 769 [45568/54000 (84%)] Loss: -411.333862\n",
      "    epoch          : 769\n",
      "    loss           : -429.3443617248535\n",
      "    val_loss       : -383.43592468919235\n",
      "    val_log_likelihood: 504.82294006347655\n",
      "    val_log_marginal: 389.4559195347316\n",
      "Train Epoch: 770 [512/54000 (1%)] Loss: -584.243286\n",
      "Train Epoch: 770 [11776/54000 (22%)] Loss: -508.250916\n",
      "Train Epoch: 770 [23040/54000 (43%)] Loss: -454.027527\n",
      "Train Epoch: 770 [34304/54000 (64%)] Loss: -463.733704\n",
      "Train Epoch: 770 [45568/54000 (84%)] Loss: -381.076263\n",
      "    epoch          : 770\n",
      "    loss           : -433.9734196472168\n",
      "    val_loss       : -388.37030648076905\n",
      "    val_log_likelihood: 507.7943420410156\n",
      "    val_log_marginal: 396.65411388464275\n",
      "Train Epoch: 771 [512/54000 (1%)] Loss: -236.823181\n",
      "Train Epoch: 771 [11776/54000 (22%)] Loss: -436.654175\n",
      "Train Epoch: 771 [23040/54000 (43%)] Loss: -465.776367\n",
      "Train Epoch: 771 [34304/54000 (64%)] Loss: -439.791382\n",
      "Train Epoch: 771 [45568/54000 (84%)] Loss: -353.283508\n",
      "    epoch          : 771\n",
      "    loss           : -409.0872018432617\n",
      "    val_loss       : -380.199356249813\n",
      "    val_log_likelihood: 498.26781311035154\n",
      "    val_log_marginal: 383.45585267059505\n",
      "Train Epoch: 772 [512/54000 (1%)] Loss: -450.355774\n",
      "Train Epoch: 772 [11776/54000 (22%)] Loss: -454.434235\n",
      "Train Epoch: 772 [23040/54000 (43%)] Loss: -436.604187\n",
      "Train Epoch: 772 [34304/54000 (64%)] Loss: -473.784607\n",
      "Train Epoch: 772 [45568/54000 (84%)] Loss: -289.747864\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   772: reducing learning rate of group 0 to 1.0000e-04.\n",
      "    epoch          : 772\n",
      "    loss           : -429.1160757446289\n",
      "    val_loss       : -384.9145217984915\n",
      "    val_log_likelihood: 502.9166717529297\n",
      "    val_log_marginal: 388.5575367167088\n",
      "Train Epoch: 773 [512/54000 (1%)] Loss: -437.443604\n",
      "Train Epoch: 773 [11776/54000 (22%)] Loss: -498.275818\n",
      "Train Epoch: 773 [23040/54000 (43%)] Loss: -608.808960\n",
      "Train Epoch: 773 [34304/54000 (64%)] Loss: -472.152435\n",
      "Train Epoch: 773 [45568/54000 (84%)] Loss: -481.116791\n",
      "    epoch          : 773\n",
      "    loss           : -447.92971633911134\n",
      "    val_loss       : -412.2089907709509\n",
      "    val_log_likelihood: 514.4686096191406\n",
      "    val_log_marginal: 418.28649548627436\n",
      "Train Epoch: 774 [512/54000 (1%)] Loss: -277.894287\n",
      "Train Epoch: 774 [11776/54000 (22%)] Loss: -254.578613\n",
      "Train Epoch: 774 [23040/54000 (43%)] Loss: -505.290283\n",
      "Train Epoch: 774 [34304/54000 (64%)] Loss: -488.932373\n",
      "Train Epoch: 774 [45568/54000 (84%)] Loss: -294.202667\n",
      "    epoch          : 774\n",
      "    loss           : -452.59365493774413\n",
      "    val_loss       : -412.12951934449376\n",
      "    val_log_likelihood: 512.0282409667968\n",
      "    val_log_marginal: 416.22202996499834\n",
      "Train Epoch: 775 [512/54000 (1%)] Loss: -463.711304\n",
      "Train Epoch: 775 [11776/54000 (22%)] Loss: -291.359924\n",
      "Train Epoch: 775 [23040/54000 (43%)] Loss: -263.412598\n",
      "Train Epoch: 775 [34304/54000 (64%)] Loss: -452.871979\n",
      "Train Epoch: 775 [45568/54000 (84%)] Loss: -600.246582\n",
      "    epoch          : 775\n",
      "    loss           : -452.63985046386716\n",
      "    val_loss       : -413.03276120293884\n",
      "    val_log_likelihood: 513.5891571044922\n",
      "    val_log_marginal: 417.01882434003056\n",
      "Train Epoch: 776 [512/54000 (1%)] Loss: -509.631439\n",
      "Train Epoch: 776 [11776/54000 (22%)] Loss: -475.541748\n",
      "Train Epoch: 776 [23040/54000 (43%)] Loss: -450.892212\n",
      "Train Epoch: 776 [34304/54000 (64%)] Loss: -601.152588\n",
      "Train Epoch: 776 [45568/54000 (84%)] Loss: -496.579651\n",
      "    epoch          : 776\n",
      "    loss           : -453.08248092651365\n",
      "    val_loss       : -413.29850161857905\n",
      "    val_log_likelihood: 513.9112274169922\n",
      "    val_log_marginal: 417.5101026321945\n",
      "Train Epoch: 777 [512/54000 (1%)] Loss: -490.937378\n",
      "Train Epoch: 777 [11776/54000 (22%)] Loss: -287.816010\n",
      "Train Epoch: 777 [23040/54000 (43%)] Loss: -269.726135\n",
      "Train Epoch: 777 [34304/54000 (64%)] Loss: -488.157501\n",
      "Train Epoch: 777 [45568/54000 (84%)] Loss: -278.083984\n",
      "    epoch          : 777\n",
      "    loss           : -453.3716879272461\n",
      "    val_loss       : -410.54364746091886\n",
      "    val_log_likelihood: 512.0794250488282\n",
      "    val_log_marginal: 415.985386400273\n",
      "Train Epoch: 778 [512/54000 (1%)] Loss: -488.301331\n",
      "Train Epoch: 778 [11776/54000 (22%)] Loss: -521.909180\n",
      "Train Epoch: 778 [23040/54000 (43%)] Loss: -483.508453\n",
      "Train Epoch: 778 [34304/54000 (64%)] Loss: -275.347473\n",
      "Train Epoch: 778 [45568/54000 (84%)] Loss: -484.813568\n",
      "    epoch          : 778\n",
      "    loss           : -453.3650119018555\n",
      "    val_loss       : -412.6031304521486\n",
      "    val_log_likelihood: 514.2759979248046\n",
      "    val_log_marginal: 417.3975522305818\n",
      "Train Epoch: 779 [512/54000 (1%)] Loss: -483.468658\n",
      "Train Epoch: 779 [11776/54000 (22%)] Loss: -452.670685\n",
      "Train Epoch: 779 [23040/54000 (43%)] Loss: -495.812439\n",
      "Train Epoch: 779 [34304/54000 (64%)] Loss: -479.513855\n",
      "Train Epoch: 779 [45568/54000 (84%)] Loss: -468.400757\n",
      "    epoch          : 779\n",
      "    loss           : -453.634189453125\n",
      "    val_loss       : -412.61034578001124\n",
      "    val_log_likelihood: 513.0874481201172\n",
      "    val_log_marginal: 417.44095804992696\n",
      "Train Epoch: 780 [512/54000 (1%)] Loss: -482.548096\n",
      "Train Epoch: 780 [11776/54000 (22%)] Loss: -512.107971\n",
      "Train Epoch: 780 [23040/54000 (43%)] Loss: -442.302887\n",
      "Train Epoch: 780 [34304/54000 (64%)] Loss: -429.719910\n",
      "Train Epoch: 780 [45568/54000 (84%)] Loss: -489.683075\n",
      "    epoch          : 780\n",
      "    loss           : -452.9914276123047\n",
      "    val_loss       : -413.568282182049\n",
      "    val_log_likelihood: 511.8041046142578\n",
      "    val_log_marginal: 415.8473244590339\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch780.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 781 [512/54000 (1%)] Loss: -308.982178\n",
      "Train Epoch: 781 [11776/54000 (22%)] Loss: -288.738708\n",
      "Train Epoch: 781 [23040/54000 (43%)] Loss: -489.106750\n",
      "Train Epoch: 781 [34304/54000 (64%)] Loss: -498.456879\n",
      "Train Epoch: 781 [45568/54000 (84%)] Loss: -429.086945\n",
      "    epoch          : 781\n",
      "    loss           : -453.0693760681152\n",
      "    val_loss       : -413.38339578900485\n",
      "    val_log_likelihood: 511.9910064697266\n",
      "    val_log_marginal: 416.3844258043915\n",
      "Train Epoch: 782 [512/54000 (1%)] Loss: -269.203125\n",
      "Train Epoch: 782 [11776/54000 (22%)] Loss: -244.298706\n",
      "Train Epoch: 782 [23040/54000 (43%)] Loss: -482.172272\n",
      "Train Epoch: 782 [34304/54000 (64%)] Loss: -430.127197\n",
      "Train Epoch: 782 [45568/54000 (84%)] Loss: -426.884216\n",
      "    epoch          : 782\n",
      "    loss           : -453.98461456298827\n",
      "    val_loss       : -411.86059659104797\n",
      "    val_log_likelihood: 513.3733032226562\n",
      "    val_log_marginal: 416.519704745732\n",
      "Train Epoch: 783 [512/54000 (1%)] Loss: -472.164612\n",
      "Train Epoch: 783 [11776/54000 (22%)] Loss: -491.276550\n",
      "Train Epoch: 783 [23040/54000 (43%)] Loss: -503.801575\n",
      "Train Epoch: 783 [34304/54000 (64%)] Loss: -480.823547\n",
      "Train Epoch: 783 [45568/54000 (84%)] Loss: -486.720459\n",
      "    epoch          : 783\n",
      "    loss           : -454.0102310180664\n",
      "    val_loss       : -410.0988726180978\n",
      "    val_log_likelihood: 513.7905914306641\n",
      "    val_log_marginal: 417.30631008408966\n",
      "Train Epoch: 784 [512/54000 (1%)] Loss: -446.894379\n",
      "Train Epoch: 784 [11776/54000 (22%)] Loss: -489.409607\n",
      "Train Epoch: 784 [23040/54000 (43%)] Loss: -494.686646\n",
      "Train Epoch: 784 [34304/54000 (64%)] Loss: -426.663605\n",
      "Train Epoch: 784 [45568/54000 (84%)] Loss: -612.809631\n",
      "    epoch          : 784\n",
      "    loss           : -453.7013424682617\n",
      "    val_loss       : -413.149996087607\n",
      "    val_log_likelihood: 516.0856262207031\n",
      "    val_log_marginal: 419.06275277563475\n",
      "Train Epoch: 785 [512/54000 (1%)] Loss: -427.316010\n",
      "Train Epoch: 785 [11776/54000 (22%)] Loss: -490.253052\n",
      "Train Epoch: 785 [23040/54000 (43%)] Loss: -443.159607\n",
      "Train Epoch: 785 [34304/54000 (64%)] Loss: -500.553284\n",
      "Train Epoch: 785 [45568/54000 (84%)] Loss: -493.100677\n",
      "    epoch          : 785\n",
      "    loss           : -453.7153286743164\n",
      "    val_loss       : -413.98360211085526\n",
      "    val_log_likelihood: 514.65849609375\n",
      "    val_log_marginal: 417.40995243005455\n",
      "Train Epoch: 786 [512/54000 (1%)] Loss: -519.594543\n",
      "Train Epoch: 786 [11776/54000 (22%)] Loss: -483.294067\n",
      "Train Epoch: 786 [23040/54000 (43%)] Loss: -618.703491\n",
      "Train Epoch: 786 [34304/54000 (64%)] Loss: -307.071106\n",
      "Train Epoch: 786 [45568/54000 (84%)] Loss: -303.033020\n",
      "    epoch          : 786\n",
      "    loss           : -453.6216931152344\n",
      "    val_loss       : -413.60404547564684\n",
      "    val_log_likelihood: 512.3237243652344\n",
      "    val_log_marginal: 416.1573786769062\n",
      "Train Epoch: 787 [512/54000 (1%)] Loss: -495.830566\n",
      "Train Epoch: 787 [11776/54000 (22%)] Loss: -472.240387\n",
      "Train Epoch: 787 [23040/54000 (43%)] Loss: -499.766235\n",
      "Train Epoch: 787 [34304/54000 (64%)] Loss: -265.630615\n",
      "Train Epoch: 787 [45568/54000 (84%)] Loss: -285.879395\n",
      "    epoch          : 787\n",
      "    loss           : -453.49401092529297\n",
      "    val_loss       : -414.4633444422856\n",
      "    val_log_likelihood: 515.4113159179688\n",
      "    val_log_marginal: 418.2789368971121\n",
      "Train Epoch: 788 [512/54000 (1%)] Loss: -451.234131\n",
      "Train Epoch: 788 [11776/54000 (22%)] Loss: -283.860352\n",
      "Train Epoch: 788 [23040/54000 (43%)] Loss: -265.389587\n",
      "Train Epoch: 788 [34304/54000 (64%)] Loss: -286.496094\n",
      "Train Epoch: 788 [45568/54000 (84%)] Loss: -490.631653\n",
      "    epoch          : 788\n",
      "    loss           : -454.39483810424804\n",
      "    val_loss       : -414.61555818570776\n",
      "    val_log_likelihood: 515.2006561279297\n",
      "    val_log_marginal: 418.62548703438205\n",
      "Train Epoch: 789 [512/54000 (1%)] Loss: -511.865540\n",
      "Train Epoch: 789 [11776/54000 (22%)] Loss: -504.643982\n",
      "Train Epoch: 789 [23040/54000 (43%)] Loss: -251.931580\n",
      "Train Epoch: 789 [34304/54000 (64%)] Loss: -498.208801\n",
      "Train Epoch: 789 [45568/54000 (84%)] Loss: -486.148315\n",
      "    epoch          : 789\n",
      "    loss           : -454.9140365600586\n",
      "    val_loss       : -415.0067899771966\n",
      "    val_log_likelihood: 516.7383575439453\n",
      "    val_log_marginal: 420.01419711746274\n",
      "Train Epoch: 790 [512/54000 (1%)] Loss: -266.273315\n",
      "Train Epoch: 790 [11776/54000 (22%)] Loss: -501.962463\n",
      "Train Epoch: 790 [23040/54000 (43%)] Loss: -621.819214\n",
      "Train Epoch: 790 [34304/54000 (64%)] Loss: -484.683960\n",
      "Train Epoch: 790 [45568/54000 (84%)] Loss: -493.128143\n",
      "    epoch          : 790\n",
      "    loss           : -454.3775427246094\n",
      "    val_loss       : -412.65746346693487\n",
      "    val_log_likelihood: 513.8317138671875\n",
      "    val_log_marginal: 417.1662686340511\n",
      "Train Epoch: 791 [512/54000 (1%)] Loss: -483.142517\n",
      "Train Epoch: 791 [11776/54000 (22%)] Loss: -418.619202\n",
      "Train Epoch: 791 [23040/54000 (43%)] Loss: -275.589722\n",
      "Train Epoch: 791 [34304/54000 (64%)] Loss: -611.300720\n",
      "Train Epoch: 791 [45568/54000 (84%)] Loss: -496.332550\n",
      "    epoch          : 791\n",
      "    loss           : -454.6213789367676\n",
      "    val_loss       : -416.35602797456085\n",
      "    val_log_likelihood: 517.4902893066406\n",
      "    val_log_marginal: 420.6099863369018\n",
      "Train Epoch: 792 [512/54000 (1%)] Loss: -263.783875\n",
      "Train Epoch: 792 [11776/54000 (22%)] Loss: -504.769409\n",
      "Train Epoch: 792 [23040/54000 (43%)] Loss: -484.243469\n",
      "Train Epoch: 792 [34304/54000 (64%)] Loss: -465.671539\n",
      "Train Epoch: 792 [45568/54000 (84%)] Loss: -489.387024\n",
      "    epoch          : 792\n",
      "    loss           : -454.9869532775879\n",
      "    val_loss       : -414.8658122483641\n",
      "    val_log_likelihood: 516.386849975586\n",
      "    val_log_marginal: 419.16337335112064\n",
      "Train Epoch: 793 [512/54000 (1%)] Loss: -463.153625\n",
      "Train Epoch: 793 [11776/54000 (22%)] Loss: -485.984711\n",
      "Train Epoch: 793 [23040/54000 (43%)] Loss: -280.517242\n",
      "Train Epoch: 793 [34304/54000 (64%)] Loss: -437.598450\n",
      "Train Epoch: 793 [45568/54000 (84%)] Loss: -477.279510\n",
      "    epoch          : 793\n",
      "    loss           : -454.48651748657227\n",
      "    val_loss       : -413.78731639683247\n",
      "    val_log_likelihood: 513.1855102539063\n",
      "    val_log_marginal: 416.60619279555976\n",
      "Train Epoch: 794 [512/54000 (1%)] Loss: -616.961853\n",
      "Train Epoch: 794 [11776/54000 (22%)] Loss: -272.061523\n",
      "Train Epoch: 794 [23040/54000 (43%)] Loss: -488.535980\n",
      "Train Epoch: 794 [34304/54000 (64%)] Loss: -509.234436\n",
      "Train Epoch: 794 [45568/54000 (84%)] Loss: -286.812073\n",
      "    epoch          : 794\n",
      "    loss           : -454.6768089294434\n",
      "    val_loss       : -412.04476113803685\n",
      "    val_log_likelihood: 513.7594146728516\n",
      "    val_log_marginal: 416.7604640461197\n",
      "Train Epoch: 795 [512/54000 (1%)] Loss: -427.180542\n",
      "Train Epoch: 795 [11776/54000 (22%)] Loss: -269.362823\n",
      "Train Epoch: 795 [23040/54000 (43%)] Loss: -490.376099\n",
      "Train Epoch: 795 [34304/54000 (64%)] Loss: -475.628296\n",
      "Train Epoch: 795 [45568/54000 (84%)] Loss: -497.840454\n",
      "    epoch          : 795\n",
      "    loss           : -454.3767869567871\n",
      "    val_loss       : -411.632332120277\n",
      "    val_log_likelihood: 515.2376678466796\n",
      "    val_log_marginal: 417.57488528392093\n",
      "Train Epoch: 796 [512/54000 (1%)] Loss: -261.997467\n",
      "Train Epoch: 796 [11776/54000 (22%)] Loss: -497.437256\n",
      "Train Epoch: 796 [23040/54000 (43%)] Loss: -490.736145\n",
      "Train Epoch: 796 [34304/54000 (64%)] Loss: -266.374359\n",
      "Train Epoch: 796 [45568/54000 (84%)] Loss: -491.168579\n",
      "    epoch          : 796\n",
      "    loss           : -454.66450241088864\n",
      "    val_loss       : -413.3694673528895\n",
      "    val_log_likelihood: 515.5941802978516\n",
      "    val_log_marginal: 418.3635084140748\n",
      "Train Epoch: 797 [512/54000 (1%)] Loss: -499.974274\n",
      "Train Epoch: 797 [11776/54000 (22%)] Loss: -425.831360\n",
      "Train Epoch: 797 [23040/54000 (43%)] Loss: -607.294617\n",
      "Train Epoch: 797 [34304/54000 (64%)] Loss: -293.204773\n",
      "Train Epoch: 797 [45568/54000 (84%)] Loss: -288.309814\n",
      "    epoch          : 797\n",
      "    loss           : -454.82653427124023\n",
      "    val_loss       : -413.19306701812894\n",
      "    val_log_likelihood: 515.7998016357421\n",
      "    val_log_marginal: 418.11894744336604\n",
      "Train Epoch: 798 [512/54000 (1%)] Loss: -491.636688\n",
      "Train Epoch: 798 [11776/54000 (22%)] Loss: -511.542816\n",
      "Train Epoch: 798 [23040/54000 (43%)] Loss: -476.176208\n",
      "Train Epoch: 798 [34304/54000 (64%)] Loss: -507.394379\n",
      "Train Epoch: 798 [45568/54000 (84%)] Loss: -497.422089\n",
      "    epoch          : 798\n",
      "    loss           : -454.64078079223634\n",
      "    val_loss       : -413.6577122348361\n",
      "    val_log_likelihood: 517.1160614013672\n",
      "    val_log_marginal: 419.807829780632\n",
      "Train Epoch: 799 [512/54000 (1%)] Loss: -497.526489\n",
      "Train Epoch: 799 [11776/54000 (22%)] Loss: -527.430542\n",
      "Train Epoch: 799 [23040/54000 (43%)] Loss: -494.966095\n",
      "Train Epoch: 799 [34304/54000 (64%)] Loss: -497.017700\n",
      "Train Epoch: 799 [45568/54000 (84%)] Loss: -496.130188\n",
      "    epoch          : 799\n",
      "    loss           : -455.2101803588867\n",
      "    val_loss       : -412.27459770347923\n",
      "    val_log_likelihood: 512.6649108886719\n",
      "    val_log_marginal: 416.0007183019071\n",
      "Train Epoch: 800 [512/54000 (1%)] Loss: -522.365845\n",
      "Train Epoch: 800 [11776/54000 (22%)] Loss: -426.742859\n",
      "Train Epoch: 800 [23040/54000 (43%)] Loss: -484.464569\n",
      "Train Epoch: 800 [34304/54000 (64%)] Loss: -501.615448\n",
      "Train Epoch: 800 [45568/54000 (84%)] Loss: -489.754089\n",
      "    epoch          : 800\n",
      "    loss           : -455.30275924682616\n",
      "    val_loss       : -410.85086751803755\n",
      "    val_log_likelihood: 512.9936828613281\n",
      "    val_log_marginal: 415.67174319065055\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch800.pth ...\n",
      "Train Epoch: 801 [512/54000 (1%)] Loss: -497.181702\n",
      "Train Epoch: 801 [11776/54000 (22%)] Loss: -504.789948\n",
      "Train Epoch: 801 [23040/54000 (43%)] Loss: -494.581512\n",
      "Train Epoch: 801 [34304/54000 (64%)] Loss: -498.439697\n",
      "Train Epoch: 801 [45568/54000 (84%)] Loss: -292.285126\n",
      "    epoch          : 801\n",
      "    loss           : -455.5827360534668\n",
      "    val_loss       : -413.59718639440837\n",
      "    val_log_likelihood: 515.383755493164\n",
      "    val_log_marginal: 417.7731076296419\n",
      "Train Epoch: 802 [512/54000 (1%)] Loss: -459.455505\n",
      "Train Epoch: 802 [11776/54000 (22%)] Loss: -295.963684\n",
      "Train Epoch: 802 [23040/54000 (43%)] Loss: -280.374084\n",
      "Train Epoch: 802 [34304/54000 (64%)] Loss: -488.529114\n",
      "Train Epoch: 802 [45568/54000 (84%)] Loss: -442.262970\n",
      "    epoch          : 802\n",
      "    loss           : -454.4592723083496\n",
      "    val_loss       : -413.24425051165747\n",
      "    val_log_likelihood: 511.60927124023436\n",
      "    val_log_marginal: 415.20712346099316\n",
      "Train Epoch: 803 [512/54000 (1%)] Loss: -608.609009\n",
      "Train Epoch: 803 [11776/54000 (22%)] Loss: -438.912018\n",
      "Train Epoch: 803 [23040/54000 (43%)] Loss: -489.962524\n",
      "Train Epoch: 803 [34304/54000 (64%)] Loss: -436.975403\n",
      "Train Epoch: 803 [45568/54000 (84%)] Loss: -496.876312\n",
      "    epoch          : 803\n",
      "    loss           : -455.4600357055664\n",
      "    val_loss       : -413.3457324061543\n",
      "    val_log_likelihood: 513.035366821289\n",
      "    val_log_marginal: 416.70749299726157\n",
      "Train Epoch: 804 [512/54000 (1%)] Loss: -609.140747\n",
      "Train Epoch: 804 [11776/54000 (22%)] Loss: -270.228455\n",
      "Train Epoch: 804 [23040/54000 (43%)] Loss: -496.098022\n",
      "Train Epoch: 804 [34304/54000 (64%)] Loss: -493.345673\n",
      "Train Epoch: 804 [45568/54000 (84%)] Loss: -498.110870\n",
      "    epoch          : 804\n",
      "    loss           : -455.15587524414065\n",
      "    val_loss       : -412.4665926305577\n",
      "    val_log_likelihood: 516.2616149902344\n",
      "    val_log_marginal: 418.5254981923848\n",
      "Train Epoch: 805 [512/54000 (1%)] Loss: -442.091675\n",
      "Train Epoch: 805 [11776/54000 (22%)] Loss: -503.534821\n",
      "Train Epoch: 805 [23040/54000 (43%)] Loss: -483.683044\n",
      "Train Epoch: 805 [34304/54000 (64%)] Loss: -255.474106\n",
      "Train Epoch: 805 [45568/54000 (84%)] Loss: -501.153961\n",
      "    epoch          : 805\n",
      "    loss           : -455.40867355346677\n",
      "    val_loss       : -415.678575776238\n",
      "    val_log_likelihood: 517.6601135253907\n",
      "    val_log_marginal: 419.9322721641511\n",
      "Train Epoch: 806 [512/54000 (1%)] Loss: -488.041565\n",
      "Train Epoch: 806 [11776/54000 (22%)] Loss: -510.062561\n",
      "Train Epoch: 806 [23040/54000 (43%)] Loss: -264.857361\n",
      "Train Epoch: 806 [34304/54000 (64%)] Loss: -613.180481\n",
      "Train Epoch: 806 [45568/54000 (84%)] Loss: -303.205078\n",
      "    epoch          : 806\n",
      "    loss           : -455.4191276550293\n",
      "    val_loss       : -412.69893504315985\n",
      "    val_log_likelihood: 515.5138366699218\n",
      "    val_log_marginal: 417.78065904267135\n",
      "Train Epoch: 807 [512/54000 (1%)] Loss: -267.837433\n",
      "Train Epoch: 807 [11776/54000 (22%)] Loss: -477.887634\n",
      "Train Epoch: 807 [23040/54000 (43%)] Loss: -458.047729\n",
      "Train Epoch: 807 [34304/54000 (64%)] Loss: -300.402710\n",
      "Train Epoch: 807 [45568/54000 (84%)] Loss: -495.929688\n",
      "    epoch          : 807\n",
      "    loss           : -455.7095443725586\n",
      "    val_loss       : -415.353454474546\n",
      "    val_log_likelihood: 517.6482727050782\n",
      "    val_log_marginal: 419.47336582951414\n",
      "Train Epoch: 808 [512/54000 (1%)] Loss: -520.461914\n",
      "Train Epoch: 808 [11776/54000 (22%)] Loss: -518.285217\n",
      "Train Epoch: 808 [23040/54000 (43%)] Loss: -425.699615\n",
      "Train Epoch: 808 [34304/54000 (64%)] Loss: -470.488190\n",
      "Train Epoch: 808 [45568/54000 (84%)] Loss: -486.175201\n",
      "    epoch          : 808\n",
      "    loss           : -455.2186605834961\n",
      "    val_loss       : -414.07136341724544\n",
      "    val_log_likelihood: 517.4296081542968\n",
      "    val_log_marginal: 419.44641292461426\n",
      "Train Epoch: 809 [512/54000 (1%)] Loss: -293.699799\n",
      "Train Epoch: 809 [11776/54000 (22%)] Loss: -616.342163\n",
      "Train Epoch: 809 [23040/54000 (43%)] Loss: -478.804199\n",
      "Train Epoch: 809 [34304/54000 (64%)] Loss: -490.380859\n",
      "Train Epoch: 809 [45568/54000 (84%)] Loss: -279.758148\n",
      "    epoch          : 809\n",
      "    loss           : -455.53052444458007\n",
      "    val_loss       : -414.45896737193686\n",
      "    val_log_likelihood: 516.9335540771484\n",
      "    val_log_marginal: 419.0229674641043\n",
      "Train Epoch: 810 [512/54000 (1%)] Loss: -516.209229\n",
      "Train Epoch: 810 [11776/54000 (22%)] Loss: -505.998413\n",
      "Train Epoch: 810 [23040/54000 (43%)] Loss: -506.517822\n",
      "Train Epoch: 810 [34304/54000 (64%)] Loss: -280.215088\n",
      "Train Epoch: 810 [45568/54000 (84%)] Loss: -287.088806\n",
      "    epoch          : 810\n",
      "    loss           : -456.6248655700684\n",
      "    val_loss       : -415.93150455653665\n",
      "    val_log_likelihood: 518.2430755615235\n",
      "    val_log_marginal: 420.1062834318727\n",
      "Train Epoch: 811 [512/54000 (1%)] Loss: -501.548859\n",
      "Train Epoch: 811 [11776/54000 (22%)] Loss: -436.542480\n",
      "Train Epoch: 811 [23040/54000 (43%)] Loss: -497.871521\n",
      "Train Epoch: 811 [34304/54000 (64%)] Loss: -513.359131\n",
      "Train Epoch: 811 [45568/54000 (84%)] Loss: -491.116547\n",
      "    epoch          : 811\n",
      "    loss           : -455.6628532409668\n",
      "    val_loss       : -415.6341827649623\n",
      "    val_log_likelihood: 516.0961700439453\n",
      "    val_log_marginal: 418.6745107214898\n",
      "Train Epoch: 812 [512/54000 (1%)] Loss: -493.323059\n",
      "Train Epoch: 812 [11776/54000 (22%)] Loss: -526.612061\n",
      "Train Epoch: 812 [23040/54000 (43%)] Loss: -267.309296\n",
      "Train Epoch: 812 [34304/54000 (64%)] Loss: -483.904602\n",
      "Train Epoch: 812 [45568/54000 (84%)] Loss: -299.086121\n",
      "    epoch          : 812\n",
      "    loss           : -455.7334086608887\n",
      "    val_loss       : -414.7688921979628\n",
      "    val_log_likelihood: 515.5047241210938\n",
      "    val_log_marginal: 418.1282937269658\n",
      "Train Epoch: 813 [512/54000 (1%)] Loss: -513.275879\n",
      "Train Epoch: 813 [11776/54000 (22%)] Loss: -524.015564\n",
      "Train Epoch: 813 [23040/54000 (43%)] Loss: -432.696655\n",
      "Train Epoch: 813 [34304/54000 (64%)] Loss: -609.726196\n",
      "Train Epoch: 813 [45568/54000 (84%)] Loss: -277.327240\n",
      "    epoch          : 813\n",
      "    loss           : -455.07694381713867\n",
      "    val_loss       : -416.58523327158764\n",
      "    val_log_likelihood: 516.9274871826171\n",
      "    val_log_marginal: 419.42719835303853\n",
      "Train Epoch: 814 [512/54000 (1%)] Loss: -610.603577\n",
      "Train Epoch: 814 [11776/54000 (22%)] Loss: -474.696136\n",
      "Train Epoch: 814 [23040/54000 (43%)] Loss: -463.345825\n",
      "Train Epoch: 814 [34304/54000 (64%)] Loss: -482.938904\n",
      "Train Epoch: 814 [45568/54000 (84%)] Loss: -467.847595\n",
      "    epoch          : 814\n",
      "    loss           : -456.1609574890137\n",
      "    val_loss       : -413.57819361314176\n",
      "    val_log_likelihood: 516.0118347167969\n",
      "    val_log_marginal: 418.5668199513108\n",
      "Train Epoch: 815 [512/54000 (1%)] Loss: -263.207489\n",
      "Train Epoch: 815 [11776/54000 (22%)] Loss: -269.950806\n",
      "Train Epoch: 815 [23040/54000 (43%)] Loss: -489.544312\n",
      "Train Epoch: 815 [34304/54000 (64%)] Loss: -481.871246\n",
      "Train Epoch: 815 [45568/54000 (84%)] Loss: -428.849396\n",
      "    epoch          : 815\n",
      "    loss           : -456.21764495849607\n",
      "    val_loss       : -413.85606759916993\n",
      "    val_log_likelihood: 518.0105773925782\n",
      "    val_log_marginal: 419.8886988546831\n",
      "Train Epoch: 816 [512/54000 (1%)] Loss: -439.316895\n",
      "Train Epoch: 816 [11776/54000 (22%)] Loss: -485.780060\n",
      "Train Epoch: 816 [23040/54000 (43%)] Loss: -284.256195\n",
      "Train Epoch: 816 [34304/54000 (64%)] Loss: -514.372864\n",
      "Train Epoch: 816 [45568/54000 (84%)] Loss: -485.151947\n",
      "    epoch          : 816\n",
      "    loss           : -455.69574096679685\n",
      "    val_loss       : -414.1413433279842\n",
      "    val_log_likelihood: 517.0504333496094\n",
      "    val_log_marginal: 418.79963528737426\n",
      "Train Epoch: 817 [512/54000 (1%)] Loss: -271.265228\n",
      "Train Epoch: 817 [11776/54000 (22%)] Loss: -625.740112\n",
      "Train Epoch: 817 [23040/54000 (43%)] Loss: -465.253296\n",
      "Train Epoch: 817 [34304/54000 (64%)] Loss: -496.663879\n",
      "Train Epoch: 817 [45568/54000 (84%)] Loss: -496.026550\n",
      "    epoch          : 817\n",
      "    loss           : -456.30602142333987\n",
      "    val_loss       : -415.5906067499891\n",
      "    val_log_likelihood: 517.9688873291016\n",
      "    val_log_marginal: 419.244474973157\n",
      "Train Epoch: 818 [512/54000 (1%)] Loss: -491.290955\n",
      "Train Epoch: 818 [11776/54000 (22%)] Loss: -468.648926\n",
      "Train Epoch: 818 [23040/54000 (43%)] Loss: -507.354187\n",
      "Train Epoch: 818 [34304/54000 (64%)] Loss: -622.888062\n",
      "Train Epoch: 818 [45568/54000 (84%)] Loss: -435.801697\n",
      "    epoch          : 818\n",
      "    loss           : -455.9893212890625\n",
      "    val_loss       : -414.31236753137784\n",
      "    val_log_likelihood: 517.600796508789\n",
      "    val_log_marginal: 419.2653042834252\n",
      "Train Epoch: 819 [512/54000 (1%)] Loss: -501.638855\n",
      "Train Epoch: 819 [11776/54000 (22%)] Loss: -498.619415\n",
      "Train Epoch: 819 [23040/54000 (43%)] Loss: -502.565979\n",
      "Train Epoch: 819 [34304/54000 (64%)] Loss: -288.776184\n",
      "Train Epoch: 819 [45568/54000 (84%)] Loss: -293.719788\n",
      "    epoch          : 819\n",
      "    loss           : -456.4847445678711\n",
      "    val_loss       : -415.2670336498879\n",
      "    val_log_likelihood: 518.7651489257812\n",
      "    val_log_marginal: 420.41597278479094\n",
      "Train Epoch: 820 [512/54000 (1%)] Loss: -527.940186\n",
      "Train Epoch: 820 [11776/54000 (22%)] Loss: -490.490753\n",
      "Train Epoch: 820 [23040/54000 (43%)] Loss: -495.164398\n",
      "Train Epoch: 820 [34304/54000 (64%)] Loss: -488.260406\n",
      "Train Epoch: 820 [45568/54000 (84%)] Loss: -291.992767\n",
      "    epoch          : 820\n",
      "    loss           : -455.86677154541013\n",
      "    val_loss       : -415.35666045732796\n",
      "    val_log_likelihood: 515.6810882568359\n",
      "    val_log_marginal: 417.7029985640198\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch820.pth ...\n",
      "Train Epoch: 821 [512/54000 (1%)] Loss: -299.785126\n",
      "Train Epoch: 821 [11776/54000 (22%)] Loss: -507.864777\n",
      "Train Epoch: 821 [23040/54000 (43%)] Loss: -499.077942\n",
      "Train Epoch: 821 [34304/54000 (64%)] Loss: -492.630249\n",
      "Train Epoch: 821 [45568/54000 (84%)] Loss: -296.265503\n",
      "    epoch          : 821\n",
      "    loss           : -456.1934732055664\n",
      "    val_loss       : -417.17047507697714\n",
      "    val_log_likelihood: 519.4413970947265\n",
      "    val_log_marginal: 421.40763257816434\n",
      "Train Epoch: 822 [512/54000 (1%)] Loss: -251.035980\n",
      "Train Epoch: 822 [11776/54000 (22%)] Loss: -266.916534\n",
      "Train Epoch: 822 [23040/54000 (43%)] Loss: -493.407623\n",
      "Train Epoch: 822 [34304/54000 (64%)] Loss: -266.526733\n",
      "Train Epoch: 822 [45568/54000 (84%)] Loss: -492.339050\n",
      "    epoch          : 822\n",
      "    loss           : -455.951350402832\n",
      "    val_loss       : -415.57250151699407\n",
      "    val_log_likelihood: 518.8884490966797\n",
      "    val_log_marginal: 420.5963686257601\n",
      "Train Epoch: 823 [512/54000 (1%)] Loss: -487.478058\n",
      "Train Epoch: 823 [11776/54000 (22%)] Loss: -434.297394\n",
      "Train Epoch: 823 [23040/54000 (43%)] Loss: -430.393738\n",
      "Train Epoch: 823 [34304/54000 (64%)] Loss: -436.856171\n",
      "Train Epoch: 823 [45568/54000 (84%)] Loss: -474.817657\n",
      "    epoch          : 823\n",
      "    loss           : -456.4744352722168\n",
      "    val_loss       : -414.01504414202645\n",
      "    val_log_likelihood: 515.6569091796875\n",
      "    val_log_marginal: 417.7074941124767\n",
      "Train Epoch: 824 [512/54000 (1%)] Loss: -453.977875\n",
      "Train Epoch: 824 [11776/54000 (22%)] Loss: -488.034668\n",
      "Train Epoch: 824 [23040/54000 (43%)] Loss: -495.670410\n",
      "Train Epoch: 824 [34304/54000 (64%)] Loss: -483.984161\n",
      "Train Epoch: 824 [45568/54000 (84%)] Loss: -500.332275\n",
      "    epoch          : 824\n",
      "    loss           : -456.6783807373047\n",
      "    val_loss       : -413.3739245607518\n",
      "    val_log_likelihood: 516.6871307373046\n",
      "    val_log_marginal: 418.0337489377707\n",
      "Train Epoch: 825 [512/54000 (1%)] Loss: -500.322601\n",
      "Train Epoch: 825 [11776/54000 (22%)] Loss: -257.421173\n",
      "Train Epoch: 825 [23040/54000 (43%)] Loss: -485.549042\n",
      "Train Epoch: 825 [34304/54000 (64%)] Loss: -261.319824\n",
      "Train Epoch: 825 [45568/54000 (84%)] Loss: -278.527863\n",
      "    epoch          : 825\n",
      "    loss           : -456.43644165039063\n",
      "    val_loss       : -413.6107602238655\n",
      "    val_log_likelihood: 517.0644104003907\n",
      "    val_log_marginal: 418.2495000571013\n",
      "Train Epoch: 826 [512/54000 (1%)] Loss: -621.068726\n",
      "Train Epoch: 826 [11776/54000 (22%)] Loss: -616.750305\n",
      "Train Epoch: 826 [23040/54000 (43%)] Loss: -509.028015\n",
      "Train Epoch: 826 [34304/54000 (64%)] Loss: -459.655609\n",
      "Train Epoch: 826 [45568/54000 (84%)] Loss: -484.966858\n",
      "    epoch          : 826\n",
      "    loss           : -456.55251678466794\n",
      "    val_loss       : -411.88160081505777\n",
      "    val_log_likelihood: 516.1037139892578\n",
      "    val_log_marginal: 417.9754638176764\n",
      "Train Epoch: 827 [512/54000 (1%)] Loss: -616.824341\n",
      "Train Epoch: 827 [11776/54000 (22%)] Loss: -528.315674\n",
      "Train Epoch: 827 [23040/54000 (43%)] Loss: -619.998718\n",
      "Train Epoch: 827 [34304/54000 (64%)] Loss: -499.491089\n",
      "Train Epoch: 827 [45568/54000 (84%)] Loss: -622.242493\n",
      "    epoch          : 827\n",
      "    loss           : -455.40286849975587\n",
      "    val_loss       : -416.4131883587688\n",
      "    val_log_likelihood: 516.4253356933593\n",
      "    val_log_marginal: 420.13864624612035\n",
      "Train Epoch: 828 [512/54000 (1%)] Loss: -627.060547\n",
      "Train Epoch: 828 [11776/54000 (22%)] Loss: -283.450378\n",
      "Train Epoch: 828 [23040/54000 (43%)] Loss: -281.539673\n",
      "Train Epoch: 828 [34304/54000 (64%)] Loss: -434.111359\n",
      "Train Epoch: 828 [45568/54000 (84%)] Loss: -497.644745\n",
      "    epoch          : 828\n",
      "    loss           : -456.75458084106447\n",
      "    val_loss       : -414.77961609568445\n",
      "    val_log_likelihood: 518.8018341064453\n",
      "    val_log_marginal: 420.1044162496951\n",
      "Train Epoch: 829 [512/54000 (1%)] Loss: -428.800690\n",
      "Train Epoch: 829 [11776/54000 (22%)] Loss: -432.730438\n",
      "Train Epoch: 829 [23040/54000 (43%)] Loss: -472.378906\n",
      "Train Epoch: 829 [34304/54000 (64%)] Loss: -507.374573\n",
      "Train Epoch: 829 [45568/54000 (84%)] Loss: -485.969177\n",
      "    epoch          : 829\n",
      "    loss           : -456.3529426574707\n",
      "    val_loss       : -414.43469631560146\n",
      "    val_log_likelihood: 517.5262969970703\n",
      "    val_log_marginal: 419.12928732991884\n",
      "Train Epoch: 830 [512/54000 (1%)] Loss: -477.440491\n",
      "Train Epoch: 830 [11776/54000 (22%)] Loss: -522.954468\n",
      "Train Epoch: 830 [23040/54000 (43%)] Loss: -506.711029\n",
      "Train Epoch: 830 [34304/54000 (64%)] Loss: -503.699402\n",
      "Train Epoch: 830 [45568/54000 (84%)] Loss: -617.874634\n",
      "    epoch          : 830\n",
      "    loss           : -456.57608123779295\n",
      "    val_loss       : -416.4423246290535\n",
      "    val_log_likelihood: 518.7996551513672\n",
      "    val_log_marginal: 420.5756314769129\n",
      "Train Epoch: 831 [512/54000 (1%)] Loss: -441.199951\n",
      "Train Epoch: 831 [11776/54000 (22%)] Loss: -509.340546\n",
      "Train Epoch: 831 [23040/54000 (43%)] Loss: -476.341797\n",
      "Train Epoch: 831 [34304/54000 (64%)] Loss: -486.994629\n",
      "Train Epoch: 831 [45568/54000 (84%)] Loss: -307.244324\n",
      "    epoch          : 831\n",
      "    loss           : -456.4919921875\n",
      "    val_loss       : -418.0500027342699\n",
      "    val_log_likelihood: 520.0283233642579\n",
      "    val_log_marginal: 421.5106966961175\n",
      "Train Epoch: 832 [512/54000 (1%)] Loss: -317.850281\n",
      "Train Epoch: 832 [11776/54000 (22%)] Loss: -486.228027\n",
      "Train Epoch: 832 [23040/54000 (43%)] Loss: -467.380554\n",
      "Train Epoch: 832 [34304/54000 (64%)] Loss: -463.154724\n",
      "Train Epoch: 832 [45568/54000 (84%)] Loss: -461.420319\n",
      "    epoch          : 832\n",
      "    loss           : -456.44340103149415\n",
      "    val_loss       : -415.7459906933829\n",
      "    val_log_likelihood: 517.464077758789\n",
      "    val_log_marginal: 419.1414448161386\n",
      "Train Epoch: 833 [512/54000 (1%)] Loss: -260.206238\n",
      "Train Epoch: 833 [11776/54000 (22%)] Loss: -508.187592\n",
      "Train Epoch: 833 [23040/54000 (43%)] Loss: -618.900879\n",
      "Train Epoch: 833 [34304/54000 (64%)] Loss: -494.065887\n",
      "Train Epoch: 833 [45568/54000 (84%)] Loss: -501.562775\n",
      "    epoch          : 833\n",
      "    loss           : -456.8976824951172\n",
      "    val_loss       : -413.38982461895796\n",
      "    val_log_likelihood: 515.7286956787109\n",
      "    val_log_marginal: 417.2600111642907\n",
      "Train Epoch: 834 [512/54000 (1%)] Loss: -622.635742\n",
      "Train Epoch: 834 [11776/54000 (22%)] Loss: -531.383606\n",
      "Train Epoch: 834 [23040/54000 (43%)] Loss: -503.124573\n",
      "Train Epoch: 834 [34304/54000 (64%)] Loss: -499.254089\n",
      "Train Epoch: 834 [45568/54000 (84%)] Loss: -436.334290\n",
      "    epoch          : 834\n",
      "    loss           : -456.8488972473144\n",
      "    val_loss       : -414.66995510235427\n",
      "    val_log_likelihood: 517.8728942871094\n",
      "    val_log_marginal: 418.9507106680423\n",
      "Train Epoch: 835 [512/54000 (1%)] Loss: -483.775787\n",
      "Train Epoch: 835 [11776/54000 (22%)] Loss: -258.455597\n",
      "Train Epoch: 835 [23040/54000 (43%)] Loss: -522.299194\n",
      "Train Epoch: 835 [34304/54000 (64%)] Loss: -615.256042\n",
      "Train Epoch: 835 [45568/54000 (84%)] Loss: -497.958740\n",
      "    epoch          : 835\n",
      "    loss           : -456.93428588867187\n",
      "    val_loss       : -411.67771192053334\n",
      "    val_log_likelihood: 514.477328491211\n",
      "    val_log_marginal: 415.233982853085\n",
      "Train Epoch: 836 [512/54000 (1%)] Loss: -267.148651\n",
      "Train Epoch: 836 [11776/54000 (22%)] Loss: -430.985504\n",
      "Train Epoch: 836 [23040/54000 (43%)] Loss: -295.429413\n",
      "Train Epoch: 836 [34304/54000 (64%)] Loss: -456.426483\n",
      "Train Epoch: 836 [45568/54000 (84%)] Loss: -278.658997\n",
      "    epoch          : 836\n",
      "    loss           : -457.17800994873045\n",
      "    val_loss       : -413.6874365648255\n",
      "    val_log_likelihood: 520.3158172607422\n",
      "    val_log_marginal: 421.3967166803777\n",
      "Train Epoch: 837 [512/54000 (1%)] Loss: -465.744873\n",
      "Train Epoch: 837 [11776/54000 (22%)] Loss: -493.696747\n",
      "Train Epoch: 837 [23040/54000 (43%)] Loss: -276.697083\n",
      "Train Epoch: 837 [34304/54000 (64%)] Loss: -447.518341\n",
      "Train Epoch: 837 [45568/54000 (84%)] Loss: -431.815674\n",
      "    epoch          : 837\n",
      "    loss           : -456.89211074829103\n",
      "    val_loss       : -416.3418188260868\n",
      "    val_log_likelihood: 515.7657867431641\n",
      "    val_log_marginal: 418.52038599327204\n",
      "Train Epoch: 838 [512/54000 (1%)] Loss: -487.092896\n",
      "Train Epoch: 838 [11776/54000 (22%)] Loss: -271.532288\n",
      "Train Epoch: 838 [23040/54000 (43%)] Loss: -607.663025\n",
      "Train Epoch: 838 [34304/54000 (64%)] Loss: -619.946777\n",
      "Train Epoch: 838 [45568/54000 (84%)] Loss: -510.315155\n",
      "    epoch          : 838\n",
      "    loss           : -457.2393128967285\n",
      "    val_loss       : -412.99427243545654\n",
      "    val_log_likelihood: 517.1544891357422\n",
      "    val_log_marginal: 418.3881216496229\n",
      "Train Epoch: 839 [512/54000 (1%)] Loss: -444.385254\n",
      "Train Epoch: 839 [11776/54000 (22%)] Loss: -499.528687\n",
      "Train Epoch: 839 [23040/54000 (43%)] Loss: -527.726746\n",
      "Train Epoch: 839 [34304/54000 (64%)] Loss: -490.036896\n",
      "Train Epoch: 839 [45568/54000 (84%)] Loss: -289.118408\n",
      "    epoch          : 839\n",
      "    loss           : -458.23450775146483\n",
      "    val_loss       : -416.41255816686896\n",
      "    val_log_likelihood: 518.2541625976562\n",
      "    val_log_marginal: 419.6488479401916\n",
      "Train Epoch: 840 [512/54000 (1%)] Loss: -502.175964\n",
      "Train Epoch: 840 [11776/54000 (22%)] Loss: -471.638153\n",
      "Train Epoch: 840 [23040/54000 (43%)] Loss: -434.621613\n",
      "Train Epoch: 840 [34304/54000 (64%)] Loss: -264.381927\n",
      "Train Epoch: 840 [45568/54000 (84%)] Loss: -436.687561\n",
      "    epoch          : 840\n",
      "    loss           : -457.55055419921877\n",
      "    val_loss       : -418.39394345022737\n",
      "    val_log_likelihood: 519.7809631347657\n",
      "    val_log_marginal: 421.9100685710023\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch840.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 841 [512/54000 (1%)] Loss: -486.665222\n",
      "Train Epoch: 841 [11776/54000 (22%)] Loss: -523.941162\n",
      "Train Epoch: 841 [23040/54000 (43%)] Loss: -472.677948\n",
      "Train Epoch: 841 [34304/54000 (64%)] Loss: -492.909302\n",
      "Train Epoch: 841 [45568/54000 (84%)] Loss: -295.340088\n",
      "    epoch          : 841\n",
      "    loss           : -457.4690214538574\n",
      "    val_loss       : -415.5656726107933\n",
      "    val_log_likelihood: 518.0883544921875\n",
      "    val_log_marginal: 419.5454823313418\n",
      "Train Epoch: 842 [512/54000 (1%)] Loss: -267.258728\n",
      "Train Epoch: 842 [11776/54000 (22%)] Loss: -510.147614\n",
      "Train Epoch: 842 [23040/54000 (43%)] Loss: -492.855835\n",
      "Train Epoch: 842 [34304/54000 (64%)] Loss: -516.962280\n",
      "Train Epoch: 842 [45568/54000 (84%)] Loss: -617.520569\n",
      "    epoch          : 842\n",
      "    loss           : -456.575984954834\n",
      "    val_loss       : -415.0475939825177\n",
      "    val_log_likelihood: 518.9921936035156\n",
      "    val_log_marginal: 419.7837743246795\n",
      "Train Epoch: 843 [512/54000 (1%)] Loss: -471.055542\n",
      "Train Epoch: 843 [11776/54000 (22%)] Loss: -509.137146\n",
      "Train Epoch: 843 [23040/54000 (43%)] Loss: -443.065002\n",
      "Train Epoch: 843 [34304/54000 (64%)] Loss: -274.941284\n",
      "Train Epoch: 843 [45568/54000 (84%)] Loss: -504.112213\n",
      "    epoch          : 843\n",
      "    loss           : -457.15745559692385\n",
      "    val_loss       : -413.6574982808903\n",
      "    val_log_likelihood: 517.8879760742187\n",
      "    val_log_marginal: 418.4493350617622\n",
      "Train Epoch: 844 [512/54000 (1%)] Loss: -497.775452\n",
      "Train Epoch: 844 [11776/54000 (22%)] Loss: -502.227173\n",
      "Train Epoch: 844 [23040/54000 (43%)] Loss: -255.367249\n",
      "Train Epoch: 844 [34304/54000 (64%)] Loss: -497.821899\n",
      "Train Epoch: 844 [45568/54000 (84%)] Loss: -497.458191\n",
      "    epoch          : 844\n",
      "    loss           : -457.0770002746582\n",
      "    val_loss       : -414.4386012296192\n",
      "    val_log_likelihood: 520.5090728759766\n",
      "    val_log_marginal: 421.200760659948\n",
      "Train Epoch: 845 [512/54000 (1%)] Loss: -264.041748\n",
      "Train Epoch: 845 [11776/54000 (22%)] Loss: -461.492249\n",
      "Train Epoch: 845 [23040/54000 (43%)] Loss: -510.244690\n",
      "Train Epoch: 845 [34304/54000 (64%)] Loss: -479.356689\n",
      "Train Epoch: 845 [45568/54000 (84%)] Loss: -506.220947\n",
      "    epoch          : 845\n",
      "    loss           : -457.2947085571289\n",
      "    val_loss       : -415.90033646039666\n",
      "    val_log_likelihood: 519.5600494384765\n",
      "    val_log_marginal: 420.2682185344398\n",
      "Train Epoch: 846 [512/54000 (1%)] Loss: -267.258514\n",
      "Train Epoch: 846 [11776/54000 (22%)] Loss: -476.643768\n",
      "Train Epoch: 846 [23040/54000 (43%)] Loss: -504.459137\n",
      "Train Epoch: 846 [34304/54000 (64%)] Loss: -513.789429\n",
      "Train Epoch: 846 [45568/54000 (84%)] Loss: -499.832550\n",
      "    epoch          : 846\n",
      "    loss           : -457.58667984008787\n",
      "    val_loss       : -413.80932525126263\n",
      "    val_log_likelihood: 518.6086944580078\n",
      "    val_log_marginal: 419.03117023743687\n",
      "Train Epoch: 847 [512/54000 (1%)] Loss: -481.762787\n",
      "Train Epoch: 847 [11776/54000 (22%)] Loss: -299.681427\n",
      "Train Epoch: 847 [23040/54000 (43%)] Loss: -309.881989\n",
      "Train Epoch: 847 [34304/54000 (64%)] Loss: -498.283203\n",
      "Train Epoch: 847 [45568/54000 (84%)] Loss: -493.348907\n",
      "    epoch          : 847\n",
      "    loss           : -457.82272857666015\n",
      "    val_loss       : -415.4297766711563\n",
      "    val_log_likelihood: 520.4746673583984\n",
      "    val_log_marginal: 420.9398513045162\n",
      "Train Epoch: 848 [512/54000 (1%)] Loss: -619.301270\n",
      "Train Epoch: 848 [11776/54000 (22%)] Loss: -619.532471\n",
      "Train Epoch: 848 [23040/54000 (43%)] Loss: -474.229767\n",
      "Train Epoch: 848 [34304/54000 (64%)] Loss: -479.444305\n",
      "Train Epoch: 848 [45568/54000 (84%)] Loss: -290.644043\n",
      "    epoch          : 848\n",
      "    loss           : -457.6414350891113\n",
      "    val_loss       : -417.4764627924189\n",
      "    val_log_likelihood: 519.3841522216796\n",
      "    val_log_marginal: 420.67941544130446\n",
      "Train Epoch: 849 [512/54000 (1%)] Loss: -610.020142\n",
      "Train Epoch: 849 [11776/54000 (22%)] Loss: -466.579224\n",
      "Train Epoch: 849 [23040/54000 (43%)] Loss: -448.070557\n",
      "Train Epoch: 849 [34304/54000 (64%)] Loss: -455.303955\n",
      "Train Epoch: 849 [45568/54000 (84%)] Loss: -493.410553\n",
      "    epoch          : 849\n",
      "    loss           : -457.6330822753906\n",
      "    val_loss       : -415.93958307392893\n",
      "    val_log_likelihood: 520.1727233886719\n",
      "    val_log_marginal: 421.0066027902707\n",
      "Train Epoch: 850 [512/54000 (1%)] Loss: -486.209778\n",
      "Train Epoch: 850 [11776/54000 (22%)] Loss: -283.151917\n",
      "Train Epoch: 850 [23040/54000 (43%)] Loss: -502.784607\n",
      "Train Epoch: 850 [34304/54000 (64%)] Loss: -492.520538\n",
      "Train Epoch: 850 [45568/54000 (84%)] Loss: -484.938354\n",
      "    epoch          : 850\n",
      "    loss           : -456.48380401611325\n",
      "    val_loss       : -417.29395586661997\n",
      "    val_log_likelihood: 518.7913818359375\n",
      "    val_log_marginal: 419.9386518161744\n",
      "Train Epoch: 851 [512/54000 (1%)] Loss: -465.861938\n",
      "Train Epoch: 851 [11776/54000 (22%)] Loss: -507.310608\n",
      "Train Epoch: 851 [23040/54000 (43%)] Loss: -438.623779\n",
      "Train Epoch: 851 [34304/54000 (64%)] Loss: -621.472473\n",
      "Train Epoch: 851 [45568/54000 (84%)] Loss: -479.739410\n",
      "    epoch          : 851\n",
      "    loss           : -457.91430099487303\n",
      "    val_loss       : -416.24903396703303\n",
      "    val_log_likelihood: 520.4526123046875\n",
      "    val_log_marginal: 421.2886683247983\n",
      "Train Epoch: 852 [512/54000 (1%)] Loss: -626.399536\n",
      "Train Epoch: 852 [11776/54000 (22%)] Loss: -296.326294\n",
      "Train Epoch: 852 [23040/54000 (43%)] Loss: -287.255859\n",
      "Train Epoch: 852 [34304/54000 (64%)] Loss: -511.682495\n",
      "Train Epoch: 852 [45568/54000 (84%)] Loss: -272.323181\n",
      "    epoch          : 852\n",
      "    loss           : -457.8507376098633\n",
      "    val_loss       : -413.53199982400986\n",
      "    val_log_likelihood: 517.6738067626953\n",
      "    val_log_marginal: 418.04128950983284\n",
      "Train Epoch: 853 [512/54000 (1%)] Loss: -618.232666\n",
      "Train Epoch: 853 [11776/54000 (22%)] Loss: -429.625793\n",
      "Train Epoch: 853 [23040/54000 (43%)] Loss: -457.311554\n",
      "Train Epoch: 853 [34304/54000 (64%)] Loss: -512.342834\n",
      "Train Epoch: 853 [45568/54000 (84%)] Loss: -432.319092\n",
      "    epoch          : 853\n",
      "    loss           : -457.69709259033203\n",
      "    val_loss       : -411.4878264829516\n",
      "    val_log_likelihood: 516.0903015136719\n",
      "    val_log_marginal: 417.2067658316344\n",
      "Train Epoch: 854 [512/54000 (1%)] Loss: -473.891663\n",
      "Train Epoch: 854 [11776/54000 (22%)] Loss: -284.899139\n",
      "Train Epoch: 854 [23040/54000 (43%)] Loss: -293.588348\n",
      "Train Epoch: 854 [34304/54000 (64%)] Loss: -525.113953\n",
      "Train Epoch: 854 [45568/54000 (84%)] Loss: -256.174011\n",
      "    epoch          : 854\n",
      "    loss           : -457.9350422668457\n",
      "    val_loss       : -415.8946529438719\n",
      "    val_log_likelihood: 519.6865753173828\n",
      "    val_log_marginal: 419.92188417433516\n",
      "Train Epoch: 855 [512/54000 (1%)] Loss: -443.602966\n",
      "Train Epoch: 855 [11776/54000 (22%)] Loss: -607.040161\n",
      "Train Epoch: 855 [23040/54000 (43%)] Loss: -498.469696\n",
      "Train Epoch: 855 [34304/54000 (64%)] Loss: -505.335266\n",
      "Train Epoch: 855 [45568/54000 (84%)] Loss: -289.831909\n",
      "    epoch          : 855\n",
      "    loss           : -457.8187823486328\n",
      "    val_loss       : -417.0961531973444\n",
      "    val_log_likelihood: 516.6873809814454\n",
      "    val_log_marginal: 418.76498829498877\n",
      "Train Epoch: 856 [512/54000 (1%)] Loss: -486.898621\n",
      "Train Epoch: 856 [11776/54000 (22%)] Loss: -486.500916\n",
      "Train Epoch: 856 [23040/54000 (43%)] Loss: -261.065887\n",
      "Train Epoch: 856 [34304/54000 (64%)] Loss: -627.661987\n",
      "Train Epoch: 856 [45568/54000 (84%)] Loss: -299.376892\n",
      "    epoch          : 856\n",
      "    loss           : -457.9767710876465\n",
      "    val_loss       : -415.6674880813807\n",
      "    val_log_likelihood: 518.3140899658204\n",
      "    val_log_marginal: 418.6613981045728\n",
      "Train Epoch: 857 [512/54000 (1%)] Loss: -520.285095\n",
      "Train Epoch: 857 [11776/54000 (22%)] Loss: -491.349060\n",
      "Train Epoch: 857 [23040/54000 (43%)] Loss: -447.267181\n",
      "Train Epoch: 857 [34304/54000 (64%)] Loss: -515.577759\n",
      "Train Epoch: 857 [45568/54000 (84%)] Loss: -282.027283\n",
      "    epoch          : 857\n",
      "    loss           : -457.6803207397461\n",
      "    val_loss       : -413.9139848805964\n",
      "    val_log_likelihood: 519.169955444336\n",
      "    val_log_marginal: 419.6232088920443\n",
      "Train Epoch: 858 [512/54000 (1%)] Loss: -524.510620\n",
      "Train Epoch: 858 [11776/54000 (22%)] Loss: -622.293945\n",
      "Train Epoch: 858 [23040/54000 (43%)] Loss: -496.460236\n",
      "Train Epoch: 858 [34304/54000 (64%)] Loss: -499.269348\n",
      "Train Epoch: 858 [45568/54000 (84%)] Loss: -494.282837\n",
      "    epoch          : 858\n",
      "    loss           : -457.90036041259765\n",
      "    val_loss       : -415.36988732926545\n",
      "    val_log_likelihood: 519.57509765625\n",
      "    val_log_marginal: 420.11583702042697\n",
      "Train Epoch: 859 [512/54000 (1%)] Loss: -267.641388\n",
      "Train Epoch: 859 [11776/54000 (22%)] Loss: -440.365295\n",
      "Train Epoch: 859 [23040/54000 (43%)] Loss: -526.998169\n",
      "Train Epoch: 859 [34304/54000 (64%)] Loss: -509.115997\n",
      "Train Epoch: 859 [45568/54000 (84%)] Loss: -500.846313\n",
      "    epoch          : 859\n",
      "    loss           : -457.78092071533206\n",
      "    val_loss       : -416.346793002449\n",
      "    val_log_likelihood: 520.8906372070312\n",
      "    val_log_marginal: 421.23249854632996\n",
      "Train Epoch: 860 [512/54000 (1%)] Loss: -521.250366\n",
      "Train Epoch: 860 [11776/54000 (22%)] Loss: -476.035706\n",
      "Train Epoch: 860 [23040/54000 (43%)] Loss: -281.028412\n",
      "Train Epoch: 860 [34304/54000 (64%)] Loss: -502.721497\n",
      "Train Epoch: 860 [45568/54000 (84%)] Loss: -306.552948\n",
      "    epoch          : 860\n",
      "    loss           : -457.93485214233397\n",
      "    val_loss       : -413.67366419229654\n",
      "    val_log_likelihood: 520.0597595214844\n",
      "    val_log_marginal: 420.1472121980041\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch860.pth ...\n",
      "Train Epoch: 861 [512/54000 (1%)] Loss: -532.523071\n",
      "Train Epoch: 861 [11776/54000 (22%)] Loss: -492.992371\n",
      "Train Epoch: 861 [23040/54000 (43%)] Loss: -519.747864\n",
      "Train Epoch: 861 [34304/54000 (64%)] Loss: -280.796478\n",
      "Train Epoch: 861 [45568/54000 (84%)] Loss: -259.362061\n",
      "    epoch          : 861\n",
      "    loss           : -457.7614274597168\n",
      "    val_loss       : -413.08484269632027\n",
      "    val_log_likelihood: 518.4467041015625\n",
      "    val_log_marginal: 418.76412680633365\n",
      "Train Epoch: 862 [512/54000 (1%)] Loss: -263.667267\n",
      "Train Epoch: 862 [11776/54000 (22%)] Loss: -249.851624\n",
      "Train Epoch: 862 [23040/54000 (43%)] Loss: -495.300568\n",
      "Train Epoch: 862 [34304/54000 (64%)] Loss: -439.008728\n",
      "Train Epoch: 862 [45568/54000 (84%)] Loss: -302.978333\n",
      "    epoch          : 862\n",
      "    loss           : -457.72172882080076\n",
      "    val_loss       : -414.54389602718874\n",
      "    val_log_likelihood: 519.8586303710938\n",
      "    val_log_marginal: 419.94375009275973\n",
      "Train Epoch: 863 [512/54000 (1%)] Loss: -489.028778\n",
      "Train Epoch: 863 [11776/54000 (22%)] Loss: -507.613098\n",
      "Train Epoch: 863 [23040/54000 (43%)] Loss: -470.442688\n",
      "Train Epoch: 863 [34304/54000 (64%)] Loss: -258.599854\n",
      "Train Epoch: 863 [45568/54000 (84%)] Loss: -271.375610\n",
      "    epoch          : 863\n",
      "    loss           : -458.4961604309082\n",
      "    val_loss       : -416.69787996066736\n",
      "    val_log_likelihood: 520.3147125244141\n",
      "    val_log_marginal: 421.2998938005418\n",
      "Train Epoch: 864 [512/54000 (1%)] Loss: -506.386169\n",
      "Train Epoch: 864 [11776/54000 (22%)] Loss: -484.546387\n",
      "Train Epoch: 864 [23040/54000 (43%)] Loss: -471.485474\n",
      "Train Epoch: 864 [34304/54000 (64%)] Loss: -503.263855\n",
      "Train Epoch: 864 [45568/54000 (84%)] Loss: -498.444611\n",
      "    epoch          : 864\n",
      "    loss           : -457.01131912231443\n",
      "    val_loss       : -411.93831899985673\n",
      "    val_log_likelihood: 517.1003936767578\n",
      "    val_log_marginal: 417.56187199715714\n",
      "Train Epoch: 865 [512/54000 (1%)] Loss: -514.006531\n",
      "Train Epoch: 865 [11776/54000 (22%)] Loss: -481.697083\n",
      "Train Epoch: 865 [23040/54000 (43%)] Loss: -507.773071\n",
      "Train Epoch: 865 [34304/54000 (64%)] Loss: -286.843262\n",
      "Train Epoch: 865 [45568/54000 (84%)] Loss: -495.102295\n",
      "    epoch          : 865\n",
      "    loss           : -458.23714752197264\n",
      "    val_loss       : -414.65665874723345\n",
      "    val_log_likelihood: 518.2289123535156\n",
      "    val_log_marginal: 418.7839805793017\n",
      "Train Epoch: 866 [512/54000 (1%)] Loss: -502.965271\n",
      "Train Epoch: 866 [11776/54000 (22%)] Loss: -265.526672\n",
      "Train Epoch: 866 [23040/54000 (43%)] Loss: -429.717377\n",
      "Train Epoch: 866 [34304/54000 (64%)] Loss: -444.708801\n",
      "Train Epoch: 866 [45568/54000 (84%)] Loss: -486.957092\n",
      "    epoch          : 866\n",
      "    loss           : -457.96364166259764\n",
      "    val_loss       : -419.8330323904753\n",
      "    val_log_likelihood: 522.3935455322265\n",
      "    val_log_marginal: 423.39210925512015\n",
      "Train Epoch: 867 [512/54000 (1%)] Loss: -248.329391\n",
      "Train Epoch: 867 [11776/54000 (22%)] Loss: -493.973877\n",
      "Train Epoch: 867 [23040/54000 (43%)] Loss: -260.785431\n",
      "Train Epoch: 867 [34304/54000 (64%)] Loss: -510.069000\n",
      "Train Epoch: 867 [45568/54000 (84%)] Loss: -441.497925\n",
      "    epoch          : 867\n",
      "    loss           : -458.011434173584\n",
      "    val_loss       : -418.092058939673\n",
      "    val_log_likelihood: 522.4894805908203\n",
      "    val_log_marginal: 422.70776565152175\n",
      "Train Epoch: 868 [512/54000 (1%)] Loss: -278.623413\n",
      "Train Epoch: 868 [11776/54000 (22%)] Loss: -290.159576\n",
      "Train Epoch: 868 [23040/54000 (43%)] Loss: -497.935272\n",
      "Train Epoch: 868 [34304/54000 (64%)] Loss: -460.507141\n",
      "Train Epoch: 868 [45568/54000 (84%)] Loss: -499.902802\n",
      "    epoch          : 868\n",
      "    loss           : -459.02794982910154\n",
      "    val_loss       : -417.720108929649\n",
      "    val_log_likelihood: 519.9191772460938\n",
      "    val_log_marginal: 420.75863135270777\n",
      "Train Epoch: 869 [512/54000 (1%)] Loss: -479.848999\n",
      "Train Epoch: 869 [11776/54000 (22%)] Loss: -439.649963\n",
      "Train Epoch: 869 [23040/54000 (43%)] Loss: -494.037323\n",
      "Train Epoch: 869 [34304/54000 (64%)] Loss: -490.261475\n",
      "Train Epoch: 869 [45568/54000 (84%)] Loss: -278.414581\n",
      "    epoch          : 869\n",
      "    loss           : -458.05859939575197\n",
      "    val_loss       : -413.59127282034603\n",
      "    val_log_likelihood: 517.8852142333984\n",
      "    val_log_marginal: 418.4231211785228\n",
      "Train Epoch: 870 [512/54000 (1%)] Loss: -491.532654\n",
      "Train Epoch: 870 [11776/54000 (22%)] Loss: -489.608917\n",
      "Train Epoch: 870 [23040/54000 (43%)] Loss: -491.662537\n",
      "Train Epoch: 870 [34304/54000 (64%)] Loss: -621.707031\n",
      "Train Epoch: 870 [45568/54000 (84%)] Loss: -499.452240\n",
      "    epoch          : 870\n",
      "    loss           : -458.66144500732423\n",
      "    val_loss       : -417.1731575181708\n",
      "    val_log_likelihood: 520.5608612060547\n",
      "    val_log_marginal: 421.44294158108534\n",
      "Train Epoch: 871 [512/54000 (1%)] Loss: -466.496216\n",
      "Train Epoch: 871 [11776/54000 (22%)] Loss: -513.422668\n",
      "Train Epoch: 871 [23040/54000 (43%)] Loss: -479.630737\n",
      "Train Epoch: 871 [34304/54000 (64%)] Loss: -281.547333\n",
      "Train Epoch: 871 [45568/54000 (84%)] Loss: -495.082977\n",
      "    epoch          : 871\n",
      "    loss           : -458.83152893066404\n",
      "    val_loss       : -417.30674495138226\n",
      "    val_log_likelihood: 519.0106170654296\n",
      "    val_log_marginal: 420.1356225240976\n",
      "Train Epoch: 872 [512/54000 (1%)] Loss: -528.281494\n",
      "Train Epoch: 872 [11776/54000 (22%)] Loss: -449.920532\n",
      "Train Epoch: 872 [23040/54000 (43%)] Loss: -433.927277\n",
      "Train Epoch: 872 [34304/54000 (64%)] Loss: -440.019775\n",
      "Train Epoch: 872 [45568/54000 (84%)] Loss: -300.188599\n",
      "    epoch          : 872\n",
      "    loss           : -457.987879486084\n",
      "    val_loss       : -417.0107433293015\n",
      "    val_log_likelihood: 520.0674102783203\n",
      "    val_log_marginal: 420.79869730435314\n",
      "Train Epoch: 873 [512/54000 (1%)] Loss: -519.799438\n",
      "Train Epoch: 873 [11776/54000 (22%)] Loss: -257.665619\n",
      "Train Epoch: 873 [23040/54000 (43%)] Loss: -307.679260\n",
      "Train Epoch: 873 [34304/54000 (64%)] Loss: -288.557678\n",
      "Train Epoch: 873 [45568/54000 (84%)] Loss: -438.275574\n",
      "    epoch          : 873\n",
      "    loss           : -458.13375366210937\n",
      "    val_loss       : -415.01099741784856\n",
      "    val_log_likelihood: 518.9787017822266\n",
      "    val_log_marginal: 419.54344149194657\n",
      "Train Epoch: 874 [512/54000 (1%)] Loss: -424.129761\n",
      "Train Epoch: 874 [11776/54000 (22%)] Loss: -626.870483\n",
      "Train Epoch: 874 [23040/54000 (43%)] Loss: -622.934631\n",
      "Train Epoch: 874 [34304/54000 (64%)] Loss: -489.826294\n",
      "Train Epoch: 874 [45568/54000 (84%)] Loss: -302.653717\n",
      "    epoch          : 874\n",
      "    loss           : -458.4657731628418\n",
      "    val_loss       : -415.5828493868932\n",
      "    val_log_likelihood: 517.4603576660156\n",
      "    val_log_marginal: 418.83938851803543\n",
      "Train Epoch: 875 [512/54000 (1%)] Loss: -624.336975\n",
      "Train Epoch: 875 [11776/54000 (22%)] Loss: -262.658875\n",
      "Train Epoch: 875 [23040/54000 (43%)] Loss: -474.372864\n",
      "Train Epoch: 875 [34304/54000 (64%)] Loss: -510.884399\n",
      "Train Epoch: 875 [45568/54000 (84%)] Loss: -500.131836\n",
      "    epoch          : 875\n",
      "    loss           : -458.28691390991213\n",
      "    val_loss       : -415.92563008796424\n",
      "    val_log_likelihood: 519.9473327636719\n",
      "    val_log_marginal: 420.1177680697292\n",
      "Train Epoch: 876 [512/54000 (1%)] Loss: -464.659515\n",
      "Train Epoch: 876 [11776/54000 (22%)] Loss: -313.732361\n",
      "Train Epoch: 876 [23040/54000 (43%)] Loss: -480.878235\n",
      "Train Epoch: 876 [34304/54000 (64%)] Loss: -317.231628\n",
      "Train Epoch: 876 [45568/54000 (84%)] Loss: -436.332458\n",
      "    epoch          : 876\n",
      "    loss           : -459.1878036499023\n",
      "    val_loss       : -415.4454498659819\n",
      "    val_log_likelihood: 519.2161254882812\n",
      "    val_log_marginal: 419.3929081801325\n",
      "Train Epoch: 877 [512/54000 (1%)] Loss: -508.521881\n",
      "Train Epoch: 877 [11776/54000 (22%)] Loss: -445.180359\n",
      "Train Epoch: 877 [23040/54000 (43%)] Loss: -494.287933\n",
      "Train Epoch: 877 [34304/54000 (64%)] Loss: -307.588104\n",
      "Train Epoch: 877 [45568/54000 (84%)] Loss: -513.529541\n",
      "    epoch          : 877\n",
      "    loss           : -458.0075039672852\n",
      "    val_loss       : -415.8131518032402\n",
      "    val_log_likelihood: 520.788784790039\n",
      "    val_log_marginal: 421.4079073276371\n",
      "Train Epoch: 878 [512/54000 (1%)] Loss: -478.388397\n",
      "Train Epoch: 878 [11776/54000 (22%)] Loss: -514.288086\n",
      "Train Epoch: 878 [23040/54000 (43%)] Loss: -528.202637\n",
      "Train Epoch: 878 [34304/54000 (64%)] Loss: -625.231323\n",
      "Train Epoch: 878 [45568/54000 (84%)] Loss: -490.355988\n",
      "    epoch          : 878\n",
      "    loss           : -458.8631806945801\n",
      "    val_loss       : -412.62485275780784\n",
      "    val_log_likelihood: 519.1991333007812\n",
      "    val_log_marginal: 418.7930398393422\n",
      "Train Epoch: 879 [512/54000 (1%)] Loss: -440.473816\n",
      "Train Epoch: 879 [11776/54000 (22%)] Loss: -503.672058\n",
      "Train Epoch: 879 [23040/54000 (43%)] Loss: -532.598145\n",
      "Train Epoch: 879 [34304/54000 (64%)] Loss: -512.586243\n",
      "Train Epoch: 879 [45568/54000 (84%)] Loss: -471.814117\n",
      "    epoch          : 879\n",
      "    loss           : -458.7881163024902\n",
      "    val_loss       : -417.13352362569424\n",
      "    val_log_likelihood: 520.9267456054688\n",
      "    val_log_marginal: 420.43730826191603\n",
      "Train Epoch: 880 [512/54000 (1%)] Loss: -484.048340\n",
      "Train Epoch: 880 [11776/54000 (22%)] Loss: -621.469299\n",
      "Train Epoch: 880 [23040/54000 (43%)] Loss: -439.248871\n",
      "Train Epoch: 880 [34304/54000 (64%)] Loss: -442.017181\n",
      "Train Epoch: 880 [45568/54000 (84%)] Loss: -518.371704\n",
      "    epoch          : 880\n",
      "    loss           : -458.91181640625\n",
      "    val_loss       : -417.7784780683927\n",
      "    val_log_likelihood: 522.6448486328125\n",
      "    val_log_marginal: 422.5288744624704\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch880.pth ...\n",
      "Train Epoch: 881 [512/54000 (1%)] Loss: -625.198792\n",
      "Train Epoch: 881 [11776/54000 (22%)] Loss: -491.888306\n",
      "Train Epoch: 881 [23040/54000 (43%)] Loss: -503.651764\n",
      "Train Epoch: 881 [34304/54000 (64%)] Loss: -428.053741\n",
      "Train Epoch: 881 [45568/54000 (84%)] Loss: -522.509155\n",
      "    epoch          : 881\n",
      "    loss           : -458.4562442016602\n",
      "    val_loss       : -417.3467181729153\n",
      "    val_log_likelihood: 520.3041076660156\n",
      "    val_log_marginal: 420.51129008532064\n",
      "Train Epoch: 882 [512/54000 (1%)] Loss: -498.985291\n",
      "Train Epoch: 882 [11776/54000 (22%)] Loss: -264.651154\n",
      "Train Epoch: 882 [23040/54000 (43%)] Loss: -524.754761\n",
      "Train Epoch: 882 [34304/54000 (64%)] Loss: -464.027832\n",
      "Train Epoch: 882 [45568/54000 (84%)] Loss: -497.972046\n",
      "    epoch          : 882\n",
      "    loss           : -459.0067803955078\n",
      "    val_loss       : -416.08829779187215\n",
      "    val_log_likelihood: 518.7037689208985\n",
      "    val_log_marginal: 419.8943910863251\n",
      "Train Epoch: 883 [512/54000 (1%)] Loss: -623.908081\n",
      "Train Epoch: 883 [11776/54000 (22%)] Loss: -477.683716\n",
      "Train Epoch: 883 [23040/54000 (43%)] Loss: -299.515259\n",
      "Train Epoch: 883 [34304/54000 (64%)] Loss: -265.960358\n",
      "Train Epoch: 883 [45568/54000 (84%)] Loss: -493.230011\n",
      "    epoch          : 883\n",
      "    loss           : -458.67421173095704\n",
      "    val_loss       : -416.04649599948897\n",
      "    val_log_likelihood: 517.46201171875\n",
      "    val_log_marginal: 418.9464545108378\n",
      "Train Epoch: 884 [512/54000 (1%)] Loss: -508.274811\n",
      "Train Epoch: 884 [11776/54000 (22%)] Loss: -623.286499\n",
      "Train Epoch: 884 [23040/54000 (43%)] Loss: -493.339050\n",
      "Train Epoch: 884 [34304/54000 (64%)] Loss: -502.386078\n",
      "Train Epoch: 884 [45568/54000 (84%)] Loss: -514.952026\n",
      "    epoch          : 884\n",
      "    loss           : -458.3489787292481\n",
      "    val_loss       : -418.2374270462431\n",
      "    val_log_likelihood: 521.0784912109375\n",
      "    val_log_marginal: 422.0338070493267\n",
      "Train Epoch: 885 [512/54000 (1%)] Loss: -522.713257\n",
      "Train Epoch: 885 [11776/54000 (22%)] Loss: -495.137238\n",
      "Train Epoch: 885 [23040/54000 (43%)] Loss: -495.664948\n",
      "Train Epoch: 885 [34304/54000 (64%)] Loss: -487.408508\n",
      "Train Epoch: 885 [45568/54000 (84%)] Loss: -426.829529\n",
      "    epoch          : 885\n",
      "    loss           : -459.216064453125\n",
      "    val_loss       : -415.7309030047618\n",
      "    val_log_likelihood: 521.4350311279297\n",
      "    val_log_marginal: 420.9093310687837\n",
      "Train Epoch: 886 [512/54000 (1%)] Loss: -495.334900\n",
      "Train Epoch: 886 [11776/54000 (22%)] Loss: -493.975677\n",
      "Train Epoch: 886 [23040/54000 (43%)] Loss: -505.253479\n",
      "Train Epoch: 886 [34304/54000 (64%)] Loss: -475.273682\n",
      "Train Epoch: 886 [45568/54000 (84%)] Loss: -509.496155\n",
      "    epoch          : 886\n",
      "    loss           : -458.6810765075684\n",
      "    val_loss       : -415.9348846898414\n",
      "    val_log_likelihood: 521.9642791748047\n",
      "    val_log_marginal: 421.4644028544004\n",
      "Train Epoch: 887 [512/54000 (1%)] Loss: -473.427429\n",
      "Train Epoch: 887 [11776/54000 (22%)] Loss: -479.236603\n",
      "Train Epoch: 887 [23040/54000 (43%)] Loss: -298.625854\n",
      "Train Epoch: 887 [34304/54000 (64%)] Loss: -489.440521\n",
      "Train Epoch: 887 [45568/54000 (84%)] Loss: -492.843872\n",
      "    epoch          : 887\n",
      "    loss           : -458.8064739990234\n",
      "    val_loss       : -415.4277091308497\n",
      "    val_log_likelihood: 520.547476196289\n",
      "    val_log_marginal: 420.55812361724674\n",
      "Train Epoch: 888 [512/54000 (1%)] Loss: -444.457031\n",
      "Train Epoch: 888 [11776/54000 (22%)] Loss: -306.347076\n",
      "Train Epoch: 888 [23040/54000 (43%)] Loss: -497.473053\n",
      "Train Epoch: 888 [34304/54000 (64%)] Loss: -253.428055\n",
      "Train Epoch: 888 [45568/54000 (84%)] Loss: -287.056122\n",
      "    epoch          : 888\n",
      "    loss           : -458.7488883972168\n",
      "    val_loss       : -416.3258110780269\n",
      "    val_log_likelihood: 522.4997192382813\n",
      "    val_log_marginal: 422.21064861677587\n",
      "Train Epoch: 889 [512/54000 (1%)] Loss: -480.352295\n",
      "Train Epoch: 889 [11776/54000 (22%)] Loss: -302.262939\n",
      "Train Epoch: 889 [23040/54000 (43%)] Loss: -499.196869\n",
      "Train Epoch: 889 [34304/54000 (64%)] Loss: -472.192719\n",
      "Train Epoch: 889 [45568/54000 (84%)] Loss: -495.939667\n",
      "    epoch          : 889\n",
      "    loss           : -459.55761352539065\n",
      "    val_loss       : -414.53755975971\n",
      "    val_log_likelihood: 519.2169372558594\n",
      "    val_log_marginal: 419.05495862029494\n",
      "Train Epoch: 890 [512/54000 (1%)] Loss: -484.323883\n",
      "Train Epoch: 890 [11776/54000 (22%)] Loss: -254.175415\n",
      "Train Epoch: 890 [23040/54000 (43%)] Loss: -478.699402\n",
      "Train Epoch: 890 [34304/54000 (64%)] Loss: -460.171783\n",
      "Train Epoch: 890 [45568/54000 (84%)] Loss: -492.684296\n",
      "    epoch          : 890\n",
      "    loss           : -459.07178146362304\n",
      "    val_loss       : -414.6050482138991\n",
      "    val_log_likelihood: 520.2355010986328\n",
      "    val_log_marginal: 419.3458227869123\n",
      "Train Epoch: 891 [512/54000 (1%)] Loss: -495.388458\n",
      "Train Epoch: 891 [11776/54000 (22%)] Loss: -499.341187\n",
      "Train Epoch: 891 [23040/54000 (43%)] Loss: -309.211914\n",
      "Train Epoch: 891 [34304/54000 (64%)] Loss: -305.848907\n",
      "Train Epoch: 891 [45568/54000 (84%)] Loss: -268.686157\n",
      "    epoch          : 891\n",
      "    loss           : -459.00161392211913\n",
      "    val_loss       : -417.9199633160606\n",
      "    val_log_likelihood: 521.4851470947266\n",
      "    val_log_marginal: 421.7176169872294\n",
      "Train Epoch: 892 [512/54000 (1%)] Loss: -478.037415\n",
      "Train Epoch: 892 [11776/54000 (22%)] Loss: -268.269287\n",
      "Train Epoch: 892 [23040/54000 (43%)] Loss: -503.066589\n",
      "Train Epoch: 892 [34304/54000 (64%)] Loss: -501.067963\n",
      "Train Epoch: 892 [45568/54000 (84%)] Loss: -489.678650\n",
      "    epoch          : 892\n",
      "    loss           : -459.18287170410156\n",
      "    val_loss       : -416.8468970953487\n",
      "    val_log_likelihood: 520.4867065429687\n",
      "    val_log_marginal: 420.9737806852907\n",
      "Train Epoch: 893 [512/54000 (1%)] Loss: -461.257751\n",
      "Train Epoch: 893 [11776/54000 (22%)] Loss: -507.407898\n",
      "Train Epoch: 893 [23040/54000 (43%)] Loss: -272.302429\n",
      "Train Epoch: 893 [34304/54000 (64%)] Loss: -526.927551\n",
      "Train Epoch: 893 [45568/54000 (84%)] Loss: -490.001160\n",
      "    epoch          : 893\n",
      "    loss           : -459.2092135620117\n",
      "    val_loss       : -414.1339534626342\n",
      "    val_log_likelihood: 519.4993743896484\n",
      "    val_log_marginal: 419.3615685176143\n",
      "Train Epoch: 894 [512/54000 (1%)] Loss: -441.005463\n",
      "Train Epoch: 894 [11776/54000 (22%)] Loss: -496.996033\n",
      "Train Epoch: 894 [23040/54000 (43%)] Loss: -608.026611\n",
      "Train Epoch: 894 [34304/54000 (64%)] Loss: -501.479340\n",
      "Train Epoch: 894 [45568/54000 (84%)] Loss: -501.969666\n",
      "    epoch          : 894\n",
      "    loss           : -459.14952926635743\n",
      "    val_loss       : -416.875732269045\n",
      "    val_log_likelihood: 519.7172149658203\n",
      "    val_log_marginal: 420.6022843229331\n",
      "Train Epoch: 895 [512/54000 (1%)] Loss: -430.707092\n",
      "Train Epoch: 895 [11776/54000 (22%)] Loss: -434.554993\n",
      "Train Epoch: 895 [23040/54000 (43%)] Loss: -452.617004\n",
      "Train Epoch: 895 [34304/54000 (64%)] Loss: -503.064514\n",
      "Train Epoch: 895 [45568/54000 (84%)] Loss: -524.364746\n",
      "    epoch          : 895\n",
      "    loss           : -459.73490264892575\n",
      "    val_loss       : -417.086408967711\n",
      "    val_log_likelihood: 520.916455078125\n",
      "    val_log_marginal: 421.7148945312947\n",
      "Train Epoch: 896 [512/54000 (1%)] Loss: -618.373352\n",
      "Train Epoch: 896 [11776/54000 (22%)] Loss: -256.604858\n",
      "Train Epoch: 896 [23040/54000 (43%)] Loss: -489.166290\n",
      "Train Epoch: 896 [34304/54000 (64%)] Loss: -312.329590\n",
      "Train Epoch: 896 [45568/54000 (84%)] Loss: -526.667969\n",
      "    epoch          : 896\n",
      "    loss           : -458.9980308532715\n",
      "    val_loss       : -415.2846306551248\n",
      "    val_log_likelihood: 521.7353363037109\n",
      "    val_log_marginal: 421.134385137103\n",
      "Train Epoch: 897 [512/54000 (1%)] Loss: -623.189575\n",
      "Train Epoch: 897 [11776/54000 (22%)] Loss: -487.723877\n",
      "Train Epoch: 897 [23040/54000 (43%)] Loss: -263.930237\n",
      "Train Epoch: 897 [34304/54000 (64%)] Loss: -473.430084\n",
      "Train Epoch: 897 [45568/54000 (84%)] Loss: -503.133057\n",
      "    epoch          : 897\n",
      "    loss           : -458.8573059082031\n",
      "    val_loss       : -415.80830086367206\n",
      "    val_log_likelihood: 521.5598907470703\n",
      "    val_log_marginal: 420.8006904277951\n",
      "Train Epoch: 898 [512/54000 (1%)] Loss: -275.392944\n",
      "Train Epoch: 898 [11776/54000 (22%)] Loss: -487.535889\n",
      "Train Epoch: 898 [23040/54000 (43%)] Loss: -488.179504\n",
      "Train Epoch: 898 [34304/54000 (64%)] Loss: -488.920288\n",
      "Train Epoch: 898 [45568/54000 (84%)] Loss: -491.316406\n",
      "    epoch          : 898\n",
      "    loss           : -458.8140951538086\n",
      "    val_loss       : -416.3730295489542\n",
      "    val_log_likelihood: 520.9827423095703\n",
      "    val_log_marginal: 420.4204599123448\n",
      "Train Epoch: 899 [512/54000 (1%)] Loss: -522.951294\n",
      "Train Epoch: 899 [11776/54000 (22%)] Loss: -477.899231\n",
      "Train Epoch: 899 [23040/54000 (43%)] Loss: -296.024017\n",
      "Train Epoch: 899 [34304/54000 (64%)] Loss: -505.095490\n",
      "Train Epoch: 899 [45568/54000 (84%)] Loss: -495.909760\n",
      "    epoch          : 899\n",
      "    loss           : -459.7444448852539\n",
      "    val_loss       : -416.7567913268693\n",
      "    val_log_likelihood: 522.1191040039063\n",
      "    val_log_marginal: 421.2152008693665\n",
      "Train Epoch: 900 [512/54000 (1%)] Loss: -538.074036\n",
      "Train Epoch: 900 [11776/54000 (22%)] Loss: -522.969910\n",
      "Train Epoch: 900 [23040/54000 (43%)] Loss: -445.347382\n",
      "Train Epoch: 900 [34304/54000 (64%)] Loss: -446.678253\n",
      "Train Epoch: 900 [45568/54000 (84%)] Loss: -486.885620\n",
      "    epoch          : 900\n",
      "    loss           : -459.6610922241211\n",
      "    val_loss       : -416.14627790404484\n",
      "    val_log_likelihood: 519.3164611816406\n",
      "    val_log_marginal: 419.9230691809207\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch900.pth ...\n",
      "Train Epoch: 901 [512/54000 (1%)] Loss: -466.862946\n",
      "Train Epoch: 901 [11776/54000 (22%)] Loss: -510.559601\n",
      "Train Epoch: 901 [23040/54000 (43%)] Loss: -494.236603\n",
      "Train Epoch: 901 [34304/54000 (64%)] Loss: -499.465088\n",
      "Train Epoch: 901 [45568/54000 (84%)] Loss: -281.769531\n",
      "    epoch          : 901\n",
      "    loss           : -459.40252822875976\n",
      "    val_loss       : -417.6135789385065\n",
      "    val_log_likelihood: 523.0911285400391\n",
      "    val_log_marginal: 422.7725907977825\n",
      "Train Epoch: 902 [512/54000 (1%)] Loss: -470.970642\n",
      "Train Epoch: 902 [11776/54000 (22%)] Loss: -619.802185\n",
      "Train Epoch: 902 [23040/54000 (43%)] Loss: -493.204010\n",
      "Train Epoch: 902 [34304/54000 (64%)] Loss: -516.121094\n",
      "Train Epoch: 902 [45568/54000 (84%)] Loss: -427.719788\n",
      "    epoch          : 902\n",
      "    loss           : -459.4597546386719\n",
      "    val_loss       : -418.26462608445433\n",
      "    val_log_likelihood: 521.3355041503906\n",
      "    val_log_marginal: 421.5009311720729\n",
      "Train Epoch: 903 [512/54000 (1%)] Loss: -531.130615\n",
      "Train Epoch: 903 [11776/54000 (22%)] Loss: -505.011810\n",
      "Train Epoch: 903 [23040/54000 (43%)] Loss: -437.718811\n",
      "Train Epoch: 903 [34304/54000 (64%)] Loss: -499.819550\n",
      "Train Epoch: 903 [45568/54000 (84%)] Loss: -262.575256\n",
      "    epoch          : 903\n",
      "    loss           : -459.87877471923827\n",
      "    val_loss       : -416.385447147768\n",
      "    val_log_likelihood: 520.8152923583984\n",
      "    val_log_marginal: 420.7032564882189\n",
      "Train Epoch: 904 [512/54000 (1%)] Loss: -500.693329\n",
      "Train Epoch: 904 [11776/54000 (22%)] Loss: -445.256226\n",
      "Train Epoch: 904 [23040/54000 (43%)] Loss: -475.544739\n",
      "Train Epoch: 904 [34304/54000 (64%)] Loss: -511.792114\n",
      "Train Epoch: 904 [45568/54000 (84%)] Loss: -306.152100\n",
      "    epoch          : 904\n",
      "    loss           : -459.92768310546876\n",
      "    val_loss       : -414.7456278695725\n",
      "    val_log_likelihood: 520.3475769042968\n",
      "    val_log_marginal: 420.0653012700379\n",
      "Train Epoch: 905 [512/54000 (1%)] Loss: -621.517090\n",
      "Train Epoch: 905 [11776/54000 (22%)] Loss: -528.950317\n",
      "Train Epoch: 905 [23040/54000 (43%)] Loss: -474.415955\n",
      "Train Epoch: 905 [34304/54000 (64%)] Loss: -504.888794\n",
      "Train Epoch: 905 [45568/54000 (84%)] Loss: -497.880920\n",
      "    epoch          : 905\n",
      "    loss           : -460.21302337646483\n",
      "    val_loss       : -414.7387426279485\n",
      "    val_log_likelihood: 521.4299255371094\n",
      "    val_log_marginal: 420.83093535639347\n",
      "Train Epoch: 906 [512/54000 (1%)] Loss: -536.701172\n",
      "Train Epoch: 906 [11776/54000 (22%)] Loss: -484.893250\n",
      "Train Epoch: 906 [23040/54000 (43%)] Loss: -460.381439\n",
      "Train Epoch: 906 [34304/54000 (64%)] Loss: -498.393066\n",
      "Train Epoch: 906 [45568/54000 (84%)] Loss: -511.113708\n",
      "    epoch          : 906\n",
      "    loss           : -460.49701110839845\n",
      "    val_loss       : -419.37375394087286\n",
      "    val_log_likelihood: 524.7456146240235\n",
      "    val_log_marginal: 424.2742620159035\n",
      "Train Epoch: 907 [512/54000 (1%)] Loss: -482.557617\n",
      "Train Epoch: 907 [11776/54000 (22%)] Loss: -630.758118\n",
      "Train Epoch: 907 [23040/54000 (43%)] Loss: -485.492981\n",
      "Train Epoch: 907 [34304/54000 (64%)] Loss: -483.179016\n",
      "Train Epoch: 907 [45568/54000 (84%)] Loss: -446.654816\n",
      "    epoch          : 907\n",
      "    loss           : -459.95601303100585\n",
      "    val_loss       : -413.81509272940457\n",
      "    val_log_likelihood: 520.6572448730469\n",
      "    val_log_marginal: 419.7510375294834\n",
      "Train Epoch: 908 [512/54000 (1%)] Loss: -531.874084\n",
      "Train Epoch: 908 [11776/54000 (22%)] Loss: -494.912109\n",
      "Train Epoch: 908 [23040/54000 (43%)] Loss: -622.277832\n",
      "Train Epoch: 908 [34304/54000 (64%)] Loss: -498.586914\n",
      "Train Epoch: 908 [45568/54000 (84%)] Loss: -508.227570\n",
      "    epoch          : 908\n",
      "    loss           : -460.09971572875975\n",
      "    val_loss       : -419.0162669160403\n",
      "    val_log_likelihood: 521.5116455078125\n",
      "    val_log_marginal: 421.6021457899362\n",
      "Train Epoch: 909 [512/54000 (1%)] Loss: -632.524780\n",
      "Train Epoch: 909 [11776/54000 (22%)] Loss: -500.128845\n",
      "Train Epoch: 909 [23040/54000 (43%)] Loss: -533.812500\n",
      "Train Epoch: 909 [34304/54000 (64%)] Loss: -613.384216\n",
      "Train Epoch: 909 [45568/54000 (84%)] Loss: -478.796143\n",
      "    epoch          : 909\n",
      "    loss           : -459.9397383117676\n",
      "    val_loss       : -418.5972733530216\n",
      "    val_log_likelihood: 523.675668334961\n",
      "    val_log_marginal: 422.7110340181738\n",
      "Train Epoch: 910 [512/54000 (1%)] Loss: -501.288818\n",
      "Train Epoch: 910 [11776/54000 (22%)] Loss: -489.047394\n",
      "Train Epoch: 910 [23040/54000 (43%)] Loss: -514.303711\n",
      "Train Epoch: 910 [34304/54000 (64%)] Loss: -298.996643\n",
      "Train Epoch: 910 [45568/54000 (84%)] Loss: -284.105652\n",
      "    epoch          : 910\n",
      "    loss           : -460.11949905395505\n",
      "    val_loss       : -415.7283161756583\n",
      "    val_log_likelihood: 519.5600189208984\n",
      "    val_log_marginal: 419.94509644918145\n",
      "Train Epoch: 911 [512/54000 (1%)] Loss: -493.612152\n",
      "Train Epoch: 911 [11776/54000 (22%)] Loss: -489.816040\n",
      "Train Epoch: 911 [23040/54000 (43%)] Loss: -494.388855\n",
      "Train Epoch: 911 [34304/54000 (64%)] Loss: -441.265228\n",
      "Train Epoch: 911 [45568/54000 (84%)] Loss: -443.600006\n",
      "    epoch          : 911\n",
      "    loss           : -459.45276916503906\n",
      "    val_loss       : -415.682805287838\n",
      "    val_log_likelihood: 521.9352630615234\n",
      "    val_log_marginal: 420.78719268054175\n",
      "Train Epoch: 912 [512/54000 (1%)] Loss: -488.544434\n",
      "Train Epoch: 912 [11776/54000 (22%)] Loss: -437.742065\n",
      "Train Epoch: 912 [23040/54000 (43%)] Loss: -469.808167\n",
      "Train Epoch: 912 [34304/54000 (64%)] Loss: -458.400452\n",
      "Train Epoch: 912 [45568/54000 (84%)] Loss: -507.290833\n",
      "    epoch          : 912\n",
      "    loss           : -460.2226072692871\n",
      "    val_loss       : -417.4698965258896\n",
      "    val_log_likelihood: 521.0172332763672\n",
      "    val_log_marginal: 420.9328439828008\n",
      "Train Epoch: 913 [512/54000 (1%)] Loss: -530.626221\n",
      "Train Epoch: 913 [11776/54000 (22%)] Loss: -470.096436\n",
      "Train Epoch: 913 [23040/54000 (43%)] Loss: -493.963562\n",
      "Train Epoch: 913 [34304/54000 (64%)] Loss: -258.274506\n",
      "Train Epoch: 913 [45568/54000 (84%)] Loss: -264.557434\n",
      "    epoch          : 913\n",
      "    loss           : -459.618857421875\n",
      "    val_loss       : -417.340950789582\n",
      "    val_log_likelihood: 519.8592071533203\n",
      "    val_log_marginal: 420.33625349096366\n",
      "Train Epoch: 914 [512/54000 (1%)] Loss: -504.968597\n",
      "Train Epoch: 914 [11776/54000 (22%)] Loss: -623.737183\n",
      "Train Epoch: 914 [23040/54000 (43%)] Loss: -622.134644\n",
      "Train Epoch: 914 [34304/54000 (64%)] Loss: -502.122253\n",
      "Train Epoch: 914 [45568/54000 (84%)] Loss: -292.421692\n",
      "    epoch          : 914\n",
      "    loss           : -460.32882019042967\n",
      "    val_loss       : -419.58243464911357\n",
      "    val_log_likelihood: 522.7956817626953\n",
      "    val_log_marginal: 422.443562464416\n",
      "Train Epoch: 915 [512/54000 (1%)] Loss: -261.667023\n",
      "Train Epoch: 915 [11776/54000 (22%)] Loss: -635.994019\n",
      "Train Epoch: 915 [23040/54000 (43%)] Loss: -528.933716\n",
      "Train Epoch: 915 [34304/54000 (64%)] Loss: -290.744568\n",
      "Train Epoch: 915 [45568/54000 (84%)] Loss: -493.957855\n",
      "    epoch          : 915\n",
      "    loss           : -459.9907800292969\n",
      "    val_loss       : -416.26940083224326\n",
      "    val_log_likelihood: 520.1379974365234\n",
      "    val_log_marginal: 419.5210723038763\n",
      "Train Epoch: 916 [512/54000 (1%)] Loss: -499.810760\n",
      "Train Epoch: 916 [11776/54000 (22%)] Loss: -505.245544\n",
      "Train Epoch: 916 [23040/54000 (43%)] Loss: -469.683533\n",
      "Train Epoch: 916 [34304/54000 (64%)] Loss: -295.764435\n",
      "Train Epoch: 916 [45568/54000 (84%)] Loss: -501.151672\n",
      "    epoch          : 916\n",
      "    loss           : -460.39820739746096\n",
      "    val_loss       : -418.54894587202\n",
      "    val_log_likelihood: 522.9204498291016\n",
      "    val_log_marginal: 421.7372599133567\n",
      "Train Epoch: 917 [512/54000 (1%)] Loss: -461.233612\n",
      "Train Epoch: 917 [11776/54000 (22%)] Loss: -437.384064\n",
      "Train Epoch: 917 [23040/54000 (43%)] Loss: -531.637939\n",
      "Train Epoch: 917 [34304/54000 (64%)] Loss: -477.454315\n",
      "Train Epoch: 917 [45568/54000 (84%)] Loss: -505.546082\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "    epoch          : 917\n",
      "    loss           : -460.1507012939453\n",
      "    val_loss       : -417.62287825522947\n",
      "    val_log_likelihood: 523.911587524414\n",
      "    val_log_marginal: 423.49954677931964\n",
      "Train Epoch: 918 [512/54000 (1%)] Loss: -274.480225\n",
      "Train Epoch: 918 [11776/54000 (22%)] Loss: -315.219299\n",
      "Train Epoch: 918 [23040/54000 (43%)] Loss: -258.220520\n",
      "Train Epoch: 918 [34304/54000 (64%)] Loss: -480.257690\n",
      "Train Epoch: 918 [45568/54000 (84%)] Loss: -510.487946\n",
      "    epoch          : 918\n",
      "    loss           : -460.90592224121093\n",
      "    val_loss       : -419.0171615486033\n",
      "    val_log_likelihood: 523.0686981201172\n",
      "    val_log_marginal: 422.9476288933307\n",
      "Train Epoch: 919 [512/54000 (1%)] Loss: -524.322632\n",
      "Train Epoch: 919 [11776/54000 (22%)] Loss: -285.606140\n",
      "Train Epoch: 919 [23040/54000 (43%)] Loss: -305.888336\n",
      "Train Epoch: 919 [34304/54000 (64%)] Loss: -491.134705\n",
      "Train Epoch: 919 [45568/54000 (84%)] Loss: -503.980042\n",
      "    epoch          : 919\n",
      "    loss           : -460.36741958618165\n",
      "    val_loss       : -417.42175703309476\n",
      "    val_log_likelihood: 523.2402618408203\n",
      "    val_log_marginal: 422.2731614787132\n",
      "Train Epoch: 920 [512/54000 (1%)] Loss: -478.667816\n",
      "Train Epoch: 920 [11776/54000 (22%)] Loss: -467.222931\n",
      "Train Epoch: 920 [23040/54000 (43%)] Loss: -437.792603\n",
      "Train Epoch: 920 [34304/54000 (64%)] Loss: -469.542542\n",
      "Train Epoch: 920 [45568/54000 (84%)] Loss: -494.622314\n",
      "    epoch          : 920\n",
      "    loss           : -460.67402389526364\n",
      "    val_loss       : -419.13695011548697\n",
      "    val_log_likelihood: 523.749365234375\n",
      "    val_log_marginal: 423.1375766161829\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch920.pth ...\n",
      "Train Epoch: 921 [512/54000 (1%)] Loss: -492.230286\n",
      "Train Epoch: 921 [11776/54000 (22%)] Loss: -490.189331\n",
      "Train Epoch: 921 [23040/54000 (43%)] Loss: -442.348816\n",
      "Train Epoch: 921 [34304/54000 (64%)] Loss: -499.028992\n",
      "Train Epoch: 921 [45568/54000 (84%)] Loss: -479.243896\n",
      "    epoch          : 921\n",
      "    loss           : -460.524488067627\n",
      "    val_loss       : -417.97448601219804\n",
      "    val_log_likelihood: 523.281118774414\n",
      "    val_log_marginal: 423.3235065903515\n",
      "Train Epoch: 922 [512/54000 (1%)] Loss: -253.876541\n",
      "Train Epoch: 922 [11776/54000 (22%)] Loss: -502.517151\n",
      "Train Epoch: 922 [23040/54000 (43%)] Loss: -506.170166\n",
      "Train Epoch: 922 [34304/54000 (64%)] Loss: -493.643677\n",
      "Train Epoch: 922 [45568/54000 (84%)] Loss: -428.190399\n",
      "    epoch          : 922\n",
      "    loss           : -460.2667823791504\n",
      "    val_loss       : -417.32014738824216\n",
      "    val_log_likelihood: 522.192660522461\n",
      "    val_log_marginal: 421.92432235591116\n",
      "Train Epoch: 923 [512/54000 (1%)] Loss: -250.673904\n",
      "Train Epoch: 923 [11776/54000 (22%)] Loss: -501.445984\n",
      "Train Epoch: 923 [23040/54000 (43%)] Loss: -480.434845\n",
      "Train Epoch: 923 [34304/54000 (64%)] Loss: -438.808044\n",
      "Train Epoch: 923 [45568/54000 (84%)] Loss: -499.909485\n",
      "    epoch          : 923\n",
      "    loss           : -460.2425527954102\n",
      "    val_loss       : -415.18687604218724\n",
      "    val_log_likelihood: 521.1968933105469\n",
      "    val_log_marginal: 420.52204939946535\n",
      "Train Epoch: 924 [512/54000 (1%)] Loss: -518.727722\n",
      "Train Epoch: 924 [11776/54000 (22%)] Loss: -482.921631\n",
      "Train Epoch: 924 [23040/54000 (43%)] Loss: -246.332428\n",
      "Train Epoch: 924 [34304/54000 (64%)] Loss: -298.741455\n",
      "Train Epoch: 924 [45568/54000 (84%)] Loss: -487.217255\n",
      "    epoch          : 924\n",
      "    loss           : -460.51806274414065\n",
      "    val_loss       : -415.9801786173135\n",
      "    val_log_likelihood: 522.0441467285157\n",
      "    val_log_marginal: 421.4301599923521\n",
      "Train Epoch: 925 [512/54000 (1%)] Loss: -534.251587\n",
      "Train Epoch: 925 [11776/54000 (22%)] Loss: -515.529419\n",
      "Train Epoch: 925 [23040/54000 (43%)] Loss: -523.668884\n",
      "Train Epoch: 925 [34304/54000 (64%)] Loss: -504.232574\n",
      "Train Epoch: 925 [45568/54000 (84%)] Loss: -481.980225\n",
      "    epoch          : 925\n",
      "    loss           : -460.5389065551758\n",
      "    val_loss       : -417.820972497575\n",
      "    val_log_likelihood: 523.3162719726563\n",
      "    val_log_marginal: 423.22136931680143\n",
      "Train Epoch: 926 [512/54000 (1%)] Loss: -438.302856\n",
      "Train Epoch: 926 [11776/54000 (22%)] Loss: -427.772583\n",
      "Train Epoch: 926 [23040/54000 (43%)] Loss: -467.725311\n",
      "Train Epoch: 926 [34304/54000 (64%)] Loss: -491.458984\n",
      "Train Epoch: 926 [45568/54000 (84%)] Loss: -447.595276\n",
      "    epoch          : 926\n",
      "    loss           : -459.8754118347168\n",
      "    val_loss       : -416.07720540063457\n",
      "    val_log_likelihood: 519.4994049072266\n",
      "    val_log_marginal: 419.5609896254602\n",
      "Train Epoch: 927 [512/54000 (1%)] Loss: -258.801453\n",
      "Train Epoch: 927 [11776/54000 (22%)] Loss: -515.754028\n",
      "Train Epoch: 927 [23040/54000 (43%)] Loss: -507.135773\n",
      "Train Epoch: 927 [34304/54000 (64%)] Loss: -296.629608\n",
      "Train Epoch: 927 [45568/54000 (84%)] Loss: -446.150696\n",
      "    epoch          : 927\n",
      "    loss           : -460.64685440063477\n",
      "    val_loss       : -416.34352156249804\n",
      "    val_log_likelihood: 519.9575439453125\n",
      "    val_log_marginal: 420.0443696986884\n",
      "Train Epoch: 928 [512/54000 (1%)] Loss: -438.596985\n",
      "Train Epoch: 928 [11776/54000 (22%)] Loss: -263.814209\n",
      "Train Epoch: 928 [23040/54000 (43%)] Loss: -309.767700\n",
      "Train Epoch: 928 [34304/54000 (64%)] Loss: -442.749817\n",
      "Train Epoch: 928 [45568/54000 (84%)] Loss: -481.448120\n",
      "    epoch          : 928\n",
      "    loss           : -460.6161526489258\n",
      "    val_loss       : -419.5955749804154\n",
      "    val_log_likelihood: 522.5576080322265\n",
      "    val_log_marginal: 422.54836434535684\n",
      "Train Epoch: 929 [512/54000 (1%)] Loss: -488.964844\n",
      "Train Epoch: 929 [11776/54000 (22%)] Loss: -314.642944\n",
      "Train Epoch: 929 [23040/54000 (43%)] Loss: -497.294983\n",
      "Train Epoch: 929 [34304/54000 (64%)] Loss: -489.656799\n",
      "Train Epoch: 929 [45568/54000 (84%)] Loss: -277.156311\n",
      "    epoch          : 929\n",
      "    loss           : -460.2191470336914\n",
      "    val_loss       : -419.1126446999609\n",
      "    val_log_likelihood: 523.5622406005859\n",
      "    val_log_marginal: 423.58257091657106\n",
      "Train Epoch: 930 [512/54000 (1%)] Loss: -503.854370\n",
      "Train Epoch: 930 [11776/54000 (22%)] Loss: -456.105957\n",
      "Train Epoch: 930 [23040/54000 (43%)] Loss: -483.366608\n",
      "Train Epoch: 930 [34304/54000 (64%)] Loss: -494.876862\n",
      "Train Epoch: 930 [45568/54000 (84%)] Loss: -316.049316\n",
      "    epoch          : 930\n",
      "    loss           : -460.07145141601563\n",
      "    val_loss       : -416.83390247719365\n",
      "    val_log_likelihood: 523.5834777832031\n",
      "    val_log_marginal: 422.741183289513\n",
      "Train Epoch: 931 [512/54000 (1%)] Loss: -280.137268\n",
      "Train Epoch: 931 [11776/54000 (22%)] Loss: -502.668640\n",
      "Train Epoch: 931 [23040/54000 (43%)] Loss: -512.406433\n",
      "Train Epoch: 931 [34304/54000 (64%)] Loss: -512.020569\n",
      "Train Epoch: 931 [45568/54000 (84%)] Loss: -436.762207\n",
      "    epoch          : 931\n",
      "    loss           : -460.6842559814453\n",
      "    val_loss       : -417.19462623260915\n",
      "    val_log_likelihood: 523.0362335205078\n",
      "    val_log_marginal: 422.36467801444235\n",
      "Train Epoch: 932 [512/54000 (1%)] Loss: -513.847900\n",
      "Train Epoch: 932 [11776/54000 (22%)] Loss: -267.300171\n",
      "Train Epoch: 932 [23040/54000 (43%)] Loss: -505.369568\n",
      "Train Epoch: 932 [34304/54000 (64%)] Loss: -516.407837\n",
      "Train Epoch: 932 [45568/54000 (84%)] Loss: -488.541138\n",
      "    epoch          : 932\n",
      "    loss           : -460.39934600830077\n",
      "    val_loss       : -416.91421216810124\n",
      "    val_log_likelihood: 521.2649566650391\n",
      "    val_log_marginal: 421.2616791555507\n",
      "Train Epoch: 933 [512/54000 (1%)] Loss: -491.731934\n",
      "Train Epoch: 933 [11776/54000 (22%)] Loss: -627.101196\n",
      "Train Epoch: 933 [23040/54000 (43%)] Loss: -440.928284\n",
      "Train Epoch: 933 [34304/54000 (64%)] Loss: -282.795410\n",
      "Train Epoch: 933 [45568/54000 (84%)] Loss: -506.635529\n",
      "    epoch          : 933\n",
      "    loss           : -460.12024993896483\n",
      "    val_loss       : -420.3360843131319\n",
      "    val_log_likelihood: 523.8567962646484\n",
      "    val_log_marginal: 423.86603474505245\n",
      "Train Epoch: 934 [512/54000 (1%)] Loss: -632.597168\n",
      "Train Epoch: 934 [11776/54000 (22%)] Loss: -235.328857\n",
      "Train Epoch: 934 [23040/54000 (43%)] Loss: -494.481049\n",
      "Train Epoch: 934 [34304/54000 (64%)] Loss: -269.687805\n",
      "Train Epoch: 934 [45568/54000 (84%)] Loss: -444.477295\n",
      "    epoch          : 934\n",
      "    loss           : -460.40287063598635\n",
      "    val_loss       : -416.37658434761687\n",
      "    val_log_likelihood: 521.3433135986328\n",
      "    val_log_marginal: 421.3032020330429\n",
      "Train Epoch: 935 [512/54000 (1%)] Loss: -502.676270\n",
      "Train Epoch: 935 [11776/54000 (22%)] Loss: -270.402069\n",
      "Train Epoch: 935 [23040/54000 (43%)] Loss: -518.041504\n",
      "Train Epoch: 935 [34304/54000 (64%)] Loss: -513.222168\n",
      "Train Epoch: 935 [45568/54000 (84%)] Loss: -281.869019\n",
      "    epoch          : 935\n",
      "    loss           : -460.5408573913574\n",
      "    val_loss       : -414.177898828499\n",
      "    val_log_likelihood: 520.4357391357422\n",
      "    val_log_marginal: 420.18454927764833\n",
      "Train Epoch: 936 [512/54000 (1%)] Loss: -498.439178\n",
      "Train Epoch: 936 [11776/54000 (22%)] Loss: -512.233521\n",
      "Train Epoch: 936 [23040/54000 (43%)] Loss: -487.170654\n",
      "Train Epoch: 936 [34304/54000 (64%)] Loss: -513.796387\n",
      "Train Epoch: 936 [45568/54000 (84%)] Loss: -289.552246\n",
      "    epoch          : 936\n",
      "    loss           : -459.9120248413086\n",
      "    val_loss       : -419.24472285583613\n",
      "    val_log_likelihood: 525.0195526123047\n",
      "    val_log_marginal: 424.293984712299\n",
      "Train Epoch: 937 [512/54000 (1%)] Loss: -294.452148\n",
      "Train Epoch: 937 [11776/54000 (22%)] Loss: -268.946960\n",
      "Train Epoch: 937 [23040/54000 (43%)] Loss: -473.044220\n",
      "Train Epoch: 937 [34304/54000 (64%)] Loss: -494.054718\n",
      "Train Epoch: 937 [45568/54000 (84%)] Loss: -457.949799\n",
      "    epoch          : 937\n",
      "    loss           : -460.2796047973633\n",
      "    val_loss       : -419.3768255615607\n",
      "    val_log_likelihood: 525.1633972167969\n",
      "    val_log_marginal: 424.776789052412\n",
      "Train Epoch: 938 [512/54000 (1%)] Loss: -465.262085\n",
      "Train Epoch: 938 [11776/54000 (22%)] Loss: -625.705322\n",
      "Train Epoch: 938 [23040/54000 (43%)] Loss: -476.913361\n",
      "Train Epoch: 938 [34304/54000 (64%)] Loss: -444.715027\n",
      "Train Epoch: 938 [45568/54000 (84%)] Loss: -476.078094\n",
      "    epoch          : 938\n",
      "    loss           : -460.479670715332\n",
      "    val_loss       : -418.6408405768685\n",
      "    val_log_likelihood: 522.8574920654297\n",
      "    val_log_marginal: 422.0651531849056\n",
      "Train Epoch: 939 [512/54000 (1%)] Loss: -623.503174\n",
      "Train Epoch: 939 [11776/54000 (22%)] Loss: -442.567169\n",
      "Train Epoch: 939 [23040/54000 (43%)] Loss: -619.936951\n",
      "Train Epoch: 939 [34304/54000 (64%)] Loss: -514.279480\n",
      "Train Epoch: 939 [45568/54000 (84%)] Loss: -292.280884\n",
      "    epoch          : 939\n",
      "    loss           : -460.43863891601563\n",
      "    val_loss       : -416.3103917114437\n",
      "    val_log_likelihood: 520.8231964111328\n",
      "    val_log_marginal: 420.9891696270558\n",
      "Train Epoch: 940 [512/54000 (1%)] Loss: -499.685516\n",
      "Train Epoch: 940 [11776/54000 (22%)] Loss: -440.709290\n",
      "Train Epoch: 940 [23040/54000 (43%)] Loss: -498.075409\n",
      "Train Epoch: 940 [34304/54000 (64%)] Loss: -472.296082\n",
      "Train Epoch: 940 [45568/54000 (84%)] Loss: -472.645660\n",
      "    epoch          : 940\n",
      "    loss           : -460.19678024291994\n",
      "    val_loss       : -415.73965846933424\n",
      "    val_log_likelihood: 521.6191436767579\n",
      "    val_log_marginal: 420.72435556389394\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch940.pth ...\n",
      "Train Epoch: 941 [512/54000 (1%)] Loss: -502.832336\n",
      "Train Epoch: 941 [11776/54000 (22%)] Loss: -262.492096\n",
      "Train Epoch: 941 [23040/54000 (43%)] Loss: -302.390137\n",
      "Train Epoch: 941 [34304/54000 (64%)] Loss: -482.846405\n",
      "Train Epoch: 941 [45568/54000 (84%)] Loss: -493.969757\n",
      "    epoch          : 941\n",
      "    loss           : -460.1890481567383\n",
      "    val_loss       : -416.24772311868145\n",
      "    val_log_likelihood: 521.4892333984375\n",
      "    val_log_marginal: 420.3939485687778\n",
      "Train Epoch: 942 [512/54000 (1%)] Loss: -496.838257\n",
      "Train Epoch: 942 [11776/54000 (22%)] Loss: -285.782135\n",
      "Train Epoch: 942 [23040/54000 (43%)] Loss: -257.674103\n",
      "Train Epoch: 942 [34304/54000 (64%)] Loss: -503.826538\n",
      "Train Epoch: 942 [45568/54000 (84%)] Loss: -486.865662\n",
      "    epoch          : 942\n",
      "    loss           : -460.82359146118165\n",
      "    val_loss       : -419.0113526536152\n",
      "    val_log_likelihood: 522.5521026611328\n",
      "    val_log_marginal: 422.4250229869038\n",
      "Train Epoch: 943 [512/54000 (1%)] Loss: -262.858948\n",
      "Train Epoch: 943 [11776/54000 (22%)] Loss: -477.043121\n",
      "Train Epoch: 943 [23040/54000 (43%)] Loss: -258.228210\n",
      "Train Epoch: 943 [34304/54000 (64%)] Loss: -508.240662\n",
      "Train Epoch: 943 [45568/54000 (84%)] Loss: -430.087891\n",
      "    epoch          : 943\n",
      "    loss           : -460.0081651306152\n",
      "    val_loss       : -418.67213051989677\n",
      "    val_log_likelihood: 523.6415222167968\n",
      "    val_log_marginal: 422.8356678482145\n",
      "Train Epoch: 944 [512/54000 (1%)] Loss: -291.930817\n",
      "Train Epoch: 944 [11776/54000 (22%)] Loss: -520.156738\n",
      "Train Epoch: 944 [23040/54000 (43%)] Loss: -495.035583\n",
      "Train Epoch: 944 [34304/54000 (64%)] Loss: -623.239380\n",
      "Train Epoch: 944 [45568/54000 (84%)] Loss: -442.805420\n",
      "    epoch          : 944\n",
      "    loss           : -459.7470222473145\n",
      "    val_loss       : -420.60433567613364\n",
      "    val_log_likelihood: 522.8018676757813\n",
      "    val_log_marginal: 422.66288591958585\n",
      "Train Epoch: 945 [512/54000 (1%)] Loss: -475.281342\n",
      "Train Epoch: 945 [11776/54000 (22%)] Loss: -294.008545\n",
      "Train Epoch: 945 [23040/54000 (43%)] Loss: -616.106201\n",
      "Train Epoch: 945 [34304/54000 (64%)] Loss: -516.706787\n",
      "Train Epoch: 945 [45568/54000 (84%)] Loss: -303.019806\n",
      "    epoch          : 945\n",
      "    loss           : -460.22494720458985\n",
      "    val_loss       : -415.3331597097218\n",
      "    val_log_likelihood: 520.6237152099609\n",
      "    val_log_marginal: 419.9721236655917\n",
      "Train Epoch: 946 [512/54000 (1%)] Loss: -494.558228\n",
      "Train Epoch: 946 [11776/54000 (22%)] Loss: -501.349091\n",
      "Train Epoch: 946 [23040/54000 (43%)] Loss: -508.779053\n",
      "Train Epoch: 946 [34304/54000 (64%)] Loss: -520.912170\n",
      "Train Epoch: 946 [45568/54000 (84%)] Loss: -441.016266\n",
      "    epoch          : 946\n",
      "    loss           : -460.587278137207\n",
      "    val_loss       : -419.2955919947475\n",
      "    val_log_likelihood: 523.8548034667969\n",
      "    val_log_marginal: 423.10347234217863\n",
      "Train Epoch: 947 [512/54000 (1%)] Loss: -624.654480\n",
      "Train Epoch: 947 [11776/54000 (22%)] Loss: -458.074707\n",
      "Train Epoch: 947 [23040/54000 (43%)] Loss: -462.359375\n",
      "Train Epoch: 947 [34304/54000 (64%)] Loss: -507.396240\n",
      "Train Epoch: 947 [45568/54000 (84%)] Loss: -509.424164\n",
      "    epoch          : 947\n",
      "    loss           : -461.39530853271486\n",
      "    val_loss       : -417.5827954720706\n",
      "    val_log_likelihood: 521.0158874511719\n",
      "    val_log_marginal: 421.137847668305\n",
      "Train Epoch: 948 [512/54000 (1%)] Loss: -485.530426\n",
      "Train Epoch: 948 [11776/54000 (22%)] Loss: -471.025635\n",
      "Train Epoch: 948 [23040/54000 (43%)] Loss: -431.252533\n",
      "Train Epoch: 948 [34304/54000 (64%)] Loss: -507.434479\n",
      "Train Epoch: 948 [45568/54000 (84%)] Loss: -492.793213\n",
      "    epoch          : 948\n",
      "    loss           : -460.86793640136716\n",
      "    val_loss       : -415.5166077298112\n",
      "    val_log_likelihood: 521.1626007080079\n",
      "    val_log_marginal: 420.5713833291087\n",
      "Train Epoch: 949 [512/54000 (1%)] Loss: -495.794861\n",
      "Train Epoch: 949 [11776/54000 (22%)] Loss: -630.383667\n",
      "Train Epoch: 949 [23040/54000 (43%)] Loss: -497.168518\n",
      "Train Epoch: 949 [34304/54000 (64%)] Loss: -496.012634\n",
      "Train Epoch: 949 [45568/54000 (84%)] Loss: -277.427795\n",
      "    epoch          : 949\n",
      "    loss           : -460.4295442199707\n",
      "    val_loss       : -418.21132902363314\n",
      "    val_log_likelihood: 518.2822448730469\n",
      "    val_log_marginal: 419.5538076873869\n",
      "Train Epoch: 950 [512/54000 (1%)] Loss: -456.340881\n",
      "Train Epoch: 950 [11776/54000 (22%)] Loss: -624.528076\n",
      "Train Epoch: 950 [23040/54000 (43%)] Loss: -266.915924\n",
      "Train Epoch: 950 [34304/54000 (64%)] Loss: -446.031921\n",
      "Train Epoch: 950 [45568/54000 (84%)] Loss: -293.636261\n",
      "    epoch          : 950\n",
      "    loss           : -460.3537672424316\n",
      "    val_loss       : -418.64337604586035\n",
      "    val_log_likelihood: 522.883187866211\n",
      "    val_log_marginal: 422.5232024181634\n",
      "Train Epoch: 951 [512/54000 (1%)] Loss: -494.505157\n",
      "Train Epoch: 951 [11776/54000 (22%)] Loss: -451.445251\n",
      "Train Epoch: 951 [23040/54000 (43%)] Loss: -271.449615\n",
      "Train Epoch: 951 [34304/54000 (64%)] Loss: -510.583557\n",
      "Train Epoch: 951 [45568/54000 (84%)] Loss: -300.080414\n",
      "    epoch          : 951\n",
      "    loss           : -460.5152256774902\n",
      "    val_loss       : -416.82796084694564\n",
      "    val_log_likelihood: 522.4245513916015\n",
      "    val_log_marginal: 421.4961993869235\n",
      "Train Epoch: 952 [512/54000 (1%)] Loss: -286.482971\n",
      "Train Epoch: 952 [11776/54000 (22%)] Loss: -520.319519\n",
      "Train Epoch: 952 [23040/54000 (43%)] Loss: -266.197296\n",
      "Train Epoch: 952 [34304/54000 (64%)] Loss: -617.720337\n",
      "Train Epoch: 952 [45568/54000 (84%)] Loss: -286.436035\n",
      "    epoch          : 952\n",
      "    loss           : -460.32704681396484\n",
      "    val_loss       : -417.2472498157993\n",
      "    val_log_likelihood: 522.8638702392578\n",
      "    val_log_marginal: 422.0808869164437\n",
      "Train Epoch: 953 [512/54000 (1%)] Loss: -479.208313\n",
      "Train Epoch: 953 [11776/54000 (22%)] Loss: -472.507843\n",
      "Train Epoch: 953 [23040/54000 (43%)] Loss: -315.440247\n",
      "Train Epoch: 953 [34304/54000 (64%)] Loss: -491.644714\n",
      "Train Epoch: 953 [45568/54000 (84%)] Loss: -296.614655\n",
      "    epoch          : 953\n",
      "    loss           : -460.52290405273436\n",
      "    val_loss       : -416.47018375154585\n",
      "    val_log_likelihood: 522.2809173583985\n",
      "    val_log_marginal: 421.1848998274654\n",
      "Train Epoch: 954 [512/54000 (1%)] Loss: -525.406494\n",
      "Train Epoch: 954 [11776/54000 (22%)] Loss: -243.544266\n",
      "Train Epoch: 954 [23040/54000 (43%)] Loss: -530.213074\n",
      "Train Epoch: 954 [34304/54000 (64%)] Loss: -627.175171\n",
      "Train Epoch: 954 [45568/54000 (84%)] Loss: -317.587463\n",
      "    epoch          : 954\n",
      "    loss           : -460.36958541870115\n",
      "    val_loss       : -415.6205886445008\n",
      "    val_log_likelihood: 521.1510589599609\n",
      "    val_log_marginal: 420.3845237646252\n",
      "Train Epoch: 955 [512/54000 (1%)] Loss: -526.078369\n",
      "Train Epoch: 955 [11776/54000 (22%)] Loss: -260.155609\n",
      "Train Epoch: 955 [23040/54000 (43%)] Loss: -294.612396\n",
      "Train Epoch: 955 [34304/54000 (64%)] Loss: -298.110046\n",
      "Train Epoch: 955 [45568/54000 (84%)] Loss: -438.370178\n",
      "    epoch          : 955\n",
      "    loss           : -460.9351501464844\n",
      "    val_loss       : -417.2743723917753\n",
      "    val_log_likelihood: 520.8174774169922\n",
      "    val_log_marginal: 421.02264348007736\n",
      "Train Epoch: 956 [512/54000 (1%)] Loss: -484.433350\n",
      "Train Epoch: 956 [11776/54000 (22%)] Loss: -287.536041\n",
      "Train Epoch: 956 [23040/54000 (43%)] Loss: -470.910645\n",
      "Train Epoch: 956 [34304/54000 (64%)] Loss: -496.562805\n",
      "Train Epoch: 956 [45568/54000 (84%)] Loss: -497.969025\n",
      "    epoch          : 956\n",
      "    loss           : -459.71331436157226\n",
      "    val_loss       : -417.151958247181\n",
      "    val_log_likelihood: 522.9437255859375\n",
      "    val_log_marginal: 422.08594738430565\n",
      "Train Epoch: 957 [512/54000 (1%)] Loss: -264.652313\n",
      "Train Epoch: 957 [11776/54000 (22%)] Loss: -459.670593\n",
      "Train Epoch: 957 [23040/54000 (43%)] Loss: -524.539795\n",
      "Train Epoch: 957 [34304/54000 (64%)] Loss: -439.561218\n",
      "Train Epoch: 957 [45568/54000 (84%)] Loss: -276.183228\n",
      "    epoch          : 957\n",
      "    loss           : -460.6864965820312\n",
      "    val_loss       : -416.44131659539414\n",
      "    val_log_likelihood: 520.2035247802735\n",
      "    val_log_marginal: 420.1066601779312\n",
      "Train Epoch: 958 [512/54000 (1%)] Loss: -432.852081\n",
      "Train Epoch: 958 [11776/54000 (22%)] Loss: -525.792969\n",
      "Train Epoch: 958 [23040/54000 (43%)] Loss: -512.739136\n",
      "Train Epoch: 958 [34304/54000 (64%)] Loss: -258.535828\n",
      "Train Epoch: 958 [45568/54000 (84%)] Loss: -433.934998\n",
      "    epoch          : 958\n",
      "    loss           : -460.50296600341795\n",
      "    val_loss       : -418.5662292652763\n",
      "    val_log_likelihood: 523.0306915283203\n",
      "    val_log_marginal: 422.12580362688533\n",
      "Train Epoch: 959 [512/54000 (1%)] Loss: -503.910065\n",
      "Train Epoch: 959 [11776/54000 (22%)] Loss: -264.970581\n",
      "Train Epoch: 959 [23040/54000 (43%)] Loss: -620.004395\n",
      "Train Epoch: 959 [34304/54000 (64%)] Loss: -493.969147\n",
      "Train Epoch: 959 [45568/54000 (84%)] Loss: -452.965851\n",
      "    epoch          : 959\n",
      "    loss           : -460.2484719848633\n",
      "    val_loss       : -418.37130263512955\n",
      "    val_log_likelihood: 523.8529479980468\n",
      "    val_log_marginal: 422.92804451622067\n",
      "Train Epoch: 960 [512/54000 (1%)] Loss: -260.608246\n",
      "Train Epoch: 960 [11776/54000 (22%)] Loss: -623.385803\n",
      "Train Epoch: 960 [23040/54000 (43%)] Loss: -495.334045\n",
      "Train Epoch: 960 [34304/54000 (64%)] Loss: -525.754883\n",
      "Train Epoch: 960 [45568/54000 (84%)] Loss: -446.097412\n",
      "    epoch          : 960\n",
      "    loss           : -460.45425201416015\n",
      "    val_loss       : -417.1781864605844\n",
      "    val_log_likelihood: 522.16826171875\n",
      "    val_log_marginal: 421.1268391489983\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch960.pth ...\n",
      "Train Epoch: 961 [512/54000 (1%)] Loss: -244.125259\n",
      "Train Epoch: 961 [11776/54000 (22%)] Loss: -495.930450\n",
      "Train Epoch: 961 [23040/54000 (43%)] Loss: -521.602478\n",
      "Train Epoch: 961 [34304/54000 (64%)] Loss: -498.098389\n",
      "Train Epoch: 961 [45568/54000 (84%)] Loss: -285.401337\n",
      "    epoch          : 961\n",
      "    loss           : -460.6060830688477\n",
      "    val_loss       : -414.93668955489994\n",
      "    val_log_likelihood: 519.423617553711\n",
      "    val_log_marginal: 419.3851290256749\n",
      "Train Epoch: 962 [512/54000 (1%)] Loss: -472.581543\n",
      "Train Epoch: 962 [11776/54000 (22%)] Loss: -525.566772\n",
      "Train Epoch: 962 [23040/54000 (43%)] Loss: -496.505981\n",
      "Train Epoch: 962 [34304/54000 (64%)] Loss: -490.126495\n",
      "Train Epoch: 962 [45568/54000 (84%)] Loss: -285.525360\n",
      "    epoch          : 962\n",
      "    loss           : -460.6594476318359\n",
      "    val_loss       : -417.09779718080534\n",
      "    val_log_likelihood: 522.8600067138672\n",
      "    val_log_marginal: 421.61186921535847\n",
      "Train Epoch: 963 [512/54000 (1%)] Loss: -495.029968\n",
      "Train Epoch: 963 [11776/54000 (22%)] Loss: -480.615417\n",
      "Train Epoch: 963 [23040/54000 (43%)] Loss: -432.136688\n",
      "Train Epoch: 963 [34304/54000 (64%)] Loss: -464.625549\n",
      "Train Epoch: 963 [45568/54000 (84%)] Loss: -295.176575\n",
      "    epoch          : 963\n",
      "    loss           : -460.6198805236816\n",
      "    val_loss       : -417.6151579272002\n",
      "    val_log_likelihood: 520.7539428710937\n",
      "    val_log_marginal: 420.8084629688412\n",
      "Train Epoch: 964 [512/54000 (1%)] Loss: -474.124725\n",
      "Train Epoch: 964 [11776/54000 (22%)] Loss: -498.459503\n",
      "Train Epoch: 964 [23040/54000 (43%)] Loss: -437.553680\n",
      "Train Epoch: 964 [34304/54000 (64%)] Loss: -302.244110\n",
      "Train Epoch: 964 [45568/54000 (84%)] Loss: -507.712280\n",
      "    epoch          : 964\n",
      "    loss           : -460.11674179077147\n",
      "    val_loss       : -418.42894811937583\n",
      "    val_log_likelihood: 523.7339813232422\n",
      "    val_log_marginal: 422.76436035484073\n",
      "Train Epoch: 965 [512/54000 (1%)] Loss: -504.486938\n",
      "Train Epoch: 965 [11776/54000 (22%)] Loss: -498.013031\n",
      "Train Epoch: 965 [23040/54000 (43%)] Loss: -515.126831\n",
      "Train Epoch: 965 [34304/54000 (64%)] Loss: -439.114319\n",
      "Train Epoch: 965 [45568/54000 (84%)] Loss: -288.495422\n",
      "    epoch          : 965\n",
      "    loss           : -460.6577575683594\n",
      "    val_loss       : -417.25783148668705\n",
      "    val_log_likelihood: 523.4106628417969\n",
      "    val_log_marginal: 422.8764709878713\n",
      "Train Epoch: 966 [512/54000 (1%)] Loss: -488.505524\n",
      "Train Epoch: 966 [11776/54000 (22%)] Loss: -501.628052\n",
      "Train Epoch: 966 [23040/54000 (43%)] Loss: -477.493225\n",
      "Train Epoch: 966 [34304/54000 (64%)] Loss: -534.182190\n",
      "Train Epoch: 966 [45568/54000 (84%)] Loss: -444.650055\n",
      "    epoch          : 966\n",
      "    loss           : -460.3843537902832\n",
      "    val_loss       : -418.5275936614722\n",
      "    val_log_likelihood: 519.9301116943359\n",
      "    val_log_marginal: 420.4864139220282\n",
      "Train Epoch: 967 [512/54000 (1%)] Loss: -275.230530\n",
      "Train Epoch: 967 [11776/54000 (22%)] Loss: -250.517960\n",
      "Train Epoch: 967 [23040/54000 (43%)] Loss: -629.676697\n",
      "Train Epoch: 967 [34304/54000 (64%)] Loss: -443.546570\n",
      "Train Epoch: 967 [45568/54000 (84%)] Loss: -513.417725\n",
      "    epoch          : 967\n",
      "    loss           : -460.83113723754883\n",
      "    val_loss       : -417.8591028892435\n",
      "    val_log_likelihood: 523.8097747802734\n",
      "    val_log_marginal: 423.20567459352316\n",
      "Train Epoch: 968 [512/54000 (1%)] Loss: -472.311432\n",
      "Train Epoch: 968 [11776/54000 (22%)] Loss: -494.517883\n",
      "Train Epoch: 968 [23040/54000 (43%)] Loss: -311.875519\n",
      "Train Epoch: 968 [34304/54000 (64%)] Loss: -505.556366\n",
      "Train Epoch: 968 [45568/54000 (84%)] Loss: -491.195343\n",
      "    epoch          : 968\n",
      "    loss           : -460.51581558227537\n",
      "    val_loss       : -414.91714642550795\n",
      "    val_log_likelihood: 519.2705322265625\n",
      "    val_log_marginal: 418.8070630621165\n",
      "Train Epoch: 969 [512/54000 (1%)] Loss: -461.124146\n",
      "Train Epoch: 969 [11776/54000 (22%)] Loss: -509.660980\n",
      "Train Epoch: 969 [23040/54000 (43%)] Loss: -500.314484\n",
      "Train Epoch: 969 [34304/54000 (64%)] Loss: -629.419434\n",
      "Train Epoch: 969 [45568/54000 (84%)] Loss: -443.507233\n",
      "    epoch          : 969\n",
      "    loss           : -460.43538528442383\n",
      "    val_loss       : -418.0229904685169\n",
      "    val_log_likelihood: 521.748779296875\n",
      "    val_log_marginal: 420.80489827878773\n",
      "Train Epoch: 970 [512/54000 (1%)] Loss: -435.472168\n",
      "Train Epoch: 970 [11776/54000 (22%)] Loss: -495.597809\n",
      "Train Epoch: 970 [23040/54000 (43%)] Loss: -307.489899\n",
      "Train Epoch: 970 [34304/54000 (64%)] Loss: -623.179565\n",
      "Train Epoch: 970 [45568/54000 (84%)] Loss: -291.920471\n",
      "    epoch          : 970\n",
      "    loss           : -461.04150329589845\n",
      "    val_loss       : -418.09661499680954\n",
      "    val_log_likelihood: 524.5039489746093\n",
      "    val_log_marginal: 423.80918057523667\n",
      "Train Epoch: 971 [512/54000 (1%)] Loss: -492.125549\n",
      "Train Epoch: 971 [11776/54000 (22%)] Loss: -495.811188\n",
      "Train Epoch: 971 [23040/54000 (43%)] Loss: -497.788422\n",
      "Train Epoch: 971 [34304/54000 (64%)] Loss: -495.359314\n",
      "Train Epoch: 971 [45568/54000 (84%)] Loss: -436.082642\n",
      "    epoch          : 971\n",
      "    loss           : -459.9017922973633\n",
      "    val_loss       : -421.04948736652733\n",
      "    val_log_likelihood: 525.3058685302734\n",
      "    val_log_marginal: 424.51280881352585\n",
      "Train Epoch: 972 [512/54000 (1%)] Loss: -494.841003\n",
      "Train Epoch: 972 [11776/54000 (22%)] Loss: -631.599487\n",
      "Train Epoch: 972 [23040/54000 (43%)] Loss: -285.424438\n",
      "Train Epoch: 972 [34304/54000 (64%)] Loss: -320.660858\n",
      "Train Epoch: 972 [45568/54000 (84%)] Loss: -474.429718\n",
      "    epoch          : 972\n",
      "    loss           : -460.51916351318357\n",
      "    val_loss       : -418.1525521607138\n",
      "    val_log_likelihood: 523.94189453125\n",
      "    val_log_marginal: 423.083811051608\n",
      "Train Epoch: 973 [512/54000 (1%)] Loss: -481.885803\n",
      "Train Epoch: 973 [11776/54000 (22%)] Loss: -440.209778\n",
      "Train Epoch: 973 [23040/54000 (43%)] Loss: -257.413757\n",
      "Train Epoch: 973 [34304/54000 (64%)] Loss: -501.454834\n",
      "Train Epoch: 973 [45568/54000 (84%)] Loss: -433.759705\n",
      "    epoch          : 973\n",
      "    loss           : -460.7363920593262\n",
      "    val_loss       : -417.61941462326797\n",
      "    val_log_likelihood: 522.7564086914062\n",
      "    val_log_marginal: 421.9442941557616\n",
      "Train Epoch: 974 [512/54000 (1%)] Loss: -492.271606\n",
      "Train Epoch: 974 [11776/54000 (22%)] Loss: -631.600952\n",
      "Train Epoch: 974 [23040/54000 (43%)] Loss: -249.787689\n",
      "Train Epoch: 974 [34304/54000 (64%)] Loss: -504.951050\n",
      "Train Epoch: 974 [45568/54000 (84%)] Loss: -486.981323\n",
      "    epoch          : 974\n",
      "    loss           : -460.91983489990236\n",
      "    val_loss       : -415.89632757855577\n",
      "    val_log_likelihood: 520.676235961914\n",
      "    val_log_marginal: 421.0436423059553\n",
      "Train Epoch: 975 [512/54000 (1%)] Loss: -493.850861\n",
      "Train Epoch: 975 [11776/54000 (22%)] Loss: -490.837463\n",
      "Train Epoch: 975 [23040/54000 (43%)] Loss: -629.275696\n",
      "Train Epoch: 975 [34304/54000 (64%)] Loss: -489.833099\n",
      "Train Epoch: 975 [45568/54000 (84%)] Loss: -282.825836\n",
      "    epoch          : 975\n",
      "    loss           : -460.4662640380859\n",
      "    val_loss       : -417.279920342844\n",
      "    val_log_likelihood: 521.3080627441407\n",
      "    val_log_marginal: 421.3542735096071\n",
      "Train Epoch: 976 [512/54000 (1%)] Loss: -274.856201\n",
      "Train Epoch: 976 [11776/54000 (22%)] Loss: -626.536987\n",
      "Train Epoch: 976 [23040/54000 (43%)] Loss: -627.468445\n",
      "Train Epoch: 976 [34304/54000 (64%)] Loss: -521.016357\n",
      "Train Epoch: 976 [45568/54000 (84%)] Loss: -508.232147\n",
      "    epoch          : 976\n",
      "    loss           : -461.05941131591794\n",
      "    val_loss       : -418.3161624535918\n",
      "    val_log_likelihood: 522.6466522216797\n",
      "    val_log_marginal: 422.198015124946\n",
      "Train Epoch: 977 [512/54000 (1%)] Loss: -499.183472\n",
      "Train Epoch: 977 [11776/54000 (22%)] Loss: -270.933716\n",
      "Train Epoch: 977 [23040/54000 (43%)] Loss: -484.446167\n",
      "Train Epoch: 977 [34304/54000 (64%)] Loss: -308.783295\n",
      "Train Epoch: 977 [45568/54000 (84%)] Loss: -490.512604\n",
      "    epoch          : 977\n",
      "    loss           : -461.0325059509277\n",
      "    val_loss       : -419.79396196668966\n",
      "    val_log_likelihood: 523.0153717041015\n",
      "    val_log_marginal: 422.40876974277273\n",
      "Train Epoch: 978 [512/54000 (1%)] Loss: -626.415222\n",
      "Train Epoch: 978 [11776/54000 (22%)] Loss: -301.322540\n",
      "Train Epoch: 978 [23040/54000 (43%)] Loss: -270.515717\n",
      "Train Epoch: 978 [34304/54000 (64%)] Loss: -507.153931\n",
      "Train Epoch: 978 [45568/54000 (84%)] Loss: -513.024414\n",
      "    epoch          : 978\n",
      "    loss           : -460.88374877929687\n",
      "    val_loss       : -416.420880481042\n",
      "    val_log_likelihood: 519.8807342529296\n",
      "    val_log_marginal: 420.15957481004295\n",
      "Train Epoch: 979 [512/54000 (1%)] Loss: -497.913788\n",
      "Train Epoch: 979 [11776/54000 (22%)] Loss: -496.570099\n",
      "Train Epoch: 979 [23040/54000 (43%)] Loss: -533.720764\n",
      "Train Epoch: 979 [34304/54000 (64%)] Loss: -470.986023\n",
      "Train Epoch: 979 [45568/54000 (84%)] Loss: -499.431824\n",
      "    epoch          : 979\n",
      "    loss           : -460.54818939208985\n",
      "    val_loss       : -419.2399003742263\n",
      "    val_log_likelihood: 523.8900360107422\n",
      "    val_log_marginal: 423.07964263893666\n",
      "Train Epoch: 980 [512/54000 (1%)] Loss: -488.144073\n",
      "Train Epoch: 980 [11776/54000 (22%)] Loss: -447.667297\n",
      "Train Epoch: 980 [23040/54000 (43%)] Loss: -270.201660\n",
      "Train Epoch: 980 [34304/54000 (64%)] Loss: -458.003479\n",
      "Train Epoch: 980 [45568/54000 (84%)] Loss: -262.825623\n",
      "    epoch          : 980\n",
      "    loss           : -460.73444625854495\n",
      "    val_loss       : -417.112443273142\n",
      "    val_log_likelihood: 523.0712982177735\n",
      "    val_log_marginal: 421.89343632869424\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch980.pth ...\n",
      "Train Epoch: 981 [512/54000 (1%)] Loss: -510.070404\n",
      "Train Epoch: 981 [11776/54000 (22%)] Loss: -504.542664\n",
      "Train Epoch: 981 [23040/54000 (43%)] Loss: -285.715088\n",
      "Train Epoch: 981 [34304/54000 (64%)] Loss: -440.429596\n",
      "Train Epoch: 981 [45568/54000 (84%)] Loss: -298.089661\n",
      "    epoch          : 981\n",
      "    loss           : -460.70361892700197\n",
      "    val_loss       : -417.35669446159153\n",
      "    val_log_likelihood: 521.3922668457031\n",
      "    val_log_marginal: 420.82247726657187\n",
      "Train Epoch: 982 [512/54000 (1%)] Loss: -416.466858\n",
      "Train Epoch: 982 [11776/54000 (22%)] Loss: -295.040894\n",
      "Train Epoch: 982 [23040/54000 (43%)] Loss: -308.405029\n",
      "Train Epoch: 982 [34304/54000 (64%)] Loss: -291.604614\n",
      "Train Epoch: 982 [45568/54000 (84%)] Loss: -259.392426\n",
      "    epoch          : 982\n",
      "    loss           : -460.59912567138673\n",
      "    val_loss       : -415.70851325690745\n",
      "    val_log_likelihood: 521.969107055664\n",
      "    val_log_marginal: 421.0152343764901\n",
      "Train Epoch: 983 [512/54000 (1%)] Loss: -529.914978\n",
      "Train Epoch: 983 [11776/54000 (22%)] Loss: -525.396545\n",
      "Train Epoch: 983 [23040/54000 (43%)] Loss: -494.138000\n",
      "Train Epoch: 983 [34304/54000 (64%)] Loss: -305.446350\n",
      "Train Epoch: 983 [45568/54000 (84%)] Loss: -628.246826\n",
      "    epoch          : 983\n",
      "    loss           : -460.9811682128906\n",
      "    val_loss       : -417.8099403013475\n",
      "    val_log_likelihood: 522.144808959961\n",
      "    val_log_marginal: 422.2587620537728\n",
      "Train Epoch: 984 [512/54000 (1%)] Loss: -470.449219\n",
      "Train Epoch: 984 [11776/54000 (22%)] Loss: -468.087555\n",
      "Train Epoch: 984 [23040/54000 (43%)] Loss: -487.030212\n",
      "Train Epoch: 984 [34304/54000 (64%)] Loss: -439.703491\n",
      "Train Epoch: 984 [45568/54000 (84%)] Loss: -487.330170\n",
      "    epoch          : 984\n",
      "    loss           : -460.61902618408203\n",
      "    val_loss       : -416.5816722759977\n",
      "    val_log_likelihood: 521.7368347167969\n",
      "    val_log_marginal: 420.5682804983109\n",
      "Train Epoch: 985 [512/54000 (1%)] Loss: -292.408508\n",
      "Train Epoch: 985 [11776/54000 (22%)] Loss: -520.038574\n",
      "Train Epoch: 985 [23040/54000 (43%)] Loss: -272.883240\n",
      "Train Epoch: 985 [34304/54000 (64%)] Loss: -523.474792\n",
      "Train Epoch: 985 [45568/54000 (84%)] Loss: -288.208374\n",
      "    epoch          : 985\n",
      "    loss           : -461.03696533203123\n",
      "    val_loss       : -417.70833336692306\n",
      "    val_log_likelihood: 523.00146484375\n",
      "    val_log_marginal: 422.15614601559935\n",
      "Train Epoch: 986 [512/54000 (1%)] Loss: -627.617371\n",
      "Train Epoch: 986 [11776/54000 (22%)] Loss: -505.227905\n",
      "Train Epoch: 986 [23040/54000 (43%)] Loss: -502.538239\n",
      "Train Epoch: 986 [34304/54000 (64%)] Loss: -300.958099\n",
      "Train Epoch: 986 [45568/54000 (84%)] Loss: -506.820129\n",
      "    epoch          : 986\n",
      "    loss           : -460.9485250854492\n",
      "    val_loss       : -418.44425129964947\n",
      "    val_log_likelihood: 524.0880340576172\n",
      "    val_log_marginal: 423.1124865297228\n",
      "Train Epoch: 987 [512/54000 (1%)] Loss: -519.781860\n",
      "Train Epoch: 987 [11776/54000 (22%)] Loss: -302.186371\n",
      "Train Epoch: 987 [23040/54000 (43%)] Loss: -473.776917\n",
      "Train Epoch: 987 [34304/54000 (64%)] Loss: -457.064362\n",
      "Train Epoch: 987 [45568/54000 (84%)] Loss: -514.292603\n",
      "    epoch          : 987\n",
      "    loss           : -460.5897332763672\n",
      "    val_loss       : -420.5377034455538\n",
      "    val_log_likelihood: 522.8475341796875\n",
      "    val_log_marginal: 424.28383754603567\n",
      "Train Epoch: 988 [512/54000 (1%)] Loss: -634.284302\n",
      "Train Epoch: 988 [11776/54000 (22%)] Loss: -465.817627\n",
      "Train Epoch: 988 [23040/54000 (43%)] Loss: -487.049835\n",
      "Train Epoch: 988 [34304/54000 (64%)] Loss: -440.649536\n",
      "Train Epoch: 988 [45568/54000 (84%)] Loss: -444.943909\n",
      "    epoch          : 988\n",
      "    loss           : -460.64958526611326\n",
      "    val_loss       : -418.7657137585804\n",
      "    val_log_likelihood: 524.0678070068359\n",
      "    val_log_marginal: 423.37679030397703\n",
      "Train Epoch: 989 [512/54000 (1%)] Loss: -278.237854\n",
      "Train Epoch: 989 [11776/54000 (22%)] Loss: -634.049072\n",
      "Train Epoch: 989 [23040/54000 (43%)] Loss: -625.171936\n",
      "Train Epoch: 989 [34304/54000 (64%)] Loss: -501.414734\n",
      "Train Epoch: 989 [45568/54000 (84%)] Loss: -481.952515\n",
      "    epoch          : 989\n",
      "    loss           : -461.14284744262693\n",
      "    val_loss       : -415.40922697642816\n",
      "    val_log_likelihood: 518.339486694336\n",
      "    val_log_marginal: 418.1265887472779\n",
      "Train Epoch: 990 [512/54000 (1%)] Loss: -628.261536\n",
      "Train Epoch: 990 [11776/54000 (22%)] Loss: -620.459534\n",
      "Train Epoch: 990 [23040/54000 (43%)] Loss: -256.235840\n",
      "Train Epoch: 990 [34304/54000 (64%)] Loss: -498.953278\n",
      "Train Epoch: 990 [45568/54000 (84%)] Loss: -483.941223\n",
      "    epoch          : 990\n",
      "    loss           : -460.1770816040039\n",
      "    val_loss       : -418.73260519467294\n",
      "    val_log_likelihood: 523.0059997558594\n",
      "    val_log_marginal: 422.0432864133269\n",
      "Train Epoch: 991 [512/54000 (1%)] Loss: -271.406952\n",
      "Train Epoch: 991 [11776/54000 (22%)] Loss: -621.763855\n",
      "Train Epoch: 991 [23040/54000 (43%)] Loss: -306.726288\n",
      "Train Epoch: 991 [34304/54000 (64%)] Loss: -437.579041\n",
      "Train Epoch: 991 [45568/54000 (84%)] Loss: -507.456940\n",
      "    epoch          : 991\n",
      "    loss           : -460.7659770202637\n",
      "    val_loss       : -418.4755409624428\n",
      "    val_log_likelihood: 524.2206481933594\n",
      "    val_log_marginal: 423.302694153983\n",
      "Train Epoch: 992 [512/54000 (1%)] Loss: -494.334229\n",
      "Train Epoch: 992 [11776/54000 (22%)] Loss: -498.903687\n",
      "Train Epoch: 992 [23040/54000 (43%)] Loss: -284.239319\n",
      "Train Epoch: 992 [34304/54000 (64%)] Loss: -498.918213\n",
      "Train Epoch: 992 [45568/54000 (84%)] Loss: -454.580688\n",
      "    epoch          : 992\n",
      "    loss           : -459.8641586303711\n",
      "    val_loss       : -416.74440632276236\n",
      "    val_log_likelihood: 520.9853820800781\n",
      "    val_log_marginal: 420.7682887241188\n",
      "Train Epoch: 993 [512/54000 (1%)] Loss: -509.590881\n",
      "Train Epoch: 993 [11776/54000 (22%)] Loss: -475.708191\n",
      "Train Epoch: 993 [23040/54000 (43%)] Loss: -501.253998\n",
      "Train Epoch: 993 [34304/54000 (64%)] Loss: -536.610535\n",
      "Train Epoch: 993 [45568/54000 (84%)] Loss: -442.659851\n",
      "    epoch          : 993\n",
      "    loss           : -460.16019134521486\n",
      "    val_loss       : -415.9634454440326\n",
      "    val_log_likelihood: 523.3398040771484\n",
      "    val_log_marginal: 422.3662090536207\n",
      "Train Epoch: 994 [512/54000 (1%)] Loss: -506.889465\n",
      "Train Epoch: 994 [11776/54000 (22%)] Loss: -459.931580\n",
      "Train Epoch: 994 [23040/54000 (43%)] Loss: -496.191315\n",
      "Train Epoch: 994 [34304/54000 (64%)] Loss: -265.608276\n",
      "Train Epoch: 994 [45568/54000 (84%)] Loss: -482.622314\n",
      "    epoch          : 994\n",
      "    loss           : -461.0464926147461\n",
      "    val_loss       : -416.8605037389323\n",
      "    val_log_likelihood: 522.3810302734375\n",
      "    val_log_marginal: 421.0339365483666\n",
      "Train Epoch: 995 [512/54000 (1%)] Loss: -515.336609\n",
      "Train Epoch: 995 [11776/54000 (22%)] Loss: -464.422180\n",
      "Train Epoch: 995 [23040/54000 (43%)] Loss: -504.698608\n",
      "Train Epoch: 995 [34304/54000 (64%)] Loss: -483.645203\n",
      "Train Epoch: 995 [45568/54000 (84%)] Loss: -532.840454\n",
      "    epoch          : 995\n",
      "    loss           : -460.599854888916\n",
      "    val_loss       : -415.9416632766835\n",
      "    val_log_likelihood: 522.6640563964844\n",
      "    val_log_marginal: 421.44688388928773\n",
      "Train Epoch: 996 [512/54000 (1%)] Loss: -626.491333\n",
      "Train Epoch: 996 [11776/54000 (22%)] Loss: -488.980774\n",
      "Train Epoch: 996 [23040/54000 (43%)] Loss: -492.977905\n",
      "Train Epoch: 996 [34304/54000 (64%)] Loss: -251.823318\n",
      "Train Epoch: 996 [45568/54000 (84%)] Loss: -310.533539\n",
      "    epoch          : 996\n",
      "    loss           : -460.3060330200195\n",
      "    val_loss       : -417.4053849229589\n",
      "    val_log_likelihood: 524.998583984375\n",
      "    val_log_marginal: 424.1289161082357\n",
      "Train Epoch: 997 [512/54000 (1%)] Loss: -620.712158\n",
      "Train Epoch: 997 [11776/54000 (22%)] Loss: -633.771484\n",
      "Train Epoch: 997 [23040/54000 (43%)] Loss: -620.348022\n",
      "Train Epoch: 997 [34304/54000 (64%)] Loss: -477.239990\n",
      "Train Epoch: 997 [45568/54000 (84%)] Loss: -295.053741\n",
      "    epoch          : 997\n",
      "    loss           : -460.8487161254883\n",
      "    val_loss       : -414.868498284556\n",
      "    val_log_likelihood: 522.5859375\n",
      "    val_log_marginal: 421.6209576364607\n",
      "Train Epoch: 998 [512/54000 (1%)] Loss: -523.968506\n",
      "Train Epoch: 998 [11776/54000 (22%)] Loss: -495.250916\n",
      "Train Epoch: 998 [23040/54000 (43%)] Loss: -630.533325\n",
      "Train Epoch: 998 [34304/54000 (64%)] Loss: -438.034515\n",
      "Train Epoch: 998 [45568/54000 (84%)] Loss: -280.544922\n",
      "    epoch          : 998\n",
      "    loss           : -461.03236785888674\n",
      "    val_loss       : -415.85198927316816\n",
      "    val_log_likelihood: 523.7081451416016\n",
      "    val_log_marginal: 422.5899694312364\n",
      "Train Epoch: 999 [512/54000 (1%)] Loss: -485.478516\n",
      "Train Epoch: 999 [11776/54000 (22%)] Loss: -465.748108\n",
      "Train Epoch: 999 [23040/54000 (43%)] Loss: -509.606567\n",
      "Train Epoch: 999 [34304/54000 (64%)] Loss: -506.044525\n",
      "Train Epoch: 999 [45568/54000 (84%)] Loss: -490.357178\n",
      "    epoch          : 999\n",
      "    loss           : -460.74666564941407\n",
      "    val_loss       : -417.99180363249036\n",
      "    val_log_likelihood: 522.8666290283203\n",
      "    val_log_marginal: 421.7688725284767\n",
      "Train Epoch: 1000 [512/54000 (1%)] Loss: -534.516174\n",
      "Train Epoch: 1000 [11776/54000 (22%)] Loss: -615.034851\n",
      "Train Epoch: 1000 [23040/54000 (43%)] Loss: -504.712250\n",
      "Train Epoch: 1000 [34304/54000 (64%)] Loss: -500.654236\n",
      "Train Epoch: 1000 [45568/54000 (84%)] Loss: -437.993866\n",
      "    epoch          : 1000\n",
      "    loss           : -460.32757080078125\n",
      "    val_loss       : -417.5942134555429\n",
      "    val_log_likelihood: 523.9890808105469\n",
      "    val_log_marginal: 423.2424655023962\n",
      "Saving checkpoint: saved/models/FashionMnist_GlimpseOperad/1011_122236/checkpoint-epoch1000.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GlimpseOperadicModel(\n",
       "  (_category): FreeOperad(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=98, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=252, bias=True)\n",
       "        (1): LayerNorm((252,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=252, out_features=252, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(28, 14, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(14, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(14, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=98, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=252, bias=True)\n",
       "        (1): LayerNorm((252,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=252, out_features=252, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(28, 14, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(14, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(14, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=49, out_features=252, bias=True)\n",
       "        (1): LayerNorm((252,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=252, out_features=252, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(28, 14, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(14, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(14, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_6): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=49, bias=True)\n",
       "        (1): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=49, out_features=49, bias=True)\n",
       "        (4): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=49, out_features=49, bias=True)\n",
       "        (7): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=49, out_features=98, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=49, bias=True)\n",
       "        (1): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=98, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (7): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=196, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=392, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_10): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_11): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_12): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=24, out_features=49, bias=True)\n",
       "        (1): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=49, out_features=49, bias=True)\n",
       "        (4): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=49, out_features=49, bias=True)\n",
       "        (7): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=49, out_features=98, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_13): SpatialTransformerWriter(\n",
       "      (glimpse_conv): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Conv2d(56, 112, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (glimpse_selector): Softmax2d()\n",
       "      (glimpse_dense): Linear(in_features=9, out_features=6, bias=True)\n",
       "      (coordinates_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_14): GaussianLikelihood()\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=32, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=32, out_features=48, bias=True)\n",
       "  )\n",
       "  (encoders): ModuleDict(\n",
       "    ($p(Z^{32} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=96, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{49} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=147, out_features=147, bias=True)\n",
       "        (3): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=147, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{49} | \\mathbb{R}^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=147, out_features=147, bias=True)\n",
       "        (3): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=147, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{49})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=98, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{32} \\mid \\mathbb{R}^{16} \\times \\mathbb{R}^{2}):\\mathbb{R}^{16} \\times \\mathbb{R}^{2} -> \\mathbb{R}^{32}$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=96, out_features=36, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{49} \\mid \\mathbb{R}^{32} \\times \\mathbb{R}^{2}):\\mathbb{R}^{32} \\times \\mathbb{R}^{2} -> \\mathbb{R}^{49}$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=147, out_features=147, bias=True)\n",
       "        (3): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=147, out_features=68, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} \\mid \\mathbb{R}^{49} \\times \\mathbb{R}^{2}):\\mathbb{R}^{49} \\times \\mathbb{R}^{2} -> \\mathbb{R}^{196}$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=102, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{784} \\mid \\mathbb{R}^{196} \\times \\mathbb{R}^{2}):\\mathbb{R}^{196} \\times \\mathbb{R}^{2} -> \\mathbb{R}^{784}$): MlpEncoder(\n",
       "      (incoder): DenseIncoder(\n",
       "        (dense): Sequential(\n",
       "          (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "          (1): LayerNorm((1568,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Linear(in_features=1568, out_features=1568, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((1568,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=1568, out_features=1568, bias=True)\n",
       "        (3): LayerNorm((1568,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=1568, out_features=396, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{16}, Z^{32}): Ty() -> \\mathbb{R}^{32}$): RecurrentEncoder(\n",
       "      (recurrent): GRUCell(48, 64)\n",
       "    )\n",
       "    ($p(Z^{8}, Z^{16}): Ty() -> \\mathbb{R}^{16}$): RecurrentEncoder(\n",
       "      (recurrent): GRUCell(24, 32)\n",
       "    )\n",
       "    ($p(Z^{24}, Z^{49}): Ty() -> \\mathbb{R}^{49}$): RecurrentEncoder(\n",
       "      (recurrent): GRUCell(73, 98)\n",
       "    )\n",
       "    ($p(Z^{3} \\mid \\mathbb{R}^{784} \\times \\mathbb{R}^{196})$): MlpEncoder(\n",
       "      (incoder): DenseIncoder(\n",
       "        (dense): Sequential(\n",
       "          (0): Linear(in_features=784, out_features=6, bias=True)\n",
       "          (1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Linear(in_features=6, out_features=6, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((787,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=787, out_features=787, bias=True)\n",
       "        (3): LayerNorm((787,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=787, out_features=1176, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} \\mid \\mathbb{R}^{784})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{2})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{32})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{49})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{16})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEFCAYAAADOqip4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAaUUlEQVR4nO2d2XNcx3nFzzcrdgzABSBEieSQkiVZshwYsmJX2XmhXF6qkodATqWSx4R6SOWVKv0J0n8gVqrihzzJfIgr2wMZV6yKK3EE0rK10ZIIcydIbIPBOmvnARfSaIg+FxxygGH7/KpQmLnf9O2ennvm3rmnv25zzkEIESaJvW6AEKJ9SOBCBIwELkTASOBCBIwELkTApPa6ASFjZpMAhgEsACgAyDvnzrS5zpMA3nLOHd/h68cBTAD4hnPu1Xa2Tew+OoO3CTPLA3jROXfGOXcWmyLPtbte59x5ANP3UeR1AG93qrjN7PJet+FRRgJvH3kA81tPnHMXcX/C2y1yzrnCXjeC8I29bsCjjATePqYAvG5mp6OzOaIzOYDNS+no7w0zyzVsWzSz8ejxW2aWj56/tbWfhtfds49mzOxU9JrTza+JLs+Ho9fkzWzSzC5Hr/9pQ7smo22T0U+AHbe1qT5vu7erO2rfhYby27Vj2zaLCOec/tr0B2AcwDkADpsHaq4h9lb0/ySANxq2nwMwHj1+A8Bpz+s+319Uz08b99Gw/Y3ocW6rzqY2nmt+HpXLN+zjdGO7G+rdUVub9k/b3Vj3Nu+FtqOxnP42/3QGbyPOuYvOuZedcwbgPDZFsBVr/M2bayq6dSk/3/B4YZv9F7bqwaaomvkLAPPRmTAf/cUxHLV7q95XAVxsiF9uqmtHbd1hu5vrboS1g5X7g0YCbxNbl5BbOOdeQ4PAosvTkyDCjSg0x++DHICL0cF/0Tn38g7KUHFGDG89eIht3Wnd27Xjfsv9wSCBt49cZJMBAKLfhtPR41MA5t3mHe+t+Pj9VtDw+zWPzSuEZn4K4OWG1993HdE+Gsu96Klrx+yg3bvSjj8EJPA2E90EmgRwCsBr0ebzAI43neWHty6lG27MvQzglUgQrwI42XTz6mS0j1cB/G1U39Y+TkVfIFs3oO65hG+qLxe9ZiL6AgLwue1W2Lq5hc3f8dMttLWR7dp9T93bvJft2nFPOfEFFt2kEI8YZnbBOffIWUiParsfVXQGFyJgJPBHkOiyNP+oXZY+qu1+lNEluhABozO4EAEjgQsRMG1PF80ku113asAbr3enaXmr+2O1jNGyiQr/+ZEo12i8nkn621UlDQNgNR4vD/L3Xe+KaXvJ/97rPbxuON5viPvVluAv6M5UvLH1UoaWtQpvW3KDhpEs+9sW95m5BK/b6vx917r8xwsAJMr++mtZfq6Nq3u1cHPOOXegeXtLAo98yAJ2kN/cnRrAt8f+yhtf+dohWld6peqNLT+epWV77vgPNADovrFM42tP+L+YumbXadnkwgqN3/jTMRovPuV/3wDQN+3/6NbH12jZeoUfTC4mnujiX4zPP37LG/vN9GFaNn2bfwHkPqFhDFwp+fe9ENMvPbzuxFqZxotP52i878qqv+yJXlo2vcq/nH75z6evbrf9vi/Rt0ZnbY3C2m4AhRCiM2jlN/iL+CKpYBpfHj4I4PMUxSkzmyrX+JlOCNE+WhF4run5vuYXuM1ZTCaccxOZZHdLDRNCPDitCLyAhmwiIUTn0orA38UXZ/E8NpPthRAdyH3fRXfOnY0yiE5ic0YNmrJX68mgOO6/Y9x9h/sehSd7vLGBa/47pgBQT/Pvr+IzORpntkl2nlsqS+MjNL76GLc9nv/qNRo/MO6/S//3I/9Jyz6T4f1yo8r79Xi6j8Z/ueG/4/vXl2NGqcY4eJllfjc5e3XeG6vt97siALCxv4vG6xn+c7Nrjrs2a4f9x/LQu3dpWVhMx3hoySZzzr0ZPVQ+rhAdjEayCREwErgQASOBCxEwErgQASOBCxEwErgQAdP2dNFEpY7uGb/XvUR8bgDomfNnVVV6efO75rjHnizx9L7M9Ky/7sP3jND9EtUu7lsOPu33awHghwfep3HGP8x9l8Z/vO9XND7AcnQBvPB/f0njq2v+LL+v5m/Ssh9WYrLN/AlZAIDyY0P+dj3Gfe7MMs+Sq8akdLphfjxmiv79rzy7n5btvcIzH33oDC5EwEjgQgSMBC5EwEjgQgSMBC5EwEjgQgRM220yq9aQvrPkjWcP8okTez/2W1ULL43yumt83y7JrazaM/7912JSLgtP8X3/aOwzGv/32edpfF/W7xe9NMCXyf7ZIl9k9KX+yzReKvPD5odPfeiN/fzaU7RseoBPbDjzTT454b6P/J9L701um1YG+Ey3fbd4Gm2ixG22jQN+m67axY+nylBrMyPpDC5EwEjgQgSMBC5EwEjgQgSMBC5EwEjgQgSMBC5EwLTdB69nU1g/4U+F673KF+lzSf930PB/XaFlK8f41MWJEl/gj65GWYtZYfMoXxvid8u8bdU6/+79eMFf/oM5vqDjy49dovF/uv0tGn/u0G0a/5f3XvDGvnqCp4tenuVpk+VhnsqaWifxmHEPpRxPH+65sO36fp9TPc77PVP0T6tcT/OFD+NWLvWhM7gQASOBCxEwErgQASOBCxEwErgQASOBCxEwErgQAdN2H9wlgGqP/3uka53n/6Lo98lrhw/QonHT5A587M9TBwDXRbonwX3wpa/w3GAsDfLwEp9OmuHWuWf6TuoEjRfWeO5xucz3/6MX/FM+/8fvnqVlEzH9ajHdmqj4y5dj8r0HPuNzMteO8LEL4E1H8Yj/eMxd4uNBUnOaNlkI0URLAjezRTM7Z2anH3aDhBAPj1Yv0V9xzp1/qC0RQjx0Wr1Ez5lZ3hc0s1NmNmVmU9VSzFozQoi20arAhwEsmNlb2wWdc2eccxPOuYlUlk+SJ4RoHy0JPBJwAUDBzCYfbpOEEA+L+xZ4dPnN590VQnQErdxkextAfuvM7Zw7y16cqDpk5/15sIWv8/zf3K/95mLxyX5adn0f//7qu85zcNk81r3T3EPvmuFe8ep+Pmd7LsfvXQx2++f4vnKVjw8oVfnH/t3H+bzo08t86eR/u/g1b6x/hPu9pfdzNG51ntPtEv5472eLtGy9l38myZtzNF49cpDGu2f98w9U+/mxuBLnwXs+svsWeHRpfjH6o+IWQuwtGugiRMBI4EIEjAQuRMBI4EIEjAQuRMC0PV0UdSC54bcHsku8CfVBf9pk96zfftuM86axKZkBPs1tdYCnota6ee5gz//wEX6LX+epjcUuktIZYyUtLPK63ykfp/HVxZilbEm3Lt/m1maGNx0uxft1Zcx/PJX7+VTW/Z9xC69+cIjGk0vrNJ4i6ce1LD8We27zpYt96AwuRMBI4EIEjAQuRMBI4EIEjAQuRMBI4EIEjAQuRMC03QevdSWw9KTfd+27zv29ap/fD17fz71iq3PPtJ7i5bvn/f598Snug5cP8KWJK/08nTRR5B9NbcP/3Tx0mKeyFq7naHx1lffL0Bjf/+LtAW/MqtzoNr46MDJLvHxqw7+D9CrfeWJ5jcarB/3vCwDSRV5+/aA/JXTgUoGWXc3zun3oDC5EwEjgQgSMBC5EwEjgQgSMBC5EwEjgQgSMBC5EwLTdB0+W6hiY9vuDFeJzA0B6xZ+T3X+Ve83ro9yrTq9xX7T7o9veWLX7MVp2eZZ37ej/8nVwb32X++S54wVvbPEqz1t2Gf6+k728Xxdv8qWPkwM8T5+R+T3PNU+v8rEN5T6/T95/hS9VXR3h72t9hE+rXO3hU4APfFTwxup9fNrkaky+uA+dwYUIGAlciICRwIUIGAlciICRwIUIGAlciICRwIUImPbPi54w1Lr91ZSGuQ/edd2fe+yyvPn9FxZo3PVxz/Xu957wxvpuca+4ludzZLt3ue+ZiJkGu1Tx91tcznXPGM9bXrvO5y5PxtjcPYf8SxuvFHmf993iHv38c/y97f+Nv3xpmPvY3TO8X/o+878vAKj18f0vP+332bML/Hiqt6hUncGFCJhYgZvZpJmd22bbSTM71b6mCSEelFiBO+fONj43s8lo+/no+cn2NE0I8aC0con+IoDp6PE0gPHmF5jZKTObMrOpcnn1QdonhHgAWhF4run5vuYXOOfOOOcmnHMTmQxf6E4I0T5aEXgBAF+mUQjREbQi8HfxxVk8D+Cc/6VCiL0k1l2LbqJNmNmkc+6sc+6smZ2Otue2brZ5y5eryNwo+BtQiFlruubPm64Ocb82XeWeaj3LPfgDv/L76Nd/cM8vky+RuMa7NlHlvqfFrPG9VvTnurssf9+rs/411wEgs8zr7nquQOPli/58dMvFtG2Un3NGf8X7bWPIn0dfipla3KV4v6SLPIe/no7pt3n/AILsVT5mIzvDx034iBV4JOChpm1vRg+puIUQe4sGuggRMBK4EAEjgQsRMBK4EAEjgQsRMG1PF60MpnHrB6Pe+KFfLNLyhW+MeGPMdgCAjaf59MGpmGmTF5/022iOz2qM1Aq3TCo9/Ls18wLvl/q63zapLXD7z6VjllWOcWSKs338BY/7P5euG7xtCe6CIVPkn3kh7z+ky3xWZAxf4tMqp6/N0Xh9mNu2tupPN41bmrhVdAYXImAkcCECRgIXImAkcCECRgIXImAkcCECRgIXImDa7oOnlyp47F9veeO1QT7jy+Al/7TJ62Pcj+37xF8WAKpDPFW1nvZ7tj13uJdc5SsXoytmmtxbd/h7G33Cn144sxJjZKe4/2+rvHzu4DKNl6b884FsHOfzQde6ed3734vpd3I49d6OWXp4kHv0y985TOO5j4o0zpYntgpPRbU6b7sPncGFCBgJXIiAkcCFCBgJXIiAkcCFCBgJXIiAkcCFCJi2++AulaS5ruUcX3I1ter3i12K51xX93GfO7nKc4u75/xmdjHP6973Pvc1Szne9aNP8Nzjmav+aZvTizxZvXqI5z0PPjdP42bck10a9b/3TA+vu0rGHgBApT9mquv3/J/pUj4mTz7JP9Oh9/jUxrUBPvghuezPB6/187KV3takqjO4EAEjgQsRMBK4EAEjgQsRMBK4EAEjgQsRMBK4EAHTdh/cNspIXrrqf8EfP0XLr4/484Pj5kVfGeMee+7DdRp3xBZN85RopDZ4zvXSUe7JFm74c6oB4OjxO97YjQtjtKwr8+/1+at8PnnXzT3+zCLZ/wFaFG6E54uvjXC/uE6GAAxc4Tn4iSr39zfG+LznXbf4QTH3Tf/YhaGPVvi+b6zRuI/YM7iZTZrZuaZti2Z2zsxOt1SrEGJX2Mn64GfN7NWmza9E64YLITqYVn+D58ws/1BbIoR46LQq8GEAC2b21nZBMztlZlNmNlV2/HeuEKJ9tCRw59wZ51wBQMHMJj3xCefcRMZ4wocQon3ct8Cjs/N4OxojhHi47OQu+kkAEw1n6rej7ZPA5k249jVPCPEg7OQu+nkAQw3PCwAuRn/x4k4lYcN+XzVT4L5n8Qn/RNfZRZ6/23ub5x7HwdbJNm5zo9zHc7JT69xzPXJklsaZ153gwwOQLPK2ZQr8e9+9wD3Zcp//sEpejllbPMv7pWuRe/CpVX+8eIzPuZ5e4XWn1/jxtnGYr/G9/79nvLH1436PHADSydZul2kkmxABI4ELETASuBABI4ELETASuBABI4ELETBtTxeFA1D3e0pW5X7T8Mf+qWbTM3x5YKT521t8gadk9l/3pxfOP8vTPQc+4amD17/vX0oWAJYLPDWx++mCN7Y6zfd98Bluwc38nls2A2medlkt+e2kA3/kT3MFgJEe3m8L54/S+OohvxVWy8RMs72fx8FnskbfO5/SeP2o39rsvsaP5dpgayNCdQYXImAkcCECRgIXImAkcCECRgIXImAkcCECRgIXImDa7oNXBtKY+d5j3vjgNE/pTNT8KXwbx7iPvb6Pv730GvfgN4ZIWmWMZVod5FM2b4zwuvP7CjR++QN/n8alXM5c4/2WHOT5psW7POXTRvyf6e1P+LzJa0d4SmdqpPVDtpTj8d5bvN/6rvvHZABA7cnDNO7S/uNp41APLZsuxuQAe9AZXIiAkcCFCBgJXIiAkcCFCBgJXIiAkcCFCBgJXIiAaf/ywQ5IEfswtRqTW9zvz7vOLPIpl5cP85zt4jH+9nOf+b3q4Uvcl8y8d5nG8WfP0PBnV0ZovHvO/91c6ed+Lhz/XrcBPjahFjMGwJL+fqv38GmPjw3N0/jNKl/aeN/PPvTGSn/zHC1b7Yl5YzHUu/nxtH7A7/GnVvm4iPVRPq7Ch87gQgSMBC5EwEjgQgSMBC5EwEjgQgSMBC5EwEjgQgRM233wZKmOgd/7jfCVI3y+556Z1pcAzn0ak7+b6aLx/k/9c3RvjPH8XRw6SMPpYswSvWP8fdvX172x6qx/yWUAyA77ywJAeYOPH0j28jEAtZI/77nnMs/3/mhglMZL3+I++tAHj3tj2UU+PiC7zL1ol+afWebuKo0vHfUfb11zvE+z8/xY9kEFbmY5APno70Xn3GvR9kkABQB559yZlmoWQrSduEv0HwOYcM6dBQAzOxWJG86589G2k+1tohCiVajAnXNnGs7QeQDTAF6M/iP6P96+5gkhHoQd3WQzszyAheisnWsK37OQVXSmnzKzqXKF/y4RQrSPnd5Fn3TOvRo9LgCgs/ZFZ/4J59xEJs1v+Agh2keswM1s0jn3ZvR4HMC7+OIsngdwrm2tE0I8EHF30U8CeMPMXo82veacO2tmp6NYbutmm496OoH1Eb81kixz66Ke8X8HuQ2e3lc4wW2wg1N8qdpar98ucgle98pXeFpjeR+3ew72r9H40qrfXkwtkemeAZRL/KrKpbldxFsOIOsvn3ppkRadPPZbGv/FT75N44mS326q9PPPbPhjbh8m3ucpwPWnjtJ4z6w/NbrSzx3ruLgPWioS7/Fttr8ZPaTiFkLsLRrJJkTASOBCBIwELkTASOBCBIwELkTASOBCBEz7p02uOmQKfv8vO8OHsq4f6ffG7o7zZWz7Zrhju5zn5UuDft90/08u0LKJE0do/Mjf8XTQ4gafJrf2qb/tLmb5YJfhPne6wH30So73a3LRf1iVr/LxAb8e8qd7AsDtb/FD9vhl/3vrucPf99oYT13urR6j8bjjiZFd4OmilYHWpKozuBABI4ELETASuBABI4ELETASuBABI4ELETASuBAB03YfHAa4VOvfI113/Dm66RW+9LBVuR+8lOe+5+Blv1dd+c7ztGz219M0fu3OURrv6eVLIxuxdHtv8P5eeYKGkSnwvGmX4D65e9z/mZUy3N8/kF2h8ctrvG1rJ/w+e9ciP16yM7zu8gGeR1/u5W3LfUqO5UWei75ymE6i5EVncCECRgIXImAkcCECRgIXImAkcCECRgIXImAkcCECpv0+OACr+/3o2gD3RdNXZ72x0nNjtGzxKF8G17gtiuwdvy9699vclzz425jlgasxXvV8zPLE+/052bUs96kHP+F+bZ13G7Lc4sdK1T++oBYzH/x6jVde7eNjG9i4iZUj3Mcu5Hmu+ug78zSe2cePidJ+//oA9Qz/zIbfL9K4D53BhQgYCVyIgJHAhQgYCVyIgJHAhQgYCVyIgJHAhQiYtvvgLgFUe/zfI6lV7smWnhz1xtja4QCQXuWeKcupBoCb39vnjfVf535uLc89+sQc93sTVd4vtS7/e0sv87JsXAIAlP1T0QMAkl0x+3/Sv+564gb3ohdK3P8f/iBmzvepD/yxYy/RsomYhc9Lo7xjXMzpsu93/rXRNx4fpGXr2TbMi25mOTMbN7NJM3ujYfuimZ0zs9Mt1SqE2BXiLtF/DGDCOXcWAMzsVLT9Fefcy865N9vaOiHEA0HP+865Mw1P8wDORY9zZpZ3zsUMWhRC7CU7uslmZnkAC86589GmYQALZvaW5/WnzGzKzKaqJb72mBCifez0Lvqkc+7VrSfOuTPOuQKAgplNNr84ik845yZSWX5TRQjRPmJvzZnZ5NZvbTMbBzABYMo5d7HdjRNCPBhU4GZ2EsAbZvZ6tOk1AG8DyG+dubduwHn3UQeSG35rwyW45ZIubJBoFy1b6fGn5wHA6iF+AZMgK/ym1rjHlrrJUwtrwzxNFvPcRuu662+745mHSLIuBdB3g1tRPXd5nu3tQf8yujEzLuOTDw/T+PEbfDrp5H6/tRlng2UL/DNN1Hh88OMlGq8O+S3A5FpM7jL/SLzE3WQ7D+D4NqGL0R8VtxBib9FINiECRgIXImAkcCECRgIXImAkcCECRgIXImDani5q1Tqys2ve+MYoTw9cPuL3i7vmubE5+HGBxy9xD/72n/in0e2+4U+JBIDKEwdo/MQ/8rbHTfnMUjprfFVkZJe5nzvw3h0aX3n2II0f/rl/AMH0n3MjvPcaPyRTMcvsujF/v9fS/PPOLvHPJHNljsbXvzJC44zlw/zz7r0T45N70BlciICRwIUIGAlciICRwIUIGAlciICRwIUIGAlciIAx51pMNN1pBWazAK42bNoPgBuKe4fa1hqd2rZObRfw8Nt2xDl3zyCAtgv8ngrNppxzE7ta6Q5R21qjU9vWqe0Cdq9tukQXImAkcCECZi8Efib+JXuG2tYandq2Tm0XsEtt2/Xf4EKI3UOX6EIEjAQuRMDsqsCjVUpPNixi2BF04mqpUV+d22bbnvefp2172odkJdw977O9XKV31wTesFDC+ej5yd2qewd03GqpzQtKdFL/eRa72Os+vGcl3A7qsz1bpXc3z+AvAthajXQawPgu1h1HLlpgsZPp5P4D9rgPo/Xwtu5M57HZRx3RZ562AbvQZ7sp8FzTc/8aM7sPXS21Q8g1Pe+k/gM6pA+bVsLNNYX3tM/ud5Xeh8FuCryAzTfUccStltohFNCh/Qd0VB82roRbQGf12X2t0vsw2E2Bv4svvlHzAM75X7p7RL/VOu1ydzs6sv+AzunDbVbC7Zg+a27bbvXZrgk8usGQj2505BouU/aat4Ev3cTqiAUVo36aaGpXR/Rfc9vQAX3YsBLuBTO7AGC4U/psu7Zhl/pMI9mECBgNdBEiYCRwIQJGAhciYCRwIQJGAhciYCRwIQJGAhciYP4fL4I+3lotghoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEFCAYAAADOqip4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAafklEQVR4nO2dW2wc133Gv/9eSJFcksurJOtiaeVbfA9DuQmcNk5Ao0hTo3VBJ0AK9KWtBBToSwvI8Evf7Zf21epDC7RBWltFkTZtH6Q6hZPYjk3LF1l25Fg0qdi68rK8cy/k6YOG9nrF8w210pKrk+8HECTnv2fmv2fm25mdb/7nmHMOQogwSWx1AkKI+iGBCxEwErgQASOBCxEwErgQAZPa6gRCxsyGAXQDmAKQB5Bzzh2t8zaHADzvnDuwwdcPABgE8BXn3OF65iY2H53B64SZ5QAcdM4ddc4dw1WRZ+u9XefcCQCj19HkGQAvNKq4zezsVudwKyOB148cgMm1f5xzJ3F9wtssss65/FYnQfjKVidwKyOB148RAM+Y2ZHobI7oTA7g6qV09POsmWUrlk2b2UD09/Nmlov+f35tPRWvu2Yd1ZjZoeg1R6pfE12ed0evyZnZsJmdjV7/YkVew9Gy4egrwIZzrdqeN+/1th3l92ZF+/XyWDdnEeGc00+dfgAMADgOwOHqgZqtiD0f/R4C8GzF8uMABqK/nwVwxPO6z9YXbefFynVULH82+ju7ts2qHI9X/x+1y1Ws40hl3hXb3VCuVeuneVdue533QvOobKefqz86g9cR59xJ59zjzjkDcAJXRbAWq/zOm61qunYpP1nx99Q668+vbQdXRVXN9wBMRmfCXPQTR3eU99p2DwM4WRE/W7WtDeW6wbyrt10Jy4O1+41GAq8Ta5eQazjnnkaFwKLL0yEQ4Ubkq+PXQRbAyejgP+mce3wDbag4I7rX/riJuW502+vlcb3tfmOQwOtHNrLJAADRd8PR6O9DACbd1Tvea/GB691AxffXHK5eIVTzIoDHK15/3duI1lHZ7qBnWxtmA3lvSh6/CUjgdSa6CTQM4BCAp6PFJwAcqDrLd69dSlfcmHscwFORIA4DGKq6eTUUreMwgD+Ptre2jkPRB8jaDahrLuGrtpeNXjMYfQAB+Mx2y6/d3MLV7/GjNeRayXp5X7Ptdd7Lenlc0058jkU3KcQthpm96Zy75SykWzXvWxWdwYUIGAn8FiS6LM3dapelt2retzK6RBciYHQGFyJgJHAhAqbu5aKpljaX7uz2xpsml2n71bbmmrdtK/zrh5VX+bbTSW8sEdO2lPG3BYDUIm9v5RUad0n/+q1c5m1TPDdb5f3G+gUAXNJqXnfcPouDbjtm3SvN/HyXKPH2jncLEmyXxnxVZu8LAOZnPp1wzvVVL69J4JEPmccG6pvTnd3I/clfeeN7/vEM3dbiIxt5utKz7Tl+oKcnF2i8sKPdG2u6zNte+noXjfe+s0jj6UszNL7S488teXGat+3vpPHEUonGCzv92waAYqf/sEou8Q+25ukCjcdR6mjyxlJz/H3N7W+h8ZbLvH25jSu8adZ/PCaKvF+K2TSN//THT4+vu17aah3Wns5aewprvQcohBCNQS3fwQ/i86KCUXzx8UEAn5UojpjZyMoSP9MJIepHLQLPVv3fU/0Cd3UUk0Hn3GCypa2mxIQQN04tAs+joppICNG41CLwN/D5WTyHq8X2QogG5LrvojvnjkUVREO4OqIGLdlrmi5i74u/9sYnvn0X3V7Xe7Pe2NJufvlf6uBvr5jN0vhin/+uaP+ZC7Rt76ltND6/l9+xbW3md2QL3f67qraLr3t2D++XjnPcfZi6h7fPnvX7QdsmuC068VCGxksZbhd1fuzPfamH94txZxKpBd4vS/38Tnfb2JI3Vm7ndnDrmF8HjJpsMufcc9GfqscVooHRk2xCBIwELkTASOBCBIwELkTASOBCBIwELkTA1L1c1KWSWOnu8Mab5nkVTTnr9wddzMdTsZ17yeVtMZ7qqL+yabUvS9su9/qrmjZCao5XVSWX/Z5sXBnslYf9+wMAmk5xv7flcly5qT926RFeiZb9qEjjrFINAJrz/oovW+E+dVwpa6mT79O2Czx3W/Tv08J+3i+lTp473lt/sc7gQgSMBC5EwEjgQgSMBC5EwEjgQgSMBC5EwNTdJkPCsJLx2wuJMrcmiqTkM26Uy9UUt8GSRd6++exlb+z8H+ylbTvGYgZ8XIixB2PKB0vt/n7JvH2ett3/r9yCu/LVXhp3MUdNsuB/bzv+4RRtu/it+2m8850JGi/s9g8oudLMj4eOd/m681/pp/GmORrG6n7/OCkL/dx6THIHzovO4EIEjAQuRMBI4EIEjAQuRMBI4EIEjAQuRMBI4EIETP3LRROGUsa/mUIH/4xJL/m96uI23rZnZJLGi/18iN5Lv7vHG2u9zH3sqXt513adiRuamPvgCTIP3uwT3KNvu8zHB07w1NDxK//wvwAw8VCrN7b85IO0bXqRP5uwtJ9P6rjton+qrFJ7zP7+1nYa7/qQPz/gjPvsS33+ks/OUW50N83UZoTrDC5EwEjgQgSMBC5EwEjgQgSMBC5EwEjgQgSMBC5EwNTdB7dVh/Sc37RtmiWGLoBCl7+WfKWJ+46XH+2h8SS3NWkNbnOem8U9p3hucUP0Jou8PrjzY3/yCzv58L7pOe6DZ8b8XjIATDzMh/ht/9TfN83TfH8vd/Pc83fw4YPbm/1e95UH+eHe/QHvl2UyZTMApOd5+0LWf0x0nuHPFiBmKGwfOoMLETA1CdzMps3suJkdudkJCSFuHrVeoj/lnDtxUzMRQtx0ar1Ez5pZzhc0s0NmNmJmI8US/z4nhKgftQq8G8CUmT2/XtA5d9Q5N+icG2xKt9WenRDihqhJ4JGA8wDyZjZ8c1MSQtwsrlvg0eX3QD2SEULcXGq5yfYCgNzamds5d4y+2gEJ4uGlLs3Q5rbiH+e65QL3Hafv5dPkdr0/S+N0PPcl7oOPf4d7xX1v89yb5rjvOf5tf734gRf4AN0TD/Hcym3cg+8Y47XJqQV/36R/zccen7qH17I7/ngByi3+c1Y65nbQUg8/3zXN82cXyi1cTn1v+vfL4h5eqx73zAc8w81ft8CjS/OT0Q8XtxBiS9GDLkIEjAQuRMBI4EIEjAQuRMBI4EIETP2HTU4ZCt1+S2e5jw9V25Qn5YUp/vnUdomXJk4McBuNWTLLPdy2yHzCLZVLj/Dcm6djprr9yL/+mTu55VLs5Ote2cZtMuzg8dUm//6ePcQPubYzPLc4m+ziXaTfV7n1aDEVmZkxvs9SZIhvAFjub/HGkkvcNi2281JVHzqDCxEwErgQASOBCxEwErgQASOBCxEwErgQASOBCxEw9ffBzbBCpvnNfMTLRYu9/hFh4oYeLrfyzy82jC0AFLr969/xC14uenkgxkuOwbgtinKrP/el/phpbPfy5wN6XueHxdztNIzEin/73a/xYZGnBnhuyQzv9+3dvASYkZ/3T3sMAKU9/HhbPMvLcJvm/MdE5hP+vppnYg4IDzqDCxEwErgQASOBCxEwErgQASOBCxEwErgQASOBCxEwdffBAQdb8fuHs3f7h0UGgJYrfl/UYqZUTS7xeO+7fPjfyfv8nu3sXt51xR7uW/aM8M/W+RiveXmXv186T/Ha4c6fcI8+c36Zxuf3+OuaAZ5boTempnqG92u6h0+zu7113htbBX8+4OvbR2n8337+CI13nuPrn9/lj6eW+fMB26b4sepDZ3AhAkYCFyJgJHAhAkYCFyJgJHAhAkYCFyJgJHAhAqb+9eAJQ4nUZXe9PUnbz97b7Y21XObeYLGTv71LB7lvmSD+fctF3vbB+8do/P3ZHI2n+QzAaDnn97pn7uG1xYs7uQ9+8RvcR7cW7pNj1d8322M8+J5DYzR++uwuGu/Y5ffJ0zEDn5cczy17e563H++h8Z7T/ucDVpv58XTlIf7sAf5v/cU6gwsRMLECN7NhMzu+zrIhMztUv9SEEDdKrMCdc8cq/zez4Wj5iej/ofqkJoS4UWq5RD8IYO2h3VEAA9UvMLNDZjZiZiPl5YUbyU8IcQPUIvBs1f/X3Flwzh11zg065wZT2/yDJgoh6kstAs8D8N/aFkI0DLUI/A18fhbPATjuf6kQYiuJ9cGjm2iDZjbsnDvmnDtmZkei5dm1m20+XJKPP3750V66/d63/Ybw1H18HOrekSkan7yP+5aFXX6fPfsAH8+9p5nfe4ib57rQzT1bR/ZcosA/t8vtfN2pPPeDu17jcba/C518bPEz5/l88d958BSNTxX9Y5u/MnaAtl2d5DXZ6ODPF7THDF2eKPn73RzfZz3vFfjKPcQKPBJwV9Wy56I/qbiFEFuLHnQRImAkcCECRgIXImAkcCECRgIXImDqXi6aKAMtk35rZDUmg+V+f5lcjLOA6Qe7aLx8Bx+C16b8tklTknsivzjPxz1eaYuxwVr5+ts+9OcWU/UIG+QW3+KFDI0v3MY3sLifDHVd5DutpZlPH/zu1G00vlz2H1C7+6Zp23Mlbptu356n8ZXpfhpf7vWX4baPLdK2lx7h+wT/u/5incGFCBgJXIiAkcCFCBgJXIiAkcCFCBgJXIiAkcCFCJi6++CrSaDQ6S8fLLfwusnWS35ftGWae8Uz+/nbiysPTG/3++S72/O0bXmVf3YuL3fQeNNF7jWXOvzPFrSe53268nM+ZTNyvF+X+7iHjxLZfkxJ5Y5OPl70lXk+QtCT+9/1xj6Y20HbXj7LPfbpc9znLvw29/C7Rthw1P4yVwDY8SrvF18Rrc7gQgSMBC5EwEjgQgSMBC5EwEjgQgSMBC5EwEjgQgRM3X3w1MQCev7+VW985o+/StuX2v1+8PwO7hWv8llwcdf9n9D4TGGbN5ZrnaBtx2b43BAJPgIvlndww7jrlP+zeX4PH5p4Jcen/217h09Vu/QQr6PHMtkvrTy3sXe5F/3o196n8Vcm/NMyf3yRD9HdMpCn8cd2j9L4T/7rmlm8vgAb+6DlCp8Ke2E398nx+vqLdQYXImAkcCECRgIXImAkcCECRgIXImAkcCECRgIXImDq7oOXe9sw+eTXvPG4sc23Tft90+YZ7qnOPMr93vliM41/f++IN/b7mdO07UsX7qLxlT08t9Q5vwcPADOP+b3o1RLvVDfPd/t8jpv0re9xn3wbGQd/apCvO1nmteyvje+j8Z5O/7TND+7hzz080HmexldiDtamh/m46+4n/nH6L3+ZH4udH8cU0nuIPYOb2bCZHa9aNm1mx83sSE1bFUJsChuZH/yYmR2uWvxUNG+4EKKBqfU7eNbM/M8ECiEagloF3g1gysyeXy9oZofMbMTMRsrL/u9EQoj6UpPAnXNHnXN5AHkzG/bEB51zg6ltfJA8IUT9uG6BR2dnXjYjhGgINnIXfQjAYMWZ+oVo+TBw9SZc/dITQtwIG7mLfgJAV8X/eQAno59Ycafnyuh/+bJ//S18bPJy1u8Hl1v451N6lPu1Tzz8Mxr/wfhBb+wvHx6nbe/rvkjjL13M0ngiZo7vlUX/rkvM8d2aiBnWfLWX1yavDvB68Jmif/utp/k+Wdwb45PHjDfP+Lt9/07jvzdSbRZ9kd4Mv5/0h2RMdgD4cfI+b6z5B3z8gNRizE7zoCfZhAgYCVyIgJHAhQgYCVyIgJHAhQgYCVyIgKn/9MHNSSzl/BbAYh9PYWGXv3yw720+XWtxH49fKPJpdH97x1lv7M0Ct5J+Ns4f1U+keflf6wVuH85l/D7aagu3VJILMeWkMeWmqx9maDyRW/TGip28xNdWeLloHPd2XfLGHvvPv6ZtH36AD4s8XeBDF//TW3wI8KZW/zGz2hPT5zG2qQ+dwYUIGAlciICRwIUIGAlciICRwIUIGAlciICRwIUImLr74C5pKHT6Tbx0TBlc31t+v3ipl6f/jbt/SeOn8ztpvLTiz/sven5K237/Hv+QywDwH+fup/HFR3nZpMv7y2i/fPcYbTs+4x++FwCmLvDnA1b28SGfd/fMeGOlrjm+7phy0Csf8imAx/r9z1wc/p2XaNtXp/izC3G53bnH78EDwEfn+7yxRJY2jS2N9q63plZCiFsCCVyIgJHAhQgYCVyIgJHAhQgYCVyIgJHAhQiYuvvgyYUiul7xT9vqMnwY3cJtHd7Y3O388+n1Hz1A44s5Xi/+2AN+Hz0dU7b8xvTtND51kXvNlubPB9yVu+CNvfXBPtq2Y/s8jSczvF9wnk9tfK7U4w8u8EMu1c09divxjh891++NPXnnD2nbnWk+/e/JhX00/srF/TTu8v4a/1Ve/o9dL/Ohqt/3LNcZXIiAkcCFCBgJXIiAkcCFCBgJXIiAkcCFCBgJXIiAqX89eDqF0m6/Lzp7gPvgCWLJtl7gY2xPD3FPFbNpGl4o+83JH81/ibb9dIb73Ejy3JvOcWP0VzO7vbFt0/xzuzzO68Ez3CbHUj/PvfnDZm8sbnpgjPGxx1umuA8+3+mP/80nT9C2v57L0vjcsv99AUA6yce6bxv3jy+Q5MPsY3Y/f/YAL6+/mArczLIActHPQefc09HyYQB5ADnn3FG+ZSHEVhF3if5dAIPOuWMAYGaHInHDOXciWjZU3xSFELVCBe6cO1pxhs4BGAVwMPqN6PdA/dITQtwIG7rJZmY5AFPRWTtbFb7mC3Z0ph8xs5FSaeHGsxRC1MRG76IPO+cOR3/nAfhHtsNnZ/5B59xgOt12I/kJIW6AWIGb2bBz7rno7wEAb+Dzs3gOwPG6ZSeEuCHi7qIPAXjWzJ6JFj3tnDtmZkeiWHbtZpsPlzSUOvx21Goqpu7S+S2ZliluS8yf4RZcE3d7cPtvTXljf/vf36Ft20f5Z2eWbxrze3lyHWf962/O87Yr3O1BIcv3SbGX93up6G9/20u8Xxb7+LZbJngZbaLsP9ZOjd1D2/Z/81Manx+PsT47eZltqse/X7pP832WiHEXvdtkwUi8B9ZZ/lz0JxW3EGJr0ZNsQgSMBC5EwEjgQgSMBC5EwEjgQgSMBC5EwNS9XBQGrKb9nyOL27nvedvLi96YS/HPp64PeXz6Hh4/lb/NH9xZoG0Lee7BZ3/F/dztv+DT7J75M/8Tgtsu8t0at+05PuIzOn7J1585719/oYPv7+xHvG6y2OEvuQSABNkti1/iPvX4Bzto3LVxMzqR5+XHWTKbdbmZ90vrPH/2wIfO4EIEjAQuRMBI4EIEjAQuRMBI4EIEjAQuRMBI4EIETN198NW0YbHf713u/Lnf5waA+b1+P3luD/98ar3Ea2zbPuHxM2f9Pnjnu9zznL2De83JmPrfywf90yYDAMzvi3Z+xLe93B3z/MAHvH3zLPdkJ+73H1aFHr7uhdv4cNFNszSM1KK/X21bjJfcxHPLvMcL6S1mfIECGQeJ7E4AwHya+/8+dAYXImAkcCECRgIXImAkcCECRgIXImAkcCECRgIXImDq7oMnCw6dH/uLdGdzvG667aK/hrfUxr3otgu8tti47YnMdye9sSujpFYc8eOiz+/k204WuKna95rfF00t8zdmkzyev4vn3j4eU7t80Z97XA3/vhcu0/jFb/bReJHUmzef5VPwljK8zwu9PN4+SsPIXPCb3XO7uc/dErPPfOgMLkTASOBCBIwELkTASOBCBIwELkTASOBCBIwELkTA1N0HdwasNBOPL6aGdrHfn2Lv69O07eRgF43HzZNd/h+/173zrWXaNn8HX3nrBC8Ajps3PVHyd1zr+SXadmFPK43v/xfuRdsCX//on/oHVm+aoU2xcCcpmkb8sws9p/3PTZRb+fnsysM8vvNVPi763B4up2TBn3yiyH3w1TQ/HrzrZUEzy5rZgJkNm9mzFcunzey4mR2paatCiE0h7hL9uwAGnXPHAMDMDkXLn3LOPe6ce66u2Qkhbgh6TeGcO1rxbw7A8ejvrJnlnHMxD+cJIbaSDd1kM7McgCnn3IloUTeAKTN73vP6Q2Y2YmYjpeLCTUpVCHG9bPQu+rBz7vDaP865o865PIC8mQ1XvziKDzrnBtNN/knyhBD1JfYuupkNr33XNrMBAIMARpxzJ+udnBDixqACN7MhAM+a2TPRoqcBvAAgt3bmXrsB52M1ZVju9lsAHee43bTc4x9G98I3uaWSHeXTxTZf4dse/aOMN5Za4Ovu++f3aXzye1+m8VRMuehy1n/xVczwq6auU3kady186OI4K2v/Dy95Y+Vef58CwNw+Xj7cfYbvs5l9/pLQ5lnusbWP0XCsDdb9Ps9tcYe/X7s+4m2tWFu5aNxNthMADqwTOhn9UHELIbYWPckmRMBI4EIEjAQuRMBI4EIEjAQuRMBI4EIETN3LRRMrDk1zfg+vkOVDHzfN+Ev00sQLBgArcy+5nOF+bzexsj/9RjttuzN1N423TPJy0fwBvmsyF2rzRQFgtZW/7+QMLwdNLfHcJ7/a7411fTBP25ZaeVnk9F186OOeU/5Ho8ee4M8H9L7Dj5f0An/f87t5iXDLpP9YjtNB5pWPadyHzuBCBIwELkTASOBCBIwELkTASOBCBIwELkTASOBCBIw5FzNu8Y1uwOwKgPGKRb0AJuq60dpRbrXRqLk1al7Azc/tdufcNXMr113g12zQbMQ5N7ipG90gyq02GjW3Rs0L2LzcdIkuRMBI4EIEzFYI/Gj8S7YM5VYbjZpbo+YFbFJum/4dXAixeegSXYiAkcCFCJhNFXg0S+lQxSSGDUEjzpYa9dXxdZZtef95ctvSPiQz4W55n23lLL2bJvCKiRJORP8Pbda2N0DDzZZaPaFEI/WfZ7KLre7Da2bCbaA+27JZejfzDH4QwNpspKMABjZx23FkowkWG5lG7j9gi/swmg9v7c50Dlf7qCH6zJMbsAl9tpkCz1b937OJ246DzpbaIGSr/m+k/gMapA+rZsLNVoW3tM+ud5bem8FmCjyPq2+o4YibLbVByKNB+w9oqD6snAk3j8bqs+uapfdmsJkCfwOff6LmABz3v3TziL6rNdrl7no0ZP8BjdOH68yE2zB9Vp3bZvXZpgk8usGQi250ZCsuU7aaF4Av3MRqiAkVo34arMqrIfqvOjc0QB9WzIT7ppm9CaC7UfpsvdywSX2mJ9mECBg96CJEwEjgQgSMBC5EwEjgQgSMBC5EwEjgQgSMBC5EwPw/nD5PYMrm+vEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEFCAYAAADOqip4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAZ4UlEQVR4nO2d2W8cV3bGv9t7s0WySUmUrdVueR9PbMvULAGyDEIHGSQPA4OeBMhz5Ic8DPIix3kIkEf7P7CQtzwE41GAbDOYQJpkBkhiJKLlLV7GHlGSF0ncm2ySTfZ288DiuKfF+12qxSJbd74fQLC7Tt+6p27XV1Vdp865xloLIUSYJPbaASFEfEjgQgSMBC5EwEjgQgSMBC5EwKT22oGQMcaMAxgGMA+gDKBkrT0Xc59jAF6z1p7c5udPARgF8Ky19sU4fRO7j87gMWGMKQE4ba09Z609jw2RF+Pu11p7EcDkHTR5GcDrvSpuY8yVvfbhXkYCj48SgLnNN9bay7gz4e0WRWttea+dIDy71w7cy0jg8TEB4GVjzNnobI7oTA5g41I6+nvFGFNsW7ZgjDkVvX7NGFOK3r+2uZ62z922jk6MMWeiz5zt/Ex0eT4cfaZkjBk3xlyJPv+DNr/Go2Xj0U+Abfva0Z/T7636jvx7s639Vn5s6bOIsNbqL6Y/AKcAXABgsbGjFttsr0X/xwC80rb8AoBT0etXAJx1fO6X64v6+UH7OtqWvxK9Lm722eHjhc73UbtS2zrOtvvd1u+2fO1YP/W7ve8ttoX60d5Ofxt/OoPHiLX2srX2OWutAXARGyLYtLX/5i12NN28lJ9rez2/xfrLm/1gQ1Sd/DGAuehMWIr+fAxHfm/2+yKAy232Kx19bcvXbfrd2Xc7zA/W7tcaCTwmNi8hN7HWvoQ2gUWXp2Mgwo0od9rvgCKAy9HOf9la+9w22lBxRgxvvthBX7fb91Z+3Gm7Xxsk8PgoRmEyAED023Ayen0GwJzduOO9aT91px20/X4tYeMKoZMfAHiu7fN33Ee0jvZ2px19bZtt+L0rfvw6IIHHTHQTaBzAGQAvRYsvAjjZcZYf3ryUbrsx9xyAFyJBvAhgrOPm1Vi0jhcB/FnU3+Y6zkQHkM0bULddwnf0V4w+MxodgAD8MuxW3ry5hY3f8ZNd+NrOVn7f1vcW27KVH7e1E19iopsU4h7DGPOmtfaeCyHdq37fq+gMLkTASOD3INFlaeleuyy9V/2+l9EluhABozO4EAEjgQsRMLGni6ZyBZvpH3baEw3fGthPCMNbeg5fvDXvOtHgP218fVvDezctvv5EveW0tTK880aO951cp2b/wBHXGwXeNLXK7a272GN930my7lsBN/u+M9q26WnrMVdWbsxaaw92Lu9quKI4ZBnbyG/O9A/jse/8hdOen3fvqAAfNJvke1q9j9t9B5dE3d13bp7vDfUCH9pWhvuWWuXjkru57LRVj+6jbeceT1N78UqT2ltp37i6x21qlKts5E2+3asHeXu2T7QytCkKN3nfPpGlV7rfl9MrfGdMrPPv5OIbf319y3a01RZsPp21+RTWVg9QCCF6g25+g5/Gl0kFk/jVxwcB/DJFccIYM9FYW7kb/4QQd0E3Ai92vN/f+QG7UcVk1Fo7msp5fnQJIWKjG4GX0ZZNJIToXboR+CV8eRYvYSPZXgjRg9zxXXRr7fkog2gMGxU1aMpesmbR/4X7DuHKIe5Cuuq+89jkN4PRN+25Te4LRZHQRfUA7zzBb3rCekJNvjvVtQN9btu+JG3r863Wz4/7Sw94nCfNM0u86dxXue8pd/AAAJBecX9nKc/toFq/J+zq2ez1Qe57ZtntW93znfkiQnhj68Vdhcmsta9GL5WPK0QPoyfZhAgYCVyIgJHAhQgYCVyIgJHAhQgYCVyIgIk9XdQmgCZJT/Sl8LHMpPVB3jg7zXMPZ04PUvvQx2tOW36WZ5MtHc9S+8CnPCdz9skctedn3du+eoiPS3qJx/8rxzwxV4957X732CT7+bg1K/z5gvz+KrUvz7ifD0iu8HFpeZ6rQJKPW3aGx7KXSTZbZoEPaprE0Bk6gwsRMBK4EAEjgQsRMBK4EAEjgQsRMBK4EAETe5gMxqBFCuGlV/ntf1qp0hM5WHhygNqHPuQhlwYpnFjv5yGRfTd5OMhXVbU4yduzlNDCLV78rzp8d8f19WG+/sJVd7yp9jQPDzY9Ibh8tkbtxx4pO203lvj+sPJZP7UXjlSofbXG1z/4sXvj+mZ5Dq+viq+zXVethBD3BBK4EAEjgQsRMBK4EAEjgQsRMBK4EAEjgQsRMPHHwT00PZPwpUmp2xQpqQwA60V+/Go+kqd2WibXE69dGeFx8sK0r64yN+cW3CWhl0548h59s6765uDr476vD7sH5/H7p2nb3FEe/x8fmaD2ny4+7u67eIu2bXnSZP/js4ep3Ttu5HmQVJU3buS7OxfrDC5EwEjgQgSMBC5EwEjgQgSMBC5EwEjgQgSMBC5EwMQeBzcti2TNHdRNr/ApfteG3PFkm+Bxy+GPeO5x5RipYwse6i7c4n7XC57Sxcs8luzbtkTdHTfNeMoir/Nq0Wi4Kw8DAEyax2zzJCf7vQ+P07bJAZ7v/eZV3n7kgHt+4oUK37BCnu8vjQZ/tqGV5+O+fNxtP/Au359Mszup6gwuRMB0JXBjzIIx5oIx5uxOOySE2Dm6vUR/wVp7cUc9EULsON1eoheNMSWX0RhzxhgzYYyZqNfIw+RCiFjpVuDDAOaNMa9tZbTWnrPWjlprR9OZQvfeCSHuiq4EHgm4DKBsjBnfWZeEEDvFHQs8uvw+FYczQoidpZubbK8DKG2eua2159mHrTFopd0x3ZX7PHnTN93x4lSVx5JpTXUAubKnFnXd3T617JkGN8dj7NWDfOgLnrrqbNt8Ofb9n/GYa3WE55PXqvw7q6y564sP3Mdriy/d4rXJU544+dRU0Wl74oEbtO37k0eoHTV+Pkwtc/vQB24bq8EP+KfZdnHHAo8uzS9Hf1TcQoi9RQ+6CBEwErgQASOBCxEwErgQASOBCxEwsaeLttLAyiH3ceTgW/xR1uqhnNPmm8K3kePHL1oWGUC27g6jffEtnno4+AueUukLZTX6+LbBuu3ZiqesMUnBBYCMO+MSgD91ce2ge9sr13muauGYZ4reSpbaUzl3ePFmhYfgYHhYNbnC96fcrCfFt+Eel+Q6/87qnjCas8+uWgkh7gkkcCECRgIXImAkcCECRgIXImAkcCECRgIXImBij4MnaxYD193piWsH3XFuH4snuPuHLvEY+1KJTx9cL7jjxff9D09bbKV4TDRT8djLfP2rh8m4WR7PTa1xe91ThKf+1DK125r7ezHzPI12dYnvD4lZnso6+s2rTtt70/fTtpk+TwpwlseqVxPc9/yM+ztfL/LtStY8cxM70BlciICRwIUIGAlciICRwIUIGAlciICRwIUIGAlciICJPQ7eTBtUjrq7yc/z+J4h5r5pT2zQ+GLRvD0r91zdz4du6NIUX3c/zydvDHrynqtk+uAyj+e2Mvy4np/h9vUvuO/JpnvcDK/YjIbh8eAHn/mC2hMkp/vAPv5cxPUPeJw8P+UZt2n+fAGrP8C+TwBYL3rqAzjQGVyIgJHAhQgYCVyIgJHAhQgYCVyIgJHAhQgYCVyIgIk9Dp5oAtlFd3xwrciPMYOT7rzoZJa3nfsKz/fu/4IHZet97vXn53jbmd+6j9obeU9Rdg+DV92x7spxHkPPz3LfVw9x35p9PC+a1Q9vDPN477EHZ6h9vcF32c+Xi05bOsn9tlnu29oBavbO8ZtZdNvYMxeAv46+C53BhQgYr8CNMePGmAtbLBszxpyJzzUhxN3iFbi19nz7e2PMeLT8YvR+LB7XhBB3SzeX6KcBTEavJwGc6vyAMeaMMWbCGDNRX+fP/woh4qMbgRc73u/v/IC19py1dtRaO5rOeir4CSFioxuBlwEM77AfQogY6Ebgl/DlWbwE4IL7o0KIvcQbB49uoo0aY8atteetteeNMWej5cXNm23O9k2L3II77trM8vzfWtHtYiPrmY+Zp0WjcoRvfnrVHb9P1HnMdOSnN6h9auwwtRdu8Zhti8RF1z3PFqwc5rXJ10b4tqWH1qi9Neju/7uPv03bvr/Ic7K/Meyuew4AP5t52GnLJnn8P7HGxy1Z4/tbqkrNaJLHMipHeL53fq67uuhegUcCHupY9mr0kopbCLG36EEXIQJGAhciYCRwIQJGAhciYCRwIQIm9nRRAGgl3eGF7BIPB+VvuUMyy8d5+d7sEg8t+FLw6n1ue6OPhzUS9xWpPbPkmeJ3hY/L+pD7q2OppAAw/5hnqtqqZ1wWPFM+59y+n/+/Z2jTkQNL1H7x1mPUvkbSSRct93vgwTK1L10tcvsTPAyXLLjt+Xd4avPAp5o+WAjRgQQuRMBI4EIEjAQuRMBI4EIEjAQuRMBI4EIETOxx8FbaoHrA3U2yxuPBjYI7Zptc57HBRJ2vu+8WjxeXH3LHJgtv82lsl752jNpNi/u2eJKndA5cd/s+NcrbtngYnE7ZDACGxHMBwJApfPv2rdO2JwfnqH2q2k/tC6vu74yPOFDIukt0A0DFM/Uxi3MDQHPFrYOqJ0XX9+wCfrj1Yp3BhQgYCVyIgJHAhQgYCVyIgJHAhQgYCVyIgJHAhQiY2OPgyWoTQx+4c3ybeR7fqx5yT4XrnXJ1wGP3TD+caLojp1PfPkHb5ud5XHN1hOeT++LktQF3+ySvaoxa0fPswRCP5z56dIra1xru73R6aR9tm/AE4R8fvEXtn8+788WPDpdp26Sn71v9PEf/0FCF2qdqg05bcx/fVwt8s53oDC5EwEjgQgSMBC5EwEjgQgSMBC5EwEjgQgSMBC5EwMQeB7fpBKr3F5z2eoEfY/qm3Tm6S8fdMXIA6P/ck997jOdNJ9fd8eLM8t3VNR/4lLf3TSfbyLnjptURvm6b5HaT5b6nEp4Yf90dBx8s8Dl2pz353tkE9+2pw+48/cnyftp2KMd9S/R58r1bfF/uH1p12h7ZP0PbXr3snhaZ4T2DG2PGjTEXOpYtGGMuGGPOdtWrEGJX2M784OeNMS92LH4hmjdcCNHDdPsbvGiMKe2oJ0KIHadbgQ8DmDfGvLaV0RhzxhgzYYyZqNdWuvdOCHFXdCVwa+05a20ZQNkYM+6wj1prR9MZ9w02IUS83LHAo7PzqTicEULsLNu5iz4GYLTtTP16tHwc2LgJF597Qoi7YTt30S8CGGp7XwZwOfqLX9wkZJuu8nju2n5PLWkPbH7wJolDA0DhCx6vXd7vGXq+eiyedB+bzQPLtG0y4ckHr3PfJmd5PDmZdMfJTw7zuudPDNyk9n+59iS159LuWPXsTXc+NgAsDfL5w79eukbtb3zwELUfPube9tUGfybDt7+50JNsQgSMBC5EwEjgQgSMBC5EwEjgQgSMBC5EwMQ/fXDSYH3QfRwp3ORT+K4ecocPcnO8bTPnKU3c5KGH9LI71JVe4n0jwdedm+Oph6k17vvSQ+712wZv2/KEyZ48eoPaP5oaofbnH3rPaZuv8ycbZ2o8XXSoj6d0fv3gNaftvb7DtO1qnYeqflE+QO3D9y1S+xND7nLT1SYP6f7isG/y463RGVyIgJHAhQgYCVyIgJHAhQgYCVyIgJHAhQgYCVyIgIm/bHKCl/idecZT+vgzd+rhwqO87cG33WVqAcCmPNMLp93Hv0aBD10rw4+dM0/xuGeCh8lhSeni4SIvkzU7w2PNa02+bQcG+Pr//r1Rp+3Pn/kZbfve8hFqf/7oW9R+Kn/NabuyzOPYJ/bNU3ujxZ8vWKjlqb1l3fvb/HofbWs8j1240BlciICRwIUIGAlciICRwIUIGAlciICRwIUIGAlciICJPQ6eqraw/z13Gd/aEI9lN3PuY1DxCi9NbOrcniDTAwPA8iP7nDZWUhkAmhlubxR437Vh7nvpkVtO29WP7qdtf3f0A2q/usTLIq+s87zpsUc/ctqm6gO0bX9qjdr/t/wgtb9TOea0Ta/y+P/bk8ep/cSRWWp/aIDbn+2/5rS9leB9f5I+Qe0udAYXImAkcCECRgIXImAkcCECRgIXImAkcCECRgIXImDir4ueSaDyoLsWdmaJx3vTFbe9epDnVDf6eI5t5SjP780uuGPVawd4nHvtN3guerPCfc8N83jw9bfcedNPnL5G2/7n5Elqz/etU3sxz307mHE/9/DtgXdo27+d/h1q/2ThILU/PDTjtC2s8HztbKFG7d974CfU/k9zz1D7pSV3DP+r/Z/Ttm99+DS1u6ACN8YUAZSiv9PW2pei5eMAygBK1tpzXfUshIgd3yX6dwGMWmvPA4Ax5kwkblhrL0bLxuJ1UQjRLVTg1tpzbWfoEoBJAKej/4j+n4rPPSHE3bCtm2zGmBKA+eisXeww3/bgcnSmnzDGTNTXef0uIUR8bPcu+ri19sXodRnAMPtwdOYftdaOprN8sjkhRHx4BW6MGbfWvhq9PgXgEr48i5cAXIjNOyHEXeG7iz4G4BVjzMvRopesteeNMWcjW3HzZpuLZhaonHAfR7ILPNy0Pui2N3gUDIZH4LyHt+oht61+lIeKfvOBa9Q+u8avbD5bKFJ74eGy05ZL8hq7+wrc95ce+zdqf3fVnZIJAN/qd6ejXq7ydM8ri7y08bGBBWrPJt31plfn+A5z3zFeNvmH809R+1P9n1F7KTPttJWb3LclHtl0QgUeife2VW+e0QFQcQsh9hY9ySZEwEjgQgSMBC5EwEjgQgSMBC5EwEjgQgRM7OmipgUkSdh1/mn3NLgAAGK2aV56GGm+7uIBd1ojAJSn3WV2//SpS7Ttf82UqP0rRXfZYwCwZKpZAJhZccfRr8zzWPIfnnif2t9ceYDaB1NVav+H+dNO25Ul7ttAlsfopzyljyv1nNOWLfJ1zy3yZxNaQ/w7+f6nz1L7Hxz+0Gn7ZuET2jZR530723XVSghxTyCBCxEwErgQASOBCxEwErgQASOBCxEwErgQARN7HBwWMO4UXZwg0+ACwPOH33ba3iZTxQLA1wcnqf3f5x+j9trQnNNWrvP83XqTl2QupnlZ5UN9S9T+9LC7zO6hNG87to9PH5xmDx8AqFhe8vl7X/yJu23VHacGgJUZPq6mwc9JX3vaHU+e9VQXSqd4AYHnD0xQ+yMFvj+O7XM/f3BuhpeLtnx3cqIzuBABI4ELETASuBABI4ELETASuBABI4ELETASuBABE3sc3CaAJpm19fqN22Y++hX+0bhrUR/fx2tk/3T+UWr38f2TP3ba3q3xmGnfQRL8B7DYylL7u1keUz2ZmXLafi/Pffurqa9R+2/3/5za/+aTP6L2lfWM27bI4+CpBb5L9j/Ba5dnEu5x/+rIDdr22hLfF48kF6k97Rn3Q0n39MQncny76v2eugkOdAYXImAkcCECRgIXImAkcCECRgIXImAkcCECRgIXImBij4MnGkBu1l2/fN+P3DFTAPjs6SNO2/X0Yd75YV4H21dp+i9z7vreCeOpye7hsfxNar+yNkLti+Thgh8vDtC2P6+Qic8B/GzqIWpfuMR9S624R3b/HB+36gj/Vmr/zWPV07/vrnW/1uB57I8W3fN3A8C/Vvj84EOpFWr/SeUrTtvPl/h3MvhxDHXRjTFFY8wpY8y4MeaVtuULxpgLxpizXfUqhNgVfJfo3wUwaq09DwDGmDPR8hestc9Za1+N1TshxF1BL9Gttefa3pYAXIheF40xJWstr4kkhNhTtnWTzRhTAjBvrb0YLRoGMG+Mec3x+TPGmAljzERjjf8uEULEx3bvoo9ba1/cfGOtPWetLQMoG2PGOz8c2UettaOpHC90J4SID+9ddGPM+OZvbWPMKQCjACastZfjdk4IcXdQgRtjxgC8Yox5OVr0EoDXAZQ2z9ybN+Cc62gBqTV3aCQ/W6cO5qfdaZUZnr2HxDskTxVA3XNx8ZM3vuG0rQ97+nZnBgIAfsTN3jK5jT73mBZu8JBK3xRPa2zk+IVd0fJQV4MM+9AnPHTZ/zk/56we5Pab/3zCaavx6CFupe6n9otH+L6aLvAvvdVyj2trnqcPD2S7C5P5brJdBHByC9Pl6I+KWwixt+hJNiECRgIXImAkcCECRgIXImAkcCECRgIXImBiTxc1LYvMsjvuml5cp+0zZXc6aZOHDlG4xUsXN/r45g997I5r5j+v0LaVR4vU3krxuKb1HHqT6+5YdHqFx7mXTvC0yeEPqtTeynDnKkfdX0xtgPedXubfWfHvLlF79TvuktAJMmYA4MsAXjnEfW+luD236C59nFrlZZFXR7o7F+sMLkTASOBCBIwELkTASOBCBIwELkTASOBCBIwELkTAGOvJ7b3rDoyZAXC9bdEBALOxdto98q07etW3XvUL2HnfTlhrD3YujF3gt3VozIS1dnRXO90m8q07etW3XvUL2D3fdIkuRMBI4EIEzF4I/Jz/I3uGfOuOXvWtV/0Cdsm3Xf8NLoTYPXSJLkTASOBCBMyuCjyapXSsbRLDnqAXZ0uNxurCFsv2fPwcvu3pGJKZcPd8zPZylt5dE3jbRAkXo/dju9X3Nui52VI7J5TopfFzTHax12N420y4PTRmezZL726ewU8D2JyNdBLAqV3s20cxmmCxl+nl8QP2eAyj+fA270yXsDFGPTFmDt+AXRiz3RR4seP9/l3s2wedLbVHKHa876XxA3pkDDtmwi12mPd0zO50lt6dYDcFXsbGBvUcvtlSe4QyenT8gJ4aw/aZcMvorTG7o1l6d4LdFPglfHlELQG44P7o7hH9Vuu1y92t6MnxA3pnDLeYCbdnxqzTt90as10TeHSDoRTd6Ci2XabsNa8Dv3ITqycmVIzGabTDr54Yv07f0ANj2DYT7pvGmDcBDPfKmG3lG3ZpzPQkmxABowddhAgYCVyIgJHAhQgYCVyIgJHAhQgYCVyIgJHAhQiY/weC1y2I4LWEbAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
