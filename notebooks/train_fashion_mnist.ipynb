{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 598543.187500\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: 399437.468750\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: 336487.843750\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: 243410.656250\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: 199241.718750\n",
      "    epoch          : 1\n",
      "    loss           : 323421.44265625\n",
      "    val_loss       : 174488.375\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: 170403.156250\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: 140246.312500\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: 143461.000000\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: 110403.453125\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: 85490.703125\n",
      "    epoch          : 2\n",
      "    loss           : 123952.788515625\n",
      "    val_loss       : 76563.05390625\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: 91415.406250\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: 62146.421875\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: 100183.367188\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: 40682.593750\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: 19823.876953\n",
      "    epoch          : 3\n",
      "    loss           : 53269.998537597654\n",
      "    val_loss       : 19453.58359375\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: 18095.097656\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: 5258.552734\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: 8461.632812\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: 593.296997\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -30213.316406\n",
      "    epoch          : 4\n",
      "    loss           : 143.24334716796875\n",
      "    val_loss       : -17041.159765625\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -16095.027344\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -32265.277344\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -17646.916016\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -18265.695312\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -50466.957031\n",
      "    epoch          : 5\n",
      "    loss           : -18535.82435058594\n",
      "    val_loss       : -57518.5095703125\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -57907.687500\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -12091.945312\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -71400.468750\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -55134.296875\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -91026.742188\n",
      "    epoch          : 6\n",
      "    loss           : -68625.54700195312\n",
      "    val_loss       : -86006.0744140625\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -86323.320312\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -111523.406250\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -55497.625000\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -53488.507812\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -69590.570312\n",
      "    epoch          : 7\n",
      "    loss           : -77272.9796875\n",
      "    val_loss       : -100163.041796875\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -98306.984375\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -126049.468750\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -113132.179688\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -118537.265625\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -114003.328125\n",
      "    epoch          : 8\n",
      "    loss           : -105309.005\n",
      "    val_loss       : -105608.7021484375\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -138730.015625\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -108641.789062\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -140117.593750\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -118783.828125\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -113911.257812\n",
      "    epoch          : 9\n",
      "    loss           : -112231.50749511718\n",
      "    val_loss       : -142246.98046875\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -139424.984375\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -164182.046875\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -132780.828125\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -130495.812500\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -131210.750000\n",
      "    epoch          : 10\n",
      "    loss           : -127926.7449609375\n",
      "    val_loss       : -128189.1353515625\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -155910.234375\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -159291.656250\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -150132.296875\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -178198.937500\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -132420.140625\n",
      "    epoch          : 11\n",
      "    loss           : -149853.325078125\n",
      "    val_loss       : -155049.162109375\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -194797.218750\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -188409.515625\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -152139.015625\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -159712.921875\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -163683.625000\n",
      "    epoch          : 12\n",
      "    loss           : -158088.28765625\n",
      "    val_loss       : -175312.983984375\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -192346.890625\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -162708.937500\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -162030.218750\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -131359.750000\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -115650.828125\n",
      "    epoch          : 13\n",
      "    loss           : -150427.25533203126\n",
      "    val_loss       : -138308.4421875\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -142916.843750\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -192473.843750\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -188631.250000\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -182733.906250\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -145856.828125\n",
      "    epoch          : 14\n",
      "    loss           : -147044.94435546876\n",
      "    val_loss       : -182344.62265625\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -124524.812500\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -131070.531250\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -205438.531250\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -197758.031250\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -167657.906250\n",
      "    epoch          : 15\n",
      "    loss           : -189830.203984375\n",
      "    val_loss       : -196111.6171875\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -201576.609375\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -218520.687500\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -199181.375000\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -228119.671875\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -185851.703125\n",
      "    epoch          : 16\n",
      "    loss           : -193998.503671875\n",
      "    val_loss       : -193609.1673828125\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -146704.468750\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -151357.562500\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -137540.984375\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -184658.281250\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -187654.046875\n",
      "    epoch          : 17\n",
      "    loss           : -193563.58171875\n",
      "    val_loss       : -208735.033984375\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -211773.937500\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -251761.562500\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -159321.406250\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -243501.375000\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -225348.859375\n",
      "    epoch          : 18\n",
      "    loss           : -220540.7553125\n",
      "    val_loss       : -209097.6748046875\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -247604.156250\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -259163.937500\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -247040.500000\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -212949.000000\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -172556.421875\n",
      "    epoch          : 19\n",
      "    loss           : -191017.92508789062\n",
      "    val_loss       : -143684.648828125\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -149673.031250\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -187668.625000\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -229652.031250\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -253335.734375\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -204179.843750\n",
      "    epoch          : 20\n",
      "    loss           : -209434.753125\n",
      "    val_loss       : -232914.528515625\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -165302.375000\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -273546.937500\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -194048.937500\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -209892.687500\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -227953.656250\n",
      "    epoch          : 21\n",
      "    loss           : -221116.34421875\n",
      "    val_loss       : -227438.898828125\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -230159.375000\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -218351.125000\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -261856.703125\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -239112.687500\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -242764.578125\n",
      "    epoch          : 22\n",
      "    loss           : -225302.99296875\n",
      "    val_loss       : -226137.8349609375\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -219073.718750\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -220507.515625\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -263378.812500\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -222857.406250\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -231744.968750\n",
      "    epoch          : 23\n",
      "    loss           : -227047.65546875\n",
      "    val_loss       : -211277.198828125\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -218646.156250\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -284762.593750\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -286372.562500\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -243277.156250\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -276086.062500\n",
      "    epoch          : 24\n",
      "    loss           : -244401.174375\n",
      "    val_loss       : -242615.8287109375\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -292347.625000\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -255245.468750\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -180215.187500\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -229736.437500\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -203793.968750\n",
      "    epoch          : 25\n",
      "    loss           : -239322.454296875\n",
      "    val_loss       : -227492.276953125\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -146361.187500\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -260391.968750\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -218709.859375\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -243849.140625\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -255083.562500\n",
      "    epoch          : 26\n",
      "    loss           : -241995.541875\n",
      "    val_loss       : -248548.7908203125\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -177104.656250\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -195170.468750\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -241307.656250\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -258531.328125\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -213573.062500\n",
      "    epoch          : 27\n",
      "    loss           : -235908.059375\n",
      "    val_loss       : -224767.4853515625\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -261425.671875\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -237015.718750\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -277172.093750\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -268172.437500\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -271371.375000\n",
      "    epoch          : 28\n",
      "    loss           : -260449.05140625\n",
      "    val_loss       : -268134.3423828125\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -312058.562500\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -329150.062500\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -246073.562500\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -258526.500000\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -244069.234375\n",
      "    epoch          : 29\n",
      "    loss           : -262699.12875\n",
      "    val_loss       : -272910.578125\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -310640.875000\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -285319.781250\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -292857.312500\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -301826.000000\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -271323.312500\n",
      "    epoch          : 30\n",
      "    loss           : -260183.960625\n",
      "    val_loss       : -242162.91328125\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -218392.937500\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -257962.140625\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -282651.000000\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -281273.343750\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -248765.875000\n",
      "    epoch          : 31\n",
      "    loss           : -266078.5296875\n",
      "    val_loss       : -269014.806640625\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -201068.000000\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -180919.656250\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -254224.093750\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -303023.093750\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -303882.937500\n",
      "    epoch          : 32\n",
      "    loss           : -265829.1303125\n",
      "    val_loss       : -280877.750390625\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -322411.218750\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -273143.812500\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -304873.750000\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -207955.937500\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -319074.656250\n",
      "    epoch          : 33\n",
      "    loss           : -288334.99015625\n",
      "    val_loss       : -295336.8533203125\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -320944.437500\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -359282.250000\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -275748.812500\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -293467.125000\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -254435.140625\n",
      "    epoch          : 34\n",
      "    loss           : -288981.413125\n",
      "    val_loss       : -276280.304296875\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -352843.031250\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -263970.562500\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -268716.968750\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -260455.375000\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -291584.437500\n",
      "    epoch          : 35\n",
      "    loss           : -239612.79875\n",
      "    val_loss       : -274535.2826171875\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -193924.406250\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -190848.531250\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -324557.281250\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -275548.125000\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -287302.312500\n",
      "    epoch          : 36\n",
      "    loss           : -278659.74328125\n",
      "    val_loss       : -270340.8583984375\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -260694.234375\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -214053.000000\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -281259.625000\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -303391.437500\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -273583.406250\n",
      "    epoch          : 37\n",
      "    loss           : -291371.86921875\n",
      "    val_loss       : -270002.3087890625\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -254368.453125\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -331322.531250\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -240227.437500\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -309255.156250\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -316394.875000\n",
      "    epoch          : 38\n",
      "    loss           : -280877.961875\n",
      "    val_loss       : -282764.7806640625\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -267784.375000\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -277696.937500\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -290378.750000\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -247399.218750\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -288814.093750\n",
      "    epoch          : 39\n",
      "    loss           : -283367.19546875\n",
      "    val_loss       : -305147.7357421875\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -299819.093750\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -309776.312500\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -286011.437500\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -304084.437500\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -292346.812500\n",
      "    epoch          : 40\n",
      "    loss           : -293320.64109375\n",
      "    val_loss       : -304200.82734375\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -366740.093750\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -226487.031250\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -295785.937500\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -317657.468750\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -258211.234375\n",
      "    epoch          : 41\n",
      "    loss           : -299155.34296875\n",
      "    val_loss       : -306035.541015625\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -316185.906250\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -282189.562500\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -384857.187500\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -328755.187500\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -304381.718750\n",
      "    epoch          : 42\n",
      "    loss           : -311043.88703125\n",
      "    val_loss       : -297440.7294921875\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -237337.562500\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -290404.875000\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -235453.968750\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -342646.812500\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -309005.562500\n",
      "    epoch          : 43\n",
      "    loss           : -314745.94296875\n",
      "    val_loss       : -322504.2341796875\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -248649.593750\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -294032.437500\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -305338.625000\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -289389.812500\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -277984.656250\n",
      "    epoch          : 44\n",
      "    loss           : -288244.406875\n",
      "    val_loss       : -297069.5224609375\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -301307.062500\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -226098.234375\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -322481.531250\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -314927.968750\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -322013.937500\n",
      "    epoch          : 45\n",
      "    loss           : -310347.2640625\n",
      "    val_loss       : -318738.5548828125\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -342076.125000\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -349469.000000\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -315753.937500\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -254733.218750\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -300159.937500\n",
      "    epoch          : 46\n",
      "    loss           : -298239.32984375\n",
      "    val_loss       : -299595.3291015625\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -293299.468750\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -344899.375000\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -239057.109375\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -232522.812500\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -309812.937500\n",
      "    epoch          : 47\n",
      "    loss           : -277140.86578125\n",
      "    val_loss       : -313587.5126953125\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -239151.265625\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -337049.562500\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -280146.531250\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -334027.000000\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -299206.875000\n",
      "    epoch          : 48\n",
      "    loss           : -315388.90359375\n",
      "    val_loss       : -334842.1064453125\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -337712.312500\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -250886.187500\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -320666.875000\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -340800.750000\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -250236.500000\n",
      "    epoch          : 49\n",
      "    loss           : -329236.00875\n",
      "    val_loss       : -329297.03828125\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -245895.000000\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -350613.875000\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -366993.968750\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -302709.593750\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -311151.562500\n",
      "    epoch          : 50\n",
      "    loss           : -313300.2903125\n",
      "    val_loss       : -338966.841015625\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1027_122354/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -331123.437500\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -340701.250000\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -370605.625000\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -362404.437500\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -308359.906250\n",
      "    epoch          : 51\n",
      "    loss           : -339860.76265625\n",
      "    val_loss       : -294595.936328125\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -375196.500000\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -280745.875000\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -201652.843750\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -368435.843750\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -317219.500000\n",
      "    epoch          : 52\n",
      "    loss           : -265798.57861328125\n",
      "    val_loss       : -304763.383203125\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -355958.656250\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -333388.875000\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -341018.625000\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -328569.906250\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -309465.687500\n",
      "    epoch          : 53\n",
      "    loss           : -332186.3415625\n",
      "    val_loss       : -318730.4328125\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -380758.593750\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -395067.093750\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -258222.750000\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -329752.843750\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -345096.000000\n",
      "    epoch          : 54\n",
      "    loss           : -327772.18859375\n",
      "    val_loss       : -316872.566015625\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -243320.234375\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -267264.125000\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -380981.312500\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -338950.656250\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -326585.375000\n",
      "    epoch          : 55\n",
      "    loss           : -339342.603125\n",
      "    val_loss       : -338380.011328125\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -331776.750000\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -347881.906250\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -275374.750000\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -317471.875000\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -325456.468750\n",
      "    epoch          : 56\n",
      "    loss           : -329532.6640625\n",
      "    val_loss       : -319426.984765625\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -361658.875000\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -308807.531250\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -371278.125000\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -304178.875000\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -303871.281250\n",
      "    epoch          : 57\n",
      "    loss           : -299678.43109375\n",
      "    val_loss       : -301059.5314453125\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -221539.375000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -368801.000000\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -271279.625000\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -326000.875000\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -333480.937500\n",
      "    epoch          : 58\n",
      "    loss           : -303538.87375\n",
      "    val_loss       : -319094.8837890625\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -343103.968750\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -339445.187500\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -259846.015625\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -361649.218750\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -328541.281250\n",
      "    epoch          : 59\n",
      "    loss           : -345279.7459375\n",
      "    val_loss       : -343950.4337890625\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -398538.437500\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -334729.500000\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -427424.218750\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -363260.093750\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -381328.500000\n",
      "    epoch          : 60\n",
      "    loss           : -353744.946875\n",
      "    val_loss       : -346825.7060546875\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -395411.531250\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -382289.000000\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -346879.281250\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -349175.718750\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -349420.406250\n",
      "    epoch          : 61\n",
      "    loss           : -358461.86203125\n",
      "    val_loss       : -360392.033203125\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -361813.875000\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -344207.687500\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -346460.093750\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -337535.562500\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -337551.937500\n",
      "    epoch          : 62\n",
      "    loss           : -343754.8390625\n",
      "    val_loss       : -348555.175\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -357084.375000\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -271486.093750\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -314042.437500\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -323140.687500\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -283723.000000\n",
      "    epoch          : 63\n",
      "    loss           : -307211.280546875\n",
      "    val_loss       : -238736.2626953125\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -110368.578125\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -279759.343750\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -295643.093750\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -318093.250000\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -324158.250000\n",
      "    epoch          : 64\n",
      "    loss           : -302647.83890625\n",
      "    val_loss       : -350376.11640625\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -426272.406250\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -364770.312500\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -395089.406250\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -365249.000000\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -362339.406250\n",
      "    epoch          : 65\n",
      "    loss           : -364239.19359375\n",
      "    val_loss       : -361765.279296875\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -407754.375000\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -349270.062500\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -376830.062500\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -268200.125000\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -331405.062500\n",
      "    epoch          : 66\n",
      "    loss           : -359684.5878125\n",
      "    val_loss       : -316930.7748046875\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -383923.312500\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -238430.750000\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -406048.718750\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -280971.125000\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -348458.750000\n",
      "    epoch          : 67\n",
      "    loss           : -318542.09953125\n",
      "    val_loss       : -231737.9458984375\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -233887.765625\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -413609.937500\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -217304.750000\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -355384.062500\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -360669.875000\n",
      "    epoch          : 68\n",
      "    loss           : -332231.90265625\n",
      "    val_loss       : -344074.169921875\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -351309.906250\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -344496.562500\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -366808.593750\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -442528.843750\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -327211.875000\n",
      "    epoch          : 69\n",
      "    loss           : -351763.39875\n",
      "    val_loss       : -335284.85078125\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -329685.531250\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -423427.968750\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -408212.812500\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -368692.750000\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -357644.250000\n",
      "    epoch          : 70\n",
      "    loss           : -361646.38\n",
      "    val_loss       : -369650.519921875\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -374103.875000\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -442901.437500\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -367611.750000\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -266521.625000\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -366411.750000\n",
      "    epoch          : 71\n",
      "    loss           : -361778.15421875\n",
      "    val_loss       : -360357.5916015625\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -364008.125000\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -334120.187500\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -336526.718750\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -324006.593750\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -183402.562500\n",
      "    epoch          : 72\n",
      "    loss           : -341788.4884375\n",
      "    val_loss       : -326964.05234375\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -289332.343750\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -336843.500000\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -263580.375000\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -332958.687500\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -350235.781250\n",
      "    epoch          : 73\n",
      "    loss           : -345820.68125\n",
      "    val_loss       : -344101.47265625\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -234870.609375\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -348832.625000\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -355473.812500\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -370350.937500\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -375127.750000\n",
      "    epoch          : 74\n",
      "    loss           : -352003.61375\n",
      "    val_loss       : -365831.1640625\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -374086.031250\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -402286.125000\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -371432.437500\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -266129.156250\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -385885.375000\n",
      "    epoch          : 75\n",
      "    loss           : -355882.9596875\n",
      "    val_loss       : -354049.5873046875\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -337415.687500\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -443966.500000\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -375449.375000\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -438305.843750\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -335890.500000\n",
      "    epoch          : 76\n",
      "    loss           : -372044.990625\n",
      "    val_loss       : -362112.742578125\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -371794.281250\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -356323.437500\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -399015.187500\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -373542.125000\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -439479.187500\n",
      "    epoch          : 77\n",
      "    loss           : -375304.778125\n",
      "    val_loss       : -374711.88671875\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -376519.531250\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -428812.468750\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -436222.093750\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -215224.671875\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -289727.437500\n",
      "    epoch          : 78\n",
      "    loss           : -345796.1965625\n",
      "    val_loss       : -318565.7181640625\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -305420.781250\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -284413.281250\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -366896.875000\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -290410.687500\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -347195.750000\n",
      "    epoch          : 79\n",
      "    loss           : -345833.92375\n",
      "    val_loss       : -291111.5326171875\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -255106.109375\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -395919.656250\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -211273.687500\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -332634.062500\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -352335.750000\n",
      "    epoch          : 80\n",
      "    loss           : -326919.10578125\n",
      "    val_loss       : -356958.3310546875\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -347733.312500\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -286218.625000\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -373187.375000\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -455651.687500\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -340313.750000\n",
      "    epoch          : 81\n",
      "    loss           : -360920.34109375\n",
      "    val_loss       : -326560.76796875\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -331529.125000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -331125.250000\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -365514.312500\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -400818.968750\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -325501.187500\n",
      "    epoch          : 82\n",
      "    loss           : -343974.63109375\n",
      "    val_loss       : -356919.8318359375\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -363433.562500\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -289537.437500\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -464319.375000\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -355285.906250\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -454860.937500\n",
      "    epoch          : 83\n",
      "    loss           : -376267.56234375\n",
      "    val_loss       : -379550.3328125\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -357131.062500\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -458514.281250\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -375760.593750\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -379433.625000\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -396316.750000\n",
      "    epoch          : 84\n",
      "    loss           : -387323.1203125\n",
      "    val_loss       : -381599.0560546875\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -389029.906250\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -349371.843750\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -359226.156250\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -366202.562500\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -344276.906250\n",
      "    epoch          : 85\n",
      "    loss           : -365634.85328125\n",
      "    val_loss       : -374576.7748046875\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -271436.812500\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -397300.062500\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -418303.187500\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -353634.968750\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -382008.562500\n",
      "    epoch          : 86\n",
      "    loss           : -363076.548125\n",
      "    val_loss       : -374561.438671875\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -457914.781250\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -371633.593750\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -389042.625000\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -404799.593750\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -361532.937500\n",
      "    epoch          : 87\n",
      "    loss           : -382212.6596875\n",
      "    val_loss       : -377077.2431640625\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -432453.875000\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -358526.343750\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -268177.312500\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -275967.875000\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -332807.062500\n",
      "    epoch          : 88\n",
      "    loss           : -358774.6728125\n",
      "    val_loss       : -363598.5642578125\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -345551.125000\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -245578.953125\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -363152.312500\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -358507.312500\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -363761.093750\n",
      "    epoch          : 89\n",
      "    loss           : -352052.07203125\n",
      "    val_loss       : -383881.1197265625\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -408126.375000\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -309696.531250\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -245174.531250\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -335926.187500\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -345853.187500\n",
      "    epoch          : 90\n",
      "    loss           : -365403.535\n",
      "    val_loss       : -353296.4255859375\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -356877.187500\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -307087.937500\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -381676.593750\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -263541.562500\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -373178.093750\n",
      "    epoch          : 91\n",
      "    loss           : -369023.335\n",
      "    val_loss       : -366042.6296875\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -392156.687500\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -374166.562500\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -396918.375000\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -365366.437500\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -372998.000000\n",
      "    epoch          : 92\n",
      "    loss           : -381710.0403125\n",
      "    val_loss       : -393129.972265625\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -411519.593750\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -419013.218750\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -371707.531250\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -369193.531250\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -363913.687500\n",
      "    epoch          : 93\n",
      "    loss           : -377768.7753125\n",
      "    val_loss       : -371243.80546875\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -460641.031250\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -297771.250000\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -138443.281250\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -338007.125000\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -406879.187500\n",
      "    epoch          : 94\n",
      "    loss           : -368298.18796875\n",
      "    val_loss       : -385531.3046875\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -379673.937500\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -397384.781250\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -438287.125000\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -363144.312500\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -325670.812500\n",
      "    epoch          : 95\n",
      "    loss           : -371318.3640625\n",
      "    val_loss       : -318461.153125\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -413197.625000\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -358010.593750\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -384669.125000\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -403528.937500\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -466498.562500\n",
      "    epoch          : 96\n",
      "    loss           : -379655.036875\n",
      "    val_loss       : -399614.7025390625\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -479478.812500\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -402566.687500\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -310981.281250\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -396205.625000\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -358024.812500\n",
      "    epoch          : 97\n",
      "    loss           : -387831.74125\n",
      "    val_loss       : -354958.027734375\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -248627.875000\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -328474.687500\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -346042.156250\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -335351.875000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -357195.750000\n",
      "    epoch          : 98\n",
      "    loss           : -352941.49546875\n",
      "    val_loss       : -379229.234375\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -437134.750000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -362836.843750\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -364583.750000\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -376798.937500\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -341191.375000\n",
      "    epoch          : 99\n",
      "    loss           : -338578.74859375\n",
      "    val_loss       : -369787.5025390625\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -276840.375000\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -391997.906250\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -379351.687500\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -370482.562500\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -284629.687500\n",
      "    epoch          : 100\n",
      "    loss           : -375341.3346875\n",
      "    val_loss       : -377186.6865234375\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1027_122354/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -372080.281250\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -409320.000000\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -418167.187500\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -388784.250000\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -380660.437500\n",
      "    epoch          : 101\n",
      "    loss           : -396573.741875\n",
      "    val_loss       : -388759.0400390625\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -311246.875000\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -408243.250000\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -393891.156250\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -405294.750000\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -396959.937500\n",
      "    epoch          : 102\n",
      "    loss           : -396525.7609375\n",
      "    val_loss       : -392017.0416015625\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -463356.750000\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -388553.718750\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -345737.687500\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -356956.312500\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -361853.062500\n",
      "    epoch          : 103\n",
      "    loss           : -366792.08234375\n",
      "    val_loss       : -347688.317578125\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -367871.687500\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -269640.656250\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -366692.500000\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -269948.656250\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -301619.875000\n",
      "    epoch          : 104\n",
      "    loss           : -378572.518125\n",
      "    val_loss       : -383676.8310546875\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -434385.625000\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -388974.656250\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -376807.625000\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -417902.125000\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -389206.500000\n",
      "    epoch          : 105\n",
      "    loss           : -365691.5815625\n",
      "    val_loss       : -374315.9138671875\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -370235.062500\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -379028.125000\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -420754.062500\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -433673.781250\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -354055.125000\n",
      "    epoch          : 106\n",
      "    loss           : -381500.4321875\n",
      "    val_loss       : -363849.5177734375\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -344484.000000\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -390294.875000\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -362739.125000\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -395190.000000\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -366382.062500\n",
      "    epoch          : 107\n",
      "    loss           : -356720.3075\n",
      "    val_loss       : -359844.0330078125\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -387674.125000\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -262740.218750\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -381132.312500\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -469204.281250\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -369025.687500\n",
      "    epoch          : 108\n",
      "    loss           : -388230.716875\n",
      "    val_loss       : -388557.3416015625\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -375435.312500\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -408694.312500\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -418582.500000\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -438968.312500\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -425698.062500\n",
      "    epoch          : 109\n",
      "    loss           : -406842.0059375\n",
      "    val_loss       : -398930.909765625\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -411677.656250\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -403133.500000\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -475184.562500\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -434010.000000\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -382487.875000\n",
      "    epoch          : 110\n",
      "    loss           : -405218.6721875\n",
      "    val_loss       : -395940.748828125\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -409695.156250\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -391370.250000\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -330833.687500\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -350582.781250\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -434970.187500\n",
      "    epoch          : 111\n",
      "    loss           : -377037.16765625\n",
      "    val_loss       : -387295.285546875\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -379204.656250\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -405427.562500\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -406529.000000\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -465977.000000\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -427795.781250\n",
      "    epoch          : 112\n",
      "    loss           : -395677.90625\n",
      "    val_loss       : -375461.203125\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -410425.250000\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -396814.875000\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -414748.187500\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -392407.875000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -372227.000000\n",
      "    epoch          : 113\n",
      "    loss           : -406351.0853125\n",
      "    val_loss       : -406362.21875\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -415991.125000\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -463236.343750\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -283561.687500\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -392661.781250\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -352953.500000\n",
      "    epoch          : 114\n",
      "    loss           : -389003.68265625\n",
      "    val_loss       : -381675.150390625\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -435259.000000\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -397907.812500\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -487226.843750\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -394208.500000\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -398975.843750\n",
      "    epoch          : 115\n",
      "    loss           : -408676.4765625\n",
      "    val_loss       : -392563.8931640625\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -317503.031250\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -464429.187500\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -385893.843750\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -381388.031250\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -357131.500000\n",
      "    epoch          : 116\n",
      "    loss           : -390912.67453125\n",
      "    val_loss       : -394254.1265625\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -414424.531250\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -404748.343750\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -426267.312500\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -472417.343750\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -420654.406250\n",
      "    epoch          : 117\n",
      "    loss           : -418814.8165625\n",
      "    val_loss       : -413997.31015625\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -325905.812500\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -503503.812500\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -405512.906250\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -327939.750000\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -230191.671875\n",
      "    epoch          : 118\n",
      "    loss           : -377394.70546875\n",
      "    val_loss       : -306271.00390625\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -390782.375000\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -305403.000000\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -283013.156250\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -273368.062500\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -345744.656250\n",
      "    epoch          : 119\n",
      "    loss           : -319439.57390625\n",
      "    val_loss       : -354139.21796875\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -423162.875000\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -436614.656250\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -275391.531250\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -464327.031250\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -379664.062500\n",
      "    epoch          : 120\n",
      "    loss           : -387072.798125\n",
      "    val_loss       : -387922.2046875\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -385130.812500\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -426698.093750\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -377443.937500\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -424533.531250\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -414172.750000\n",
      "    epoch          : 121\n",
      "    loss           : -386205.4221875\n",
      "    val_loss       : -380043.613671875\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -389642.843750\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -450347.125000\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -383991.343750\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -415936.625000\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -427867.843750\n",
      "    epoch          : 122\n",
      "    loss           : -398183.95375\n",
      "    val_loss       : -391233.1935546875\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -475006.562500\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -399352.625000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -306243.125000\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -416727.562500\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -399987.375000\n",
      "    epoch          : 123\n",
      "    loss           : -409991.5703125\n",
      "    val_loss       : -415944.0578125\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -445431.812500\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -424462.437500\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -307227.000000\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -339619.500000\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -260488.343750\n",
      "    epoch          : 124\n",
      "    loss           : -379147.74546875\n",
      "    val_loss       : -372844.47265625\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -365996.531250\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -360092.562500\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -382874.687500\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -385154.500000\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -301079.562500\n",
      "    epoch          : 125\n",
      "    loss           : -384827.57296875\n",
      "    val_loss       : -389539.9146484375\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -312775.187500\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -402433.718750\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -491710.187500\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -501707.375000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -388059.812500\n",
      "    epoch          : 126\n",
      "    loss           : -405489.6403125\n",
      "    val_loss       : -396403.6609375\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -304227.437500\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -403471.812500\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -389406.656250\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -389561.093750\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -382691.187500\n",
      "    epoch          : 127\n",
      "    loss           : -395098.29375\n",
      "    val_loss       : -394561.0666015625\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -461413.718750\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -463878.968750\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -431481.468750\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -475155.343750\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -372579.593750\n",
      "    epoch          : 128\n",
      "    loss           : -399370.45546875\n",
      "    val_loss       : -367168.8810546875\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -416200.125000\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -506892.062500\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -421097.500000\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -395647.312500\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -488563.343750\n",
      "    epoch          : 129\n",
      "    loss           : -409766.4771875\n",
      "    val_loss       : -404488.080078125\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -423072.312500\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -394822.906250\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -324934.343750\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -419072.812500\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -446074.781250\n",
      "    epoch          : 130\n",
      "    loss           : -422373.1696875\n",
      "    val_loss       : -422430.4177734375\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -411406.875000\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -336350.812500\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -485337.375000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -420394.375000\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -401364.843750\n",
      "    epoch          : 131\n",
      "    loss           : -426588.678125\n",
      "    val_loss       : -392459.08984375\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -479442.687500\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -455158.187500\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -387434.687500\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -352229.125000\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -362297.468750\n",
      "    epoch          : 132\n",
      "    loss           : -378532.26328125\n",
      "    val_loss       : -381588.6716796875\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -391835.062500\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -416411.718750\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -436812.656250\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -453057.375000\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -390275.937500\n",
      "    epoch          : 133\n",
      "    loss           : -416220.15875\n",
      "    val_loss       : -404652.0490234375\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -396086.375000\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -288416.500000\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -420558.843750\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -321217.718750\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -381574.937500\n",
      "    epoch          : 134\n",
      "    loss           : -394423.5953125\n",
      "    val_loss       : -405327.9947265625\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -498495.375000\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -402569.031250\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -398423.687500\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -291204.937500\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -373431.343750\n",
      "    epoch          : 135\n",
      "    loss           : -397526.14625\n",
      "    val_loss       : -338920.337109375\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -413145.125000\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -331335.406250\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -357646.187500\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -369258.000000\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -287314.093750\n",
      "    epoch          : 136\n",
      "    loss           : -372588.160625\n",
      "    val_loss       : -390520.0287109375\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -444226.156250\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -374043.156250\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -385027.593750\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -434740.125000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -306569.500000\n",
      "    epoch          : 137\n",
      "    loss           : -397101.12375\n",
      "    val_loss       : -395463.5548828125\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -470554.250000\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -406850.718750\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -413447.343750\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -485014.875000\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -328266.218750\n",
      "    epoch          : 138\n",
      "    loss           : -402946.7503125\n",
      "    val_loss       : -399051.0666015625\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -297936.343750\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -432248.062500\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -414395.718750\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -324597.250000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -462947.093750\n",
      "    epoch          : 139\n",
      "    loss           : -430788.2065625\n",
      "    val_loss       : -420128.5498046875\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -452284.500000\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -409210.187500\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -525084.500000\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -406691.437500\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -440078.531250\n",
      "    epoch          : 140\n",
      "    loss           : -425828.265625\n",
      "    val_loss       : -401155.1810546875\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -414154.250000\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -286186.875000\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -427118.000000\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -443015.812500\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -384874.062500\n",
      "    epoch          : 141\n",
      "    loss           : -398783.30625\n",
      "    val_loss       : -356548.9734375\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -357834.593750\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -362499.375000\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -398399.937500\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -380274.062500\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -353914.000000\n",
      "    epoch          : 142\n",
      "    loss           : -383975.86390625\n",
      "    val_loss       : -394948.3470703125\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -461615.687500\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -510931.531250\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -338918.125000\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -454286.562500\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -445940.281250\n",
      "    epoch          : 143\n",
      "    loss           : -417543.87875\n",
      "    val_loss       : -362076.60546875\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -304545.437500\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -308959.312500\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -380785.718750\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -352602.656250\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -415340.125000\n",
      "    epoch          : 144\n",
      "    loss           : -374404.30265625\n",
      "    val_loss       : -375565.6158203125\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -457449.000000\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -432317.500000\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -375659.250000\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -450382.062500\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -432954.843750\n",
      "    epoch          : 145\n",
      "    loss           : -416755.221875\n",
      "    val_loss       : -427715.4802734375\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -446411.656250\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -414432.312500\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -399925.062500\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -429860.937500\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -442109.718750\n",
      "    epoch          : 146\n",
      "    loss           : -425410.941875\n",
      "    val_loss       : -397795.7021484375\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -402288.562500\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -425517.687500\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -467895.812500\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -470850.937500\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -370449.593750\n",
      "    epoch          : 147\n",
      "    loss           : -388916.06453125\n",
      "    val_loss       : -336406.64296875\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -252896.531250\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -375156.718750\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -383029.687500\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -401236.093750\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -422631.968750\n",
      "    epoch          : 148\n",
      "    loss           : -382609.93375\n",
      "    val_loss       : -393099.752734375\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -290568.656250\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -365174.062500\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -383028.312500\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -302472.250000\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -438639.312500\n",
      "    epoch          : 149\n",
      "    loss           : -399270.99484375\n",
      "    val_loss       : -407210.96015625\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -406328.250000\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -506854.187500\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -402947.625000\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -416098.875000\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -451973.250000\n",
      "    epoch          : 150\n",
      "    loss           : -413004.810625\n",
      "    val_loss       : -411891.2037109375\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1027_122354/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -404902.937500\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -410147.968750\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -438825.250000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -509890.937500\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -428733.031250\n",
      "    epoch          : 151\n",
      "    loss           : -423895.2459375\n",
      "    val_loss       : -409592.3478515625\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -424894.500000\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -329819.093750\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -513607.906250\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -413948.812500\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -407320.625000\n",
      "    epoch          : 152\n",
      "    loss           : -426346.840625\n",
      "    val_loss       : -423921.2228515625\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -433475.625000\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -433494.375000\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -383300.406250\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -397995.562500\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -405353.000000\n",
      "    epoch          : 153\n",
      "    loss           : -400346.00671875\n",
      "    val_loss       : -391600.846484375\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -294664.750000\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -374948.875000\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -400779.125000\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -403148.593750\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -407800.750000\n",
      "    epoch          : 154\n",
      "    loss           : -426206.789375\n",
      "    val_loss       : -422991.3435546875\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -352740.562500\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -535621.187500\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -420307.500000\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -420100.687500\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -407569.312500\n",
      "    epoch          : 155\n",
      "    loss           : -430018.1071875\n",
      "    val_loss       : -413116.6609375\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -420833.062500\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -525524.187500\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -323423.937500\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -316213.281250\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -375854.812500\n",
      "    epoch          : 156\n",
      "    loss           : -415629.0790625\n",
      "    val_loss       : -392258.4697265625\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -437774.125000\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -302660.781250\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -392644.031250\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -419529.312500\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -368382.062500\n",
      "    epoch          : 157\n",
      "    loss           : -408104.9675\n",
      "    val_loss       : -361585.3119140625\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -455945.750000\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -363183.781250\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -375902.781250\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -440951.375000\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -453742.937500\n",
      "    epoch          : 158\n",
      "    loss           : -392986.6284375\n",
      "    val_loss       : -396576.5193359375\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -432492.343750\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -403812.687500\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -421794.312500\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -336355.437500\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -403097.687500\n",
      "    epoch          : 159\n",
      "    loss           : -409492.01984375\n",
      "    val_loss       : -419230.35625\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -474446.875000\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -422651.250000\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -425060.781250\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -460146.625000\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -427749.937500\n",
      "    epoch          : 160\n",
      "    loss           : -432135.91125\n",
      "    val_loss       : -429791.58203125\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -535589.750000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -347539.812500\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -431864.187500\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -441779.343750\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -390417.468750\n",
      "    epoch          : 161\n",
      "    loss           : -436065.2603125\n",
      "    val_loss       : -356168.4806640625\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -433199.843750\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -403189.968750\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -407833.375000\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -421184.312500\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -439272.125000\n",
      "    epoch          : 162\n",
      "    loss           : -395054.81921875\n",
      "    val_loss       : -400666.092578125\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -424342.625000\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -538088.500000\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -352650.531250\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -489497.468750\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -481719.375000\n",
      "    epoch          : 163\n",
      "    loss           : -430975.2265625\n",
      "    val_loss       : -412368.684375\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -438615.906250\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -310512.312500\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -412473.312500\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -322248.062500\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -360963.625000\n",
      "    epoch          : 164\n",
      "    loss           : -372176.0391796875\n",
      "    val_loss       : -301430.03828125\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -373572.187500\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -341144.875000\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -419384.250000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -321423.875000\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -412922.531250\n",
      "    epoch          : 165\n",
      "    loss           : -371560.70765625\n",
      "    val_loss       : -406676.4841796875\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -493274.000000\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -409654.687500\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -469327.062500\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -300684.687500\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -371517.437500\n",
      "    epoch          : 166\n",
      "    loss           : -422687.736875\n",
      "    val_loss       : -389648.893359375\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -386888.812500\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -425817.687500\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -430768.312500\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -397948.812500\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -427254.875000\n",
      "    epoch          : 167\n",
      "    loss           : -404938.8365625\n",
      "    val_loss       : -403164.0765625\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -476442.437500\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -399012.718750\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -397321.906250\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -454066.625000\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -462404.750000\n",
      "    epoch          : 168\n",
      "    loss           : -416081.73875\n",
      "    val_loss       : -427994.31484375\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -496646.812500\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -422894.750000\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -419834.187500\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -348468.312500\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -423350.625000\n",
      "    epoch          : 169\n",
      "    loss           : -438414.796875\n",
      "    val_loss       : -430751.5146484375\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -352190.812500\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -500831.312500\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -471689.000000\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -432551.875000\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -421749.062500\n",
      "    epoch          : 170\n",
      "    loss           : -442716.923125\n",
      "    val_loss       : -418801.399609375\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -334944.343750\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -413010.437500\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -386045.375000\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -458583.218750\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -438197.281250\n",
      "    epoch          : 171\n",
      "    loss           : -426709.5053125\n",
      "    val_loss       : -426486.280859375\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -331810.625000\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -425182.625000\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -430903.718750\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -540717.750000\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -416500.062500\n",
      "    epoch          : 172\n",
      "    loss           : -449410.215625\n",
      "    val_loss       : -434330.1697265625\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -351999.718750\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -451746.062500\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -519505.062500\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -444493.937500\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -435270.968750\n",
      "    epoch          : 173\n",
      "    loss           : -442303.18\n",
      "    val_loss       : -435876.739453125\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -501524.687500\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -523959.593750\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -425624.812500\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -436517.968750\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -407414.625000\n",
      "    epoch          : 174\n",
      "    loss           : -427077.9234375\n",
      "    val_loss       : -409420.676171875\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -481821.968750\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -513806.312500\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -431926.625000\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -430788.906250\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -466350.375000\n",
      "    epoch          : 175\n",
      "    loss           : -439696.235\n",
      "    val_loss       : -409715.9439453125\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -472391.156250\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -401584.500000\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -374467.437500\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -437728.562500\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -348700.875000\n",
      "    epoch          : 176\n",
      "    loss           : -399569.7171875\n",
      "    val_loss       : -328777.57578125\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -244988.500000\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -459344.000000\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -315581.375000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -413431.968750\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -424855.187500\n",
      "    epoch          : 177\n",
      "    loss           : -389636.1034375\n",
      "    val_loss       : -426553.128515625\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -532692.562500\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -345477.625000\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -431955.562500\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -368122.937500\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -383803.218750\n",
      "    epoch          : 178\n",
      "    loss           : -425970.613125\n",
      "    val_loss       : -410932.4912109375\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -385888.281250\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -419679.718750\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -493706.156250\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -332952.218750\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -331790.437500\n",
      "    epoch          : 179\n",
      "    loss           : -418720.1821875\n",
      "    val_loss       : -415295.3560546875\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -405897.812500\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -416002.656250\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -542777.500000\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -428715.000000\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -373164.031250\n",
      "    epoch          : 180\n",
      "    loss           : -431536.055625\n",
      "    val_loss       : -426256.6525390625\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -340445.687500\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -493450.906250\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -523114.000000\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -424117.375000\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -435706.187500\n",
      "    epoch          : 181\n",
      "    loss           : -426403.985\n",
      "    val_loss       : -404494.8501953125\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -378878.062500\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -486599.250000\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -400486.125000\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -360846.406250\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -412342.687500\n",
      "    epoch          : 182\n",
      "    loss           : -398040.0365625\n",
      "    val_loss       : -373900.0328125\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -441282.500000\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -340435.875000\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -487371.843750\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -436866.406250\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -440683.031250\n",
      "    epoch          : 183\n",
      "    loss           : -423930.739375\n",
      "    val_loss       : -423298.440234375\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -419214.218750\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -457436.125000\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -487255.906250\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -414872.156250\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -363595.843750\n",
      "    epoch          : 184\n",
      "    loss           : -418876.311875\n",
      "    val_loss       : -414628.06484375\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -418915.750000\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -428029.781250\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -489016.656250\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -300936.937500\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -368300.000000\n",
      "    epoch          : 185\n",
      "    loss           : -405710.6796875\n",
      "    val_loss       : -352585.59609375\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -467291.375000\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -435457.375000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -465817.781250\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -233439.609375\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -1355.437744\n",
      "    epoch          : 186\n",
      "    loss           : -349371.2307055664\n",
      "    val_loss       : -278324.6501953125\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -308195.500000\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -518436.906250\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -480799.250000\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -432280.781250\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -400072.625000\n",
      "    epoch          : 187\n",
      "    loss           : -407784.783125\n",
      "    val_loss       : -393077.1619140625\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -392940.937500\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -412084.093750\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -440133.187500\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -445322.812500\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -441838.281250\n",
      "    epoch          : 188\n",
      "    loss           : -443057.9925\n",
      "    val_loss       : -445507.713671875\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -541480.562500\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -360884.875000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -451217.156250\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -461384.625000\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -458907.937500\n",
      "    epoch          : 189\n",
      "    loss           : -449900.10125\n",
      "    val_loss       : -424297.43828125\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -430372.437500\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -445158.750000\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -339513.968750\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -429764.375000\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -428079.500000\n",
      "    epoch          : 190\n",
      "    loss           : -440609.3353125\n",
      "    val_loss       : -436887.9697265625\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -505458.906250\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -447565.000000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -427980.093750\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -417729.250000\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -375903.812500\n",
      "    epoch          : 191\n",
      "    loss           : -438884.798125\n",
      "    val_loss       : -420211.9396484375\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -401737.843750\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -334843.312500\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -499463.687500\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -461615.000000\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -458969.000000\n",
      "    epoch          : 192\n",
      "    loss           : -446960.821875\n",
      "    val_loss       : -438205.951171875\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -485874.593750\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -327984.250000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -443767.437500\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -422971.812500\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -437063.125000\n",
      "    epoch          : 193\n",
      "    loss           : -439229.6175\n",
      "    val_loss       : -418379.2529296875\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -431880.937500\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -551133.875000\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -406451.687500\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -420323.437500\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -414779.718750\n",
      "    epoch          : 194\n",
      "    loss           : -425643.8209375\n",
      "    val_loss       : -414388.6033203125\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -424303.593750\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -432699.468750\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -435410.906250\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -407404.875000\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -453354.250000\n",
      "    epoch          : 195\n",
      "    loss           : -437347.8590625\n",
      "    val_loss       : -418831.988671875\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -509013.625000\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -435748.906250\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -438169.500000\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -432155.125000\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -402098.406250\n",
      "    epoch          : 196\n",
      "    loss           : -404682.91265625\n",
      "    val_loss       : -412411.965625\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -477624.718750\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -545577.375000\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -422687.062500\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -441611.375000\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -424065.593750\n",
      "    epoch          : 197\n",
      "    loss           : -449478.563125\n",
      "    val_loss       : -413611.17734375\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -375518.656250\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -432519.968750\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -435963.625000\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -429080.562500\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -416291.500000\n",
      "    epoch          : 198\n",
      "    loss           : -432647.95875\n",
      "    val_loss       : -417425.0951171875\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -460733.437500\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -517276.562500\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -410673.531250\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -384497.843750\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -445621.468750\n",
      "    epoch          : 199\n",
      "    loss           : -430942.9725\n",
      "    val_loss       : -429098.2841796875\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -435137.968750\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -416728.500000\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -442742.687500\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -474352.812500\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -322536.750000\n",
      "    epoch          : 200\n",
      "    loss           : -436997.0525\n",
      "    val_loss       : -433884.708203125\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1027_122354/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -433051.500000\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -355551.750000\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -398932.375000\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -393126.218750\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -413346.437500\n",
      "    epoch          : 201\n",
      "    loss           : -403074.14953125\n",
      "    val_loss       : -434223.2001953125\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -502283.625000\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -410209.937500\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -429650.500000\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -442700.375000\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -469089.312500\n",
      "    epoch          : 202\n",
      "    loss           : -438822.758125\n",
      "    val_loss       : -439008.2630859375\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -457440.250000\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -367581.968750\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -444224.843750\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -508948.125000\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -445623.500000\n",
      "    epoch          : 203\n",
      "    loss           : -458772.675625\n",
      "    val_loss       : -433976.4015625\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -419460.750000\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -449465.156250\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -451702.593750\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -428815.562500\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -358961.812500\n",
      "    epoch          : 204\n",
      "    loss           : -418718.3909375\n",
      "    val_loss       : -404048.0369140625\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -388035.031250\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -340780.281250\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -433220.593750\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -474244.875000\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -461166.343750\n",
      "    epoch          : 205\n",
      "    loss           : -436622.4559375\n",
      "    val_loss       : -420983.43046875\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -430224.625000\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -442702.437500\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -349692.375000\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -465231.562500\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -373798.250000\n",
      "    epoch          : 206\n",
      "    loss           : -447972.7853125\n",
      "    val_loss       : -432327.3478515625\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -328163.781250\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -399267.562500\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -398707.562500\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -392888.250000\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -413666.812500\n",
      "    epoch          : 207\n",
      "    loss           : -419963.74875\n",
      "    val_loss       : -434648.5345703125\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -430169.968750\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -493142.812500\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -425443.625000\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -387335.875000\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -377810.312500\n",
      "    epoch          : 208\n",
      "    loss           : -421091.99265625\n",
      "    val_loss       : -415634.130078125\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -509629.000000\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -413500.250000\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -515260.375000\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -372310.187500\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -235348.000000\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   208: reducing learning rate of group 0 to 1.0000e-04.\n",
      "    epoch          : 209\n",
      "    loss           : -399156.57109375\n",
      "    val_loss       : -414899.45625\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -449442.125000\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -446154.437500\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -473630.437500\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -445043.031250\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -499568.687500\n",
      "    epoch          : 210\n",
      "    loss           : -464956.1153125\n",
      "    val_loss       : -456384.2861328125\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -563197.750000\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -372910.875000\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -482910.406250\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -443944.875000\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -463398.156250\n",
      "    epoch          : 211\n",
      "    loss           : -474069.3665625\n",
      "    val_loss       : -456979.3123046875\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -460365.718750\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -480747.343750\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -454945.781250\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -472123.281250\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -471360.906250\n",
      "    epoch          : 212\n",
      "    loss           : -475220.0209375\n",
      "    val_loss       : -459960.4828125\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -455828.375000\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -449805.312500\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -497825.593750\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -466930.750000\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -451620.937500\n",
      "    epoch          : 213\n",
      "    loss           : -475770.4625\n",
      "    val_loss       : -459747.65546875\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -441291.906250\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -505707.218750\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -557532.312500\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -376931.187500\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -451091.812500\n",
      "    epoch          : 214\n",
      "    loss           : -476888.878125\n",
      "    val_loss       : -459398.233203125\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -448678.562500\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -458430.375000\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -464572.937500\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -466948.968750\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -450754.750000\n",
      "    epoch          : 215\n",
      "    loss           : -476893.5340625\n",
      "    val_loss       : -459972.1296875\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -480223.312500\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -486353.875000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -487291.750000\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -564252.625000\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -477560.000000\n",
      "    epoch          : 216\n",
      "    loss           : -477418.7753125\n",
      "    val_loss       : -464256.694921875\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -452128.937500\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -452145.937500\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -501431.656250\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -503843.406250\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -507119.562500\n",
      "    epoch          : 217\n",
      "    loss           : -478047.2990625\n",
      "    val_loss       : -460190.61953125\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -531514.625000\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -574137.875000\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -574666.375000\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -459742.156250\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -484072.781250\n",
      "    epoch          : 218\n",
      "    loss           : -479064.0484375\n",
      "    val_loss       : -462797.0685546875\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -455329.187500\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -485007.562500\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -454342.031250\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -368694.218750\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -505272.312500\n",
      "    epoch          : 219\n",
      "    loss           : -479039.5440625\n",
      "    val_loss       : -463329.373828125\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -386088.062500\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -459191.656250\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -500703.062500\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -457319.875000\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -480095.031250\n",
      "    epoch          : 220\n",
      "    loss           : -479566.19875\n",
      "    val_loss       : -465535.935546875\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -485694.406250\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -453092.250000\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -530121.312500\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -465141.312500\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -473248.875000\n",
      "    epoch          : 221\n",
      "    loss           : -479906.4265625\n",
      "    val_loss       : -464609.783203125\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -463999.406250\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -459859.593750\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -482877.468750\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -504879.812500\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -509219.687500\n",
      "    epoch          : 222\n",
      "    loss           : -480621.025625\n",
      "    val_loss       : -463461.18203125\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -465749.781250\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -455524.906250\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -451900.781250\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -488979.937500\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -452574.156250\n",
      "    epoch          : 223\n",
      "    loss           : -480518.8521875\n",
      "    val_loss       : -464675.1216796875\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -494012.593750\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -476647.281250\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -478927.000000\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -530301.125000\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -488360.000000\n",
      "    epoch          : 224\n",
      "    loss           : -480829.2115625\n",
      "    val_loss       : -466974.648828125\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -536680.625000\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -531182.375000\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -508494.375000\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -483444.562500\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -443933.187500\n",
      "    epoch          : 225\n",
      "    loss           : -482240.3234375\n",
      "    val_loss       : -463030.6798828125\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -379060.125000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -463422.718750\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -566706.500000\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -486587.875000\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -477371.125000\n",
      "    epoch          : 226\n",
      "    loss           : -481358.8753125\n",
      "    val_loss       : -464109.8228515625\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -456142.812500\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -470620.156250\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -530753.062500\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -480978.125000\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -578020.500000\n",
      "    epoch          : 227\n",
      "    loss           : -481415.0690625\n",
      "    val_loss       : -467547.5568359375\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -533790.937500\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -528556.000000\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -482418.781250\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -467873.406250\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -463525.812500\n",
      "    epoch          : 228\n",
      "    loss           : -482067.7253125\n",
      "    val_loss       : -466189.02421875\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -466225.000000\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -391827.437500\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -452801.468750\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -466980.375000\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -491579.937500\n",
      "    epoch          : 229\n",
      "    loss           : -482962.4921875\n",
      "    val_loss       : -467308.409375\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -511745.000000\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -533029.250000\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -469742.937500\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -454829.781250\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -488668.750000\n",
      "    epoch          : 230\n",
      "    loss           : -482685.405\n",
      "    val_loss       : -466725.9283203125\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -483152.375000\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -383936.156250\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -533832.312500\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -516568.281250\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -456271.375000\n",
      "    epoch          : 231\n",
      "    loss           : -482969.878125\n",
      "    val_loss       : -465495.2005859375\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -460588.062500\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -485001.875000\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -455756.187500\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -507018.687500\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -468648.250000\n",
      "    epoch          : 232\n",
      "    loss           : -483839.2875\n",
      "    val_loss       : -466424.5470703125\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -470885.531250\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -508141.718750\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -489242.812500\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -530315.687500\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -484585.125000\n",
      "    epoch          : 233\n",
      "    loss           : -483077.9596875\n",
      "    val_loss       : -466551.566015625\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -479463.812500\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -391912.062500\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -457144.000000\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -457936.062500\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -490212.562500\n",
      "    epoch          : 234\n",
      "    loss           : -483832.94375\n",
      "    val_loss       : -467142.530078125\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -488389.531250\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -458290.812500\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -378031.343750\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -451905.375000\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -471523.812500\n",
      "    epoch          : 235\n",
      "    loss           : -484290.651875\n",
      "    val_loss       : -469442.323046875\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -465250.500000\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -511613.437500\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -485644.218750\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -488322.781250\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -450539.562500\n",
      "    epoch          : 236\n",
      "    loss           : -484212.8971875\n",
      "    val_loss       : -466915.315234375\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -482967.125000\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -453844.718750\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -472688.906250\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -510447.156250\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -491325.781250\n",
      "    epoch          : 237\n",
      "    loss           : -484673.07125\n",
      "    val_loss       : -467508.843359375\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -576727.000000\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -530319.500000\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -571226.937500\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -576937.937500\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -514542.906250\n",
      "    epoch          : 238\n",
      "    loss           : -484990.7684375\n",
      "    val_loss       : -466862.435546875\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -385105.625000\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -580761.750000\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -571462.000000\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -531194.250000\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -456039.468750\n",
      "    epoch          : 239\n",
      "    loss           : -484845.5953125\n",
      "    val_loss       : -467989.4583984375\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -460868.125000\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -532349.062500\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -490381.250000\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -483576.562500\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -580452.687500\n",
      "    epoch          : 240\n",
      "    loss           : -484974.01\n",
      "    val_loss       : -466808.300390625\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -487640.312500\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -464998.687500\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -385147.750000\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -458092.187500\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -492715.593750\n",
      "    epoch          : 241\n",
      "    loss           : -484732.989375\n",
      "    val_loss       : -469205.078515625\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -479337.812500\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -461178.531250\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -483403.187500\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -515948.500000\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -460143.937500\n",
      "    epoch          : 242\n",
      "    loss           : -485491.74\n",
      "    val_loss       : -467374.0609375\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -487204.000000\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -468593.812500\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -587979.250000\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -505758.343750\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -513832.187500\n",
      "    epoch          : 243\n",
      "    loss           : -485681.7234375\n",
      "    val_loss       : -466727.3296875\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -465222.125000\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -498989.625000\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -372539.281250\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -467790.187500\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -484346.312500\n",
      "    epoch          : 244\n",
      "    loss           : -485462.9465625\n",
      "    val_loss       : -467180.473828125\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -484995.812500\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -486384.375000\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -463077.500000\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -488615.375000\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -492178.031250\n",
      "    epoch          : 245\n",
      "    loss           : -486047.546875\n",
      "    val_loss       : -469497.7568359375\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -491389.187500\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -461710.750000\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -536035.125000\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -508312.687500\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -467881.812500\n",
      "    epoch          : 246\n",
      "    loss           : -486420.0125\n",
      "    val_loss       : -469391.593359375\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -489992.687500\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -575808.500000\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -451677.781250\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -466548.468750\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -479578.750000\n",
      "    epoch          : 247\n",
      "    loss           : -486230.37625\n",
      "    val_loss       : -469359.6826171875\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -465654.093750\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -530580.375000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -494188.406250\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -480652.656250\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -469806.312500\n",
      "    epoch          : 248\n",
      "    loss           : -486790.1890625\n",
      "    val_loss       : -467444.0099609375\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -572794.125000\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -471836.093750\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -457149.218750\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -496269.375000\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -461452.250000\n",
      "    epoch          : 249\n",
      "    loss           : -486914.688125\n",
      "    val_loss       : -470014.31015625\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -450958.500000\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -489164.562500\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -533169.125000\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -384404.218750\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -460527.093750\n",
      "    epoch          : 250\n",
      "    loss           : -487056.775\n",
      "    val_loss       : -470114.8498046875\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1027_122354/checkpoint-epoch250.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -461431.343750\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -535295.500000\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -484654.156250\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -487536.562500\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -515834.812500\n",
      "    epoch          : 251\n",
      "    loss           : -486877.76125\n",
      "    val_loss       : -469969.6912109375\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -534925.687500\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -536327.312500\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -513423.218750\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -465577.843750\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -467490.375000\n",
      "    epoch          : 252\n",
      "    loss           : -487694.4571875\n",
      "    val_loss       : -469264.9802734375\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -380718.906250\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -468670.906250\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -581706.500000\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -470993.250000\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -515315.500000\n",
      "    epoch          : 253\n",
      "    loss           : -487417.98375\n",
      "    val_loss       : -467818.462890625\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -467587.281250\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -488045.000000\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -495125.406250\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -457728.437500\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -489385.812500\n",
      "    epoch          : 254\n",
      "    loss           : -487449.01\n",
      "    val_loss       : -472174.24296875\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -489152.062500\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -489831.937500\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -581677.000000\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -513032.562500\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -482804.250000\n",
      "    epoch          : 255\n",
      "    loss           : -488185.66875\n",
      "    val_loss       : -468746.5822265625\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -536837.687500\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -383232.312500\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -578568.937500\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -491712.093750\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -514438.625000\n",
      "    epoch          : 256\n",
      "    loss           : -487806.363125\n",
      "    val_loss       : -471243.50234375\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -455854.125000\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -455520.406250\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -585753.375000\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -490974.906250\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -455984.625000\n",
      "    epoch          : 257\n",
      "    loss           : -488360.865625\n",
      "    val_loss       : -470591.83828125\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -497200.718750\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -461266.000000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -461988.500000\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -479701.312500\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -490587.031250\n",
      "    epoch          : 258\n",
      "    loss           : -488578.5825\n",
      "    val_loss       : -471386.76796875\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -455121.656250\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -515755.000000\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -492879.406250\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -459051.812500\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -485649.875000\n",
      "    epoch          : 259\n",
      "    loss           : -488586.770625\n",
      "    val_loss       : -472984.338671875\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -493530.250000\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -469848.156250\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -474980.656250\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -482354.718750\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -489305.125000\n",
      "    epoch          : 260\n",
      "    loss           : -488577.609375\n",
      "    val_loss       : -471801.7099609375\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -536820.500000\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -537362.250000\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -497226.781250\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -483423.687500\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -497071.000000\n",
      "    epoch          : 261\n",
      "    loss           : -488862.4209375\n",
      "    val_loss       : -470593.1326171875\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -391241.375000\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -539376.687500\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -463425.656250\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -570026.187500\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -484885.281250\n",
      "    epoch          : 262\n",
      "    loss           : -489111.935625\n",
      "    val_loss       : -469808.9548828125\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -493761.250000\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -473893.906250\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -456828.031250\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -511617.031250\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -461827.906250\n",
      "    epoch          : 263\n",
      "    loss           : -489134.8053125\n",
      "    val_loss       : -471324.8142578125\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -477737.250000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -536923.375000\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -481586.625000\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -535244.687500\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -464090.937500\n",
      "    epoch          : 264\n",
      "    loss           : -489401.0096875\n",
      "    val_loss       : -471520.279296875\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -499033.031250\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -389240.531250\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -519969.156250\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -455305.468750\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -463088.250000\n",
      "    epoch          : 265\n",
      "    loss           : -489112.8603125\n",
      "    val_loss       : -470506.8283203125\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -457851.406250\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -469103.031250\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -541909.750000\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -481536.937500\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -490419.906250\n",
      "    epoch          : 266\n",
      "    loss           : -489534.9409375\n",
      "    val_loss       : -471817.137109375\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -512363.250000\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -520250.562500\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -495578.875000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -515583.187500\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -517011.718750\n",
      "    epoch          : 267\n",
      "    loss           : -490176.3609375\n",
      "    val_loss       : -472254.6609375\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -489124.375000\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -516559.750000\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -533932.000000\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -500204.250000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -473371.562500\n",
      "    epoch          : 268\n",
      "    loss           : -489688.28\n",
      "    val_loss       : -473666.615625\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -461899.375000\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -538994.812500\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -492427.781250\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -588516.250000\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -489062.093750\n",
      "    epoch          : 269\n",
      "    loss           : -490105.5540625\n",
      "    val_loss       : -471122.027734375\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -497313.562500\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -493618.750000\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -498294.375000\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -471086.656250\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -468548.906250\n",
      "    epoch          : 270\n",
      "    loss           : -490446.956875\n",
      "    val_loss       : -472169.896484375\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -468565.125000\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -573357.437500\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -489011.093750\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -494798.187500\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -456935.281250\n",
      "    epoch          : 271\n",
      "    loss           : -490479.9809375\n",
      "    val_loss       : -472334.5373046875\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -465715.625000\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -466042.718750\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -517606.875000\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -495385.906250\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -469310.875000\n",
      "    epoch          : 272\n",
      "    loss           : -490543.229375\n",
      "    val_loss       : -470898.8888671875\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -489061.500000\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -492523.562500\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -538086.125000\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -489445.312500\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -472623.937500\n",
      "    epoch          : 273\n",
      "    loss           : -491004.03625\n",
      "    val_loss       : -473129.2896484375\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -536224.562500\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -494381.781250\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -513625.625000\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -388903.156250\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -471314.062500\n",
      "    epoch          : 274\n",
      "    loss           : -490909.6703125\n",
      "    val_loss       : -472995.74921875\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -539566.312500\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -391901.937500\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -520029.281250\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -516758.093750\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -457124.437500\n",
      "    epoch          : 275\n",
      "    loss           : -491726.55625\n",
      "    val_loss       : -472842.700390625\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -584359.312500\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -492841.500000\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -383733.125000\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -522623.531250\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -457612.312500\n",
      "    epoch          : 276\n",
      "    loss           : -491116.4871875\n",
      "    val_loss       : -471797.298828125\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -517783.437500\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -472820.500000\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -387759.750000\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -538976.875000\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -468394.937500\n",
      "    epoch          : 277\n",
      "    loss           : -491474.27625\n",
      "    val_loss       : -471557.28828125\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -389380.093750\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -585053.312500\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -456484.250000\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -494069.937500\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -493844.750000\n",
      "    epoch          : 278\n",
      "    loss           : -492170.209375\n",
      "    val_loss       : -473403.210546875\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -541463.625000\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -540375.000000\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -388144.812500\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -494690.718750\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -498572.687500\n",
      "    epoch          : 279\n",
      "    loss           : -491839.228125\n",
      "    val_loss       : -473148.3130859375\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -455168.937500\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -474329.000000\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -500059.437500\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -468315.843750\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -485864.562500\n",
      "    epoch          : 280\n",
      "    loss           : -491964.4703125\n",
      "    val_loss       : -473669.0654296875\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -493152.968750\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -498985.843750\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -395014.000000\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -472971.031250\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -485645.031250\n",
      "    epoch          : 281\n",
      "    loss           : -492281.2640625\n",
      "    val_loss       : -472979.5998046875\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -543184.062500\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -392886.750000\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -466746.812500\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -460618.125000\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -493417.843750\n",
      "    epoch          : 282\n",
      "    loss           : -492463.313125\n",
      "    val_loss       : -473440.0806640625\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -466802.937500\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -536927.687500\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -391670.687500\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -490115.625000\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -497853.500000\n",
      "    epoch          : 283\n",
      "    loss           : -492325.5703125\n",
      "    val_loss       : -473583.2576171875\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -470215.906250\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -492005.125000\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -389301.000000\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -540665.500000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -538476.125000\n",
      "    epoch          : 284\n",
      "    loss           : -492236.8396875\n",
      "    val_loss       : -473870.196484375\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -493921.250000\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -581912.125000\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -481008.000000\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -588538.250000\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -580322.625000\n",
      "    epoch          : 285\n",
      "    loss           : -492415.4028125\n",
      "    val_loss       : -476061.3873046875\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -580495.187500\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -578490.750000\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -476912.156250\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -463086.718750\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -496924.125000\n",
      "    epoch          : 286\n",
      "    loss           : -492446.0834375\n",
      "    val_loss       : -473279.7953125\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -474926.031250\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -491511.062500\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -527590.375000\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -478330.718750\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -522457.187500\n",
      "    epoch          : 287\n",
      "    loss           : -493010.2628125\n",
      "    val_loss       : -473344.9908203125\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -543901.625000\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -543002.750000\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -497927.968750\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -494807.062500\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -467932.687500\n",
      "    epoch          : 288\n",
      "    loss           : -492909.70125\n",
      "    val_loss       : -473687.2681640625\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -382228.468750\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -518524.937500\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -497270.875000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -537686.750000\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -470873.187500\n",
      "    epoch          : 289\n",
      "    loss           : -492830.8984375\n",
      "    val_loss       : -474946.8140625\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -582575.750000\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -477163.406250\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -517416.562500\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -520966.750000\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -496912.906250\n",
      "    epoch          : 290\n",
      "    loss           : -492742.128125\n",
      "    val_loss       : -473492.203125\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -470203.437500\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -395410.187500\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -460233.125000\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -473589.343750\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -469173.406250\n",
      "    epoch          : 291\n",
      "    loss           : -493331.845\n",
      "    val_loss       : -473376.2474609375\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -580283.062500\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -591005.375000\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -480623.562500\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -540851.812500\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -493844.000000\n",
      "    epoch          : 292\n",
      "    loss           : -493498.7209375\n",
      "    val_loss       : -473957.9654296875\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -460026.093750\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -519531.312500\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -467623.562500\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -523591.750000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -490629.187500\n",
      "    epoch          : 293\n",
      "    loss           : -493303.169375\n",
      "    val_loss       : -474061.7072265625\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -583065.250000\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -473260.125000\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -492223.968750\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -466775.062500\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -522769.062500\n",
      "    epoch          : 294\n",
      "    loss           : -493868.9278125\n",
      "    val_loss       : -474039.7693359375\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -490984.875000\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -518639.718750\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -495610.437500\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -467969.718750\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -471341.937500\n",
      "    epoch          : 295\n",
      "    loss           : -494131.665\n",
      "    val_loss       : -475499.857421875\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -498504.812500\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -503957.906250\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -478686.968750\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -399023.875000\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -473337.375000\n",
      "    epoch          : 296\n",
      "    loss           : -493936.606875\n",
      "    val_loss       : -475582.776171875\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -514889.375000\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -479544.156250\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -516814.250000\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -520071.843750\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -462510.656250\n",
      "    epoch          : 297\n",
      "    loss           : -493698.7453125\n",
      "    val_loss       : -476481.54921875\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -475157.750000\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -390685.437500\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -543410.625000\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -495081.468750\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -468947.187500\n",
      "    epoch          : 298\n",
      "    loss           : -494196.73875\n",
      "    val_loss       : -474241.7814453125\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -467970.781250\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -494287.625000\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -464944.812500\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -546659.437500\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -470322.718750\n",
      "    epoch          : 299\n",
      "    loss           : -494165.795\n",
      "    val_loss       : -474979.50625\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -492786.156250\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -389331.125000\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -497525.750000\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -475711.000000\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -471636.437500\n",
      "    epoch          : 300\n",
      "    loss           : -494623.8375\n",
      "    val_loss       : -475238.9146484375\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1027_122354/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -583127.375000\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -483018.312500\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -474474.031250\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -500272.031250\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -472709.375000\n",
      "    epoch          : 301\n",
      "    loss           : -494666.25875\n",
      "    val_loss       : -473878.7265625\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -392506.218750\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -497445.156250\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -394492.906250\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -589745.000000\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -518689.187500\n",
      "    epoch          : 302\n",
      "    loss           : -494569.1915625\n",
      "    val_loss       : -476268.0080078125\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -393905.125000\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -491491.562500\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -389767.406250\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -591149.312500\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -465454.218750\n",
      "    epoch          : 303\n",
      "    loss           : -494482.528125\n",
      "    val_loss       : -476734.1140625\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -477108.406250\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -543342.187500\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -545768.000000\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -402018.187500\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -495444.281250\n",
      "    epoch          : 304\n",
      "    loss           : -494948.1503125\n",
      "    val_loss       : -475628.82265625\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -502184.500000\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -470838.312500\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -463722.531250\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -544921.250000\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -468386.375000\n",
      "    epoch          : 305\n",
      "    loss           : -495159.943125\n",
      "    val_loss       : -477904.5771484375\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -469686.062500\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -469608.531250\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -481778.875000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -501967.968750\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -465010.125000\n",
      "    epoch          : 306\n",
      "    loss           : -495490.5603125\n",
      "    val_loss       : -476749.555078125\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -395287.375000\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -478528.343750\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -486958.625000\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -524978.000000\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -471124.000000\n",
      "    epoch          : 307\n",
      "    loss           : -495724.625625\n",
      "    val_loss       : -475795.3689453125\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -478647.562500\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -495201.187500\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -542221.187500\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -493369.687500\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -469232.625000\n",
      "    epoch          : 308\n",
      "    loss           : -495817.631875\n",
      "    val_loss       : -475885.231640625\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -484252.312500\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -515567.156250\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -473277.937500\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -594201.750000\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -470093.406250\n",
      "    epoch          : 309\n",
      "    loss           : -495946.1125\n",
      "    val_loss       : -476832.7021484375\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -395861.062500\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -482807.125000\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -498519.968750\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -522689.437500\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -470846.937500\n",
      "    epoch          : 310\n",
      "    loss           : -495465.0971875\n",
      "    val_loss       : -476612.02265625\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -496121.156250\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -526874.875000\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -468770.500000\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -396976.437500\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -478278.593750\n",
      "    epoch          : 311\n",
      "    loss           : -496318.828125\n",
      "    val_loss       : -478399.2416015625\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -501676.312500\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -499849.156250\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -544293.125000\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -492172.062500\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -523639.312500\n",
      "    epoch          : 312\n",
      "    loss           : -496275.5909375\n",
      "    val_loss       : -477363.7677734375\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -504875.468750\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -470914.187500\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -596194.437500\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -494132.250000\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -501167.812500\n",
      "    epoch          : 313\n",
      "    loss           : -496133.590625\n",
      "    val_loss       : -477070.2046875\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -503522.250000\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -481848.281250\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -461165.187500\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -464534.281250\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -474027.062500\n",
      "    epoch          : 314\n",
      "    loss           : -496204.8025\n",
      "    val_loss       : -479210.1537109375\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -481969.406250\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -475748.812500\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -487952.875000\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -485499.312500\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -464254.062500\n",
      "    epoch          : 315\n",
      "    loss           : -496849.48375\n",
      "    val_loss       : -477232.1392578125\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -545517.750000\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -500967.656250\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -498863.250000\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -506262.500000\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -493638.187500\n",
      "    epoch          : 316\n",
      "    loss           : -496432.7715625\n",
      "    val_loss       : -477158.7607421875\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -482469.281250\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -598113.562500\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -521183.125000\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -543287.000000\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -495947.250000\n",
      "    epoch          : 317\n",
      "    loss           : -497092.1003125\n",
      "    val_loss       : -477576.95859375\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -496421.875000\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -493907.343750\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -396002.781250\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -479209.187500\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -523637.062500\n",
      "    epoch          : 318\n",
      "    loss           : -496648.8446875\n",
      "    val_loss       : -476626.0498046875\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -493312.625000\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -537596.750000\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -585547.125000\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -465665.843750\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -499869.812500\n",
      "    epoch          : 319\n",
      "    loss           : -497125.74\n",
      "    val_loss       : -476925.1470703125\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -395534.718750\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -479211.531250\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -480718.343750\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -498916.843750\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -462756.843750\n",
      "    epoch          : 320\n",
      "    loss           : -497107.690625\n",
      "    val_loss       : -476706.2462890625\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -546254.187500\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -490234.093750\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -397868.218750\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -473604.281250\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -467801.312500\n",
      "    epoch          : 321\n",
      "    loss           : -497292.4215625\n",
      "    val_loss       : -478631.9962890625\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -502521.375000\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -520687.000000\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -527372.312500\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -587261.187500\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -473702.031250\n",
      "    epoch          : 322\n",
      "    loss           : -497130.5196875\n",
      "    val_loss       : -477757.076171875\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -498193.281250\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -546367.250000\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -476400.875000\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -547428.937500\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -499842.875000\n",
      "    epoch          : 323\n",
      "    loss           : -497174.583125\n",
      "    val_loss       : -478309.1517578125\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -591823.375000\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -589155.062500\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -496267.406250\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -474091.062500\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -496546.000000\n",
      "    epoch          : 324\n",
      "    loss           : -497762.116875\n",
      "    val_loss       : -476847.4935546875\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -588750.000000\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -524571.375000\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -543918.625000\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -502251.031250\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -494355.250000\n",
      "    epoch          : 325\n",
      "    loss           : -497271.395\n",
      "    val_loss       : -478741.6400390625\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -525714.250000\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -590369.750000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -500413.062500\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -471655.437500\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -499521.656250\n",
      "    epoch          : 326\n",
      "    loss           : -497875.545625\n",
      "    val_loss       : -476815.0599609375\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -482873.718750\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -505665.187500\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -520297.062500\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -544072.125000\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -457758.562500\n",
      "    epoch          : 327\n",
      "    loss           : -498237.8775\n",
      "    val_loss       : -479080.630859375\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -546169.812500\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -549106.687500\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -521588.593750\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -526192.937500\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -471566.312500\n",
      "    epoch          : 328\n",
      "    loss           : -498443.2146875\n",
      "    val_loss       : -478816.4732421875\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -477645.843750\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -546878.250000\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -524057.531250\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -597326.375000\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -528364.875000\n",
      "    epoch          : 329\n",
      "    loss           : -498460.9728125\n",
      "    val_loss       : -477650.3357421875\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -499853.843750\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -498785.125000\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -601208.250000\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -463809.875000\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -476056.375000\n",
      "    epoch          : 330\n",
      "    loss           : -498410.70625\n",
      "    val_loss       : -478262.2603515625\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -542999.500000\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -500840.437500\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -461797.406250\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -465052.500000\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -470145.468750\n",
      "    epoch          : 331\n",
      "    loss           : -498574.5178125\n",
      "    val_loss       : -479185.92734375\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -502191.281250\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -474762.187500\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -483702.156250\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -401673.000000\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -465105.968750\n",
      "    epoch          : 332\n",
      "    loss           : -498956.3871875\n",
      "    val_loss       : -479956.221875\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -546262.125000\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -477334.718750\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -472813.875000\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -524719.250000\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -474112.937500\n",
      "    epoch          : 333\n",
      "    loss           : -498958.4675\n",
      "    val_loss       : -478483.359375\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -593518.875000\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -503213.875000\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -596475.875000\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -499684.437500\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -501580.687500\n",
      "    epoch          : 334\n",
      "    loss           : -499378.1403125\n",
      "    val_loss       : -478861.610546875\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -524427.250000\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -502368.343750\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -398683.781250\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -395653.750000\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -474129.250000\n",
      "    epoch          : 335\n",
      "    loss           : -498759.303125\n",
      "    val_loss       : -477317.0390625\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -501033.000000\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -502499.562500\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -505548.937500\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -596331.250000\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -476444.687500\n",
      "    epoch          : 336\n",
      "    loss           : -498946.6140625\n",
      "    val_loss       : -478399.1845703125\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -528239.437500\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -521985.812500\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -394742.937500\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -526255.312500\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -480987.406250\n",
      "    epoch          : 337\n",
      "    loss           : -499228.175\n",
      "    val_loss       : -478832.94375\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -597410.812500\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -595083.312500\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -505352.875000\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -507130.625000\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -522527.593750\n",
      "    epoch          : 338\n",
      "    loss           : -499219.5978125\n",
      "    val_loss       : -479314.4365234375\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -549311.250000\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -477383.187500\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -502028.625000\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -591832.125000\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -527926.875000\n",
      "    epoch          : 339\n",
      "    loss           : -499212.1196875\n",
      "    val_loss       : -479458.25625\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -527327.437500\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -498540.406250\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -470593.687500\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -390575.843750\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -478987.000000\n",
      "    epoch          : 340\n",
      "    loss           : -499546.3234375\n",
      "    val_loss       : -478768.881640625\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -474430.343750\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -546645.312500\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -466988.156250\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -500371.750000\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -496864.687500\n",
      "    epoch          : 341\n",
      "    loss           : -500031.220625\n",
      "    val_loss       : -478900.164453125\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -471699.062500\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -395959.031250\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -470854.062500\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -480813.375000\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -499880.656250\n",
      "    epoch          : 342\n",
      "    loss           : -500293.831875\n",
      "    val_loss       : -478857.5033203125\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -551713.125000\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -472075.687500\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -498110.687500\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -472508.312500\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -528928.250000\n",
      "    epoch          : 343\n",
      "    loss           : -499846.804375\n",
      "    val_loss       : -478963.3359375\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -391798.687500\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -595137.750000\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -495891.031250\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -470974.781250\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -498305.593750\n",
      "    epoch          : 344\n",
      "    loss           : -500243.419375\n",
      "    val_loss       : -479678.85078125\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -386383.781250\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -548653.187500\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -595330.562500\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -512001.562500\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -477963.187500\n",
      "    epoch          : 345\n",
      "    loss           : -500058.8940625\n",
      "    val_loss       : -480317.8068359375\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -396817.031250\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -529034.375000\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -507257.000000\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -501030.437500\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -481022.500000\n",
      "    epoch          : 346\n",
      "    loss           : -500510.7490625\n",
      "    val_loss       : -479924.3150390625\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -499729.312500\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -551230.375000\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -506098.500000\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -527597.250000\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -482177.937500\n",
      "    epoch          : 347\n",
      "    loss           : -500233.5078125\n",
      "    val_loss       : -480839.3431640625\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -597390.500000\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -468687.750000\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -484419.500000\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -483964.593750\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -535798.500000\n",
      "    epoch          : 348\n",
      "    loss           : -500911.0384375\n",
      "    val_loss       : -480434.725\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -396391.562500\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -498943.187500\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -395681.718750\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -595301.062500\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -501338.437500\n",
      "    epoch          : 349\n",
      "    loss           : -500506.3265625\n",
      "    val_loss       : -480820.7673828125\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -503568.000000\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -529332.312500\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -469149.375000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -471927.281250\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -503839.625000\n",
      "    epoch          : 350\n",
      "    loss           : -500652.7459375\n",
      "    val_loss       : -479864.7265625\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1027_122354/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -509021.843750\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -551612.437500\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -504206.062500\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -596863.500000\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -530462.437500\n",
      "    epoch          : 351\n",
      "    loss           : -500896.366875\n",
      "    val_loss       : -482156.31171875\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -483874.718750\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -482723.343750\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -497608.937500\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -473742.312500\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -473864.750000\n",
      "    epoch          : 352\n",
      "    loss           : -501066.8425\n",
      "    val_loss       : -480419.5447265625\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -597379.250000\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -556445.562500\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -592872.937500\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -476590.312500\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -531576.625000\n",
      "    epoch          : 353\n",
      "    loss           : -500968.47\n",
      "    val_loss       : -480913.7013671875\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -504336.281250\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -471566.625000\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -402947.656250\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -596141.562500\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -533814.062500\n",
      "    epoch          : 354\n",
      "    loss           : -501683.125\n",
      "    val_loss       : -480038.78125\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -519713.906250\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -401321.656250\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -482282.250000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -406577.781250\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -485102.156250\n",
      "    epoch          : 355\n",
      "    loss           : -500950.845\n",
      "    val_loss       : -480377.28984375\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -497321.968750\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -507127.625000\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -500787.250000\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -475487.750000\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -502212.437500\n",
      "    epoch          : 356\n",
      "    loss           : -501623.0015625\n",
      "    val_loss       : -481817.1486328125\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -498788.218750\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -507547.343750\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -501773.687500\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -473873.125000\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -496299.687500\n",
      "    epoch          : 357\n",
      "    loss           : -501553.7096875\n",
      "    val_loss       : -480195.749609375\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -548738.375000\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -504642.312500\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -479712.500000\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -477115.687500\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -593377.875000\n",
      "    epoch          : 358\n",
      "    loss           : -501955.1596875\n",
      "    val_loss       : -480318.313671875\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -399805.500000\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -501301.562500\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -547772.687500\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -476655.312500\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -502211.187500\n",
      "    epoch          : 359\n",
      "    loss           : -501806.51375\n",
      "    val_loss       : -481038.4853515625\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -493542.312500\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -503300.062500\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -592776.187500\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -534371.125000\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -463798.468750\n",
      "    epoch          : 360\n",
      "    loss           : -502167.31375\n",
      "    val_loss       : -480167.0708984375\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -524976.687500\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -471617.687500\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -539268.875000\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -470249.437500\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -479082.218750\n",
      "    epoch          : 361\n",
      "    loss           : -502471.3421875\n",
      "    val_loss       : -481361.0658203125\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -531163.125000\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -474078.312500\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -550275.687500\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -397755.750000\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -470721.312500\n",
      "    epoch          : 362\n",
      "    loss           : -502281.379375\n",
      "    val_loss       : -482747.2998046875\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -481369.593750\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -399912.687500\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -468948.375000\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -528528.250000\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -479180.531250\n",
      "    epoch          : 363\n",
      "    loss           : -502365.9228125\n",
      "    val_loss       : -481044.4193359375\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -499690.312500\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -548469.500000\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -465101.312500\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -504728.656250\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -470997.843750\n",
      "    epoch          : 364\n",
      "    loss           : -502291.6078125\n",
      "    val_loss       : -480399.9625\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -549598.000000\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -599550.250000\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -482503.406250\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -476879.875000\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -498889.312500\n",
      "    epoch          : 365\n",
      "    loss           : -502443.6053125\n",
      "    val_loss       : -481586.160546875\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -503328.156250\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -598146.875000\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -526535.062500\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -599118.375000\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -476798.187500\n",
      "    epoch          : 366\n",
      "    loss           : -502744.9859375\n",
      "    val_loss       : -481747.38984375\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -597727.437500\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -508835.437500\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -482354.375000\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -529561.000000\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -479056.062500\n",
      "    epoch          : 367\n",
      "    loss           : -502754.3715625\n",
      "    val_loss       : -481232.81953125\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -548737.250000\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -478807.062500\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -473815.593750\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -500179.375000\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -507746.312500\n",
      "    epoch          : 368\n",
      "    loss           : -502779.7215625\n",
      "    val_loss       : -482432.5423828125\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -394929.062500\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -476978.343750\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -478279.562500\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -465714.375000\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -397929.218750\n",
      "    epoch          : 369\n",
      "    loss           : -503417.184375\n",
      "    val_loss       : -482520.0259765625\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -505960.031250\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -536631.937500\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -509606.250000\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -484103.625000\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -506277.500000\n",
      "    epoch          : 370\n",
      "    loss           : -502765.4371875\n",
      "    val_loss       : -480441.2619140625\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -480990.625000\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -555416.375000\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -475561.312500\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -482916.125000\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -501680.593750\n",
      "    epoch          : 371\n",
      "    loss           : -503242.63625\n",
      "    val_loss       : -483117.6345703125\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -602295.312500\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -596042.500000\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -531104.875000\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -481715.156250\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -472971.843750\n",
      "    epoch          : 372\n",
      "    loss           : -502986.9515625\n",
      "    val_loss       : -481330.341796875\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -597111.812500\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -475449.875000\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -479836.187500\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -536739.187500\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -475411.218750\n",
      "    epoch          : 373\n",
      "    loss           : -503497.3815625\n",
      "    val_loss       : -482053.0375\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -481247.187500\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -477596.500000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -488342.812500\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -475424.937500\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -526436.375000\n",
      "    epoch          : 374\n",
      "    loss           : -503922.0171875\n",
      "    val_loss       : -483080.9376953125\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -470792.812500\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -551729.500000\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -509189.343750\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -597142.687500\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -503681.437500\n",
      "    epoch          : 375\n",
      "    loss           : -503239.1565625\n",
      "    val_loss       : -482184.6943359375\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -531603.625000\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -504277.093750\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -477709.062500\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -507243.437500\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -480043.875000\n",
      "    epoch          : 376\n",
      "    loss           : -503709.1271875\n",
      "    val_loss       : -481826.6306640625\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -401250.531250\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -401007.437500\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -400259.687500\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -529143.250000\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -504312.375000\n",
      "    epoch          : 377\n",
      "    loss           : -503292.16\n",
      "    val_loss       : -482325.7712890625\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -527708.625000\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -482535.781250\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -485021.562500\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -549959.187500\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -478809.156250\n",
      "    epoch          : 378\n",
      "    loss           : -504090.0346875\n",
      "    val_loss       : -480878.9134765625\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -588426.062500\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -486546.968750\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -504699.562500\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -507845.312500\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -481609.312500\n",
      "    epoch          : 379\n",
      "    loss           : -503993.1415625\n",
      "    val_loss       : -482355.1388671875\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -485624.531250\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -534544.562500\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -597894.000000\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -481288.437500\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -503260.437500\n",
      "    epoch          : 380\n",
      "    loss           : -503737.775625\n",
      "    val_loss       : -483537.24453125\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -511502.187500\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -505017.843750\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -476840.937500\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -484742.218750\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -480820.781250\n",
      "    epoch          : 381\n",
      "    loss           : -504397.435\n",
      "    val_loss       : -481734.4708984375\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -501231.468750\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -402841.500000\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -603193.625000\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -406174.218750\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -509061.000000\n",
      "    epoch          : 382\n",
      "    loss           : -504286.170625\n",
      "    val_loss       : -483434.175390625\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -488754.125000\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -507193.562500\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -554496.625000\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -396411.031250\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -529619.312500\n",
      "    epoch          : 383\n",
      "    loss           : -504279.0803125\n",
      "    val_loss       : -483903.410546875\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -554951.312500\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -604729.250000\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -529992.375000\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -531129.000000\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -536192.250000\n",
      "    epoch          : 384\n",
      "    loss           : -504353.4021875\n",
      "    val_loss       : -483535.7576171875\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -500892.187500\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -478756.218750\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -508920.250000\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -599641.375000\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -599469.125000\n",
      "    epoch          : 385\n",
      "    loss           : -504950.534375\n",
      "    val_loss       : -482898.533984375\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -602792.625000\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -598440.500000\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -477672.812500\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -506608.000000\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -508278.156250\n",
      "    epoch          : 386\n",
      "    loss           : -504749.9984375\n",
      "    val_loss       : -483839.5671875\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -513262.875000\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -478544.187500\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -509207.125000\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -551547.437500\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -533982.937500\n",
      "    epoch          : 387\n",
      "    loss           : -505140.64375\n",
      "    val_loss       : -483615.4578125\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -553281.000000\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -607245.000000\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -508447.312500\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -511150.718750\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -510182.218750\n",
      "    epoch          : 388\n",
      "    loss           : -504934.7440625\n",
      "    val_loss       : -484860.0158203125\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -509380.812500\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -475731.250000\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -514087.562500\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -502140.375000\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -477529.406250\n",
      "    epoch          : 389\n",
      "    loss           : -505269.7278125\n",
      "    val_loss       : -484481.5349609375\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -500132.781250\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -510389.468750\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -481339.468750\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -511660.093750\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -527051.250000\n",
      "    epoch          : 390\n",
      "    loss           : -505092.35875\n",
      "    val_loss       : -483328.3505859375\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -558956.937500\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -535209.500000\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -550429.375000\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -481663.750000\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -483014.750000\n",
      "    epoch          : 391\n",
      "    loss           : -505712.71875\n",
      "    val_loss       : -483013.046484375\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -531279.250000\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -506081.156250\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -475273.031250\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -503939.343750\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -510081.875000\n",
      "    epoch          : 392\n",
      "    loss           : -505365.7121875\n",
      "    val_loss       : -484028.9271484375\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -603673.500000\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -603826.687500\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -556951.125000\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -487343.187500\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -533551.375000\n",
      "    epoch          : 393\n",
      "    loss           : -505446.589375\n",
      "    val_loss       : -482470.3796875\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -479267.375000\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -404771.312500\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -478738.250000\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -509913.312500\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -535927.000000\n",
      "    epoch          : 394\n",
      "    loss           : -505564.953125\n",
      "    val_loss       : -483703.713671875\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -535565.875000\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -476796.343750\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -512892.031250\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -507043.062500\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -511271.875000\n",
      "    epoch          : 395\n",
      "    loss           : -505688.6753125\n",
      "    val_loss       : -484333.9328125\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -510590.500000\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -475766.406250\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -504741.500000\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -473675.531250\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -478229.156250\n",
      "    epoch          : 396\n",
      "    loss           : -505745.896875\n",
      "    val_loss       : -484038.6576171875\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -507317.156250\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -478709.625000\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -517771.781250\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -481220.156250\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -554281.375000\n",
      "    epoch          : 397\n",
      "    loss           : -506220.4325\n",
      "    val_loss       : -483803.7869140625\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -515924.906250\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -476108.468750\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -547965.375000\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -506508.750000\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -535938.312500\n",
      "    epoch          : 398\n",
      "    loss           : -506253.830625\n",
      "    val_loss       : -484084.2642578125\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -511634.281250\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -595545.625000\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -504537.812500\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -509561.656250\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -481483.312500\n",
      "    epoch          : 399\n",
      "    loss           : -506107.8325\n",
      "    val_loss       : -483353.5837890625\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -602569.625000\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -588415.875000\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -483333.812500\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -503707.937500\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -470537.125000\n",
      "    epoch          : 400\n",
      "    loss           : -506270.8378125\n",
      "    val_loss       : -485671.3283203125\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1027_122354/checkpoint-epoch400.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -539001.500000\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -477650.437500\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -489778.625000\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -598214.500000\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -474537.468750\n",
      "    epoch          : 401\n",
      "    loss           : -506772.7340625\n",
      "    val_loss       : -483340.1341796875\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -403724.062500\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -483160.437500\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -510166.000000\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -601046.250000\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -599740.125000\n",
      "    epoch          : 402\n",
      "    loss           : -506879.439375\n",
      "    val_loss       : -485550.28203125\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -600569.375000\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -484740.156250\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -399680.812500\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -538143.375000\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -485735.187500\n",
      "    epoch          : 403\n",
      "    loss           : -506706.755625\n",
      "    val_loss       : -483384.67890625\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -502795.968750\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -402881.000000\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -535768.250000\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -532778.000000\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -476555.000000\n",
      "    epoch          : 404\n",
      "    loss           : -506750.9915625\n",
      "    val_loss       : -483449.2103515625\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -488492.281250\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -511365.750000\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -511228.812500\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -508636.781250\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -477394.156250\n",
      "    epoch          : 405\n",
      "    loss           : -506848.69875\n",
      "    val_loss       : -484406.613671875\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -508669.125000\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -508403.593750\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -480935.500000\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -488075.187500\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -598047.937500\n",
      "    epoch          : 406\n",
      "    loss           : -506996.0421875\n",
      "    val_loss       : -485901.0921875\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -491782.343750\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -598828.625000\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -516305.375000\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -489479.968750\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -531649.562500\n",
      "    epoch          : 407\n",
      "    loss           : -507348.0103125\n",
      "    val_loss       : -485134.4419921875\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -510022.000000\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -556353.125000\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -407892.687500\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -596163.375000\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -535086.312500\n",
      "    epoch          : 408\n",
      "    loss           : -507359.73875\n",
      "    val_loss       : -485981.5615234375\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -511010.968750\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -503885.406250\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -489038.187500\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -595461.812500\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -537963.375000\n",
      "    epoch          : 409\n",
      "    loss           : -507149.9403125\n",
      "    val_loss       : -485225.3126953125\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -482805.687500\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -485543.593750\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -492041.187500\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -466133.187500\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -400246.843750\n",
      "    epoch          : 410\n",
      "    loss           : -507383.0084375\n",
      "    val_loss       : -484417.0650390625\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -510295.312500\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -559490.687500\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -512826.687500\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -510818.125000\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -509965.968750\n",
      "    epoch          : 411\n",
      "    loss           : -507666.1190625\n",
      "    val_loss       : -485312.124609375\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -503724.250000\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -492584.375000\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -560035.500000\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -534913.375000\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -512255.375000\n",
      "    epoch          : 412\n",
      "    loss           : -507396.755625\n",
      "    val_loss       : -484881.9\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -503466.937500\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -506899.000000\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -556736.437500\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -595135.250000\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -480978.031250\n",
      "    epoch          : 413\n",
      "    loss           : -507299.571875\n",
      "    val_loss       : -484535.783203125\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -470191.562500\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -600010.250000\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -512918.437500\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -506173.062500\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -492797.687500\n",
      "    epoch          : 414\n",
      "    loss           : -507728.7609375\n",
      "    val_loss       : -486280.1673828125\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -601632.187500\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -480921.281250\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -508145.000000\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -511541.156250\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -502710.968750\n",
      "    epoch          : 415\n",
      "    loss           : -507520.6459375\n",
      "    val_loss       : -487353.9490234375\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -557055.875000\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -507555.312500\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -512454.281250\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -506593.000000\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -509461.906250\n",
      "    epoch          : 416\n",
      "    loss           : -507974.220625\n",
      "    val_loss       : -486922.19609375\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -505619.875000\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -558451.750000\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -411636.687500\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -479491.781250\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -516321.406250\n",
      "    epoch          : 417\n",
      "    loss           : -507743.7403125\n",
      "    val_loss       : -487552.1767578125\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -481078.843750\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -531930.187500\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -557868.812500\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -475202.656250\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -510860.593750\n",
      "    epoch          : 418\n",
      "    loss           : -508248.3921875\n",
      "    val_loss       : -488460.39140625\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -409085.718750\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -407890.312500\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -473292.375000\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -561700.062500\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -594216.625000\n",
      "    epoch          : 419\n",
      "    loss           : -508064.1828125\n",
      "    val_loss       : -484311.82734375\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -483656.656250\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -483975.093750\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -474963.125000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -403433.062500\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -471612.312500\n",
      "    epoch          : 420\n",
      "    loss           : -508101.5734375\n",
      "    val_loss       : -486949.16484375\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -409874.375000\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -509330.781250\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -600466.250000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -503462.937500\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -483215.406250\n",
      "    epoch          : 421\n",
      "    loss           : -508788.148125\n",
      "    val_loss       : -485883.5453125\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -509388.156250\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -535041.062500\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -480756.093750\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -483024.750000\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -539037.000000\n",
      "    epoch          : 422\n",
      "    loss           : -508614.1759375\n",
      "    val_loss       : -485541.6576171875\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -557851.625000\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -563838.437500\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -507370.281250\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -510046.187500\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -478815.375000\n",
      "    epoch          : 423\n",
      "    loss           : -508469.95625\n",
      "    val_loss       : -484888.9625\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -478132.125000\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -561582.000000\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -480446.812500\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -507268.343750\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -509585.000000\n",
      "    epoch          : 424\n",
      "    loss           : -508738.305\n",
      "    val_loss       : -485149.0994140625\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -510319.968750\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -562856.937500\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -537029.125000\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -559458.750000\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -528742.375000\n",
      "    epoch          : 425\n",
      "    loss           : -509232.12375\n",
      "    val_loss       : -486594.8984375\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -482182.125000\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -559402.062500\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -508282.093750\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -530843.562500\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -506507.843750\n",
      "    epoch          : 426\n",
      "    loss           : -508656.4496875\n",
      "    val_loss       : -485709.00546875\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -488756.250000\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -509183.281250\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -602971.187500\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -536486.937500\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -610690.937500\n",
      "    epoch          : 427\n",
      "    loss           : -509156.5603125\n",
      "    val_loss       : -485659.630078125\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -605980.125000\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -611290.625000\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -481018.812500\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -555798.750000\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -479921.406250\n",
      "    epoch          : 428\n",
      "    loss           : -509296.5471875\n",
      "    val_loss       : -486295.600390625\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -405288.906250\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -486160.093750\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -558393.375000\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -491097.500000\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -514771.281250\n",
      "    epoch          : 429\n",
      "    loss           : -508905.865\n",
      "    val_loss       : -487592.441015625\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -540492.937500\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -611599.625000\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -513378.156250\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -404665.875000\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -609019.437500\n",
      "    epoch          : 430\n",
      "    loss           : -509163.163125\n",
      "    val_loss       : -486328.7255859375\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -557234.750000\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -515679.343750\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -397297.468750\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -513926.343750\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -540639.000000\n",
      "    epoch          : 431\n",
      "    loss           : -509293.2975\n",
      "    val_loss       : -487149.407421875\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -408253.343750\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -606562.875000\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -536873.250000\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -480242.875000\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -478865.968750\n",
      "    epoch          : 432\n",
      "    loss           : -509793.951875\n",
      "    val_loss       : -486131.8529296875\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -601546.562500\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -501774.468750\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -554788.937500\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -532510.625000\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -483419.562500\n",
      "    epoch          : 433\n",
      "    loss           : -509569.42625\n",
      "    val_loss       : -487562.801171875\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -543660.875000\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -536193.875000\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -479344.562500\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -558630.812500\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -595896.187500\n",
      "    epoch          : 434\n",
      "    loss           : -510198.016875\n",
      "    val_loss       : -485598.24765625\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -509129.562500\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -400150.000000\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -539467.937500\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -539345.937500\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -478922.531250\n",
      "    epoch          : 435\n",
      "    loss           : -510004.7859375\n",
      "    val_loss       : -487789.271484375\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -487601.375000\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -532123.125000\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -510896.906250\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -560941.312500\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -486471.375000\n",
      "    epoch          : 436\n",
      "    loss           : -509998.4815625\n",
      "    val_loss       : -487524.6267578125\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -559342.937500\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -515309.437500\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -606199.250000\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -474999.125000\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -479016.250000\n",
      "    epoch          : 437\n",
      "    loss           : -510151.3096875\n",
      "    val_loss       : -487684.5482421875\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -494859.312500\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -504613.156250\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -476502.750000\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -511419.562500\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -480582.187500\n",
      "    epoch          : 438\n",
      "    loss           : -509921.9171875\n",
      "    val_loss       : -487483.6236328125\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -399146.375000\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -406518.843750\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -487235.406250\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -515105.593750\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -476739.593750\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "    epoch          : 439\n",
      "    loss           : -510446.0434375\n",
      "    val_loss       : -487972.3234375\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -482628.906250\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -608434.562500\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -518002.500000\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -525050.687500\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -515512.656250\n",
      "    epoch          : 440\n",
      "    loss           : -510690.8965625\n",
      "    val_loss       : -488853.373046875\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -404449.187500\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -539462.875000\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -487484.375000\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -513564.312500\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -605427.625000\n",
      "    epoch          : 441\n",
      "    loss           : -510774.0003125\n",
      "    val_loss       : -486863.9259765625\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -486763.000000\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -488316.875000\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -533738.500000\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -560356.750000\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -482775.812500\n",
      "    epoch          : 442\n",
      "    loss           : -510781.9946875\n",
      "    val_loss       : -486215.3830078125\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -514740.000000\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -510156.625000\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -488508.843750\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -539621.625000\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -483696.031250\n",
      "    epoch          : 443\n",
      "    loss           : -510950.9409375\n",
      "    val_loss       : -488472.5548828125\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -487119.437500\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -412136.781250\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -514024.562500\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -488598.968750\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -486847.687500\n",
      "    epoch          : 444\n",
      "    loss           : -511116.8240625\n",
      "    val_loss       : -487469.809765625\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -602126.062500\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -414787.375000\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -611938.250000\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -510568.531250\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -506532.062500\n",
      "    epoch          : 445\n",
      "    loss           : -510851.48375\n",
      "    val_loss       : -487609.3330078125\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -553243.062500\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -489886.125000\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -513404.031250\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -489748.343750\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -541889.875000\n",
      "    epoch          : 446\n",
      "    loss           : -510588.9690625\n",
      "    val_loss       : -487371.5625\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -511518.062500\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -476833.875000\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -607014.562500\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -485691.906250\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -514357.000000\n",
      "    epoch          : 447\n",
      "    loss           : -510415.1646875\n",
      "    val_loss       : -487656.1759765625\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -479806.187500\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -511930.218750\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -410027.937500\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -556495.187500\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -514837.218750\n",
      "    epoch          : 448\n",
      "    loss           : -510544.5509375\n",
      "    val_loss       : -487494.2998046875\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -560856.375000\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -566295.250000\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -507560.875000\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -514993.500000\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -517244.000000\n",
      "    epoch          : 449\n",
      "    loss           : -511061.87875\n",
      "    val_loss       : -488717.7048828125\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -512218.687500\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -540909.937500\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -481961.062500\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -409436.218750\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -483680.062500\n",
      "    epoch          : 450\n",
      "    loss           : -510649.0046875\n",
      "    val_loss       : -487660.385546875\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1027_122354/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -483702.968750\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -538733.937500\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -513956.562500\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -539352.062500\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -519985.562500\n",
      "    epoch          : 451\n",
      "    loss           : -510532.77\n",
      "    val_loss       : -490063.3193359375\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -407441.687500\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -517372.812500\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -507934.531250\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -484133.312500\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -514433.000000\n",
      "    epoch          : 452\n",
      "    loss           : -510861.0625\n",
      "    val_loss       : -487888.3017578125\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -482254.937500\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -478507.093750\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -511024.875000\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -473216.000000\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -401663.781250\n",
      "    epoch          : 453\n",
      "    loss           : -511055.090625\n",
      "    val_loss       : -487655.817578125\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -482464.437500\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -601815.812500\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -408449.250000\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -536550.937500\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -476879.562500\n",
      "    epoch          : 454\n",
      "    loss           : -510813.3259375\n",
      "    val_loss       : -487452.34296875\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -516667.343750\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -404993.062500\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -509854.000000\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -483254.156250\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -513468.500000\n",
      "    epoch          : 455\n",
      "    loss           : -511047.784375\n",
      "    val_loss       : -487782.7447265625\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -484446.125000\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -561310.250000\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -511709.937500\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -530746.312500\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -477508.218750\n",
      "    epoch          : 456\n",
      "    loss           : -510951.8825\n",
      "    val_loss       : -485740.0466796875\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -561355.562500\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -513598.375000\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -508552.937500\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -484538.343750\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -522508.500000\n",
      "    epoch          : 457\n",
      "    loss           : -510671.455\n",
      "    val_loss       : -488465.82421875\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -497288.156250\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -495052.750000\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -546955.125000\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -508006.625000\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -481685.062500\n",
      "    epoch          : 458\n",
      "    loss           : -511146.7728125\n",
      "    val_loss       : -489352.123046875\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -512593.375000\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -482445.125000\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -556955.875000\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -481274.812500\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -507700.687500\n",
      "    epoch          : 459\n",
      "    loss           : -510952.1203125\n",
      "    val_loss       : -487839.5607421875\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -516852.656250\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -520810.781250\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -610664.375000\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -541912.375000\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -544124.250000\n",
      "    epoch          : 460\n",
      "    loss           : -510739.918125\n",
      "    val_loss       : -488123.517578125\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -603982.625000\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -560215.812500\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -510407.875000\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -559991.187500\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -485409.437500\n",
      "    epoch          : 461\n",
      "    loss           : -510865.558125\n",
      "    val_loss       : -487873.0837890625\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -490708.937500\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -489980.750000\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -512915.156250\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -603286.750000\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -607619.375000\n",
      "    epoch          : 462\n",
      "    loss           : -510589.9940625\n",
      "    val_loss       : -488774.6701171875\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -559321.500000\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -482820.375000\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -488125.375000\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -521768.218750\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -511461.593750\n",
      "    epoch          : 463\n",
      "    loss           : -511031.5559375\n",
      "    val_loss       : -487923.1455078125\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -540459.250000\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -487737.750000\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -612580.500000\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -478621.343750\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -519107.750000\n",
      "    epoch          : 464\n",
      "    loss           : -511323.4165625\n",
      "    val_loss       : -487845.643359375\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -493644.625000\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -561573.312500\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -521668.937500\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -488010.468750\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -490710.375000\n",
      "    epoch          : 465\n",
      "    loss           : -511065.8503125\n",
      "    val_loss       : -488410.373828125\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -559698.750000\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -540057.687500\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -409861.718750\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -403161.093750\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -519744.625000\n",
      "    epoch          : 466\n",
      "    loss           : -511170.4421875\n",
      "    val_loss       : -487665.079296875\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -414113.250000\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -507406.781250\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -495984.437500\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -408181.375000\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -521313.656250\n",
      "    epoch          : 467\n",
      "    loss           : -511050.939375\n",
      "    val_loss       : -487662.751171875\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -566460.812500\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -475181.187500\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -482020.500000\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -483050.062500\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -515351.937500\n",
      "    epoch          : 468\n",
      "    loss           : -510999.178125\n",
      "    val_loss       : -487017.7576171875\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -411942.812500\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -512207.281250\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -472881.656250\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -540693.250000\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -476251.187500\n",
      "    epoch          : 469\n",
      "    loss           : -510877.564375\n",
      "    val_loss       : -489393.021875\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -514403.906250\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -510054.125000\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -511115.500000\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -515208.093750\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -539174.687500\n",
      "    epoch          : 470\n",
      "    loss           : -511136.286875\n",
      "    val_loss       : -487406.841015625\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -502220.343750\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -510755.281250\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -482649.937500\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -500688.500000\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -475149.406250\n",
      "    epoch          : 471\n",
      "    loss           : -511181.6496875\n",
      "    val_loss       : -486819.1396484375\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -562505.500000\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -404329.531250\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -515805.968750\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -521757.687500\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -518591.656250\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "    epoch          : 472\n",
      "    loss           : -511164.8346875\n",
      "    val_loss       : -487318.01328125\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -488377.531250\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -410592.625000\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -540363.750000\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -472064.531250\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -490405.343750\n",
      "    epoch          : 473\n",
      "    loss           : -511235.1659375\n",
      "    val_loss       : -487296.121484375\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -559003.000000\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -483051.250000\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -517175.218750\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -515008.375000\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -507927.468750\n",
      "    epoch          : 474\n",
      "    loss           : -511025.2659375\n",
      "    val_loss       : -488792.776171875\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -409575.875000\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -478112.812500\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -471740.375000\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -537325.750000\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -512031.562500\n",
      "    epoch          : 475\n",
      "    loss           : -510862.688125\n",
      "    val_loss       : -487877.3828125\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -511505.437500\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -564875.937500\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -485382.468750\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -604827.625000\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -474414.437500\n",
      "    epoch          : 476\n",
      "    loss           : -511039.7225\n",
      "    val_loss       : -487592.665234375\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -401916.687500\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -411314.968750\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -555921.000000\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -606651.875000\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -475013.937500\n",
      "    epoch          : 477\n",
      "    loss           : -511273.1775\n",
      "    val_loss       : -486543.3201171875\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -487950.500000\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -491969.968750\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -413543.937500\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -513956.375000\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -513068.531250\n",
      "    epoch          : 478\n",
      "    loss           : -510668.4775\n",
      "    val_loss       : -487657.8357421875\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -496055.718750\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -560085.125000\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -561146.312500\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -505988.531250\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -472461.375000\n",
      "    epoch          : 479\n",
      "    loss           : -511183.653125\n",
      "    val_loss       : -488947.390625\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -541591.000000\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -489077.718750\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -508597.000000\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -606395.500000\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -608374.812500\n",
      "    epoch          : 480\n",
      "    loss           : -511429.94\n",
      "    val_loss       : -487397.8283203125\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -536192.000000\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -510867.906250\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -513596.218750\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -517286.125000\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -477057.250000\n",
      "    epoch          : 481\n",
      "    loss           : -510920.205625\n",
      "    val_loss       : -486567.43984375\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -599967.312500\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -513697.187500\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -606524.250000\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -605827.875000\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -484377.156250\n",
      "    epoch          : 482\n",
      "    loss           : -510937.153125\n",
      "    val_loss       : -489132.319140625\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -486206.281250\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -482800.312500\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -561925.500000\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -561637.875000\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -490418.781250\n",
      "    epoch          : 483\n",
      "    loss           : -510964.1259375\n",
      "    val_loss       : -487101.6111328125\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -491447.750000\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -516622.625000\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -476719.312500\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -479574.187500\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -478658.562500\n",
      "    epoch          : 484\n",
      "    loss           : -511411.5878125\n",
      "    val_loss       : -488079.4361328125\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -487229.500000\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -485313.656250\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -520899.718750\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -510981.437500\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -521853.531250\n",
      "    epoch          : 485\n",
      "    loss           : -510886.33\n",
      "    val_loss       : -486797.1302734375\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -403046.593750\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -559446.125000\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -514407.156250\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -487376.968750\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -490018.687500\n",
      "    epoch          : 486\n",
      "    loss           : -510941.313125\n",
      "    val_loss       : -488495.5685546875\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -561631.250000\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -513426.468750\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -513528.781250\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -538539.812500\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -484998.406250\n",
      "    epoch          : 487\n",
      "    loss           : -510960.7284375\n",
      "    val_loss       : -490142.702734375\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -489631.312500\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -489207.718750\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -609767.125000\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -535573.375000\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -488496.468750\n",
      "    epoch          : 488\n",
      "    loss           : -511117.7915625\n",
      "    val_loss       : -488209.578125\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -517866.218750\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -608556.000000\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -495105.343750\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -509705.906250\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -477352.656250\n",
      "    epoch          : 489\n",
      "    loss           : -511041.9003125\n",
      "    val_loss       : -487481.8482421875\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -514583.500000\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -484352.875000\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -514639.750000\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -485289.781250\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -513723.062500\n",
      "    epoch          : 490\n",
      "    loss           : -511315.44625\n",
      "    val_loss       : -486560.7921875\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -507597.625000\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -514673.250000\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -534912.437500\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -609634.375000\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -564168.500000\n",
      "    epoch          : 491\n",
      "    loss           : -511273.95\n",
      "    val_loss       : -487657.1017578125\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -559158.125000\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -604144.125000\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -562140.250000\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -513496.750000\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -470454.406250\n",
      "    epoch          : 492\n",
      "    loss           : -510879.571875\n",
      "    val_loss       : -488305.9861328125\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -403789.343750\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -543826.000000\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -516036.218750\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -514427.218750\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -490406.562500\n",
      "    epoch          : 493\n",
      "    loss           : -511398.901875\n",
      "    val_loss       : -487512.9419921875\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -515591.093750\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -540107.187500\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -503240.000000\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -604213.937500\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -559957.000000\n",
      "    epoch          : 494\n",
      "    loss           : -511006.055625\n",
      "    val_loss       : -486684.626953125\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -407250.625000\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -519499.906250\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -513339.156250\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -486212.375000\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -489565.125000\n",
      "    epoch          : 495\n",
      "    loss           : -510824.954375\n",
      "    val_loss       : -487645.5240234375\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -511688.750000\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -485465.093750\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -512678.406250\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -481455.125000\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -511561.875000\n",
      "    epoch          : 496\n",
      "    loss           : -510722.2253125\n",
      "    val_loss       : -487476.44296875\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -410360.875000\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -401773.375000\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -511962.843750\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -517051.281250\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -511396.968750\n",
      "    epoch          : 497\n",
      "    loss           : -511058.2996875\n",
      "    val_loss       : -487035.9916015625\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -609185.875000\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -611734.250000\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -489982.937500\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -488351.062500\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -506961.687500\n",
      "    epoch          : 498\n",
      "    loss           : -510817.1396875\n",
      "    val_loss       : -487686.333984375\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -484097.875000\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -606782.312500\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -517353.718750\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -480696.968750\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -604571.250000\n",
      "    epoch          : 499\n",
      "    loss           : -511366.4934375\n",
      "    val_loss       : -488004.883203125\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -408537.375000\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -539680.312500\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -405355.093750\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -414455.406250\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -487002.187500\n",
      "    epoch          : 500\n",
      "    loss           : -511261.20875\n",
      "    val_loss       : -486971.8291015625\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/1027_122354/checkpoint-epoch500.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VlaeCategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (7): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=196, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=392, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (noise_linear): Linear(in_features=2744, out_features=4, bias=True)\n",
       "      (ladder_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (ladder_linear): Linear(in_features=2744, out_features=392, bias=True)\n",
       "    )\n",
       "    (generator_5): LadderPrior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): LadderPrior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): LadderPrior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): LadderPrior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (7): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=196, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (7): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=196, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): LadderPrior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=16, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=42, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
