{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"cooldown\": 25,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 589535.000000\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: 340648.625000\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: 252412.281250\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: 206262.390625\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: 180849.578125\n",
      "    epoch          : 1\n",
      "    loss           : 268241.47046875\n",
      "    val_loss       : 151011.15299351217\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: 134453.281250\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: 132337.156250\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: 89147.375000\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: 99529.078125\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: 61223.781250\n",
      "    epoch          : 2\n",
      "    loss           : 105513.8817578125\n",
      "    val_loss       : 65733.23068048953\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: 43332.296875\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: 67679.281250\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: 91645.070312\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -2356.110840\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: 54888.515625\n",
      "    epoch          : 3\n",
      "    loss           : 44126.787778320315\n",
      "    val_loss       : 23100.198850131033\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -11417.961914\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: 12509.278320\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -23839.925781\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: 28903.111328\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: 10664.695312\n",
      "    epoch          : 4\n",
      "    loss           : 17491.58587890625\n",
      "    val_loss       : -9659.464503598214\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -44050.707031\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -16780.308594\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: 45554.367188\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -840.068359\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -11434.758789\n",
      "    epoch          : 5\n",
      "    loss           : -15054.081166992188\n",
      "    val_loss       : -28315.572622537613\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -36509.429688\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -76681.679688\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -44891.035156\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -23428.693359\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -37602.945312\n",
      "    epoch          : 6\n",
      "    loss           : -38639.77245117188\n",
      "    val_loss       : -43052.44372227192\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -53116.601562\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -41600.753906\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -55368.675781\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -73236.687500\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -72232.976562\n",
      "    epoch          : 7\n",
      "    loss           : -48374.52418945313\n",
      "    val_loss       : -44400.645007121566\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -86597.835938\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -3316.808594\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -26904.031250\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -79706.781250\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -45297.054688\n",
      "    epoch          : 8\n",
      "    loss           : -50399.5485546875\n",
      "    val_loss       : -72318.32204184533\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -62887.726562\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -67283.148438\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: 24297.652344\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -80739.492188\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -136236.328125\n",
      "    epoch          : 9\n",
      "    loss           : -77553.99216796875\n",
      "    val_loss       : -84610.82471449375\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -72034.867188\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -89827.718750\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -83573.703125\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -92858.554688\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -68862.250000\n",
      "    epoch          : 10\n",
      "    loss           : -76842.4440234375\n",
      "    val_loss       : -65440.24410481453\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -116322.718750\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -94948.781250\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -8699.220703\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -107214.257812\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -89629.640625\n",
      "    epoch          : 11\n",
      "    loss           : -93988.23599609375\n",
      "    val_loss       : -101772.63576067686\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -108044.453125\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -103201.179688\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -159067.843750\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -100951.468750\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -88446.812500\n",
      "    epoch          : 12\n",
      "    loss           : -106747.74821289063\n",
      "    val_loss       : -110908.13334169387\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -103671.203125\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -150.207031\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -158942.765625\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -123453.351562\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -106389.046875\n",
      "    epoch          : 13\n",
      "    loss           : -107167.25924804687\n",
      "    val_loss       : -107046.77832546234\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -112320.929688\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -159273.250000\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -173842.453125\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -109003.265625\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -51867.550781\n",
      "    epoch          : 14\n",
      "    loss           : -111159.72811523438\n",
      "    val_loss       : -110026.03670066595\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -113016.218750\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -171809.531250\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -119101.210938\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -123592.640625\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -120994.078125\n",
      "    epoch          : 15\n",
      "    loss           : -122248.55692382813\n",
      "    val_loss       : -113806.11919378043\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -126822.343750\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -132702.281250\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -12182.201172\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -62694.101562\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -71567.101562\n",
      "    epoch          : 16\n",
      "    loss           : -116897.68385742187\n",
      "    val_loss       : -111229.88218299151\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -101574.703125\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -150023.250000\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -184789.687500\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -142743.984375\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -145504.875000\n",
      "    epoch          : 17\n",
      "    loss           : -134939.4164453125\n",
      "    val_loss       : -142130.21397551298\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -151440.937500\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -118190.171875\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -142590.218750\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -29386.876953\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -110955.234375\n",
      "    epoch          : 18\n",
      "    loss           : -129937.44395507812\n",
      "    val_loss       : -125741.02474365235\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -116097.625000\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -194757.375000\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -207006.640625\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -128835.085938\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -44921.695312\n",
      "    epoch          : 19\n",
      "    loss           : -138873.563203125\n",
      "    val_loss       : -147300.71161630153\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -149160.515625\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -129097.234375\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -153891.656250\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -120036.664062\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -150581.828125\n",
      "    epoch          : 20\n",
      "    loss           : -135886.75583984374\n",
      "    val_loss       : -149767.68187594414\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -50895.074219\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -52568.796875\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -155408.984375\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -155346.734375\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -120082.609375\n",
      "    epoch          : 21\n",
      "    loss           : -151141.80359375\n",
      "    val_loss       : -153489.1947900653\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -149055.812500\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -50798.851562\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -171608.968750\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -144789.796875\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -192675.468750\n",
      "    epoch          : 22\n",
      "    loss           : -135611.77186523436\n",
      "    val_loss       : -119054.96756894588\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -113107.546875\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -147599.609375\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -138719.000000\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -223930.718750\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -153028.218750\n",
      "    epoch          : 23\n",
      "    loss           : -152196.948515625\n",
      "    val_loss       : -161721.79500927925\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -144396.406250\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -160380.093750\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -223012.890625\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -230114.500000\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -167884.312500\n",
      "    epoch          : 24\n",
      "    loss           : -166523.2306640625\n",
      "    val_loss       : -168640.3349951625\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -164620.203125\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -164923.125000\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -171413.828125\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -165031.515625\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -171306.812500\n",
      "    epoch          : 25\n",
      "    loss           : -157578.2905859375\n",
      "    val_loss       : -155504.86428622008\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -48987.988281\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -231294.765625\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -229252.812500\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -155506.421875\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -176417.093750\n",
      "    epoch          : 26\n",
      "    loss           : -163625.3478125\n",
      "    val_loss       : -160458.18621826172\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -156986.156250\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -236099.062500\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -174755.562500\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -178182.718750\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -161205.906250\n",
      "    epoch          : 27\n",
      "    loss           : -164462.88451171876\n",
      "    val_loss       : -144953.08049280645\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -151359.375000\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -157281.156250\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -173191.593750\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -185635.875000\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -185867.546875\n",
      "    epoch          : 28\n",
      "    loss           : -175998.098046875\n",
      "    val_loss       : -183073.45124354362\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -181406.687500\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -77729.156250\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -159140.078125\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -56832.281250\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -177934.031250\n",
      "    epoch          : 29\n",
      "    loss           : -176155.2977734375\n",
      "    val_loss       : -164214.80032156705\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -237676.906250\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -192589.187500\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -202020.093750\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -188471.437500\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -146847.281250\n",
      "    epoch          : 30\n",
      "    loss           : -178287.1259375\n",
      "    val_loss       : -176227.2804690361\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -230341.640625\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -71594.570312\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -160344.125000\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -163886.687500\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -151644.890625\n",
      "    epoch          : 31\n",
      "    loss           : -165097.676171875\n",
      "    val_loss       : -95665.69576793909\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -157665.093750\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -32161.136719\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -60783.742188\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -193270.593750\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -171999.328125\n",
      "    epoch          : 32\n",
      "    loss           : -163436.57705078126\n",
      "    val_loss       : -190190.12853939534\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -84901.523438\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -255683.734375\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -188268.718750\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -169365.750000\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -191434.187500\n",
      "    epoch          : 33\n",
      "    loss           : -177078.516796875\n",
      "    val_loss       : -176856.1818037629\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -189035.125000\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -140904.015625\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -176135.390625\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -193688.093750\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -178267.812500\n",
      "    epoch          : 34\n",
      "    loss           : -169974.2696875\n",
      "    val_loss       : -182533.34506919383\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -244857.765625\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -86994.679688\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -199313.546875\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -192758.453125\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -175726.437500\n",
      "    epoch          : 35\n",
      "    loss           : -193254.630546875\n",
      "    val_loss       : -194563.07289706467\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -208549.781250\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -263595.531250\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -198775.187500\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -190187.828125\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -187200.093750\n",
      "    epoch          : 36\n",
      "    loss           : -194602.08640625\n",
      "    val_loss       : -196215.83671663998\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -194485.843750\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -208215.156250\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -187255.296875\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -158574.156250\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -191922.109375\n",
      "    epoch          : 37\n",
      "    loss           : -181499.4078125\n",
      "    val_loss       : -182319.30532010793\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -160777.187500\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -182165.000000\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -212805.687500\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -270264.281250\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -210248.343750\n",
      "    epoch          : 38\n",
      "    loss           : -196578.668125\n",
      "    val_loss       : -193228.23391315938\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -264165.000000\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -170765.390625\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -201886.781250\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -267286.187500\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -77792.718750\n",
      "    epoch          : 39\n",
      "    loss           : -179836.83970703126\n",
      "    val_loss       : -175527.22192400694\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -179524.640625\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -200886.718750\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -70701.585938\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -199381.531250\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -166407.515625\n",
      "    epoch          : 40\n",
      "    loss           : -195650.307265625\n",
      "    val_loss       : -197873.1415988207\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -175520.468750\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -194534.218750\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -277497.937500\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -264561.875000\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -205228.046875\n",
      "    epoch          : 41\n",
      "    loss           : -204104.28296875\n",
      "    val_loss       : -197182.48474493026\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -177788.750000\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -266114.906250\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -274039.093750\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -191815.343750\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -197351.437500\n",
      "    epoch          : 42\n",
      "    loss           : -188752.58154296875\n",
      "    val_loss       : -184214.28849164248\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -268590.937500\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -255600.765625\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -183390.500000\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -198951.953125\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -211489.296875\n",
      "    epoch          : 43\n",
      "    loss           : -197279.38220703125\n",
      "    val_loss       : -200029.94515684844\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -100297.609375\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -183226.406250\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -145423.906250\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -131979.625000\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -148535.140625\n",
      "    epoch          : 44\n",
      "    loss           : -167877.56974609374\n",
      "    val_loss       : -185090.93388319016\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -164731.203125\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -222859.734375\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -194574.875000\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -191722.031250\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -213437.750000\n",
      "    epoch          : 45\n",
      "    loss           : -206071.2223046875\n",
      "    val_loss       : -201854.68176345824\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -223465.140625\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -162627.500000\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -210218.187500\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -236097.875000\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -193947.218750\n",
      "    epoch          : 46\n",
      "    loss           : -206961.61265625\n",
      "    val_loss       : -210249.0594994068\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -187417.734375\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -203561.578125\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -268561.062500\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -188303.937500\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -177213.312500\n",
      "    epoch          : 47\n",
      "    loss           : -195390.3903515625\n",
      "    val_loss       : -202563.8732488513\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -267217.000000\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -186252.937500\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -294239.937500\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -210266.671875\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -211673.093750\n",
      "    epoch          : 48\n",
      "    loss           : -214205.39265625\n",
      "    val_loss       : -215140.23074923753\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -103115.546875\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -228491.578125\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -215443.187500\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -221511.687500\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -182568.921875\n",
      "    epoch          : 49\n",
      "    loss           : -211567.254765625\n",
      "    val_loss       : -210620.73161598443\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -95852.625000\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -200823.843750\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -283538.031250\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -186106.000000\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -186173.296875\n",
      "    epoch          : 50\n",
      "    loss           : -211081.5486328125\n",
      "    val_loss       : -201058.5803570032\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0326_214204/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -209426.984375\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -177576.843750\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -177449.843750\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -215629.109375\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -216932.031250\n",
      "    epoch          : 51\n",
      "    loss           : -194438.1259375\n",
      "    val_loss       : -194107.91339029075\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -196740.078125\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -202363.593750\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -229275.968750\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -232463.843750\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -227124.500000\n",
      "    epoch          : 52\n",
      "    loss           : -218925.935625\n",
      "    val_loss       : -223545.4160079956\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -309293.406250\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -208650.468750\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -209485.406250\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -246096.843750\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -186088.890625\n",
      "    epoch          : 53\n",
      "    loss           : -221148.223359375\n",
      "    val_loss       : -217608.27090847492\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -225000.000000\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -205393.812500\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -109738.007812\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -223854.093750\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -199439.343750\n",
      "    epoch          : 54\n",
      "    loss           : -223684.9134375\n",
      "    val_loss       : -225391.55258592367\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -212634.875000\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -300916.750000\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -192498.093750\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -253087.578125\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -221904.718750\n",
      "    epoch          : 55\n",
      "    loss           : -210155.375625\n",
      "    val_loss       : -209960.457492733\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -192103.015625\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -39069.101562\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -281899.843750\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -222869.781250\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -185999.000000\n",
      "    epoch          : 56\n",
      "    loss           : -198845.2075\n",
      "    val_loss       : -206038.43183870317\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -272300.125000\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -262930.343750\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -127036.421875\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -227484.531250\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -229804.453125\n",
      "    epoch          : 57\n",
      "    loss           : -200167.011875\n",
      "    val_loss       : -201628.97917318344\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -183385.625000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -205466.656250\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -194823.531250\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -247773.890625\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -223087.890625\n",
      "    epoch          : 58\n",
      "    loss           : -220866.82171875\n",
      "    val_loss       : -218287.9997321129\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -222962.968750\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -229960.359375\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -114353.921875\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -240800.140625\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -241304.921875\n",
      "    epoch          : 59\n",
      "    loss           : -229873.21703125\n",
      "    val_loss       : -228230.04155597687\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -210975.500000\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -309681.781250\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -305427.312500\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -196059.312500\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -184732.562500\n",
      "    epoch          : 60\n",
      "    loss           : -217099.030546875\n",
      "    val_loss       : -197244.40961260797\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -293916.531250\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -224879.171875\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -173737.468750\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -201069.343750\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -206436.687500\n",
      "    epoch          : 61\n",
      "    loss           : -179455.80338867186\n",
      "    val_loss       : -212352.3945492506\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -192905.437500\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -226135.343750\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -212825.921875\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -315135.312500\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -235472.031250\n",
      "    epoch          : 62\n",
      "    loss           : -229562.06546875\n",
      "    val_loss       : -230084.8410550356\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -203390.468750\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -110397.625000\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -316976.250000\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -210005.640625\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -157590.312500\n",
      "    epoch          : 63\n",
      "    loss           : -230449.72765625\n",
      "    val_loss       : -206942.55118407012\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -195356.140625\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -220230.875000\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -238068.515625\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -231906.562500\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -238970.578125\n",
      "    epoch          : 64\n",
      "    loss           : -225504.1273828125\n",
      "    val_loss       : -230931.9847352028\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -222757.468750\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -322916.000000\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -325485.593750\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -239930.718750\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -243470.968750\n",
      "    epoch          : 65\n",
      "    loss           : -239129.080078125\n",
      "    val_loss       : -235193.0723241806\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -238336.828125\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -321935.093750\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -125684.265625\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -255512.187500\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -243826.687500\n",
      "    epoch          : 66\n",
      "    loss           : -241307.953515625\n",
      "    val_loss       : -235970.2562006116\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -217100.187500\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -264989.812500\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -119349.554688\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -207125.406250\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -204256.562500\n",
      "    epoch          : 67\n",
      "    loss           : -235161.931953125\n",
      "    val_loss       : -226903.02398302555\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -206076.343750\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -287709.562500\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -202573.687500\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -173929.390625\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: 80021.164062\n",
      "    epoch          : 68\n",
      "    loss           : -191743.51857421876\n",
      "    val_loss       : -188671.25646579266\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -287719.000000\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -95836.320312\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -303498.500000\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -256547.625000\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -233536.765625\n",
      "    epoch          : 69\n",
      "    loss           : -222281.81706054686\n",
      "    val_loss       : -232522.91499586107\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -238006.375000\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -230360.875000\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -225011.718750\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -248302.593750\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -251073.718750\n",
      "    epoch          : 70\n",
      "    loss           : -241209.49890625\n",
      "    val_loss       : -233576.48671672345\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -230222.906250\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -216692.328125\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -328250.437500\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -232625.156250\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -317655.750000\n",
      "    epoch          : 71\n",
      "    loss           : -241564.580078125\n",
      "    val_loss       : -236915.73343317508\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -336685.656250\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -118167.882812\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -120166.421875\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -259754.921875\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -200851.656250\n",
      "    epoch          : 72\n",
      "    loss           : -235997.632890625\n",
      "    val_loss       : -217065.85545011162\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -199158.187500\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -204124.156250\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -242665.406250\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -245317.156250\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -217951.953125\n",
      "    epoch          : 73\n",
      "    loss           : -230843.016171875\n",
      "    val_loss       : -239884.68267986775\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -227252.828125\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -235238.781250\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -243451.796875\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -107304.304688\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -263930.625000\n",
      "    epoch          : 74\n",
      "    loss           : -235160.588046875\n",
      "    val_loss       : -224996.65918370485\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -203321.656250\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -335142.125000\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -213285.609375\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -188435.750000\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -119860.890625\n",
      "    epoch          : 75\n",
      "    loss           : -210632.937265625\n",
      "    val_loss       : -190148.88313184975\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -310267.312500\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -142892.296875\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -229768.515625\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -223806.984375\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -249438.421875\n",
      "    epoch          : 76\n",
      "    loss           : -177611.92888671876\n",
      "    val_loss       : -234919.00700736046\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -320432.000000\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -205032.843750\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -331366.656250\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -331478.500000\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -255523.968750\n",
      "    epoch          : 77\n",
      "    loss           : -247236.766015625\n",
      "    val_loss       : -243188.29758973123\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -208915.390625\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -223619.656250\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -256592.093750\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -259684.812500\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -207722.781250\n",
      "    epoch          : 78\n",
      "    loss           : -248861.822109375\n",
      "    val_loss       : -241440.72832260132\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -239523.656250\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -225847.156250\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -114870.484375\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -337094.625000\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -262950.375000\n",
      "    epoch          : 79\n",
      "    loss           : -243322.5884375\n",
      "    val_loss       : -236019.07863576413\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -129648.468750\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -273838.250000\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -225815.843750\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -234773.593750\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -338844.281250\n",
      "    epoch          : 80\n",
      "    loss           : -252351.856796875\n",
      "    val_loss       : -246203.85813480616\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -230978.859375\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -143198.375000\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -277724.000000\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -255096.531250\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -227202.968750\n",
      "    epoch          : 81\n",
      "    loss           : -255086.49515625\n",
      "    val_loss       : -246149.94454495906\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -132308.296875\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -237960.953125\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -339067.562500\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -265740.156250\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -224362.984375\n",
      "    epoch          : 82\n",
      "    loss           : -254557.723828125\n",
      "    val_loss       : -246202.91514165403\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -229590.187500\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -227631.750000\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -140515.343750\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -251426.531250\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -225889.843750\n",
      "    epoch          : 83\n",
      "    loss           : -256225.556328125\n",
      "    val_loss       : -247154.96400830746\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -347264.437500\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -345048.906250\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -343014.125000\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -228227.406250\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -250278.609375\n",
      "    epoch          : 84\n",
      "    loss           : -247741.121015625\n",
      "    val_loss       : -233249.67977719306\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -105232.257812\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -317353.531250\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -209837.671875\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -242071.359375\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -198634.062500\n",
      "    epoch          : 85\n",
      "    loss           : -226683.493125\n",
      "    val_loss       : -233856.28114949464\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -217758.406250\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -243735.125000\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -336160.218750\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -268707.187500\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -266548.906250\n",
      "    epoch          : 86\n",
      "    loss           : -252297.24375\n",
      "    val_loss       : -231744.76497101784\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -106246.593750\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -261628.375000\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -329134.062500\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -241488.125000\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -262229.812500\n",
      "    epoch          : 87\n",
      "    loss           : -241560.50765625\n",
      "    val_loss       : -244827.07843670846\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -338363.250000\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -216905.421875\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -216730.281250\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -222286.390625\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -205708.859375\n",
      "    epoch          : 88\n",
      "    loss           : -242693.5190625\n",
      "    val_loss       : -183351.78963242768\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -184208.156250\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -208725.468750\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -242985.750000\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -330505.375000\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -223554.796875\n",
      "    epoch          : 89\n",
      "    loss           : -231371.62984375\n",
      "    val_loss       : -247962.18532094956\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -232550.453125\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -275383.000000\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -270350.906250\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -223603.875000\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -266762.125000\n",
      "    epoch          : 90\n",
      "    loss           : -261099.77390625\n",
      "    val_loss       : -254683.96607789994\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -238308.703125\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -147342.890625\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -228063.265625\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -267995.687500\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -244858.312500\n",
      "    epoch          : 91\n",
      "    loss           : -265254.01140625\n",
      "    val_loss       : -255632.40707802773\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -289473.187500\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -351794.437500\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -283080.468750\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -205597.343750\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -226706.156250\n",
      "    epoch          : 92\n",
      "    loss           : -247310.36109375\n",
      "    val_loss       : -215731.05859342814\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -206518.750000\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -211862.968750\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -349947.781250\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -241126.578125\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -225435.093750\n",
      "    epoch          : 93\n",
      "    loss           : -246198.549609375\n",
      "    val_loss       : -248710.42419103385\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -128921.437500\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -251672.421875\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -273190.875000\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -246281.968750\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -257484.937500\n",
      "    epoch          : 94\n",
      "    loss           : -258725.454453125\n",
      "    val_loss       : -253596.52655079364\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -263916.250000\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -351037.468750\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -236703.593750\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -152962.234375\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -213894.125000\n",
      "    epoch          : 95\n",
      "    loss           : -241467.700078125\n",
      "    val_loss       : -223422.30766028166\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -247404.609375\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -226883.937500\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -188530.093750\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -195154.031250\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -218427.859375\n",
      "    epoch          : 96\n",
      "    loss           : -236308.761640625\n",
      "    val_loss       : -243110.1849409461\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -216681.953125\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -358494.656250\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -259107.062500\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -139593.593750\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -273770.062500\n",
      "    epoch          : 97\n",
      "    loss           : -266665.12328125\n",
      "    val_loss       : -259901.50156903267\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -284763.187500\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -359175.187500\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -259904.640625\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -235167.625000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -129085.781250\n",
      "    epoch          : 98\n",
      "    loss           : -266980.60796875\n",
      "    val_loss       : -254130.98577606678\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -228797.750000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -285278.437500\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -243498.890625\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -236824.593750\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -240860.984375\n",
      "    epoch          : 99\n",
      "    loss           : -258937.792421875\n",
      "    val_loss       : -242813.3224990964\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -334004.718750\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -72151.664062\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -193531.640625\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -233358.468750\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -269534.093750\n",
      "    epoch          : 100\n",
      "    loss           : -227020.63328125\n",
      "    val_loss       : -242481.34496545792\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0326_214204/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -227190.343750\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -250802.734375\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -145884.265625\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -143420.781250\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -282636.437500\n",
      "    epoch          : 101\n",
      "    loss           : -261316.79234375\n",
      "    val_loss       : -256057.65606156588\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -294484.062500\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -285766.937500\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -353350.343750\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -231755.015625\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -265750.812500\n",
      "    epoch          : 102\n",
      "    loss           : -260958.300078125\n",
      "    val_loss       : -246274.02327449323\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -254579.984375\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -240774.343750\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -244748.890625\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -232476.140625\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -230971.640625\n",
      "    epoch          : 103\n",
      "    loss           : -253269.986328125\n",
      "    val_loss       : -211750.20649232864\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -184748.343750\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -138663.562500\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -137438.390625\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -226337.187500\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -265821.593750\n",
      "    epoch          : 104\n",
      "    loss           : -172688.60717773438\n",
      "    val_loss       : -244152.53141174317\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -239853.281250\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -233270.093750\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -356351.875000\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -243722.250000\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -285071.875000\n",
      "    epoch          : 105\n",
      "    loss           : -265062.926875\n",
      "    val_loss       : -259420.50839486122\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -253687.140625\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -242440.968750\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -234467.515625\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -261131.531250\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -275455.125000\n",
      "    epoch          : 106\n",
      "    loss           : -271722.26375\n",
      "    val_loss       : -259755.88750433922\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -273042.250000\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -242740.625000\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -251201.750000\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -148525.484375\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -279469.250000\n",
      "    epoch          : 107\n",
      "    loss           : -272164.95953125\n",
      "    val_loss       : -259529.60364912747\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -287460.437500\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -298721.781250\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -361433.875000\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -294110.781250\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -295174.125000\n",
      "    epoch          : 108\n",
      "    loss           : -265797.690625\n",
      "    val_loss       : -256571.6453029394\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -240740.187500\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -155359.343750\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -250241.484375\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -362114.000000\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -291923.375000\n",
      "    epoch          : 109\n",
      "    loss           : -272328.025\n",
      "    val_loss       : -260365.44853815436\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -249228.296875\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -157890.890625\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -366760.437500\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -253466.156250\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -219210.718750\n",
      "    epoch          : 110\n",
      "    loss           : -269524.571875\n",
      "    val_loss       : -250419.22926840783\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -282901.625000\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -345580.406250\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -260761.046875\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -363356.781250\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -283817.187500\n",
      "    epoch          : 111\n",
      "    loss           : -261688.373828125\n",
      "    val_loss       : -257761.97521578072\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -363743.250000\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -237281.437500\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -270057.000000\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -236091.453125\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -274682.312500\n",
      "    epoch          : 112\n",
      "    loss           : -268181.24640625\n",
      "    val_loss       : -246708.88289967776\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -229428.328125\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -232956.234375\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -232288.390625\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -362017.062500\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -237152.187500\n",
      "    epoch          : 113\n",
      "    loss           : -261164.662265625\n",
      "    val_loss       : -258179.40666191577\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -265930.062500\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -245299.109375\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -263949.531250\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -246742.359375\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -150565.984375\n",
      "    epoch          : 114\n",
      "    loss           : -272757.1725\n",
      "    val_loss       : -264774.3778880358\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -289286.531250\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -364203.093750\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -236048.937500\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -305883.156250\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -234061.093750\n",
      "    epoch          : 115\n",
      "    loss           : -275630.279296875\n",
      "    val_loss       : -254434.71675169468\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -260131.890625\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -358665.187500\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -365930.500000\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -288393.281250\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -254108.156250\n",
      "    epoch          : 116\n",
      "    loss           : -270806.8721875\n",
      "    val_loss       : -257448.56634216307\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -270018.937500\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -258486.046875\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -366453.000000\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -353662.750000\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -252174.343750\n",
      "    epoch          : 117\n",
      "    loss           : -255882.2975\n",
      "    val_loss       : -241337.163734746\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -257310.781250\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -211079.687500\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -126371.968750\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -208052.968750\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -269058.687500\n",
      "    epoch          : 118\n",
      "    loss           : -213557.57599609374\n",
      "    val_loss       : -248988.7179475546\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -365655.093750\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -355344.625000\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -243448.281250\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -269246.562500\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -85537.656250\n",
      "    epoch          : 119\n",
      "    loss           : -260198.022421875\n",
      "    val_loss       : -238098.81751515865\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -347550.812500\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -348250.125000\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -260826.640625\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -227050.734375\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -270795.375000\n",
      "    epoch          : 120\n",
      "    loss           : -266208.00515625\n",
      "    val_loss       : -264083.7066688061\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -249467.093750\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -267867.500000\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -382331.875000\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -270170.156250\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -292475.562500\n",
      "    epoch          : 121\n",
      "    loss           : -279913.11390625\n",
      "    val_loss       : -261540.49383702278\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -369205.875000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -373254.187500\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -308699.750000\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -151625.031250\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -283639.875000\n",
      "    epoch          : 122\n",
      "    loss           : -280105.5775\n",
      "    val_loss       : -263665.33002209663\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -266655.281250\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -299218.625000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -243864.281250\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -267298.000000\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -263899.437500\n",
      "    epoch          : 123\n",
      "    loss           : -273245.1228125\n",
      "    val_loss       : -249319.95816450118\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -285489.593750\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -284489.375000\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -56097.070312\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -193081.593750\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -364993.500000\n",
      "    epoch          : 124\n",
      "    loss           : -226878.85998046875\n",
      "    val_loss       : -262728.0078118086\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -256102.093750\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -146587.390625\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -150363.312500\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -255536.437500\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -285901.062500\n",
      "    epoch          : 125\n",
      "    loss           : -279946.15421875\n",
      "    val_loss       : -261760.82542493343\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -137915.265625\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -246661.421875\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -271909.125000\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -294044.437500\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -284428.562500\n",
      "    epoch          : 126\n",
      "    loss           : -277796.37859375\n",
      "    val_loss       : -266247.3392794132\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -382654.875000\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -301924.625000\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -150732.625000\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -272258.562500\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -297078.562500\n",
      "    epoch          : 127\n",
      "    loss           : -278477.853125\n",
      "    val_loss       : -264853.3485521555\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -289144.875000\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -144306.328125\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -256937.546875\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -362481.906250\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -236397.531250\n",
      "    epoch          : 128\n",
      "    loss           : -267864.80515625\n",
      "    val_loss       : -231270.1349093914\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -211584.906250\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -252237.343750\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -294629.843750\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -362836.531250\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -289336.375000\n",
      "    epoch          : 129\n",
      "    loss           : -264076.2034375\n",
      "    val_loss       : -269109.65499628783\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -286865.125000\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -275026.125000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -275410.312500\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -157659.968750\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -237796.140625\n",
      "    epoch          : 130\n",
      "    loss           : -277020.1059375\n",
      "    val_loss       : -248432.42360364198\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -299357.562500\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -377758.875000\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -277654.125000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -239036.906250\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -292386.968750\n",
      "    epoch          : 131\n",
      "    loss           : -274058.75140625\n",
      "    val_loss       : -257442.69437952043\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -268270.125000\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -372141.562500\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -264635.687500\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -367964.812500\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -310162.187500\n",
      "    epoch          : 132\n",
      "    loss           : -276806.94984375\n",
      "    val_loss       : -272177.89391851425\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -153538.140625\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -371610.375000\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -257525.109375\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -170082.375000\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -190040.390625\n",
      "    epoch          : 133\n",
      "    loss           : -259976.71322265625\n",
      "    val_loss       : -241401.24205665587\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -251252.375000\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -148788.343750\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -246037.031250\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -310633.250000\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -158508.234375\n",
      "    epoch          : 134\n",
      "    loss           : -275791.3271875\n",
      "    val_loss       : -268083.42584196327\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -152319.500000\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -280422.156250\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -250434.015625\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -15728.709961\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -143011.734375\n",
      "    epoch          : 135\n",
      "    loss           : -215205.32813476562\n",
      "    val_loss       : -210992.11525743006\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -343004.218750\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -125851.726562\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -143217.718750\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -297078.125000\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -159656.015625\n",
      "    epoch          : 136\n",
      "    loss           : -268901.9840625\n",
      "    val_loss       : -273496.22506370547\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -157758.125000\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -291939.093750\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -293444.625000\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -305060.375000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -291470.843750\n",
      "    epoch          : 137\n",
      "    loss           : -285883.85046875\n",
      "    val_loss       : -271607.07225637435\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -379034.062500\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -227581.343750\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -241936.937500\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -375189.750000\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -265633.062500\n",
      "    epoch          : 138\n",
      "    loss           : -280879.449375\n",
      "    val_loss       : -270822.4575133741\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -260823.453125\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -385207.187500\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -301507.468750\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -307684.750000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -318251.687500\n",
      "    epoch          : 139\n",
      "    loss           : -287229.31875\n",
      "    val_loss       : -274177.93570280075\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -384536.062500\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -259219.296875\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -320839.843750\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -154322.593750\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -319602.187500\n",
      "    epoch          : 140\n",
      "    loss           : -290101.60546875\n",
      "    val_loss       : -274552.2180062294\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -376336.250000\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -205511.093750\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -96844.046875\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -259723.218750\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -208557.390625\n",
      "    epoch          : 141\n",
      "    loss           : -238209.09900390625\n",
      "    val_loss       : -248168.21221239568\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -131010.882812\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -272689.875000\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -306972.625000\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -314178.875000\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -302056.187500\n",
      "    epoch          : 142\n",
      "    loss           : -278785.5640625\n",
      "    val_loss       : -275939.5319835186\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -282610.125000\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -278783.843750\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -303716.781250\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -313552.375000\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -248365.062500\n",
      "    epoch          : 143\n",
      "    loss           : -290933.92234375\n",
      "    val_loss       : -271939.6609474897\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -310545.687500\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -280702.937500\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -380986.812500\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -280484.187500\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -197807.656250\n",
      "    epoch          : 144\n",
      "    loss           : -263289.815390625\n",
      "    val_loss       : -207763.2071779847\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -252639.125000\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -88209.234375\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -199270.765625\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -267047.437500\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -245769.062500\n",
      "    epoch          : 145\n",
      "    loss           : -242869.0875\n",
      "    val_loss       : -265969.2567095518\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -155768.562500\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -314312.812500\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -290754.406250\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -299414.562500\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -387410.093750\n",
      "    epoch          : 146\n",
      "    loss           : -288918.89\n",
      "    val_loss       : -273693.02447710035\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -252999.156250\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -263939.812500\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -391209.593750\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -388259.812500\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -248076.468750\n",
      "    epoch          : 147\n",
      "    loss           : -294227.7259375\n",
      "    val_loss       : -277989.41367406846\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -306114.250000\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -254816.562500\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -287861.125000\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -134213.687500\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -292366.531250\n",
      "    epoch          : 148\n",
      "    loss           : -280780.82578125\n",
      "    val_loss       : -274097.15183296206\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -291679.125000\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -158397.562500\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -295739.187500\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -241816.406250\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -250904.250000\n",
      "    epoch          : 149\n",
      "    loss           : -275627.624453125\n",
      "    val_loss       : -258977.4035569191\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -240150.828125\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -370444.062500\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -316897.968750\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -290890.125000\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -305138.125000\n",
      "    epoch          : 150\n",
      "    loss           : -283621.69265625\n",
      "    val_loss       : -277121.1309122086\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0326_214204/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -297392.843750\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -323866.843750\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -302364.375000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -300326.500000\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -316448.656250\n",
      "    epoch          : 151\n",
      "    loss           : -294928.306875\n",
      "    val_loss       : -282583.71921277046\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -166611.828125\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -399633.062500\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -278338.031250\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -267694.250000\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -246729.687500\n",
      "    epoch          : 152\n",
      "    loss           : -293817.54421875\n",
      "    val_loss       : -280672.9332016945\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -282117.312500\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -176003.656250\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -159156.687500\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -266000.125000\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -305528.781250\n",
      "    epoch          : 153\n",
      "    loss           : -298525.20625\n",
      "    val_loss       : -280262.2250870943\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -266274.062500\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -206750.843750\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -279131.781250\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -322941.250000\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -261748.031250\n",
      "    epoch          : 154\n",
      "    loss           : -279559.3940625\n",
      "    val_loss       : -264068.0871320724\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -251752.312500\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -276015.125000\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -384653.781250\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -267812.500000\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -321406.750000\n",
      "    epoch          : 155\n",
      "    loss           : -288740.75875\n",
      "    val_loss       : -277334.3804357767\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -317934.937500\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -269489.000000\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -175219.156250\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -311913.187500\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -285558.437500\n",
      "    epoch          : 156\n",
      "    loss           : -281967.926953125\n",
      "    val_loss       : -236058.8736425519\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -275979.000000\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -273517.437500\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -307907.468750\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -161888.843750\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -74885.515625\n",
      "    epoch          : 157\n",
      "    loss           : -183937.31087890625\n",
      "    val_loss       : -149947.611420393\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -124440.039062\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -130158.523438\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -267617.406250\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -259251.250000\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -280397.062500\n",
      "    epoch          : 158\n",
      "    loss           : -260861.58845703126\n",
      "    val_loss       : -279403.65173966886\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -319913.968750\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -325323.843750\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -392189.562500\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -304033.593750\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -285490.875000\n",
      "    epoch          : 159\n",
      "    loss           : -295826.60140625\n",
      "    val_loss       : -280004.5583237648\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -176436.531250\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -296685.625000\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -256203.453125\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -284687.812500\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -401564.531250\n",
      "    epoch          : 160\n",
      "    loss           : -298086.605\n",
      "    val_loss       : -283299.44412368536\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -169407.640625\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -280203.000000\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -267939.250000\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -275032.000000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -177475.187500\n",
      "    epoch          : 161\n",
      "    loss           : -300560.97171875\n",
      "    val_loss       : -280701.14398046734\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -318728.562500\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -402519.437500\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -395483.593750\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -261136.000000\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -328601.468750\n",
      "    epoch          : 162\n",
      "    loss           : -301084.53015625\n",
      "    val_loss       : -281861.9200469971\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -294014.500000\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -257186.156250\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -378586.937500\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -301171.718750\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -368730.500000\n",
      "    epoch          : 163\n",
      "    loss           : -294759.66234375\n",
      "    val_loss       : -269595.1239818573\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -307346.781250\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -373001.843750\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -305578.375000\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -308665.218750\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -284683.218750\n",
      "    epoch          : 164\n",
      "    loss           : -285115.782734375\n",
      "    val_loss       : -279779.4915421486\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -264137.625000\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -285270.875000\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -246686.859375\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -284699.156250\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -232712.687500\n",
      "    epoch          : 165\n",
      "    loss           : -273235.8409765625\n",
      "    val_loss       : -243511.9309811592\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -257827.906250\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -366283.218750\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -364758.750000\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -239194.453125\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -246756.781250\n",
      "    epoch          : 166\n",
      "    loss           : -265134.17140625\n",
      "    val_loss       : -276530.31995470525\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -264918.406250\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -397451.875000\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -277272.406250\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -284224.625000\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -258914.156250\n",
      "    epoch          : 167\n",
      "    loss           : -296963.769375\n",
      "    val_loss       : -283524.0374997139\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -179558.859375\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -327713.375000\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -279708.406250\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -333102.593750\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -303211.593750\n",
      "    epoch          : 168\n",
      "    loss           : -303140.6159375\n",
      "    val_loss       : -282593.235588789\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -317589.875000\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -261336.562500\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -276932.250000\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -324115.906250\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -287798.937500\n",
      "    epoch          : 169\n",
      "    loss           : -302506.65484375\n",
      "    val_loss       : -282757.11359148024\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -310698.812500\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -326777.562500\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -281814.156250\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -390812.781250\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -404439.500000\n",
      "    epoch          : 170\n",
      "    loss           : -295692.91609375\n",
      "    val_loss       : -262087.61912502052\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -349282.062500\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -260563.671875\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -268413.187500\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -315152.406250\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -309525.875000\n",
      "    epoch          : 171\n",
      "    loss           : -296303.9103125\n",
      "    val_loss       : -279285.12808532716\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -257985.593750\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -232081.796875\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -156235.484375\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -140269.546875\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -228477.000000\n",
      "    epoch          : 172\n",
      "    loss           : -239386.80068359376\n",
      "    val_loss       : -264028.53282079694\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -240882.937500\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -258167.781250\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -402077.156250\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -302471.750000\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -258370.062500\n",
      "    epoch          : 173\n",
      "    loss           : -294875.19359375\n",
      "    val_loss       : -278653.92960066797\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -275682.500000\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -382348.875000\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -245897.406250\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -297634.593750\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -292401.156250\n",
      "    epoch          : 174\n",
      "    loss           : -287787.64078125\n",
      "    val_loss       : -276059.16527843475\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -242984.484375\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -260331.546875\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -151351.937500\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -357399.312500\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -279731.812500\n",
      "    epoch          : 175\n",
      "    loss           : -289970.5053125\n",
      "    val_loss       : -284389.19979925157\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -318208.406250\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -281298.906250\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -292933.250000\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -280748.843750\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -306799.812500\n",
      "    epoch          : 176\n",
      "    loss           : -304806.93921875\n",
      "    val_loss       : -281211.3152994156\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -280120.875000\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -411903.687500\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -306264.031250\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -390046.250000\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -229792.015625\n",
      "    epoch          : 177\n",
      "    loss           : -300158.2875\n",
      "    val_loss       : -276557.9462333679\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -309170.718750\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -272002.937500\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -321818.906250\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -399780.000000\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -248959.750000\n",
      "    epoch          : 178\n",
      "    loss           : -296588.15890625\n",
      "    val_loss       : -283565.5053367615\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -280742.281250\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -324665.687500\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -283324.968750\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -278435.531250\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -310762.656250\n",
      "    epoch          : 179\n",
      "    loss           : -306685.15234375\n",
      "    val_loss       : -281992.72640028\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -266126.468750\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -275315.625000\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -316849.312500\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -309202.093750\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -305458.812500\n",
      "    epoch          : 180\n",
      "    loss           : -300830.37859375\n",
      "    val_loss       : -267235.67176427844\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -403364.812500\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -79809.437500\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -158303.234375\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -232335.468750\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -70082.625000\n",
      "    epoch          : 181\n",
      "    loss           : -208505.36282226563\n",
      "    val_loss       : -226035.26769075394\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -258892.312500\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -379801.437500\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -396210.750000\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -331246.593750\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -304489.125000\n",
      "    epoch          : 182\n",
      "    loss           : -289519.84046875\n",
      "    val_loss       : -275750.30421733856\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -304528.187500\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -394894.812500\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -246708.484375\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -406488.375000\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -327949.718750\n",
      "    epoch          : 183\n",
      "    loss           : -291492.26921875\n",
      "    val_loss       : -289563.09215583804\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -285458.750000\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -412620.968750\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -281814.875000\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -308625.062500\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -316117.625000\n",
      "    epoch          : 184\n",
      "    loss           : -307392.7140625\n",
      "    val_loss       : -288055.7248547554\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -277595.562500\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -329840.312500\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -412527.656250\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -320062.250000\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -297949.187500\n",
      "    epoch          : 185\n",
      "    loss           : -295955.9390625\n",
      "    val_loss       : -279759.76368484495\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -323028.625000\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -150304.875000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -291688.468750\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -260831.875000\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -307457.468750\n",
      "    epoch          : 186\n",
      "    loss           : -277509.67625\n",
      "    val_loss       : -270462.9649328232\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -302740.187500\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -246739.875000\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -213794.531250\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -142076.312500\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -214035.562500\n",
      "    epoch          : 187\n",
      "    loss           : -227223.2432421875\n",
      "    val_loss       : -243821.5236356735\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -243696.687500\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -255576.312500\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -281833.312500\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -389338.000000\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -300187.593750\n",
      "    epoch          : 188\n",
      "    loss           : -287502.711328125\n",
      "    val_loss       : -278353.06560087204\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -272567.625000\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -400282.093750\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -407158.750000\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -279158.593750\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -317621.000000\n",
      "    epoch          : 189\n",
      "    loss           : -305621.92609375\n",
      "    val_loss       : -288109.09356937406\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -405885.812500\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -310273.718750\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -269138.343750\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -311618.937500\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -322316.375000\n",
      "    epoch          : 190\n",
      "    loss           : -306559.00796875\n",
      "    val_loss       : -287257.68814783095\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -314958.656250\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -335633.500000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -286989.500000\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -281405.656250\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -290459.312500\n",
      "    epoch          : 191\n",
      "    loss           : -303823.315\n",
      "    val_loss       : -281299.90288410184\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -333039.437500\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -410977.125000\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -389374.562500\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -275160.250000\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -285211.687500\n",
      "    epoch          : 192\n",
      "    loss           : -291043.54484375\n",
      "    val_loss       : -275158.334126091\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -295882.375000\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -281573.125000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -263670.843750\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -319577.406250\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -409048.093750\n",
      "    epoch          : 193\n",
      "    loss           : -303031.6296875\n",
      "    val_loss       : -285879.93790521624\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -278961.000000\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -282324.906250\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -183553.125000\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -413795.625000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -268441.125000\n",
      "    epoch          : 194\n",
      "    loss           : -307690.37953125\n",
      "    val_loss       : -289205.21015081403\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -317462.906250\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -312936.062500\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -343887.062500\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -405507.875000\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -278453.156250\n",
      "    epoch          : 195\n",
      "    loss           : -306938.1534375\n",
      "    val_loss       : -273216.2137413025\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -273395.375000\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -256873.875000\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -335296.312500\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -283755.250000\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -329160.375000\n",
      "    epoch          : 196\n",
      "    loss           : -294107.09421875\n",
      "    val_loss       : -248105.54344129562\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -237099.687500\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -217149.468750\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -198722.781250\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -185409.937500\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -274189.500000\n",
      "    epoch          : 197\n",
      "    loss           : -244033.88446289062\n",
      "    val_loss       : -256127.7053343773\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -211843.484375\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -254952.718750\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -328755.906250\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -408957.812500\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -309393.750000\n",
      "    epoch          : 198\n",
      "    loss           : -303884.6021875\n",
      "    val_loss       : -285709.9193749428\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -412960.625000\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -339372.031250\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -259869.234375\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -263471.156250\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -252771.171875\n",
      "    epoch          : 199\n",
      "    loss           : -308163.5340625\n",
      "    val_loss       : -287840.4820346832\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -416227.843750\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -411944.531250\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -317458.625000\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -266786.781250\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -307879.312500\n",
      "    epoch          : 200\n",
      "    loss           : -302352.01125\n",
      "    val_loss       : -290825.89308795927\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0326_214204/checkpoint-epoch200.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -418984.625000\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -185636.359375\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -301364.406250\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -195669.437500\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -346098.625000\n",
      "    epoch          : 201\n",
      "    loss           : -314549.53\n",
      "    val_loss       : -292724.39457998273\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -319998.625000\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -408365.093750\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -340734.375000\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -187376.875000\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -192072.187500\n",
      "    epoch          : 202\n",
      "    loss           : -315364.16890625\n",
      "    val_loss       : -291502.2505892754\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -332079.781250\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -299678.562500\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -286728.500000\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -325490.625000\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -299671.156250\n",
      "    epoch          : 203\n",
      "    loss           : -314461.81359375\n",
      "    val_loss       : -279405.1101474762\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -294773.500000\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -278776.875000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -296271.625000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -242563.171875\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -277325.937500\n",
      "    epoch          : 204\n",
      "    loss           : -261269.711171875\n",
      "    val_loss       : -286083.30184278486\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -286216.437500\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -273711.781250\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -269311.250000\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -193866.625000\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -292831.093750\n",
      "    epoch          : 205\n",
      "    loss           : -310630.77953125\n",
      "    val_loss       : -292901.2819759369\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -186175.781250\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -422034.187500\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -267806.812500\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -203120.906250\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -289039.093750\n",
      "    epoch          : 206\n",
      "    loss           : -293110.672109375\n",
      "    val_loss       : -264865.5838104248\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -254839.171875\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -274073.718750\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -253825.687500\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -240333.125000\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -308252.593750\n",
      "    epoch          : 207\n",
      "    loss           : -276175.160078125\n",
      "    val_loss       : -273483.35631713865\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -402268.281250\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -140564.484375\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -260441.671875\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -281234.906250\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -310147.250000\n",
      "    epoch          : 208\n",
      "    loss           : -299973.6728125\n",
      "    val_loss       : -282862.19348049164\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -273527.718750\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -329590.031250\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -279538.750000\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -408764.375000\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -296320.687500\n",
      "    epoch          : 209\n",
      "    loss           : -293723.0028125\n",
      "    val_loss       : -261532.41749620438\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -271231.843750\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -219416.828125\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -220168.875000\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -258446.375000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -351379.625000\n",
      "    epoch          : 210\n",
      "    loss           : -244351.6536328125\n",
      "    val_loss       : -271063.2378168702\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -287876.437500\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -338763.625000\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -342650.562500\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -281995.312500\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -262578.312500\n",
      "    epoch          : 211\n",
      "    loss           : -307558.08765625\n",
      "    val_loss       : -293183.69813976286\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -344194.000000\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -423336.718750\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -425814.375000\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -294880.875000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -259850.406250\n",
      "    epoch          : 212\n",
      "    loss           : -312233.92375\n",
      "    val_loss       : -289578.269906044\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -325328.125000\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -344293.125000\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -301173.000000\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -325169.187500\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -319948.062500\n",
      "    epoch          : 213\n",
      "    loss           : -318043.77359375\n",
      "    val_loss       : -294415.43184547423\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -426475.812500\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -322273.125000\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -189760.312500\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -406425.500000\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -273610.906250\n",
      "    epoch          : 214\n",
      "    loss           : -316451.29796875\n",
      "    val_loss       : -292335.33567705157\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -302578.500000\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -296985.906250\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -340645.000000\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -414462.187500\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -273599.375000\n",
      "    epoch          : 215\n",
      "    loss           : -311688.36734375\n",
      "    val_loss       : -288616.07752189634\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -312056.656250\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -409444.125000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -279295.093750\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -339908.625000\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -403165.375000\n",
      "    epoch          : 216\n",
      "    loss           : -309950.746875\n",
      "    val_loss       : -289413.3071125507\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -270729.343750\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -292308.343750\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -280764.000000\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -335376.375000\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -317272.968750\n",
      "    epoch          : 217\n",
      "    loss           : -308286.33265625\n",
      "    val_loss       : -286587.94596099854\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -321560.062500\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -286142.218750\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -424798.125000\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -423274.281250\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -285509.250000\n",
      "    epoch          : 218\n",
      "    loss           : -317973.7834375\n",
      "    val_loss       : -294512.3779683113\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -430558.968750\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -334347.656250\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -331620.218750\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -366819.531250\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -221370.890625\n",
      "    epoch          : 219\n",
      "    loss           : -263739.30421875\n",
      "    val_loss       : -212006.74502506256\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -42310.695312\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -246933.500000\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -162090.453125\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -207974.843750\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -272010.343750\n",
      "    epoch          : 220\n",
      "    loss           : -257030.1437890625\n",
      "    val_loss       : -290767.95036096574\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -332666.437500\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -292637.531250\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -352367.187500\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -286775.375000\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -332304.218750\n",
      "    epoch          : 221\n",
      "    loss           : -317940.11359375\n",
      "    val_loss       : -295905.80256490706\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -192055.656250\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -424745.906250\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -296423.812500\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -271554.468750\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -324265.093750\n",
      "    epoch          : 222\n",
      "    loss           : -321609.05046875\n",
      "    val_loss       : -286867.78185405734\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -414623.312500\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -293975.312500\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -344310.250000\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -342665.062500\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -345721.812500\n",
      "    epoch          : 223\n",
      "    loss           : -311557.33046875\n",
      "    val_loss       : -288158.4521566391\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -289363.718750\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -291769.750000\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -294166.625000\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -339054.187500\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -335000.062500\n",
      "    epoch          : 224\n",
      "    loss           : -306846.69234375\n",
      "    val_loss       : -293413.8680809975\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -283951.312500\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -267839.375000\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -273916.875000\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -265312.937500\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -337963.375000\n",
      "    epoch          : 225\n",
      "    loss           : -317368.49515625\n",
      "    val_loss       : -294557.51680059434\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -299385.437500\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -301438.781250\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -191948.968750\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -259524.781250\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -361214.187500\n",
      "    epoch          : 226\n",
      "    loss           : -295995.9795703125\n",
      "    val_loss       : -188642.35163803102\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -243531.375000\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -278836.718750\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -416340.437500\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -167274.953125\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -291231.125000\n",
      "    epoch          : 227\n",
      "    loss           : -293992.671015625\n",
      "    val_loss       : -288948.17033786775\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -275315.031250\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -314933.593750\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -284210.500000\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -292471.437500\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -305947.312500\n",
      "    epoch          : 228\n",
      "    loss           : -318386.79078125\n",
      "    val_loss       : -297303.93145685195\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -269524.875000\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -270625.500000\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -304316.468750\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -417970.375000\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -271298.031250\n",
      "    epoch          : 229\n",
      "    loss           : -320335.40421875\n",
      "    val_loss       : -297441.1369867086\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -300909.875000\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -284305.187500\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -428345.468750\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -304841.281250\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -342972.031250\n",
      "    epoch          : 230\n",
      "    loss           : -320545.39765625\n",
      "    val_loss       : -282444.2012831688\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -328032.000000\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -429254.937500\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -412912.187500\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -344701.093750\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -289615.375000\n",
      "    epoch          : 231\n",
      "    loss           : -308194.29984375\n",
      "    val_loss       : -270065.42732152937\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -157179.312500\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -231677.218750\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -401739.343750\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -370761.750000\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -284260.406250\n",
      "    epoch          : 232\n",
      "    loss           : -281230.417578125\n",
      "    val_loss       : -185052.20582695008\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -209229.296875\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -163868.375000\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -260992.281250\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -413355.906250\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -269177.312500\n",
      "    epoch          : 233\n",
      "    loss           : -289769.15578125\n",
      "    val_loss       : -292544.54523620603\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -344038.000000\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -283939.781250\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -355509.375000\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -282840.406250\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -308891.250000\n",
      "    epoch          : 234\n",
      "    loss           : -323963.5565625\n",
      "    val_loss       : -297943.44517183304\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -193282.234375\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -357055.656250\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -336879.843750\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -342532.312500\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -303618.843750\n",
      "    epoch          : 235\n",
      "    loss           : -323116.78953125\n",
      "    val_loss       : -286760.42927951814\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -426969.593750\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -339352.000000\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -333859.031250\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -423734.906250\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -343099.250000\n",
      "    epoch          : 236\n",
      "    loss           : -314572.48890625\n",
      "    val_loss       : -294491.3849076271\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -351384.281250\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -293751.281250\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -282432.468750\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -303994.312500\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -271957.843750\n",
      "    epoch          : 237\n",
      "    loss           : -312337.81421875\n",
      "    val_loss       : -296221.92443771363\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -308710.218750\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -280065.875000\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -310712.343750\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -277584.906250\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -271541.062500\n",
      "    epoch          : 238\n",
      "    loss           : -325099.49203125\n",
      "    val_loss       : -299656.7733884811\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -435586.031250\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -434499.437500\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -258175.187500\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -126181.414062\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -79572.718750\n",
      "    epoch          : 239\n",
      "    loss           : -138038.13494140626\n",
      "    val_loss       : -139240.427017498\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -256708.734375\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -73809.968750\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -409878.187500\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -304106.437500\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -316526.062500\n",
      "    epoch          : 240\n",
      "    loss           : -276177.2273242187\n",
      "    val_loss       : -294864.743502903\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -303764.406250\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -348665.656250\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -325134.562500\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -288743.000000\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -335005.750000\n",
      "    epoch          : 241\n",
      "    loss           : -320967.4734375\n",
      "    val_loss       : -295693.0071599007\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -421926.468750\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -288888.593750\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -356194.281250\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -350104.375000\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -358549.625000\n",
      "    epoch          : 242\n",
      "    loss           : -324482.20765625\n",
      "    val_loss       : -296131.1843878746\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -277321.093750\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -355655.750000\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -273122.812500\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -279559.187500\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -326596.187500\n",
      "    epoch          : 243\n",
      "    loss           : -320461.45046875\n",
      "    val_loss       : -292811.2792325973\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -338321.625000\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -306262.375000\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -342074.750000\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -328935.593750\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -306591.781250\n",
      "    epoch          : 244\n",
      "    loss           : -323927.45640625\n",
      "    val_loss       : -292725.9420104027\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -358608.125000\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -429028.281250\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -435292.687500\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -300205.687500\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -348860.656250\n",
      "    epoch          : 245\n",
      "    loss           : -322764.916875\n",
      "    val_loss       : -295693.3535323143\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -283541.625000\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -322852.781250\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -168523.921875\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -347581.312500\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -346133.875000\n",
      "    epoch          : 246\n",
      "    loss           : -325813.46171875\n",
      "    val_loss       : -302475.5132720947\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -436697.656250\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -436107.906250\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -432097.562500\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -210306.156250\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -329716.468750\n",
      "    epoch          : 247\n",
      "    loss           : -328896.6053125\n",
      "    val_loss       : -298350.44930448534\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -199583.937500\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -200903.250000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -321977.625000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -300187.781250\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -430852.562500\n",
      "    epoch          : 248\n",
      "    loss           : -326344.89421875\n",
      "    val_loss       : -300134.29795637133\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -358456.875000\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -301166.625000\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -436578.125000\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -422681.187500\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -353514.125000\n",
      "    epoch          : 249\n",
      "    loss           : -322811.46828125\n",
      "    val_loss       : -290190.3694069862\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -277631.187500\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -282360.812500\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -297734.875000\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -293566.500000\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -395816.531250\n",
      "    epoch          : 250\n",
      "    loss           : -305232.388203125\n",
      "    val_loss       : -292152.6921297073\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0326_214204/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -328281.437500\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -352470.062500\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -188605.125000\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -351194.562500\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -253293.500000\n",
      "    epoch          : 251\n",
      "    loss           : -316315.60671875\n",
      "    val_loss       : -290546.1728446007\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -184836.203125\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -311240.562500\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -299738.687500\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -357181.156250\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -319050.281250\n",
      "    epoch          : 252\n",
      "    loss           : -321132.99375\n",
      "    val_loss       : -296157.6072727203\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -422684.750000\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -170565.125000\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -432057.812500\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -202814.609375\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -277444.500000\n",
      "    epoch          : 253\n",
      "    loss           : -323127.57125\n",
      "    val_loss       : -296245.5774328232\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -183818.500000\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -328377.250000\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -362417.031250\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -301432.125000\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -335827.062500\n",
      "    epoch          : 254\n",
      "    loss           : -331761.02\n",
      "    val_loss       : -302152.65190057753\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -282292.625000\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -340029.375000\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -200943.328125\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -444637.375000\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -347286.562500\n",
      "    epoch          : 255\n",
      "    loss           : -331370.1690625\n",
      "    val_loss       : -300707.1636686325\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -442183.250000\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -182304.156250\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -268201.375000\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -90088.195312\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -180962.671875\n",
      "    epoch          : 256\n",
      "    loss           : -266295.8892578125\n",
      "    val_loss       : -276470.87630119326\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -406185.781250\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -301296.250000\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -347716.750000\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -332650.125000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -365096.093750\n",
      "    epoch          : 257\n",
      "    loss           : -316617.34640625\n",
      "    val_loss       : -300771.6904273033\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -300268.718750\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -438795.625000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -179000.250000\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -363104.437500\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -327327.156250\n",
      "    epoch          : 258\n",
      "    loss           : -327093.39421875\n",
      "    val_loss       : -267774.1889657974\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -324066.625000\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -359621.718750\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -158691.187500\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -416668.281250\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -356017.500000\n",
      "    epoch          : 259\n",
      "    loss           : -319129.72984375\n",
      "    val_loss       : -293294.8214903831\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -441469.468750\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -286204.843750\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -181034.468750\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -368274.062500\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -293525.562500\n",
      "    epoch          : 260\n",
      "    loss           : -320446.1496875\n",
      "    val_loss       : -288648.3789779663\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -332340.437500\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -307736.562500\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -311102.437500\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -428144.750000\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -425768.656250\n",
      "    epoch          : 261\n",
      "    loss           : -320438.2871875\n",
      "    val_loss       : -267847.289985466\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -300747.343750\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -275624.625000\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -440957.312500\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -330957.000000\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -266397.625000\n",
      "    epoch          : 262\n",
      "    loss           : -315558.96625\n",
      "    val_loss       : -302285.9504647255\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -445310.625000\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -313093.437500\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -305334.875000\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -179385.343750\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -312538.750000\n",
      "    epoch          : 263\n",
      "    loss           : -327107.53484375\n",
      "    val_loss       : -294652.67392454145\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -440102.375000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -185551.531250\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -234891.906250\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -283292.812500\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -278000.437500\n",
      "    epoch          : 264\n",
      "    loss           : -308378.612890625\n",
      "    val_loss       : -297930.44727830886\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -301706.062500\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -276542.687500\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -249427.218750\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -299699.500000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -263278.062500\n",
      "    epoch          : 265\n",
      "    loss           : -311327.3921875\n",
      "    val_loss       : -292134.0243598938\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -415793.281250\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -143720.437500\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -199692.187500\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -333199.750000\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -208789.171875\n",
      "    epoch          : 266\n",
      "    loss           : -245770.76712890624\n",
      "    val_loss       : -174295.73022699356\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: 41663.480469\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -241428.312500\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -240444.484375\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -284588.375000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -331759.562500\n",
      "    epoch          : 267\n",
      "    loss           : -293472.4423828125\n",
      "    val_loss       : -289805.16979589465\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -413381.562500\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -285068.562500\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -277158.562500\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -179355.640625\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -266128.187500\n",
      "    epoch          : 268\n",
      "    loss           : -318283.0196875\n",
      "    val_loss       : -253222.14024772643\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -254883.062500\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -261936.843750\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -297202.250000\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -343792.781250\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -342585.718750\n",
      "    epoch          : 269\n",
      "    loss           : -307491.5441015625\n",
      "    val_loss       : -297853.7613289833\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -314752.625000\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -312286.531250\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -202418.656250\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -444195.250000\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -339915.375000\n",
      "    epoch          : 270\n",
      "    loss           : -334164.32296875\n",
      "    val_loss       : -302014.06205377576\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -319030.125000\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -283689.375000\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -314654.937500\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -308405.531250\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -322259.000000\n",
      "    epoch          : 271\n",
      "    loss           : -336005.92375\n",
      "    val_loss       : -302182.0732773781\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -212999.015625\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -213313.093750\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -316774.000000\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -289558.343750\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -341797.187500\n",
      "    epoch          : 272\n",
      "    loss           : -336055.62328125\n",
      "    val_loss       : -298843.64430150983\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -310342.000000\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -195125.421875\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -355479.937500\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -442862.531250\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -337551.750000\n",
      "    epoch          : 273\n",
      "    loss           : -330455.13875\n",
      "    val_loss       : -300323.8631147385\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -309911.687500\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -138370.843750\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -375422.656250\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -283038.187500\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -347598.187500\n",
      "    epoch          : 274\n",
      "    loss           : -297667.78578125\n",
      "    val_loss       : -283204.2771323204\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -411260.281250\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -269963.125000\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -277445.750000\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -310372.000000\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -173380.593750\n",
      "    epoch          : 275\n",
      "    loss           : -318172.605234375\n",
      "    val_loss       : -291390.9539509773\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -176248.484375\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -358166.281250\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -306850.937500\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -292451.500000\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -371473.125000\n",
      "    epoch          : 276\n",
      "    loss           : -325578.24796875\n",
      "    val_loss       : -288369.5465970993\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -158101.656250\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -321077.750000\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -340474.906250\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -358832.562500\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -354648.250000\n",
      "    epoch          : 277\n",
      "    loss           : -326425.67328125\n",
      "    val_loss       : -304507.7626630783\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -202879.937500\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -200867.968750\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -349439.625000\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -440524.906250\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -364797.625000\n",
      "    epoch          : 278\n",
      "    loss           : -328674.34734375\n",
      "    val_loss       : -289850.12820544245\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -347772.125000\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -307261.468750\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -448316.906250\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -336448.718750\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -336770.375000\n",
      "    epoch          : 279\n",
      "    loss           : -334656.18125\n",
      "    val_loss       : -301881.8974064827\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -311418.187500\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -294234.062500\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -432590.593750\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -356002.687500\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -350961.187500\n",
      "    epoch          : 280\n",
      "    loss           : -324325.043125\n",
      "    val_loss       : -289577.6571771622\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -169617.500000\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -281091.375000\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -266619.562500\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -289272.750000\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -235527.843750\n",
      "    epoch          : 281\n",
      "    loss           : -295059.565390625\n",
      "    val_loss       : -256188.0513192177\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -322845.125000\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -248106.843750\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -401031.187500\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -293590.437500\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -250422.156250\n",
      "    epoch          : 282\n",
      "    loss           : -305607.9234375\n",
      "    val_loss       : -298785.45812397\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -317701.781250\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -343076.812500\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -414767.562500\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -179393.406250\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -286223.343750\n",
      "    epoch          : 283\n",
      "    loss           : -314869.8565625\n",
      "    val_loss       : -296197.5679502487\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -296286.906250\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -163412.234375\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -311991.250000\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -282738.937500\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -305529.375000\n",
      "    epoch          : 284\n",
      "    loss           : -305564.49125\n",
      "    val_loss       : -260096.78472833632\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -261549.281250\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -181034.187500\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -274584.250000\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -291088.500000\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -277881.562500\n",
      "    epoch          : 285\n",
      "    loss           : -312015.378125\n",
      "    val_loss       : -294249.91765184404\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -346702.125000\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -291854.875000\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -308093.687500\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -363563.125000\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -200678.984375\n",
      "    epoch          : 286\n",
      "    loss           : -317415.4325\n",
      "    val_loss       : -288442.31708498\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -182407.531250\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -253369.203125\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -288879.281250\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -345159.125000\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -432616.500000\n",
      "    epoch          : 287\n",
      "    loss           : -325886.285\n",
      "    val_loss       : -299904.4634996414\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -448075.937500\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -321274.718750\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -311840.906250\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -276820.187500\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -352655.968750\n",
      "    epoch          : 288\n",
      "    loss           : -332881.86953125\n",
      "    val_loss       : -293726.87477846147\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -278654.031250\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -313646.125000\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -427391.437500\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -294413.500000\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -328299.156250\n",
      "    epoch          : 289\n",
      "    loss           : -335750.57765625\n",
      "    val_loss       : -305046.92520427704\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -449634.812500\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -314470.562500\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -291769.218750\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -342470.500000\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -362696.562500\n",
      "    epoch          : 290\n",
      "    loss           : -339362.1240625\n",
      "    val_loss       : -304416.98038549424\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -316954.312500\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -441050.156250\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -314390.843750\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -353873.562500\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -324307.218750\n",
      "    epoch          : 291\n",
      "    loss           : -321494.7324609375\n",
      "    val_loss       : -302090.9497027397\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -336595.843750\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -307002.125000\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -276873.468750\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -281603.375000\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -436943.562500\n",
      "    epoch          : 292\n",
      "    loss           : -316672.80546875\n",
      "    val_loss       : -295422.5412659645\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -359017.312500\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -273206.437500\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -295583.312500\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -297351.312500\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -446667.468750\n",
      "    epoch          : 293\n",
      "    loss           : -337789.83671875\n",
      "    val_loss       : -306729.9724312782\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -459388.968750\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -452667.125000\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -441550.687500\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -319483.812500\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -331560.937500\n",
      "    epoch          : 294\n",
      "    loss           : -340965.41734375\n",
      "    val_loss       : -304039.9595065117\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -308948.125000\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -356322.187500\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -277762.500000\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -278769.750000\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -249349.421875\n",
      "    epoch          : 295\n",
      "    loss           : -271892.1101171875\n",
      "    val_loss       : -278706.1998553276\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -237555.375000\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -264836.875000\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -122152.273438\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -288933.875000\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -207329.703125\n",
      "    epoch          : 296\n",
      "    loss           : -289567.9175390625\n",
      "    val_loss       : -134939.79280529023\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -258322.828125\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -270965.875000\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -57905.679688\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -188090.312500\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -277931.437500\n",
      "    epoch          : 297\n",
      "    loss           : -217226.8297265625\n",
      "    val_loss       : -281090.6598093033\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -286103.593750\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -355548.187500\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -310075.000000\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -318168.093750\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -319675.250000\n",
      "    epoch          : 298\n",
      "    loss           : -321934.87640625\n",
      "    val_loss       : -300931.4035150528\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -448705.000000\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -273408.093750\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -305497.812500\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -322565.750000\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -353603.593750\n",
      "    epoch          : 299\n",
      "    loss           : -332774.45171875\n",
      "    val_loss       : -305217.008558178\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -316316.156250\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -442763.000000\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -214839.421875\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -307737.781250\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -291476.312500\n",
      "    epoch          : 300\n",
      "    loss           : -341339.935625\n",
      "    val_loss       : -305140.3812552452\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0326_214204/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -306044.500000\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -293595.343750\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -216620.906250\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -361053.968750\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -354556.125000\n",
      "    epoch          : 301\n",
      "    loss           : -342136.69046875\n",
      "    val_loss       : -305244.23417873384\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -353885.687500\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -370937.593750\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -311916.000000\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -377276.187500\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -373228.625000\n",
      "    epoch          : 302\n",
      "    loss           : -342751.71046875\n",
      "    val_loss       : -305814.6058992386\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -347610.156250\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -377109.875000\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -320797.656250\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -283152.312500\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -364717.531250\n",
      "    epoch          : 303\n",
      "    loss           : -340658.1378125\n",
      "    val_loss       : -309006.54230451584\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -316974.750000\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -348234.531250\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -449554.187500\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -371417.875000\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -355648.156250\n",
      "    epoch          : 304\n",
      "    loss           : -339038.1321875\n",
      "    val_loss       : -303730.1931111336\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -344256.656250\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -321149.687500\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -281522.875000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -288502.375000\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -347242.156250\n",
      "    epoch          : 305\n",
      "    loss           : -327038.41921875\n",
      "    val_loss       : -284203.01007175446\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -308055.656250\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -322243.406250\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -438008.125000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -360756.125000\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -375526.343750\n",
      "    epoch          : 306\n",
      "    loss           : -333566.4571875\n",
      "    val_loss       : -295660.7508339882\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -273532.468750\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -317251.968750\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -320007.218750\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -348315.375000\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -268757.750000\n",
      "    epoch          : 307\n",
      "    loss           : -329685.37125\n",
      "    val_loss       : -261461.4989163399\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -311431.468750\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -326465.500000\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -233763.390625\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -319666.187500\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -310194.750000\n",
      "    epoch          : 308\n",
      "    loss           : -288465.813359375\n",
      "    val_loss       : -287671.27912540437\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -273503.781250\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -443618.250000\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -179861.781250\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -307428.281250\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -365028.843750\n",
      "    epoch          : 309\n",
      "    loss           : -327028.47171875\n",
      "    val_loss       : -308218.4073641777\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -290778.250000\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -453325.500000\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -348605.250000\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -341877.437500\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -349328.156250\n",
      "    epoch          : 310\n",
      "    loss           : -344270.69765625\n",
      "    val_loss       : -308357.30351848603\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -450516.406250\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -304868.656250\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -452880.625000\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -294846.625000\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -371843.375000\n",
      "    epoch          : 311\n",
      "    loss           : -340792.32015625\n",
      "    val_loss       : -298648.9312328339\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -323545.125000\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -356703.156250\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -313053.062500\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -232003.687500\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -111569.968750\n",
      "    epoch          : 312\n",
      "    loss           : -291185.6548828125\n",
      "    val_loss       : -192717.54719200134\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -204857.640625\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -161987.625000\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -179862.953125\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -132804.937500\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -298310.343750\n",
      "    epoch          : 313\n",
      "    loss           : -221817.86747070312\n",
      "    val_loss       : -289889.12634449004\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -330784.500000\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -292146.500000\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -291442.250000\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -437757.750000\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -332753.781250\n",
      "    epoch          : 314\n",
      "    loss           : -331555.87890625\n",
      "    val_loss       : -307513.5056351662\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -287512.875000\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -304900.687500\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -222390.218750\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -373496.437500\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -312795.687500\n",
      "    epoch          : 315\n",
      "    loss           : -343196.41234375\n",
      "    val_loss       : -307668.00744981767\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -343103.343750\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -452168.593750\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -327028.000000\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -348306.281250\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -358242.437500\n",
      "    epoch          : 316\n",
      "    loss           : -345216.53984375\n",
      "    val_loss       : -307766.04720964434\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -323438.875000\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -456069.593750\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -380571.406250\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -216714.609375\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -344057.843750\n",
      "    epoch          : 317\n",
      "    loss           : -345996.265\n",
      "    val_loss       : -305187.77419843676\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -343343.125000\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -450713.500000\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -336916.937500\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -288318.250000\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -347373.500000\n",
      "    epoch          : 318\n",
      "    loss           : -340441.78609375\n",
      "    val_loss       : -305725.212515831\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -456861.625000\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -210992.593750\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -454885.031250\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -375079.500000\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -299590.687500\n",
      "    epoch          : 319\n",
      "    loss           : -345375.82515625\n",
      "    val_loss       : -309076.5775391579\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -294265.125000\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -349282.656250\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -214662.312500\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -343635.281250\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -320214.312500\n",
      "    epoch          : 320\n",
      "    loss           : -345049.41046875\n",
      "    val_loss       : -305634.8316978455\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -299683.375000\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -421418.343750\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -297171.093750\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -333553.125000\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -271552.500000\n",
      "    epoch          : 321\n",
      "    loss           : -325970.90609375\n",
      "    val_loss       : -305050.6278042793\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -219174.468750\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -326995.375000\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -286630.468750\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -422424.156250\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -277500.000000\n",
      "    epoch          : 322\n",
      "    loss           : -304526.750703125\n",
      "    val_loss       : -285231.02600297926\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -287682.250000\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -300451.125000\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -426795.000000\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -302917.156250\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -299692.125000\n",
      "    epoch          : 323\n",
      "    loss           : -311113.64703125\n",
      "    val_loss       : -298466.310310173\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -321744.250000\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -416520.750000\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -200273.468750\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -374932.437500\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -353292.812500\n",
      "    epoch          : 324\n",
      "    loss           : -336377.42796875\n",
      "    val_loss       : -308320.1081873894\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -326580.375000\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -327324.531250\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -458980.187500\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -375564.500000\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -288210.125000\n",
      "    epoch          : 325\n",
      "    loss           : -345116.6928125\n",
      "    val_loss       : -309270.73140687944\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -323892.218750\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -309052.000000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -335068.562500\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -406181.062500\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -325704.437500\n",
      "    epoch          : 326\n",
      "    loss           : -338198.626875\n",
      "    val_loss       : -299002.0077091217\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -450490.437500\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -205326.000000\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -319661.437500\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -322914.375000\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -375995.437500\n",
      "    epoch          : 327\n",
      "    loss           : -338847.55\n",
      "    val_loss       : -303453.03388500214\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -379059.437500\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -323203.812500\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -289939.250000\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -282221.812500\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -372765.187500\n",
      "    epoch          : 328\n",
      "    loss           : -337708.6496875\n",
      "    val_loss       : -296453.2586229324\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -341879.687500\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -428535.562500\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -418201.406250\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -261295.187500\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -269778.406250\n",
      "    epoch          : 329\n",
      "    loss           : -321717.160859375\n",
      "    val_loss       : -295487.9665174484\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -429332.218750\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -298742.125000\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -312002.812500\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -152817.328125\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -314740.281250\n",
      "    epoch          : 330\n",
      "    loss           : -322611.7109375\n",
      "    val_loss       : -296269.99895658495\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -286304.750000\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -451793.375000\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -325307.937500\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -458838.562500\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -322487.031250\n",
      "    epoch          : 331\n",
      "    loss           : -332163.4940625\n",
      "    val_loss       : -283621.70846996305\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -273318.687500\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -373142.093750\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -328640.312500\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -237678.265625\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -181901.234375\n",
      "    epoch          : 332\n",
      "    loss           : -229954.2494921875\n",
      "    val_loss       : -251954.1051082611\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -248686.718750\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -171960.250000\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -320274.406250\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -291249.437500\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -336567.687500\n",
      "    epoch          : 333\n",
      "    loss           : -326460.69515625\n",
      "    val_loss       : -304862.4363189697\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -315547.875000\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -323572.562500\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -220768.625000\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -459225.656250\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -377448.187500\n",
      "    epoch          : 334\n",
      "    loss           : -345777.62625\n",
      "    val_loss       : -307220.89546079637\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -456579.312500\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -320385.062500\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -303083.875000\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -345473.062500\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -380036.156250\n",
      "    epoch          : 335\n",
      "    loss           : -347198.898125\n",
      "    val_loss       : -309937.5872068405\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -307395.312500\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -349558.937500\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -345791.687500\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -453467.000000\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -263180.187500\n",
      "    epoch          : 336\n",
      "    loss           : -342607.01578125\n",
      "    val_loss       : -299903.60193948745\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -201725.609375\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -347703.781250\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -361850.000000\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -435371.437500\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -258047.546875\n",
      "    epoch          : 337\n",
      "    loss           : -321764.8055078125\n",
      "    val_loss       : -286076.4231101036\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -358916.187500\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -195959.500000\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -448914.406250\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -349648.625000\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -343576.562500\n",
      "    epoch          : 338\n",
      "    loss           : -338096.3328125\n",
      "    val_loss       : -309702.826731205\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -317508.656250\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -449571.687500\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -442761.062500\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -325260.687500\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -295360.375000\n",
      "    epoch          : 339\n",
      "    loss           : -347655.370625\n",
      "    val_loss       : -308077.7993778229\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -286522.406250\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -378260.625000\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -295568.281250\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -459425.750000\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -461803.656250\n",
      "    epoch          : 340\n",
      "    loss           : -350836.6546875\n",
      "    val_loss       : -311448.13118658063\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -463784.687500\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -466914.656250\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -297614.468750\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -376596.187500\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -258616.796875\n",
      "    epoch          : 341\n",
      "    loss           : -342761.6303125\n",
      "    val_loss       : -274561.45940876007\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -416960.312500\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -212830.187500\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -420067.031250\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -310663.656250\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -428279.031250\n",
      "    epoch          : 342\n",
      "    loss           : -300919.881015625\n",
      "    val_loss       : -281511.3955578804\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -448813.187500\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -415785.500000\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -292028.562500\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -158615.468750\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: 23143.568359\n",
      "    epoch          : 343\n",
      "    loss           : -282293.7501269531\n",
      "    val_loss       : 185345.9630642891\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: 47159.652344\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -181440.609375\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -244488.687500\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -312004.843750\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -333990.406250\n",
      "    epoch          : 344\n",
      "    loss           : -195682.7328515625\n",
      "    val_loss       : -308198.2688564301\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -303350.250000\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -452124.500000\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -444356.625000\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -455604.562500\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -287871.156250\n",
      "    epoch          : 345\n",
      "    loss           : -342440.11984375\n",
      "    val_loss       : -310137.78664398193\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -454214.843750\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -302291.312500\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -341962.468750\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -342211.375000\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -361998.875000\n",
      "    epoch          : 346\n",
      "    loss           : -347494.36265625\n",
      "    val_loss       : -309661.8870288849\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -329970.937500\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -458733.750000\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -456757.500000\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -454614.781250\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -367618.687500\n",
      "    epoch          : 347\n",
      "    loss           : -350208.6040625\n",
      "    val_loss       : -309995.0979909897\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -459697.750000\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -316611.281250\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -343809.187500\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -328998.000000\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -374199.156250\n",
      "    epoch          : 348\n",
      "    loss           : -351134.07453125\n",
      "    val_loss       : -308696.4122104645\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -347198.781250\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -461646.000000\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -332434.968750\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -376864.125000\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -286756.625000\n",
      "    epoch          : 349\n",
      "    loss           : -350924.51640625\n",
      "    val_loss       : -306296.08986940386\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -287112.687500\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -333717.562500\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -327114.125000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -323597.125000\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -294828.593750\n",
      "    epoch          : 350\n",
      "    loss           : -347909.73921875\n",
      "    val_loss       : -307171.25067157747\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0326_214204/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -460053.968750\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -289410.906250\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -223284.906250\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -456758.875000\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -458875.562500\n",
      "    epoch          : 351\n",
      "    loss           : -351262.08171875\n",
      "    val_loss       : -306499.02837085724\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -465674.625000\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -301794.687500\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -459922.687500\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -309905.250000\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -311544.281250\n",
      "    epoch          : 352\n",
      "    loss           : -331801.4813671875\n",
      "    val_loss       : -282755.50901727675\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -268855.468750\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -454919.375000\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -454202.812500\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -322987.875000\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -325058.406250\n",
      "    epoch          : 353\n",
      "    loss           : -343626.650625\n",
      "    val_loss       : -312747.1999264717\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -346821.375000\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -214894.265625\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -329766.750000\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -450907.843750\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -361784.687500\n",
      "    epoch          : 354\n",
      "    loss           : -343697.7153125\n",
      "    val_loss       : -310170.767473793\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -302628.437500\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -325089.500000\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -383628.625000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -332402.406250\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -346793.437500\n",
      "    epoch          : 355\n",
      "    loss           : -341980.34953125\n",
      "    val_loss       : -285536.7724970818\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -281060.500000\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -327655.593750\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -364847.500000\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -373953.562500\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -285714.625000\n",
      "    epoch          : 356\n",
      "    loss           : -338329.36921875\n",
      "    val_loss       : -307778.08163061144\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -462848.125000\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -300474.062500\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -368998.312500\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -306066.812500\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -339006.625000\n",
      "    epoch          : 357\n",
      "    loss           : -342640.061875\n",
      "    val_loss       : -300028.04492444993\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -341291.250000\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -218613.328125\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -281943.375000\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -309546.000000\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -292460.281250\n",
      "    epoch          : 358\n",
      "    loss           : -337802.2453125\n",
      "    val_loss       : -298755.78322229383\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -324490.375000\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -324997.375000\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -461279.562500\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -339170.187500\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -373406.531250\n",
      "    epoch          : 359\n",
      "    loss           : -345488.15171875\n",
      "    val_loss       : -306998.81158123014\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -335894.406250\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -378514.281250\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -372869.343750\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -291128.468750\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -186353.953125\n",
      "    epoch          : 360\n",
      "    loss           : -338587.06609375\n",
      "    val_loss       : -271603.8012518883\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -270966.281250\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -285857.531250\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -369344.000000\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -303874.062500\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -318263.906250\n",
      "    epoch          : 361\n",
      "    loss           : -284178.892265625\n",
      "    val_loss       : -278114.6217132568\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -273028.687500\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -295423.125000\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -378501.562500\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -340748.031250\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -355360.125000\n",
      "    epoch          : 362\n",
      "    loss           : -336332.93984375\n",
      "    val_loss       : -288225.84013528825\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -342025.250000\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -357552.375000\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -292832.750000\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -283944.875000\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -344065.218750\n",
      "    epoch          : 363\n",
      "    loss           : -333182.69703125\n",
      "    val_loss       : -287510.99858665466\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -194380.093750\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -161069.031250\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -273138.437500\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -287128.375000\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -277798.312500\n",
      "    epoch          : 364\n",
      "    loss           : -315476.60765625\n",
      "    val_loss       : -305365.4107093811\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -313426.218750\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -370728.937500\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -461089.875000\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -362714.500000\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -297228.437500\n",
      "    epoch          : 365\n",
      "    loss           : -351475.4840625\n",
      "    val_loss       : -314704.9545487404\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -461994.312500\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -328146.250000\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -307056.656250\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -322266.437500\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -355659.625000\n",
      "    epoch          : 366\n",
      "    loss           : -355015.99609375\n",
      "    val_loss       : -308978.1383760452\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -227005.781250\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -468024.812500\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -356889.125000\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -336397.937500\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -298248.031250\n",
      "    epoch          : 367\n",
      "    loss           : -354683.22890625\n",
      "    val_loss       : -309328.89663705823\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -306669.625000\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -290545.625000\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -465531.937500\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -322363.937500\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -333168.062500\n",
      "    epoch          : 368\n",
      "    loss           : -347852.16703125\n",
      "    val_loss       : -306007.48197460175\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -467830.531250\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -348098.187500\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -244611.343750\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -315134.406250\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -360020.656250\n",
      "    epoch          : 369\n",
      "    loss           : -327483.1265625\n",
      "    val_loss       : -310538.61673851014\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -320027.500000\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -330092.125000\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -330761.281250\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -335945.000000\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -310439.937500\n",
      "    epoch          : 370\n",
      "    loss           : -352595.521875\n",
      "    val_loss       : -309438.6913204193\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -355214.468750\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -331620.250000\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -312085.750000\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -327823.812500\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -348045.750000\n",
      "    epoch          : 371\n",
      "    loss           : -351507.00375\n",
      "    val_loss       : -304698.83096551895\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -467392.906250\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -432286.218750\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -294084.875000\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -439476.375000\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -342916.156250\n",
      "    epoch          : 372\n",
      "    loss           : -337744.94515625\n",
      "    val_loss       : -307191.85738449096\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -461162.312500\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -362024.625000\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -351448.000000\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -448988.875000\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -367845.750000\n",
      "    epoch          : 373\n",
      "    loss           : -344067.8584375\n",
      "    val_loss       : -308997.47919893265\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -296793.562500\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -318229.750000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -460509.937500\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -361820.562500\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -375652.937500\n",
      "    epoch          : 374\n",
      "    loss           : -356136.0171875\n",
      "    val_loss       : -312392.99372463225\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -336744.093750\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -220406.218750\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -463995.687500\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -335388.562500\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -293741.843750\n",
      "    epoch          : 375\n",
      "    loss           : -356513.05171875\n",
      "    val_loss       : -312835.766642189\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -330072.906250\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -224087.234375\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -359970.406250\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -342254.562500\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -359303.156250\n",
      "    epoch          : 376\n",
      "    loss           : -343244.716875\n",
      "    val_loss       : -305073.98805789946\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -306793.312500\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -448908.218750\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -369612.062500\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -331604.500000\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -235352.265625\n",
      "    epoch          : 377\n",
      "    loss           : -337385.063125\n",
      "    val_loss       : -291319.64830875397\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -321178.750000\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -319260.375000\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -268551.968750\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -322995.812500\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -313604.968750\n",
      "    epoch          : 378\n",
      "    loss           : -298226.67109375\n",
      "    val_loss       : -302476.8715245247\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -302966.062500\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -301446.625000\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -291601.562500\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -329154.000000\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -387784.625000\n",
      "    epoch          : 379\n",
      "    loss           : -344380.22484375\n",
      "    val_loss       : -313772.0156702042\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -221949.484375\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -353565.125000\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -375074.031250\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -390063.593750\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -377881.937500\n",
      "    epoch          : 380\n",
      "    loss           : -357643.07609375\n",
      "    val_loss       : -314673.7963204384\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -230138.359375\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -337722.468750\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -338651.781250\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -473933.343750\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -314550.812500\n",
      "    epoch          : 381\n",
      "    loss           : -360455.85328125\n",
      "    val_loss       : -314464.116592598\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -335069.937500\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -344115.062500\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -332752.593750\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -459344.000000\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -333024.375000\n",
      "    epoch          : 382\n",
      "    loss           : -351241.855625\n",
      "    val_loss       : -230538.47546014786\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -271565.625000\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -430132.250000\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -285292.593750\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -340061.375000\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -204551.500000\n",
      "    epoch          : 383\n",
      "    loss           : -328314.06734375\n",
      "    val_loss       : -267415.6258480072\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -436726.093750\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -290311.625000\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -246203.390625\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -288461.812500\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -277673.312500\n",
      "    epoch          : 384\n",
      "    loss           : -261194.2024609375\n",
      "    val_loss       : -269398.03895168303\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -429633.062500\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -282657.156250\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -292303.343750\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -338422.812500\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -354313.093750\n",
      "    epoch          : 385\n",
      "    loss           : -334699.8978125\n",
      "    val_loss       : -311501.99528074265\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -335871.562500\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -453716.031250\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -375521.687500\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -378911.156250\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -341630.625000\n",
      "    epoch          : 386\n",
      "    loss           : -353701.208125\n",
      "    val_loss       : -306014.32332963945\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -331767.687500\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -294457.812500\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -375707.906250\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -332167.875000\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -354799.187500\n",
      "    epoch          : 387\n",
      "    loss           : -350503.96359375\n",
      "    val_loss       : -310138.3917771339\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -336639.750000\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -324351.656250\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -320904.062500\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -324256.625000\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -457338.625000\n",
      "    epoch          : 388\n",
      "    loss           : -347816.4078125\n",
      "    val_loss       : -301720.303153038\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -319245.312500\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -471765.031250\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -288581.218750\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -343867.000000\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -291654.375000\n",
      "    epoch          : 389\n",
      "    loss           : -342468.57625\n",
      "    val_loss       : -277557.9484249115\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -353847.531250\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -442815.281250\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -318649.312500\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -203536.156250\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -334419.812500\n",
      "    epoch          : 390\n",
      "    loss           : -346384.23953125\n",
      "    val_loss       : -311146.8397760391\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -358591.625000\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -467016.750000\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -307818.031250\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -376549.187500\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -338940.437500\n",
      "    epoch          : 391\n",
      "    loss           : -345071.6678125\n",
      "    val_loss       : -265680.1000333786\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -308654.281250\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -441140.656250\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -291022.625000\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -196679.062500\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -230254.437500\n",
      "    epoch          : 392\n",
      "    loss           : -270150.41828125\n",
      "    val_loss       : -269253.9688584328\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -266410.187500\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -320257.843750\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -328706.375000\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -466244.750000\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -346290.343750\n",
      "    epoch          : 393\n",
      "    loss           : -342624.03515625\n",
      "    val_loss       : -307005.3023808479\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -470532.125000\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -285801.062500\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -459478.625000\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -329143.250000\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -372584.281250\n",
      "    epoch          : 394\n",
      "    loss           : -344139.12265625\n",
      "    val_loss       : -310250.42661304475\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -228014.625000\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -380794.562500\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -470034.937500\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -356240.875000\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -472877.781250\n",
      "    epoch          : 395\n",
      "    loss           : -359006.83984375\n",
      "    val_loss       : -313189.6318536758\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -241396.750000\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -228534.281250\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -324888.437500\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -389270.375000\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -316048.875000\n",
      "    epoch          : 396\n",
      "    loss           : -359191.2990625\n",
      "    val_loss       : -312089.89802846906\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -473660.843750\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -374972.375000\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -328269.437500\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -313435.312500\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -324097.937500\n",
      "    epoch          : 397\n",
      "    loss           : -356024.9171875\n",
      "    val_loss       : -312989.0577269554\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -225618.984375\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -237695.468750\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -457230.531250\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -354126.468750\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -372304.062500\n",
      "    epoch          : 398\n",
      "    loss           : -352346.733125\n",
      "    val_loss       : -311649.99885816575\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -325598.500000\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -472045.750000\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -394154.562500\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -472524.312500\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -361022.843750\n",
      "    epoch          : 399\n",
      "    loss           : -356969.56828125\n",
      "    val_loss       : -314395.0373781204\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -334868.937500\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -308059.562500\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -220702.187500\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -357837.500000\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -312017.625000\n",
      "    epoch          : 400\n",
      "    loss           : -345167.5921875\n",
      "    val_loss       : -280367.823783493\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0326_214204/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -201131.343750\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -224663.343750\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -332850.750000\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -311038.500000\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -340131.750000\n",
      "    epoch          : 401\n",
      "    loss           : -294495.747265625\n",
      "    val_loss       : -297198.47332258226\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -453143.312500\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -202123.718750\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -284156.812500\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -465522.406250\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -372319.000000\n",
      "    epoch          : 402\n",
      "    loss           : -346630.96875\n",
      "    val_loss       : -311019.4488467217\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -469297.312500\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -296138.968750\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -323785.625000\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -337273.562500\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -397785.406250\n",
      "    epoch          : 403\n",
      "    loss           : -358010.12390625\n",
      "    val_loss       : -318384.71324748994\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -474888.812500\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -340462.750000\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -471996.062500\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -469064.562500\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -398736.281250\n",
      "    epoch          : 404\n",
      "    loss           : -358581.499375\n",
      "    val_loss       : -314366.67914943694\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -339717.500000\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -305668.625000\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -347253.562500\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -383893.687500\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -265291.750000\n",
      "    epoch          : 405\n",
      "    loss           : -330060.390078125\n",
      "    val_loss       : 27106.64635486603\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -157021.781250\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -273294.562500\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -178769.015625\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -346610.531250\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -294569.281250\n",
      "    epoch          : 406\n",
      "    loss           : -306266.55627929687\n",
      "    val_loss       : -286030.73248987197\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -288634.406250\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -282672.750000\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -438046.312500\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -393711.468750\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -254643.390625\n",
      "    epoch          : 407\n",
      "    loss           : -269302.87646484375\n",
      "    val_loss       : -297144.48418502806\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -357877.656250\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -412079.906250\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -192155.656250\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -339149.093750\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -371819.562500\n",
      "    epoch          : 408\n",
      "    loss           : -324756.01953125\n",
      "    val_loss       : -313802.7808999062\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -342326.562500\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -306518.125000\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -330230.156250\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -220010.843750\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -380397.375000\n",
      "    epoch          : 409\n",
      "    loss           : -352528.2853125\n",
      "    val_loss       : -313929.48975782393\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -478837.906250\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -466510.375000\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -330667.187500\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -327371.937500\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -315684.062500\n",
      "    epoch          : 410\n",
      "    loss           : -361038.14515625\n",
      "    val_loss       : -312380.03071365354\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -474136.625000\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -334712.812500\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -398791.531250\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -315985.312500\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -362678.812500\n",
      "    epoch          : 411\n",
      "    loss           : -363353.4525\n",
      "    val_loss       : -316784.8486824989\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -339533.187500\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -478159.375000\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -474480.593750\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -326990.343750\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -365360.781250\n",
      "    epoch          : 412\n",
      "    loss           : -364146.21578125\n",
      "    val_loss       : -319618.9584241867\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -469730.500000\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -366620.500000\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -474452.031250\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -401562.843750\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -384196.593750\n",
      "    epoch          : 413\n",
      "    loss           : -362761.94984375\n",
      "    val_loss       : -312564.4864644051\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -337460.125000\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -474699.125000\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -361897.062500\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -220732.984375\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -382477.031250\n",
      "    epoch          : 414\n",
      "    loss           : -360047.7375\n",
      "    val_loss       : -310513.50654087064\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -337690.062500\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -271862.718750\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -223714.312500\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -155480.468750\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -92464.664062\n",
      "    epoch          : 415\n",
      "    loss           : -230995.69064453125\n",
      "    val_loss       : 159281.26039733886\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -9772.272461\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -60174.632812\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -333299.187500\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -127749.914062\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -322411.125000\n",
      "    epoch          : 416\n",
      "    loss           : -226562.6241308594\n",
      "    val_loss       : -261117.9447851181\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -320602.875000\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -401056.437500\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -208534.093750\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -326080.062500\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -371573.781250\n",
      "    epoch          : 417\n",
      "    loss           : -316569.99046875\n",
      "    val_loss       : -313069.9673989296\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -472158.843750\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -340694.750000\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -475614.437500\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -360262.531250\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -364020.000000\n",
      "    epoch          : 418\n",
      "    loss           : -359975.609375\n",
      "    val_loss       : -314817.2174556732\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -378795.343750\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -298876.812500\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -471624.343750\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -306129.625000\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -333034.437500\n",
      "    epoch          : 419\n",
      "    loss           : -362296.0728125\n",
      "    val_loss       : -317262.7022233963\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -470760.375000\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -386955.500000\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -477786.906250\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -341623.312500\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -404567.500000\n",
      "    epoch          : 420\n",
      "    loss           : -363119.78171875\n",
      "    val_loss       : -316521.09378147125\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -338520.906250\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -339550.187500\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -401196.062500\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -307781.062500\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -317140.875000\n",
      "    epoch          : 421\n",
      "    loss           : -362483.81484375\n",
      "    val_loss       : -312753.60308237077\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -346179.125000\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -335240.656250\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -328151.531250\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -339224.000000\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -358906.000000\n",
      "    epoch          : 422\n",
      "    loss           : -361580.46015625\n",
      "    val_loss       : -314811.37387199403\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -334432.187500\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -363809.656250\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -329090.000000\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -471903.187500\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -403240.000000\n",
      "    epoch          : 423\n",
      "    loss           : -365256.10640625\n",
      "    val_loss       : -314483.01431207656\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -474859.375000\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -361672.500000\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -364082.812500\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -336206.156250\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -393697.312500\n",
      "    epoch          : 424\n",
      "    loss           : -364902.00203125\n",
      "    val_loss       : -316405.4990179062\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -332843.125000\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -366531.062500\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -297217.812500\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -304566.843750\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -299564.593750\n",
      "    epoch          : 425\n",
      "    loss           : -359264.50375\n",
      "    val_loss       : -308812.7003389358\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -323025.406250\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -475426.000000\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -341400.281250\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -344435.312500\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -389648.156250\n",
      "    epoch          : 426\n",
      "    loss           : -360006.179375\n",
      "    val_loss       : -315533.5324215889\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -478219.562500\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -337016.062500\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -388425.218750\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -341664.000000\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -361145.687500\n",
      "    epoch          : 427\n",
      "    loss           : -363869.58640625\n",
      "    val_loss       : -318437.1656419754\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -484363.906250\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -325981.875000\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -319136.656250\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -369193.718750\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -353817.031250\n",
      "    epoch          : 428\n",
      "    loss           : -347165.040625\n",
      "    val_loss       : -301918.84558086394\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -349473.031250\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -429133.000000\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -179468.656250\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -463766.812500\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -353582.125000\n",
      "    epoch          : 429\n",
      "    loss           : -327966.490703125\n",
      "    val_loss       : -305274.9876805305\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -306326.000000\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -319693.406250\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -354260.437500\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -288995.750000\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -302231.187500\n",
      "    epoch          : 430\n",
      "    loss           : -347831.16453125\n",
      "    val_loss       : -294978.1293359756\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -327476.062500\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -461469.656250\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -285322.375000\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: 524178.781250\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: 43736.882812\n",
      "    epoch          : 431\n",
      "    loss           : -191395.84685546876\n",
      "    val_loss       : -221146.7165822029\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -218681.750000\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -309681.312500\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -168025.500000\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -349692.875000\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -309975.500000\n",
      "    epoch          : 432\n",
      "    loss           : -311626.04041015625\n",
      "    val_loss       : -312928.05430517194\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -329101.375000\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -471965.125000\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -324196.187500\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -356914.250000\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -361418.312500\n",
      "    epoch          : 433\n",
      "    loss           : -360542.8259375\n",
      "    val_loss       : -317495.22136240004\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -340303.750000\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -409525.656250\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -476967.468750\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -314943.625000\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -402324.406250\n",
      "    epoch          : 434\n",
      "    loss           : -363925.541875\n",
      "    val_loss       : -313782.4855272293\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -339698.062500\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -358634.656250\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -363222.937500\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -364017.750000\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -302501.781250\n",
      "    epoch          : 435\n",
      "    loss           : -365146.59625\n",
      "    val_loss       : -315441.01064605714\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -246124.562500\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -481126.000000\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -364188.187500\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -482191.718750\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -309807.937500\n",
      "    epoch          : 436\n",
      "    loss           : -366637.8990625\n",
      "    val_loss       : -317981.73675527575\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -344006.562500\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -324322.187500\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -303474.312500\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -479179.406250\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -322758.250000\n",
      "    epoch          : 437\n",
      "    loss           : -359594.29859375\n",
      "    val_loss       : -310264.2246848106\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -328889.468750\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -240945.531250\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -238033.734375\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -487021.218750\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -404649.750000\n",
      "    epoch          : 438\n",
      "    loss           : -365484.6340625\n",
      "    val_loss       : -315422.9304718971\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -476481.843750\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -403858.062500\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -387206.812500\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -306325.937500\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -296139.812500\n",
      "    epoch          : 439\n",
      "    loss           : -365366.55375\n",
      "    val_loss       : -315602.2499616623\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -480790.750000\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -365672.562500\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -370701.093750\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -352250.531250\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -296492.437500\n",
      "    epoch          : 440\n",
      "    loss           : -350399.47875\n",
      "    val_loss       : -319124.2409585953\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -337000.031250\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -311491.500000\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -474538.562500\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -339994.125000\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -475924.718750\n",
      "    epoch          : 441\n",
      "    loss           : -358303.35890625\n",
      "    val_loss       : -314338.35973854066\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -406314.687500\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -240678.546875\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -231484.265625\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -386356.968750\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -305237.375000\n",
      "    epoch          : 442\n",
      "    loss           : -361235.80890625\n",
      "    val_loss       : -303772.26698141097\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -320037.656250\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -331945.937500\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -336088.687500\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -380864.843750\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -380894.000000\n",
      "    epoch          : 443\n",
      "    loss           : -361659.41453125\n",
      "    val_loss       : -300798.99480924604\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -307557.906250\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -341509.750000\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -477724.031250\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -381175.500000\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -305185.312500\n",
      "    epoch          : 444\n",
      "    loss           : -366020.39671875\n",
      "    val_loss       : -316033.4390666008\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -321926.562500\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -340522.812500\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -486097.593750\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -474528.625000\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -304908.312500\n",
      "    epoch          : 445\n",
      "    loss           : -368342.90265625\n",
      "    val_loss       : -311986.5161187172\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -483620.312500\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -368869.687500\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -323241.687500\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -326445.093750\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -387441.000000\n",
      "    epoch          : 446\n",
      "    loss           : -361851.4746875\n",
      "    val_loss       : -315779.0469344139\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -481075.218750\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -338495.593750\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -489550.750000\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -339601.000000\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -295603.843750\n",
      "    epoch          : 447\n",
      "    loss           : -363319.155625\n",
      "    val_loss       : -309060.1875588417\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -330798.781250\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -324436.187500\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -319234.093750\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -342657.250000\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -309986.875000\n",
      "    epoch          : 448\n",
      "    loss           : -340858.0790625\n",
      "    val_loss       : -232706.937385273\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -297313.718750\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -125345.156250\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -311334.718750\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -435644.875000\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -394764.531250\n",
      "    epoch          : 449\n",
      "    loss           : -297848.1734863281\n",
      "    val_loss       : -308162.8201998711\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -392032.312500\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -478645.687500\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -336481.187500\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -302534.093750\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -310354.312500\n",
      "    epoch          : 450\n",
      "    loss           : -363001.28015625\n",
      "    val_loss       : -318877.6985970497\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0326_214204/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -326850.250000\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -477082.312500\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -366391.187500\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -309590.843750\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -366038.718750\n",
      "    epoch          : 451\n",
      "    loss           : -365261.7453125\n",
      "    val_loss       : -310496.57004709245\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -226680.468750\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -390069.375000\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -225044.375000\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -402565.687500\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -346320.625000\n",
      "    epoch          : 452\n",
      "    loss           : -365495.09125\n",
      "    val_loss       : -319021.0825311661\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -478127.000000\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -483149.875000\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -315658.562500\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -368399.718750\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -404868.062500\n",
      "    epoch          : 453\n",
      "    loss           : -369595.3565625\n",
      "    val_loss       : -317880.98981313704\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -388603.593750\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -481675.843750\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -472927.375000\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -300835.718750\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -482553.125000\n",
      "    epoch          : 454\n",
      "    loss           : -367005.9840625\n",
      "    val_loss       : -314193.08576421737\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -485435.093750\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -475557.687500\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -398923.875000\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -291852.218750\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -349014.750000\n",
      "    epoch          : 455\n",
      "    loss           : -321541.09890625\n",
      "    val_loss       : -203932.3204372406\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -48782.867188\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -300670.937500\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -317009.937500\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -375299.000000\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -364513.750000\n",
      "    epoch          : 456\n",
      "    loss           : -331596.743046875\n",
      "    val_loss       : -314344.95225753787\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -345891.593750\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -404116.406250\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -481444.093750\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -314874.843750\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -314541.937500\n",
      "    epoch          : 457\n",
      "    loss           : -368971.08375\n",
      "    val_loss       : -318386.1311278343\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -349912.625000\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -472059.312500\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -485656.500000\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -408970.812500\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -362913.906250\n",
      "    epoch          : 458\n",
      "    loss           : -366147.020625\n",
      "    val_loss       : -310540.9594063759\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -489312.031250\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -484248.875000\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -237133.468750\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -393221.000000\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -214971.375000\n",
      "    epoch          : 459\n",
      "    loss           : -363978.22265625\n",
      "    val_loss       : -316726.58743743895\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -369975.187500\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -484660.687500\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -323177.125000\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -333133.375000\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -215676.531250\n",
      "    epoch          : 460\n",
      "    loss           : -296373.531171875\n",
      "    val_loss       : -110893.12182922363\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -198408.125000\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -339889.375000\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -373467.281250\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -329201.656250\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -357879.156250\n",
      "    epoch          : 461\n",
      "    loss           : -303636.660234375\n",
      "    val_loss       : -317348.29828939436\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -486814.531250\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -390906.000000\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -342235.406250\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -483178.125000\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -401531.531250\n",
      "    epoch          : 462\n",
      "    loss           : -368519.27296875\n",
      "    val_loss       : -318404.52431259153\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -334121.562500\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -392482.656250\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -360152.031250\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -488395.093750\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -387204.812500\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.0000e-04.\n",
      "    epoch          : 463\n",
      "    loss           : -369510.39015625\n",
      "    val_loss       : -315549.8217726707\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -403867.687500\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -342911.250000\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -486235.125000\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -348245.562500\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -412186.125000\n",
      "    epoch          : 464\n",
      "    loss           : -373254.105\n",
      "    val_loss       : -318374.02898283006\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -485304.062500\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -491566.250000\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -233451.593750\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -489252.500000\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -318647.687500\n",
      "    epoch          : 465\n",
      "    loss           : -373880.94453125\n",
      "    val_loss       : -319316.22760572436\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -482165.312500\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -407048.750000\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -482825.906250\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -339991.312500\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -408735.875000\n",
      "    epoch          : 466\n",
      "    loss           : -374454.6634375\n",
      "    val_loss       : -323227.38467960357\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -356173.375000\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -337956.125000\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -242644.812500\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -347210.937500\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -370522.875000\n",
      "    epoch          : 467\n",
      "    loss           : -374323.511875\n",
      "    val_loss       : -317595.87930688856\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -411191.000000\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -349377.625000\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -417683.937500\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -378476.187500\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -395317.750000\n",
      "    epoch          : 468\n",
      "    loss           : -374524.40390625\n",
      "    val_loss       : -315699.7101837158\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -483699.656250\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -486791.156250\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -483876.656250\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -372600.250000\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -403256.312500\n",
      "    epoch          : 469\n",
      "    loss           : -374670.471875\n",
      "    val_loss       : -318299.12542562484\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -405815.312500\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -370105.593750\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -411366.375000\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -317531.843750\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -390305.500000\n",
      "    epoch          : 470\n",
      "    loss           : -374647.275625\n",
      "    val_loss       : -317747.1618842125\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -368785.937500\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -489267.343750\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -416043.656250\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -351249.750000\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -311998.906250\n",
      "    epoch          : 471\n",
      "    loss           : -374908.720625\n",
      "    val_loss       : -318472.6086069107\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -232748.937500\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -409754.625000\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -350159.812500\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -399175.875000\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -398195.687500\n",
      "    epoch          : 472\n",
      "    loss           : -374928.59703125\n",
      "    val_loss       : -319439.9177476883\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -312534.562500\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -360569.437500\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -350050.250000\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -238842.984375\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -408549.625000\n",
      "    epoch          : 473\n",
      "    loss           : -374847.18609375\n",
      "    val_loss       : -319216.01572494506\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -489073.593750\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -410512.687500\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -331149.156250\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -413581.156250\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -410398.750000\n",
      "    epoch          : 474\n",
      "    loss           : -375213.85296875\n",
      "    val_loss       : -319104.63698968885\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -248166.656250\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -493315.625000\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -253098.437500\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -328531.625000\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -376530.812500\n",
      "    epoch          : 475\n",
      "    loss           : -375039.55796875\n",
      "    val_loss       : -319319.77503099444\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -346487.531250\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -334480.687500\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -491367.687500\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -409389.625000\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -488751.718750\n",
      "    epoch          : 476\n",
      "    loss           : -375207.03953125\n",
      "    val_loss       : -318247.6553489685\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -392457.125000\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -377741.531250\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -373783.250000\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -376934.312500\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -407790.468750\n",
      "    epoch          : 477\n",
      "    loss           : -375320.70234375\n",
      "    val_loss       : -319074.5145708084\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -393007.468750\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -239971.781250\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -245673.875000\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -350648.812500\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -370463.437500\n",
      "    epoch          : 478\n",
      "    loss           : -375445.23546875\n",
      "    val_loss       : -317724.4416604042\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -405337.906250\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -397343.312500\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -485669.312500\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -411732.125000\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -373528.031250\n",
      "    epoch          : 479\n",
      "    loss           : -375391.58359375\n",
      "    val_loss       : -319691.9211535454\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -411828.937500\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -356935.812500\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -414372.281250\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -485658.437500\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -398626.656250\n",
      "    epoch          : 480\n",
      "    loss           : -375597.28953125\n",
      "    val_loss       : -316854.3364954948\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -346113.125000\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -489245.906250\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -333910.500000\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -313131.093750\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -312574.437500\n",
      "    epoch          : 481\n",
      "    loss           : -375929.236875\n",
      "    val_loss       : -316051.02505254745\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -348574.062500\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -354095.968750\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -337963.843750\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -315493.906250\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -406337.125000\n",
      "    epoch          : 482\n",
      "    loss           : -375797.0809375\n",
      "    val_loss       : -318978.3384969711\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -243807.812500\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -357518.875000\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -491198.500000\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -349137.843750\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -321498.187500\n",
      "    epoch          : 483\n",
      "    loss           : -375935.38734375\n",
      "    val_loss       : -317027.8250787735\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -247772.312500\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -245988.343750\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -347981.281250\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -378142.937500\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -320449.906250\n",
      "    epoch          : 484\n",
      "    loss           : -376053.7484375\n",
      "    val_loss       : -319875.51413764956\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -354529.281250\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -489071.718750\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -317771.187500\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -328466.656250\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -413172.500000\n",
      "    epoch          : 485\n",
      "    loss           : -375793.3175\n",
      "    val_loss       : -320804.2013082504\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -392355.937500\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -393277.156250\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -390521.437500\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -393683.468750\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -319809.312500\n",
      "    epoch          : 486\n",
      "    loss           : -376183.60140625\n",
      "    val_loss       : -319435.8161860466\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -246808.031250\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -357713.468750\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -362314.875000\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -354945.656250\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -418217.218750\n",
      "    epoch          : 487\n",
      "    loss           : -376120.858125\n",
      "    val_loss       : -318333.44139909744\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -404769.000000\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -239995.000000\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -486324.812500\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -374258.750000\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -400408.250000\n",
      "    epoch          : 488\n",
      "    loss           : -376122.44125\n",
      "    val_loss       : -317418.8194231987\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -366873.593750\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -492830.437500\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -324880.125000\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -324628.656250\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -377125.562500\n",
      "    epoch          : 489\n",
      "    loss           : -376176.101875\n",
      "    val_loss       : -319882.26274585724\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -319101.125000\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -340423.062500\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -416564.000000\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -492224.500000\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -417226.437500\n",
      "    epoch          : 490\n",
      "    loss           : -376181.85359375\n",
      "    val_loss       : -316160.56634178164\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -333393.187500\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -349624.250000\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -373745.125000\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -373832.906250\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -314161.531250\n",
      "    epoch          : 491\n",
      "    loss           : -376233.72546875\n",
      "    val_loss       : -318399.38983449934\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -379516.812500\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -350648.125000\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -316215.343750\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -490754.343750\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -491051.062500\n",
      "    epoch          : 492\n",
      "    loss           : -376329.5278125\n",
      "    val_loss       : -315573.17199201585\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -407273.125000\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -245662.859375\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -486866.218750\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -369684.281250\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -414977.093750\n",
      "    epoch          : 493\n",
      "    loss           : -376635.72109375\n",
      "    val_loss       : -317246.03995990753\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -246262.218750\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -328020.375000\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -347090.937500\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -323883.312500\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -390390.687500\n",
      "    epoch          : 494\n",
      "    loss           : -376619.9534375\n",
      "    val_loss       : -317558.5447540283\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -355294.437500\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -356024.875000\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -374671.281250\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -388544.531250\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -402837.906250\n",
      "    epoch          : 495\n",
      "    loss           : -376415.08078125\n",
      "    val_loss       : -319717.5976676941\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -241188.984375\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -397098.812500\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -324593.312500\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -421482.437500\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -404970.562500\n",
      "    epoch          : 496\n",
      "    loss           : -376662.8090625\n",
      "    val_loss       : -318473.9815361023\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -334252.187500\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -239892.484375\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -352786.875000\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -353607.625000\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -397671.312500\n",
      "    epoch          : 497\n",
      "    loss           : -376560.95046875\n",
      "    val_loss       : -317916.89936418534\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -318565.062500\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -407409.312500\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -238151.562500\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -375180.468750\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -412603.468750\n",
      "    epoch          : 498\n",
      "    loss           : -376701.613125\n",
      "    val_loss       : -318261.5532144547\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -409874.968750\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -338317.812500\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -321929.312500\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -375609.687500\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -398601.625000\n",
      "    epoch          : 499\n",
      "    loss           : -376734.54578125\n",
      "    val_loss       : -316248.83516893385\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -353043.781250\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -399522.218750\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -353455.312500\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -252876.640625\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -390054.250000\n",
      "    epoch          : 500\n",
      "    loss           : -377021.3684375\n",
      "    val_loss       : -318067.4212207794\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0326_214204/checkpoint-epoch500.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VlaeCategoryModel(\n",
       "  (_category): FreeCategory(\n",
       "    (generator_0): LadderDecoder(\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): LadderDecoder(\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): LadderDecoder(\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): LadderDecoder(\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (7): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=196, out_features=196, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=392, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_5): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=98, out_features=196, bias=True)\n",
       "        (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (4): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (7): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=196, out_features=196, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): LadderPrior(\n",
       "      (noise_distribution): StandardNormal()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=32, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (encoders): ModuleDict(\n",
       "    ($p( \\mid (\\mathbb{R}^{16} \\times \\mathbb{R}^{2}))$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=64, out_features=36, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p( \\mid (\\mathbb{R}^{32} \\times \\mathbb{R}^{2}))$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=128, out_features=68, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p( \\mid (\\mathbb{R}^{64} \\times \\mathbb{R}^{2}))$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=256, out_features=132, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p( \\mid (\\mathbb{R}^{128} \\times \\mathbb{R}^{2}))$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((392,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=392, out_features=392, bias=True)\n",
       "        (3): LayerNorm((392,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=392, out_features=260, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(X^{784} \\mid (\\mathbb{R}^{196} \\times \\mathbb{R}^{2}))$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=396, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{16} \\mid \\mathbb{R}^{16})$†): MlpEncoder(\n",
       "      (incoder): DenseIncoder(\n",
       "        (dense): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{32} \\mid \\mathbb{R}^{32})$†): MlpEncoder(\n",
       "      (incoder): DenseIncoder(\n",
       "        (dense): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{64} \\mid \\mathbb{R}^{64})$†): MlpEncoder(\n",
       "      (incoder): DenseIncoder(\n",
       "        (dense): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{98} \\mid \\mathbb{R}^{98})$†): MlpEncoder(\n",
       "      (incoder): DenseIncoder(\n",
       "        (dense): Sequential(\n",
       "          (0): Linear(in_features=392, out_features=196, bias=True)\n",
       "          (1): LayerNorm((196,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Linear(in_features=196, out_features=196, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{8} \\mid \\mathbb{R}^{8})$†): MlpEncoder(\n",
       "      (incoder): DenseIncoder(\n",
       "        (dense): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "          (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{2})$†): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEFCAYAAADOqip4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYDklEQVR4nO2dy28cV3bGv9MvvqUm9fKMPHbUtgeeTAwkNDVJgCDIgl5kkU1ATxZZRwLyB8hwgPwB8n8gZRcgCGAL2QVBQiGLmWSyEC04A0xeGGnssS1ZL7L5Zj9PFizaPS3e71JNdrN55/sBBLvr9K17qrq/ruo6dc4xd4cQIk1yx+2AEKJ/SOBCJIwELkTCSOBCJIwELkTCFI7bgZQxswUAMwCWAVQBVNz9Zp/nnAdww91fO+DrZwHMAXjb3a/20zcxeHQE7xNmVgFw2d1vuvst7Iq83O953f02gPsvMOR9AB8Oq7jN7N5x+3CSkcD7RwXAs70n7n4XLya8QVF29+pxO0F4+7gdOMlI4P1jCcD7ZnYtO5ojO5ID2D2Vzv6um1m5Y9mKmc1mj2+YWSV7fmNvPR2ve24d3ZjZlew117pfk52ez2SvqZjZgpndy17/UYdfC9myhewnwIF97Zov6Pd+c2f+fdwxfj8/9vVZZLi7/vr0B2AWwCIAx+4Htdxhu5H9nwdwvWP5IoDZ7PF1ANcCr/t6fdk8H3Wuo2P59exxeW/OLh8Xu59n4yod67jW6XfHvAfytWv91O/OuffZFupH5zj97f7pCN5H3P2uu7/j7gbgNnZFsGfr/M1b7hq6dyr/rOPx8j7rr+7Ng11RdfNnAJ5lR8JK9hdjJvN7b96rAO522O91zXUgXw/od/fcnTA/2LhfayTwPrF3CrmHu7+HDoFlp6fzIMLNqHbbX4AygLvZh/+uu79zgDFUnBkzew+O0NeDzr2fHy867tcGCbx/lLMwGQAg+214P3t8BcAz373ivWeffdEJOn6/VrB7htDNRwDe6Xj9C8+RraNz3OXAXAfmAH4PxI9fByTwPpNdBFoAcAXAe9ni2wBe6zrKz+ydSndcmHsHwLuZIK4CmO+6eDWfreMqgL/I5ttbx5XsC2TvAtRzp/Bd85Wz18xlX0AAvg67VfcubmH3d/z9HnztZD+/n5t7n23Zz4/nxolvsOwihThhmNnH7n7iQkgn1e+Tio7gQiSMBH4CyU5LKyfttPSk+n2S0Sm6EAmjI7gQCSOBC5EwfU8XLdmoj9pE0G6HWHfsx4UZX7t7m49n3uXzkcm52UtFam+X+Hdva4SMjbyrxjcbXozs2TbfOGuGbfkaX3W+wee2ZsT5RiNsax/y52js52zk89ZP1trPnrr7ue7lPQk8i0NWcYD85lGbwO+N/DFbVy8uAABi1w8sIkKv1/n4Qnj32OlTfGyOC7Rx6QK1b3xnjNrXLoXXv3OG75f8DjWjfp4oFIDt8P06+iTs26lPuUAnv+DvSfHpFrXbw8dBm9eJ+AGg1eL2duTLJfKe0y+A2NiIb/+y+bef7btavtbn2bs7a+8urP1uoBBCDAe9/Aa/jG+SCu7jV28fBPB1iuKSmS01PHK4EEL0jV4EXu56fqb7Bb5bxWTO3eeKNtqTY0KIw9OLwKvoyCYSQgwvvQj8Dr45ilewm2wvhBhCXvgqurvfyjKI5rFbUYOm7JlZ9Go2JXblkhGZl10lB4DcheeiDl+z8/p5OnbtlRK1b73Eowc7v7lN7X/6/U+Ctr869+907HokPHghT2JwAEaMh/h+RC67/OUnf07HPvlsitqnf8Z9O/VpOCQ79vMndKyvb3D7Jr+CHw2TsagPC+8B8avsAXoKk7n7B9lD5eMKMcToTjYhEkYCFyJhJHAhEkYCFyJhJHAhEkYCFyJhjr+7aCTO7S1iz0XSFmNxyTGesVX/jbNB22qFx7lX3+BTN8/wrKnxcW7/6crFoO2jsdfp2ItFXkZ8vbBC7Xd2wnMDwN99+btB23fPhrO9AOCTDR7nXn2D22vTYfvYRe732Z88onZfXaP2WAYhJXavSI+Vl3QEFyJhJHAhEkYCFyJhJHAhEkYCFyJhJHAhEqb/YTJ3GgqzUR72QIMUAIwUovOIPXeW1614+lY4jLY6x8uDXrhQpfZXT/FQ1P3qc4VyfoViPrxtn2y8Qsf+FN+h9t+Z3Ld+39f8ZJWH4fKkbGuJ+A0A09M8ZTM3w+3b9XAq6/I9Xihz6otpai9++gW1x0K+bVL00WIh30hIN4SO4EIkjAQuRMJI4EIkjAQuRMJI4EIkjAQuRMJI4EIkzLGni0YbwrHYYqwscpFv3s4lHmuuvh2OdV98icexW87jmp+vl/n4SAfPZ9vjQdtonu/T8QJPRf3HJ29R+0otPDcAnC6F6ybHtjsSDkazxY9Jp8fD5aZPv8VLUT95+BK1v/zLl6m9HYmT5ybIfos1PozZQ3P2NEoIcSKQwIVIGAlciISRwIVIGAlciISRwIVIGAlciIQ59jh4DJYvHouh+w7P2X7y2zwX/fVXPg/aJiKx5Cfb4Ta2ALCywWPJM5O8VW29Fb4H4ItIrPns+Ca111qR+wea3P5oNdx2ObZdG9v8PZkaJ72JAYwWwvUDijkeS37wW/w99X8dpfZobQNGrGxyrL1wAB3BhUiYngRuZitmtmhm147aISHE0dHrKfq77n77SD0RQhw5vZ6il82sEjKa2RUzWzKzpTr472AhRP/oVeAzAJbN7MZ+Rne/6e5z7j5XwiEuPAghDkVPAs8EXAVQNbOFo3VJCHFUvLDAs9Pv2X44I4Q4Wnq5yPYhgMrekdvdb9FXm8VjfIx2uG1qrO55fobXuW5M8alfnqgGbY+2+WAWjwWAyTF+beLZOo+jj42EY7YeyUVnueQHGb+yzscztkjdcgAYLfF4b854G10Woz83xWuqW46vuznJW0bn6zyObkwHh9EI4YUFnp2a383+uLiFEMeKbnQRImEkcCESRgIXImEkcCESRgIXImEGky7qJPwQabnKQmG5EX6XnG/zMrm1GR5mK+XCoa4/OHOPjv2qzlvV/tuD4J2+AICZKZ7SubET3vbJUR6Ci1QmRo2kogJAPh95z0iYbTOSDjoRCR+22vyYNHcunOJbiKSL/uCNX1D7p69+l9pP/4SHRmmYLEaRhxcR+KjrCC5EwkjgQiSMBC5EwkjgQiSMBC5EwkjgQiSMBC5Ewgx92WRv8tgiI/dt3g42f5bHXN+e+ixoO19Yo2NPF3h54J+Nf4vaH63xdFR2a0EsVmyRlMsYsXRSFsNf2+alh5sR31+aWqX23z/186Dt7sardGw7sl3NMW6n7YEBOEknNePr7lUHOoILkTASuBAJI4ELkTASuBAJI4ELkTASuBAJI4ELkTD9j4ObwUrhXNZYC2Ar8VK1FBYsBjB9mudc31m7FLT99bf+mY593OT54CtbY9T+yvQKta/VwvHklU2+7jORFr7FHM/3JpWsAQCrZNvGImWRY+uOxfjfLD0M2v6vwO+LmC7x/bJ9nseq2eccAG8BHPmsWqE3qeoILkTCSOBCJIwELkTCSOBCJIwELkTCSOBCJIwELkTC9D0O7u022pukPnkuUqWb1UWf5C12fZTH0M9P8HayI6Qu+pMWX/ebIw+ofZK0/wWARpvX0GZ506fGd+jYzUgL33MTkZrsNb7tU2Ph+ZuRmuvFSM31Wot/ZM/lw/t1xHqvLQAArdFYkJ7XXWe1zVmuOBDPFw+hI7gQCRMVuJktmNniPsvmzexK/1wTQhyWqMDd/VbnczNbyJbfzp7P98c1IcRh6eUU/TKA+9nj+wBmu19gZlfMbMnMlhrOfw8KIfpHLwIvdz0/0/0Cd7/p7nPuPlc0XmRPCNE/ehF4FcDMEfshhOgDvQj8Dr45ilcALIZfKoQ4TqJx8Owi2pyZLbj7LXe/ZWbXsuXlvYttZDzNk432TCZ2r/G65ijxzTtV5NcHWD/pVqTL9o823qT2fCTnuhjpZc1qeE8Uec51LKc6FmsuRGLVo4VwvHm1wdfddu7bRInPfWfn29TOyEfqxddn+NyxZPZYrJuOjeSLh4gKPBPwdNeyD7KHVNxCiONFN7oIkTASuBAJI4ELkTASuBAJI4ELkTD9Txd1561PD5Fix0JoAFA/y9u5nh9d5/Zi2F7O8ZBH0fh2bdRGqH0kz1MbG00SPizyEF4pz31rRFI6YyE+FobL5w7XuvjL5dP8Ba+ETZWRx3ToL7bPUrudjqR0RtoHGwuT5SLH2nYkRBdabU+jhBAnAglciISRwIVIGAlciISRwIVIGAlciISRwIVImP63D44Ri/+xOHkkNlib5ps3XeDtYtskJbTa5qWD/+nh96l9coSnuu40eWnjcVJ2eafJt3sqMvd2g889Eoujkzh4q81j9LFy0m3eGRk/Xv9u0PZHp/6Hjp0p8XLRxRF+b4JP8TLe7YdfhY05fu9BLtaaODSup1FCiBOBBC5EwkjgQiSMBC5EwkjgQiSMBC5EwkjgQiRM3+PgBt761BuRlq6kvXCsbPLOaf79tdbkXVfOl8L54A+a00EbACxv8YDtK+UqtY8XeDz43spzDWW+ZioSS84bv39gLFJ2ObZtLOfbSbnng1Ct8ljzL0+H35fx0/zzcm+D54O3IzF8FLmcbCRcAyBaPrxHdAQXImEkcCESRgIXImEkcCESRgIXImEkcCESRgIXImH6nw9uxnO+I21RrUBcjNRUb0zyuOWj2ilqvzhSDdp+vBbOOwaAYiRnuhlp4fvVJvdtohSOVY8WeBx7tcbj/9Oj29Qeg+V8tyJx8FykhW8uz+2fr4Xj4G+8ukLHvjxepfafgrcmbk3wGgF0y8m9IgCiOgkRPYKb2YKZLXYtWzGzRTO71tOsQoiBcJD+4LfM7GrX4nezvuFCiCGm19/gZTOrHKknQogjp1eBzwBYNrMb+xnN7IqZLZnZUt13evdOCHEoehK4u9909yqAqpktBOxz7j5XMn5BRwjRP15Y4NnRebYfzgghjpaDXEWfBzDXcaT+MFu+AOxehOufe0KIw3CQq+i3AUx3PK8CuJv9xcXtTuuXR/NgSfwvNzVFh+6c5bHDYo7Hqifz4esHL42s0rGb27z/N4tjHwQWZx+N9BbPReLc7UiseqwYqQ9ObFs7fL/Eeo8Xivw9a5PJ7zd5b/Fmm38Wpyb49aTW+CS1F9lnPXJPB3rMF9edbEIkjAQuRMJI4EIkjAQuRMJI4EIkjAQuRMIMJl20GG59ykoqA4CzNLk8/35q84gMHm3zMNvWVDj9b6PF79CrLUfKJr/8ObX/Ym2G2ndq4Y1bJzYAOD+xQe2tSCrreiTUNTkaLk9cnoy0bI6E6Oq1SGlikm7acr5dEwVeVjnGzhne4rfASoST8uAAYLEwWmi1PY0SQpwIJHAhEkYCFyJhJHAhEkYCFyJhJHAhEkYCFyJh+h8Hj0Dj3AC8Hm6Fa5E4uEdK7MZK9L5UCKeE3n7yPTp29AxPyWxGYrL1Zu9vTSFSsrm6w2P05Ug66XikPTGj2eJpj61IumixxFNVdzbCMfot5/H77499Qe3/UbpE7VaLlAAvhePk3oy10e7tWKwjuBAJI4ELkTASuBAJI4ELkTASuBAJI4ELkTASuBAJM5g4eI+5rACQGyN51+fP8mnHeEx1ZmST2kdz4dLGsfa/sbzliTyPJU+UuH2rEY6pjhZ4TPXcGM8Hr7e47+NFXvKZ3V9QjMS5T43w0sSNyH5/9Gg8aNtp83xt9n4D8fsmGmO9twCO1kVgueQEHcGFSBgJXIiEkcCFSBgJXIiEkcCFSBgJXIiEkcCFSJjjzwePxPeYPb/N61jn6vz768vNMrU/K4fbwX5R5WO9yefebIVrrgNALZI3zWKyHqkt/mCDt9GdLB2uPjiL0e80+EduLVJzPXb/Adrhba8736dvlh7zVUf2a+10xDcS67YS/zz4Nr8/IATd22ZWBlDJ/i67+3vZ8gUAVQAVd7/Z08xCiL4TO0X/IYA5d78FAGZ2JRM33P12tmy+vy4KIXqFCtzdb3YcoSsA7gO4nP1H9n+2f+4JIQ7DgS6ymVkFwHJ21C53mc/s8/orZrZkZkt17+23gxDi8Bz0KvqCu1/NHlcB0M542ZF/zt3nSsab9Akh+kdU4Ga24O4fZI9nAdzBN0fxCoDFvnknhDgUsavo8wCum9n72aL33P2WmV3LbOW9i21B3IF2OEUwVvqYUuBhDy/z9L8LY+vU/ubIg6Bte5OHNazA0yILxu2x8sKTI+FQVixMFgtF8aRIYG2bn5Wx9sFjRR4WrUXKRe/U+X4ZeRb+PC23wmFPAFhv8+2qR96TSCVsWvq4XeOhSVZyGQAQyC6mezMT72v7LP8ge8jFLYQ4VnQnmxAJI4ELkTASuBAJI4ELkTASuBAJI4ELkTD9Txc1A4rhGF6sXGxrI1zaOBctNcu/v6p13kb3GYmb+lokvS9SYvfJNo/J1ps85tog9wBMFHnJ5XOTvFx0K5KS2Whw30DCybEY+3adx3tzuUjp4mLY/rONi3TseI7vt7FIuegmv7WB3w8S+Sz3WnpcR3AhEkYCFyJhJHAhEkYCFyJhJHAhEkYCFyJhJHAhEqb/cXB3oBGOHzqJkQNAjuXB1nlc0rb599db5XC+NwB8vHUpaMuv83W3pnhQ9OcPzlF7ocTjnizn+sHaKTp2ZzsSw6dWoFjkvj2rhmP8rUg56XbEXhrn73lpPRxPfrjN98vFM8vUHiWy47zZWwvgw6AjuBAJI4ELkTASuBAJI4ELkTASuBAJI4ELkTASuBAJM5h8cFIPmsXIoxS5+z7O47WPa1PUfra0EbSNLvP83XokF70e8b35lNcu/4rkZLc3I29rIRaw5eZWg8fR85uk/vcEvz8gN8U/D/VnvHb5KEnp/t8HF+jYry6UqT2WJ38YovngPaIjuBAJI4ELkTASuBAJI4ELkTASuBAJI4ELkTASuBAJM4A4OGCF8DRe57WoGR7pmVxY5vbZqV9S++LT7wVtOd7OGeMbPK5Z2OK+tXmoGXULx8kLO3zuXKzEdqS+d6PMX8B6dDeaEd8eH+4jeeqz8MZt/oCPXW2NU/tYgcfotyKh7Nx4eP1RHfSjLrqZlc1s1swWzOx6x/IVM1s0s2s9zSqEGAixU/QfAphz91sAYGZXsuXvuvs77v5BX70TQhwKej7k7jc7nlYALGaPy2ZWcff7ffNMCHFoDnSRzcwqAJbd/Xa2aAbAspndCLz+ipktmdlSvb1zRK4KIV6Ug15FX3D3q3tP3P2mu1cBVM1sofvFmX3O3edKOZ4cIIToH9FLlma2sPdb28xmAcwBWHL3u/12TghxOKjAzWwewHUzez9b9B6ADwFU9o7cexfggjjg7BJ/pGyyb28Hbbm1cDonADQnZ6j9aZO38F2rhc8+yvd5CdzCFg9rbJ2PhMmKPOZSWwmffFkkotLk0aDo+NFl3j7YieulzyNhsgbPVS2EPw4AgPEvt4K29pMJOnYkx8Ngm5E02VykKrLXSGw1li6aj7RsDhC7yHYbwGv7mO5mf1zcQohjRXeyCZEwErgQCSOBC5EwErgQCSOBC5EwErgQCTOY9sE9proBgJXCscfWU97u9dI/XKT2v1/9Q2of+yocm7z43w/pWGvzeO7If65TezTuWQ63wm1P8pLL7VKkZPMEtxfXeGqjsU1v81TT/NM1akeNz918/DRo+875t+nYv7n3J9Te4GF0vPxfkfeUxLpjrYVZyjVDR3AhEkYCFyJhJHAhEkYCFyJhJHAhEkYCFyJhJHAhEsbcI71iDzuB2RMAn3UsOgsgHKw8XuRbbwyrb8PqF3D0vr3q7ue6F/Zd4M9NaLbk7nMDnfSAyLfeGFbfhtUvYHC+6RRdiISRwIVImOMQ+M34S44N+dYbw+rbsPoFDMi3gf8GF0IMDp2iC5EwErgQCTNQgWddSuc7mhgOBcPYLTXbV4v7LDv2/Rfw7Vj3IemEe+z77Di79A5M4B2NEm5nz+cHNfcBGLpuqd0NJYZp/wWaXRz3PnyuE+4Q7bNj69I7yCP4ZQB73UjvA5gd4NwxylmDxWFmmPcfcMz7MOuHt3dluoLdfTQU+yzgGzCAfTZIgZe7np8Z4NwxaLfUIaHc9XyY9h8wJPuwqxNuuct8rPvsRbv0HgWDFHgVuxs0dMS6pQ4JVQzp/gOGah92dsKtYrj22Qt16T0KBinwO/jmG7UCYDH80sGR/VYbttPd/RjK/QcMzz7cpxPu0Oyzbt8Gtc8GJvDsAkMlu9BR7jhNOW4+BH7lItZQNFTM9tNcl19Dsf+6fcMQ7MOOTrgfm9nHAGaGZZ/t5xsGtM90J5sQCaMbXYRIGAlciISRwIVIGAlciISRwIVIGAlciISRwIVImP8HBuzW16oTFtgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
