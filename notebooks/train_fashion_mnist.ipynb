{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='fashion_mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 109662.820312\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -63065.140625\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -151431.250000\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -167460.968750\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -191473.875000\n",
      "    epoch          : 1\n",
      "    loss           : -115735.8235546875\n",
      "    val_loss       : -269262.255859375\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -231387.250000\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -184541.906250\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -384805.812500\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -246993.687500\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -342097.000000\n",
      "    epoch          : 2\n",
      "    loss           : -305440.656796875\n",
      "    val_loss       : -377017.5171875\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -485423.468750\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -398851.812500\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -325202.562500\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -363631.500000\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -391471.562500\n",
      "    epoch          : 3\n",
      "    loss           : -393267.68953125\n",
      "    val_loss       : -441873.8921875\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -338251.062500\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -326303.468750\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -508337.937500\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -516334.312500\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -508156.437500\n",
      "    epoch          : 4\n",
      "    loss           : -446711.338125\n",
      "    val_loss       : -437008.56953125\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -492883.093750\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -342227.875000\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -410485.312500\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -363132.281250\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -554578.437500\n",
      "    epoch          : 5\n",
      "    loss           : -465246.75875\n",
      "    val_loss       : -495284.72109375\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -417338.312500\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -308934.968750\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -614824.500000\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -373742.750000\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -510590.812500\n",
      "    epoch          : 6\n",
      "    loss           : -473120.31109375\n",
      "    val_loss       : -503307.38203125\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -667066.375000\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -674066.250000\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -338659.375000\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -635142.500000\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -443187.187500\n",
      "    epoch          : 7\n",
      "    loss           : -485840.3215625\n",
      "    val_loss       : -512443.7671875\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -438368.937500\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -401631.718750\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -547883.750000\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -555270.625000\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -594568.250000\n",
      "    epoch          : 8\n",
      "    loss           : -523496.0928125\n",
      "    val_loss       : -524614.14921875\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -569344.625000\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -386325.312500\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -666459.625000\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -429854.468750\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -360653.093750\n",
      "    epoch          : 9\n",
      "    loss           : -512164.021875\n",
      "    val_loss       : -529608.9296875\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -446420.531250\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -398924.875000\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -699226.625000\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -370045.687500\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -574607.812500\n",
      "    epoch          : 10\n",
      "    loss           : -472109.16625\n",
      "    val_loss       : -482320.68125\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -404389.687500\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -710628.750000\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -665549.625000\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -420740.906250\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -646312.500000\n",
      "    epoch          : 11\n",
      "    loss           : -527657.513125\n",
      "    val_loss       : -540198.97890625\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -659369.437500\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -669752.437500\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -715532.937500\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -410125.625000\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -411762.468750\n",
      "    epoch          : 12\n",
      "    loss           : -542580.574375\n",
      "    val_loss       : -549479.93203125\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -406690.468750\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -669691.937500\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -475250.750000\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -478772.375000\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -421601.875000\n",
      "    epoch          : 13\n",
      "    loss           : -549394.419375\n",
      "    val_loss       : -540904.23984375\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -448954.750000\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -481526.062500\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -411149.000000\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -636201.500000\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -676125.000000\n",
      "    epoch          : 14\n",
      "    loss           : -529415.4715625\n",
      "    val_loss       : -545879.315625\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -576015.187500\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -551572.000000\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -474654.562500\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -427324.750000\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -642785.375000\n",
      "    epoch          : 15\n",
      "    loss           : -550042.99625\n",
      "    val_loss       : -567342.54140625\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -483742.250000\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -511423.500000\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -487526.500000\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -654733.875000\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -490290.062500\n",
      "    epoch          : 16\n",
      "    loss           : -560542.5071875\n",
      "    val_loss       : -554574.33984375\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -493102.218750\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -473463.406250\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -466579.000000\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -488394.312500\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -404529.312500\n",
      "    epoch          : 17\n",
      "    loss           : -547612.3571875\n",
      "    val_loss       : -526664.525\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -596308.312500\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -424380.437500\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -416893.968750\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -501353.406250\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -684736.312500\n",
      "    epoch          : 18\n",
      "    loss           : -556415.446875\n",
      "    val_loss       : -568402.2109375\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -425650.937500\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -731515.375000\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -506364.125000\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -434806.125000\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -500907.281250\n",
      "    epoch          : 19\n",
      "    loss           : -567870.6340625\n",
      "    val_loss       : -580210.59140625\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -501164.062500\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -690377.375000\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -411846.062500\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -706003.500000\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -649857.000000\n",
      "    epoch          : 20\n",
      "    loss           : -559839.795\n",
      "    val_loss       : -569574.2\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -718880.437500\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -590611.750000\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -461367.687500\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -684424.625000\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -427231.093750\n",
      "    epoch          : 21\n",
      "    loss           : -563532.105625\n",
      "    val_loss       : -575569.70390625\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -716285.625000\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -497856.687500\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -446318.875000\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -702021.937500\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -699675.625000\n",
      "    epoch          : 22\n",
      "    loss           : -571877.54875\n",
      "    val_loss       : -567110.59765625\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -471851.906250\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -593500.687500\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -580175.875000\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -638145.875000\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -441534.187500\n",
      "    epoch          : 23\n",
      "    loss           : -576448.9215625\n",
      "    val_loss       : -572692.50859375\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -437402.375000\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -598985.937500\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -738695.875000\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -605591.750000\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -711888.750000\n",
      "    epoch          : 24\n",
      "    loss           : -576213.8896875\n",
      "    val_loss       : -581275.0625\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -731492.500000\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -450328.156250\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -713033.062500\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -493443.687500\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -491855.250000\n",
      "    epoch          : 25\n",
      "    loss           : -567373.5\n",
      "    val_loss       : -570972.0640625\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -693151.000000\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -472592.468750\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -421591.062500\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -508254.437500\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -586726.000000\n",
      "    epoch          : 26\n",
      "    loss           : -560241.800625\n",
      "    val_loss       : -585167.1046875\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -604461.125000\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -438151.531250\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -446072.187500\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -484891.343750\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -436006.062500\n",
      "    epoch          : 27\n",
      "    loss           : -576884.638125\n",
      "    val_loss       : -571864.8515625\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -476580.593750\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -442731.125000\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -659063.375000\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -506234.218750\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -441947.750000\n",
      "    epoch          : 28\n",
      "    loss           : -580125.773125\n",
      "    val_loss       : -590798.06953125\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -524949.937500\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -476954.843750\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -470418.062500\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -731876.000000\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -457247.437500\n",
      "    epoch          : 29\n",
      "    loss           : -591282.588125\n",
      "    val_loss       : -592348.925\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -508758.937500\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -747381.250000\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -520937.031250\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -501272.656250\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -703855.187500\n",
      "    epoch          : 30\n",
      "    loss           : -583079.7971875\n",
      "    val_loss       : -579694.60546875\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -721660.250000\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -466614.000000\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -743216.437500\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -747255.312500\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -742298.625000\n",
      "    epoch          : 31\n",
      "    loss           : -588341.41375\n",
      "    val_loss       : -599525.1984375\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -526746.812500\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -510732.625000\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -485646.750000\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -424380.937500\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -463561.968750\n",
      "    epoch          : 32\n",
      "    loss           : -580709.3375\n",
      "    val_loss       : -582267.15703125\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -505775.750000\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -459014.375000\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -719079.437500\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -575838.062500\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -708057.000000\n",
      "    epoch          : 33\n",
      "    loss           : -570138.9446875\n",
      "    val_loss       : -571352.42421875\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -639575.250000\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -454016.968750\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -477764.250000\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -738211.250000\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -707174.375000\n",
      "    epoch          : 34\n",
      "    loss           : -574437.764375\n",
      "    val_loss       : -580529.85546875\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -618398.000000\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -516034.375000\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -481376.156250\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -643642.187500\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -703581.625000\n",
      "    epoch          : 35\n",
      "    loss           : -584485.6765625\n",
      "    val_loss       : -590034.40390625\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -719313.750000\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -701771.812500\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -404017.875000\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -661421.312500\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -523378.968750\n",
      "    epoch          : 36\n",
      "    loss           : -581837.789375\n",
      "    val_loss       : -599579.08046875\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -524766.000000\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -750965.250000\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -465790.968750\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -625513.937500\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -601019.312500\n",
      "    epoch          : 37\n",
      "    loss           : -583833.2915625\n",
      "    val_loss       : -593841.840625\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -746031.062500\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -732634.875000\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -493322.125000\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -663648.750000\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -444233.281250\n",
      "    epoch          : 38\n",
      "    loss           : -585763.830625\n",
      "    val_loss       : -597992.77578125\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -661963.750000\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -515383.750000\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -655079.125000\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -688955.750000\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -590266.062500\n",
      "    epoch          : 39\n",
      "    loss           : -575177.025625\n",
      "    val_loss       : -546955.6796875\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -452252.062500\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -462050.500000\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -461413.687500\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -457758.062500\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -734418.312500\n",
      "    epoch          : 40\n",
      "    loss           : -576899.418125\n",
      "    val_loss       : -599562.92421875\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -750623.250000\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -483949.500000\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -751558.750000\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -706627.125000\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -467046.437500\n",
      "    epoch          : 41\n",
      "    loss           : -591432.268125\n",
      "    val_loss       : -596850.81640625\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -670693.000000\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -741949.937500\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -667407.750000\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -527778.750000\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -731483.500000\n",
      "    epoch          : 42\n",
      "    loss           : -599715.96125\n",
      "    val_loss       : -603103.6015625\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -488269.000000\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -499959.250000\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -484186.750000\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -628701.500000\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -733670.125000\n",
      "    epoch          : 43\n",
      "    loss           : -598324.8690625\n",
      "    val_loss       : -598495.4921875\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -624270.687500\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -752842.625000\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -476737.937500\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -527272.875000\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -463956.968750\n",
      "    epoch          : 44\n",
      "    loss           : -597276.325\n",
      "    val_loss       : -603949.74609375\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -515589.250000\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -755201.500000\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -741099.625000\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -621604.625000\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -675014.375000\n",
      "    epoch          : 45\n",
      "    loss           : -596129.8809375\n",
      "    val_loss       : -603330.19140625\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -758925.500000\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -522217.875000\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -491770.687500\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -468966.312500\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -585554.437500\n",
      "    epoch          : 46\n",
      "    loss           : -588147.1271875\n",
      "    val_loss       : -595438.1375\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -450662.125000\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -491490.937500\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -740435.312500\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -445447.812500\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -736074.625000\n",
      "    epoch          : 47\n",
      "    loss           : -591548.0059375\n",
      "    val_loss       : -602882.425\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -635824.625000\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -464708.562500\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -672828.562500\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -522892.843750\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -456420.000000\n",
      "    epoch          : 48\n",
      "    loss           : -598278.321875\n",
      "    val_loss       : -600384.8390625\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -758570.625000\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -677515.812500\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -686764.687500\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -469294.093750\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -742645.812500\n",
      "    epoch          : 49\n",
      "    loss           : -604630.480625\n",
      "    val_loss       : -606661.67109375\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -685890.375000\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -537198.375000\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -677330.062500\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -736536.625000\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -479229.250000\n",
      "    epoch          : 50\n",
      "    loss           : -604246.94875\n",
      "    val_loss       : -610688.01640625\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0826_103239/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -634547.875000\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -537642.125000\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -489616.031250\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -757630.000000\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -753463.125000\n",
      "    epoch          : 51\n",
      "    loss           : -606334.4059375\n",
      "    val_loss       : -603477.5390625\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -673223.125000\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -628284.062500\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -652910.875000\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -511801.000000\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -522459.843750\n",
      "    epoch          : 52\n",
      "    loss           : -594615.8253125\n",
      "    val_loss       : -596701.8\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -666366.562500\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -479171.062500\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -740733.625000\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -610144.750000\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -654913.750000\n",
      "    epoch          : 53\n",
      "    loss           : -593067.7696875\n",
      "    val_loss       : -593374.31171875\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -727851.187500\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -740121.687500\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -508284.375000\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -533070.125000\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -718478.250000\n",
      "    epoch          : 54\n",
      "    loss           : -593126.71375\n",
      "    val_loss       : -598811.0953125\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -760131.750000\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -515625.562500\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -499182.812500\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -531535.312500\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -631723.062500\n",
      "    epoch          : 55\n",
      "    loss           : -599959.9009375\n",
      "    val_loss       : -607370.36875\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -525716.937500\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -758005.500000\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -751155.875000\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -621264.000000\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -463098.281250\n",
      "    epoch          : 56\n",
      "    loss           : -601184.268125\n",
      "    val_loss       : -603942.64765625\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -469110.437500\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -471492.375000\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -671783.875000\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -512019.125000\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -751900.437500\n",
      "    epoch          : 57\n",
      "    loss           : -592789.81625\n",
      "    val_loss       : -588717.525\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -754709.875000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -526471.312500\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -756544.562500\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -729427.437500\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -734384.437500\n",
      "    epoch          : 58\n",
      "    loss           : -599298.638125\n",
      "    val_loss       : -609592.67265625\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -635895.875000\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -743447.125000\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -474602.750000\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -638310.000000\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -471879.531250\n",
      "    epoch          : 59\n",
      "    loss           : -610714.474375\n",
      "    val_loss       : -612939.94609375\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -476711.875000\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -532688.250000\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -482894.000000\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -538264.812500\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -629758.812500\n",
      "    epoch          : 60\n",
      "    loss           : -611261.0140625\n",
      "    val_loss       : -603110.99375\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -470279.937500\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -497408.468750\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -740930.187500\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -740721.437500\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -741206.125000\n",
      "    epoch          : 61\n",
      "    loss           : -601822.228125\n",
      "    val_loss       : -611072.33203125\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -764442.187500\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -754170.500000\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -626319.750000\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -624646.375000\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -740208.625000\n",
      "    epoch          : 62\n",
      "    loss           : -601168.9784375\n",
      "    val_loss       : -607618.92734375\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -541830.687500\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -760636.812500\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -539677.500000\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -481516.625000\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -656383.125000\n",
      "    epoch          : 63\n",
      "    loss           : -608283.08625\n",
      "    val_loss       : -602714.1171875\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -474762.375000\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -484559.593750\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -484726.093750\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -626245.687500\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -534945.750000\n",
      "    epoch          : 64\n",
      "    loss           : -607607.881875\n",
      "    val_loss       : -613098.590625\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -478416.218750\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -508328.937500\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -535576.875000\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -748773.062500\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -648373.875000\n",
      "    epoch          : 65\n",
      "    loss           : -607453.2046875\n",
      "    val_loss       : -599635.446875\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -665070.312500\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -533033.375000\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -725592.750000\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -726409.812500\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -731245.187500\n",
      "    epoch          : 66\n",
      "    loss           : -602678.449375\n",
      "    val_loss       : -606906.25390625\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -526419.250000\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -475573.156250\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -760082.812500\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -501946.250000\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -691610.875000\n",
      "    epoch          : 67\n",
      "    loss           : -609456.8459375\n",
      "    val_loss       : -617136.08046875\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -766280.312500\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -766688.062500\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -634534.437500\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -483555.875000\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -669117.812500\n",
      "    epoch          : 68\n",
      "    loss           : -609340.1325\n",
      "    val_loss       : -612519.02578125\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -757964.500000\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -751844.687500\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -762292.937500\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -743500.375000\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -494645.437500\n",
      "    epoch          : 69\n",
      "    loss           : -612167.625625\n",
      "    val_loss       : -611466.64765625\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -536923.375000\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -529631.625000\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -743911.125000\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -497197.000000\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -680157.750000\n",
      "    epoch          : 70\n",
      "    loss           : -610881.27125\n",
      "    val_loss       : -611644.1859375\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -490932.031250\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -489244.500000\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -469191.250000\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -746527.625000\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -676051.875000\n",
      "    epoch          : 71\n",
      "    loss           : -607128.938125\n",
      "    val_loss       : -608902.37421875\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -532257.062500\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -544715.375000\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -760561.375000\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -741483.000000\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -625319.000000\n",
      "    epoch          : 72\n",
      "    loss           : -607945.639375\n",
      "    val_loss       : -609755.78359375\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -757396.125000\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -528887.625000\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -757850.187500\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -515376.937500\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -483539.250000\n",
      "    epoch          : 73\n",
      "    loss           : -606604.1265625\n",
      "    val_loss       : -617926.5828125\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -541684.625000\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -631178.500000\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -770354.750000\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -626295.687500\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -497760.500000\n",
      "    epoch          : 74\n",
      "    loss           : -613600.47875\n",
      "    val_loss       : -611819.39453125\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -540593.625000\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -752830.562500\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -767636.875000\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -755747.312500\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -632175.312500\n",
      "    epoch          : 75\n",
      "    loss           : -613154.3325\n",
      "    val_loss       : -610257.67734375\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -526502.500000\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -510044.375000\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -525285.750000\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -750565.500000\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -476118.500000\n",
      "    epoch          : 76\n",
      "    loss           : -592321.2971875\n",
      "    val_loss       : -604197.52578125\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -753564.937500\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -481145.531250\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -545554.937500\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -537869.625000\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -680079.312500\n",
      "    epoch          : 77\n",
      "    loss           : -608927.5390625\n",
      "    val_loss       : -608855.4921875\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -498865.406250\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -678417.062500\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -634026.375000\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -675364.625000\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -517275.312500\n",
      "    epoch          : 78\n",
      "    loss           : -612712.331875\n",
      "    val_loss       : -614500.06328125\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -689199.125000\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -757077.375000\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -527851.187500\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -504544.562500\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -464389.031250\n",
      "    epoch          : 79\n",
      "    loss           : -612331.0371875\n",
      "    val_loss       : -612361.91796875\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -484217.687500\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -683418.250000\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -480454.125000\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -761084.750000\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -693157.625000\n",
      "    epoch          : 80\n",
      "    loss           : -615046.6428125\n",
      "    val_loss       : -620223.0890625\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -766284.750000\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -494268.250000\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -620442.312500\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -679930.375000\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -665301.812500\n",
      "    epoch          : 81\n",
      "    loss           : -610308.9784375\n",
      "    val_loss       : -608683.634375\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -678930.250000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -500552.375000\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -750093.250000\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -757068.500000\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -473776.843750\n",
      "    epoch          : 82\n",
      "    loss           : -612653.3971875\n",
      "    val_loss       : -610796.41796875\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -524348.625000\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -600375.375000\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -733006.000000\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -691730.000000\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -532028.625000\n",
      "    epoch          : 83\n",
      "    loss           : -609332.3396875\n",
      "    val_loss       : -612963.77734375\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -746811.125000\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -636190.750000\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -764834.625000\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -688055.812500\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -740178.125000\n",
      "    epoch          : 84\n",
      "    loss           : -613486.2421875\n",
      "    val_loss       : -605923.57578125\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -764770.125000\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -759004.312500\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -752777.312500\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -531261.687500\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -754144.125000\n",
      "    epoch          : 85\n",
      "    loss           : -613936.986875\n",
      "    val_loss       : -618884.27421875\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -697823.062500\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -772331.687500\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -507684.031250\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -545180.937500\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -483028.125000\n",
      "    epoch          : 86\n",
      "    loss           : -617625.7365625\n",
      "    val_loss       : -618342.8546875\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -478213.656250\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -475825.750000\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -522224.468750\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -526341.500000\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -739694.250000\n",
      "    epoch          : 87\n",
      "    loss           : -606514.9640625\n",
      "    val_loss       : -617534.44140625\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -545703.562500\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -542076.500000\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -509594.687500\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -470917.218750\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -485376.625000\n",
      "    epoch          : 88\n",
      "    loss           : -613697.40625\n",
      "    val_loss       : -599145.709375\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -524920.625000\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -751698.000000\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -474916.687500\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -479407.718750\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -651403.750000\n",
      "    epoch          : 89\n",
      "    loss           : -604313.160625\n",
      "    val_loss       : -594785.34375\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -482531.687500\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -764447.625000\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -536429.375000\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -643969.437500\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -483450.093750\n",
      "    epoch          : 90\n",
      "    loss           : -613483.0065625\n",
      "    val_loss       : -616896.30078125\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -764590.687500\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -639337.625000\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -638750.875000\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -474612.937500\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -510243.187500\n",
      "    epoch          : 91\n",
      "    loss           : -610645.1596875\n",
      "    val_loss       : -612952.28125\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -494915.250000\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -547206.062500\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -646032.500000\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -767077.375000\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -543754.500000\n",
      "    epoch          : 92\n",
      "    loss           : -620428.4165625\n",
      "    val_loss       : -615473.18203125\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -484337.437500\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -762043.062500\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -485214.781250\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -484412.437500\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -751387.750000\n",
      "    epoch          : 93\n",
      "    loss           : -608576.38875\n",
      "    val_loss       : -614123.54453125\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -539691.937500\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -741408.250000\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -667340.500000\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -677274.125000\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -634085.375000\n",
      "    epoch          : 94\n",
      "    loss           : -605579.3821875\n",
      "    val_loss       : -615250.57265625\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -497224.031250\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -511305.406250\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -638516.750000\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -697207.250000\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -501947.250000\n",
      "    epoch          : 95\n",
      "    loss           : -617883.9953125\n",
      "    val_loss       : -619949.11875\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -498536.312500\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -760692.375000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -506186.281250\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -632403.375000\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -763144.000000\n",
      "    epoch          : 96\n",
      "    loss           : -614650.3784375\n",
      "    val_loss       : -616244.96484375\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -755392.250000\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -768354.500000\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -514623.156250\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -641026.000000\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -732276.500000\n",
      "    epoch          : 97\n",
      "    loss           : -611262.729375\n",
      "    val_loss       : -609501.909375\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -497540.468750\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -638801.375000\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -676076.250000\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -615650.000000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -751491.937500\n",
      "    epoch          : 98\n",
      "    loss           : -609524.5965625\n",
      "    val_loss       : -616715.07265625\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -529685.875000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -528499.500000\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -757285.375000\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -770574.250000\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -646835.750000\n",
      "    epoch          : 99\n",
      "    loss           : -612599.2578125\n",
      "    val_loss       : -619207.5171875\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -750980.250000\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -515124.187500\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -760639.937500\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -770384.000000\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -744969.062500\n",
      "    epoch          : 100\n",
      "    loss           : -614782.2725\n",
      "    val_loss       : -610454.79765625\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0826_103239/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -525986.625000\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -762922.000000\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -553408.437500\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -757746.250000\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -507523.531250\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "    epoch          : 101\n",
      "    loss           : -619690.2884375\n",
      "    val_loss       : -620028.40078125\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -505686.156250\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -647105.125000\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -553947.000000\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -498427.312500\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -704464.125000\n",
      "    epoch          : 102\n",
      "    loss           : -626182.28875\n",
      "    val_loss       : -627082.215625\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -706874.687500\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -686381.437500\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -761820.000000\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -549195.500000\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -498837.125000\n",
      "    epoch          : 103\n",
      "    loss           : -627482.478125\n",
      "    val_loss       : -628090.85\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -776135.125000\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -765471.625000\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -512799.562500\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -773693.812500\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -648533.875000\n",
      "    epoch          : 104\n",
      "    loss           : -627755.5859375\n",
      "    val_loss       : -627139.1828125\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -548895.625000\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -704714.125000\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -758032.875000\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -490647.031250\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -548716.000000\n",
      "    epoch          : 105\n",
      "    loss           : -627872.5171875\n",
      "    val_loss       : -628610.51953125\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -772714.625000\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -518759.437500\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -760743.437500\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -776227.062500\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -649730.187500\n",
      "    epoch          : 106\n",
      "    loss           : -627958.00625\n",
      "    val_loss       : -628617.5625\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -774554.125000\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -501237.062500\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -517506.781250\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -646117.000000\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -690177.687500\n",
      "    epoch          : 107\n",
      "    loss           : -627952.800625\n",
      "    val_loss       : -628528.50703125\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -779573.875000\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -547380.250000\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -494646.343750\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -553777.375000\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -702183.562500\n",
      "    epoch          : 108\n",
      "    loss           : -628122.54\n",
      "    val_loss       : -627975.45234375\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -512017.687500\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -517029.968750\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -754016.000000\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -765249.187500\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -771873.062500\n",
      "    epoch          : 109\n",
      "    loss           : -628252.3521875\n",
      "    val_loss       : -628175.5859375\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -517812.250000\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -546567.125000\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -550737.750000\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -495714.437500\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -546569.625000\n",
      "    epoch          : 110\n",
      "    loss           : -628607.78375\n",
      "    val_loss       : -628446.1125\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -500837.562500\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -777636.625000\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -495465.375000\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -498506.875000\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -695696.750000\n",
      "    epoch          : 111\n",
      "    loss           : -628345.095\n",
      "    val_loss       : -628513.20390625\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -503280.125000\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -651705.125000\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -758057.625000\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -497057.625000\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -645760.375000\n",
      "    epoch          : 112\n",
      "    loss           : -628408.278125\n",
      "    val_loss       : -628621.77734375\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -767846.875000\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -544249.125000\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -781164.437500\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -780330.375000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -496741.312500\n",
      "    epoch          : 113\n",
      "    loss           : -628550.60375\n",
      "    val_loss       : -629358.17890625\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -774029.750000\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -762698.000000\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -767302.687500\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -766109.500000\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -650073.625000\n",
      "    epoch          : 114\n",
      "    loss           : -628656.065\n",
      "    val_loss       : -628504.55703125\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -552244.875000\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -519124.625000\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -504517.125000\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -525397.125000\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -777308.125000\n",
      "    epoch          : 115\n",
      "    loss           : -628776.035625\n",
      "    val_loss       : -628177.12265625\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -552610.625000\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -773318.562500\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -773447.812500\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -764016.312500\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -509166.781250\n",
      "    epoch          : 116\n",
      "    loss           : -628852.4253125\n",
      "    val_loss       : -629125.4546875\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -497463.312500\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -523531.937500\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -516738.625000\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -545485.875000\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -499968.906250\n",
      "    epoch          : 117\n",
      "    loss           : -628822.3496875\n",
      "    val_loss       : -628903.73359375\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -517071.125000\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -504469.437500\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -655175.750000\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -646510.187500\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -501569.500000\n",
      "    epoch          : 118\n",
      "    loss           : -628980.308125\n",
      "    val_loss       : -628913.32890625\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -697407.625000\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -764960.062500\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -517603.468750\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -776137.062500\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -775473.750000\n",
      "    epoch          : 119\n",
      "    loss           : -629016.3465625\n",
      "    val_loss       : -628570.446875\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -553562.750000\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -764093.125000\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -776671.500000\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -771957.937500\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -544895.250000\n",
      "    epoch          : 120\n",
      "    loss           : -629179.6175\n",
      "    val_loss       : -628508.221875\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -656085.000000\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -700331.500000\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -555181.500000\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -779312.875000\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -490218.718750\n",
      "    epoch          : 121\n",
      "    loss           : -629009.7565625\n",
      "    val_loss       : -627799.85546875\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -548721.250000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -781309.625000\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -706839.250000\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -776160.250000\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -649773.250000\n",
      "    epoch          : 122\n",
      "    loss           : -629173.981875\n",
      "    val_loss       : -628438.17109375\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -774490.625000\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -552503.875000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -496275.468750\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -556371.500000\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -771280.625000\n",
      "    epoch          : 123\n",
      "    loss           : -629262.6496875\n",
      "    val_loss       : -628461.00703125\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -698821.750000\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -703250.750000\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -551891.812500\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -652035.875000\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -553128.500000\n",
      "    epoch          : 124\n",
      "    loss           : -629231.3440625\n",
      "    val_loss       : -628476.684375\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -777471.250000\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -765596.562500\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -494217.125000\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -552412.250000\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -504510.687500\n",
      "    epoch          : 125\n",
      "    loss           : -629269.4415625\n",
      "    val_loss       : -628574.18671875\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -774790.875000\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -783634.250000\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -694179.000000\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -704607.500000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -491095.156250\n",
      "    epoch          : 126\n",
      "    loss           : -629273.278125\n",
      "    val_loss       : -629315.39921875\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -705612.375000\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -552837.250000\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -500538.468750\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -776672.125000\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -660870.375000\n",
      "    epoch          : 127\n",
      "    loss           : -629450.6653125\n",
      "    val_loss       : -628944.06640625\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -778807.125000\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -496841.562500\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -523972.687500\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -496993.718750\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -765047.750000\n",
      "    epoch          : 128\n",
      "    loss           : -629461.228125\n",
      "    val_loss       : -629309.93359375\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -549100.375000\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -779123.687500\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -695971.687500\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -543265.187500\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -759687.875000\n",
      "    epoch          : 129\n",
      "    loss           : -629311.3865625\n",
      "    val_loss       : -629755.921875\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -775643.500000\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -652399.687500\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -783005.437500\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -521688.718750\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -765335.750000\n",
      "    epoch          : 130\n",
      "    loss           : -629632.16875\n",
      "    val_loss       : -629097.284375\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -706378.875000\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -492984.687500\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -758881.625000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -503592.437500\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -690550.062500\n",
      "    epoch          : 131\n",
      "    loss           : -629587.484375\n",
      "    val_loss       : -629344.22265625\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -779776.000000\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -500840.250000\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -512663.531250\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -496410.312500\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -500422.687500\n",
      "    epoch          : 132\n",
      "    loss           : -629583.236875\n",
      "    val_loss       : -628698.09453125\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -496747.156250\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -772165.687500\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -776690.937500\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -521887.437500\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -758872.125000\n",
      "    epoch          : 133\n",
      "    loss           : -629789.4925\n",
      "    val_loss       : -628770.8828125\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -762684.437500\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -555984.937500\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -504666.375000\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -554289.000000\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -503185.812500\n",
      "    epoch          : 134\n",
      "    loss           : -629472.2128125\n",
      "    val_loss       : -628930.48984375\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -504013.750000\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -544378.187500\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -759899.062500\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -704556.062500\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -768408.000000\n",
      "    epoch          : 135\n",
      "    loss           : -629697.0290625\n",
      "    val_loss       : -629732.44140625\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -499225.687500\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -701998.375000\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -778864.125000\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -764782.687500\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -655429.375000\n",
      "    epoch          : 136\n",
      "    loss           : -629479.1890625\n",
      "    val_loss       : -630137.24375\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -699979.937500\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -782357.437500\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -498898.750000\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -492989.812500\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -757364.062500\n",
      "    epoch          : 137\n",
      "    loss           : -629708.1653125\n",
      "    val_loss       : -628553.38046875\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -492235.906250\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -764413.875000\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -696242.125000\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -547391.812500\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -771259.187500\n",
      "    epoch          : 138\n",
      "    loss           : -629787.7625\n",
      "    val_loss       : -629247.79921875\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -521675.468750\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -547040.000000\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -655301.000000\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -702488.625000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -778440.625000\n",
      "    epoch          : 139\n",
      "    loss           : -629673.6146875\n",
      "    val_loss       : -629301.30625\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -520303.375000\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -551442.437500\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -496667.625000\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -656818.937500\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -493050.937500\n",
      "    epoch          : 140\n",
      "    loss           : -629842.3765625\n",
      "    val_loss       : -629061.4984375\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -521721.656250\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -693872.875000\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -765440.500000\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -494110.750000\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -497586.531250\n",
      "    epoch          : 141\n",
      "    loss           : -629688.930625\n",
      "    val_loss       : -629623.87421875\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -548628.562500\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -652173.375000\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -775618.187500\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -565874.250000\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -493898.812500\n",
      "    epoch          : 142\n",
      "    loss           : -629872.0853125\n",
      "    val_loss       : -629992.05546875\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -780538.687500\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -521408.656250\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -764333.250000\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -550712.062500\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -709721.375000\n",
      "    epoch          : 143\n",
      "    loss           : -629897.9315625\n",
      "    val_loss       : -630246.03984375\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -511406.437500\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -780960.812500\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -774404.625000\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -500637.812500\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -503612.562500\n",
      "    epoch          : 144\n",
      "    loss           : -630072.0609375\n",
      "    val_loss       : -629979.19765625\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -554571.625000\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -551679.875000\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -498086.250000\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -505275.031250\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -489274.625000\n",
      "    epoch          : 145\n",
      "    loss           : -630141.0403125\n",
      "    val_loss       : -629953.134375\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -787227.875000\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -698700.312500\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -508181.562500\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -775598.437500\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -766159.437500\n",
      "    epoch          : 146\n",
      "    loss           : -630052.3759375\n",
      "    val_loss       : -630056.2328125\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -709192.187500\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -503978.875000\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -551400.500000\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -774807.875000\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -500059.750000\n",
      "    epoch          : 147\n",
      "    loss           : -630226.6971875\n",
      "    val_loss       : -629709.221875\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -494541.937500\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -775968.625000\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -520020.562500\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -771553.375000\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -516805.062500\n",
      "    epoch          : 148\n",
      "    loss           : -629991.423125\n",
      "    val_loss       : -629429.60546875\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -776406.625000\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -781745.187500\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -652525.500000\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -650189.562500\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -650041.187500\n",
      "    epoch          : 149\n",
      "    loss           : -630103.225\n",
      "    val_loss       : -630510.90078125\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -781534.812500\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -487170.125000\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -783143.625000\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -552175.625000\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -550802.875000\n",
      "    epoch          : 150\n",
      "    loss           : -630277.51625\n",
      "    val_loss       : -631407.678125\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0826_103239/checkpoint-epoch150.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -778422.312500\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -501589.406250\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -702733.375000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -548333.375000\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -765853.687500\n",
      "    epoch          : 151\n",
      "    loss           : -630317.0884375\n",
      "    val_loss       : -630555.46796875\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -518352.093750\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -519002.343750\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -554465.625000\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -551288.625000\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -703134.812500\n",
      "    epoch          : 152\n",
      "    loss           : -630261.654375\n",
      "    val_loss       : -630676.34765625\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -779720.437500\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -768562.750000\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -770510.625000\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -706874.875000\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -766746.875000\n",
      "    epoch          : 153\n",
      "    loss           : -630459.5359375\n",
      "    val_loss       : -630349.909375\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -774416.625000\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -553967.750000\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -504861.718750\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -490918.500000\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -701422.750000\n",
      "    epoch          : 154\n",
      "    loss           : -630390.6265625\n",
      "    val_loss       : -629827.90390625\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -495470.531250\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -556896.562500\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -778003.250000\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -555200.375000\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -502835.812500\n",
      "    epoch          : 155\n",
      "    loss           : -630359.38625\n",
      "    val_loss       : -629837.88203125\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -501846.375000\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -502864.812500\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -491399.312500\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -703471.750000\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -500036.187500\n",
      "    epoch          : 156\n",
      "    loss           : -630551.9940625\n",
      "    val_loss       : -630453.77578125\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -706434.562500\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -519559.343750\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -555509.000000\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -495155.937500\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -693784.812500\n",
      "    epoch          : 157\n",
      "    loss           : -630424.1628125\n",
      "    val_loss       : -630428.659375\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -525652.250000\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -653538.125000\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -502705.437500\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -699612.437500\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -654152.312500\n",
      "    epoch          : 158\n",
      "    loss           : -630603.094375\n",
      "    val_loss       : -630106.546875\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -777834.437500\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -549738.625000\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -648981.875000\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -702408.312500\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -554393.500000\n",
      "    epoch          : 159\n",
      "    loss           : -630543.4125\n",
      "    val_loss       : -630420.72421875\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -781527.500000\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -763265.250000\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -766853.875000\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -702381.000000\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -777495.250000\n",
      "    epoch          : 160\n",
      "    loss           : -630349.7334375\n",
      "    val_loss       : -630342.7515625\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -774034.875000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -525826.687500\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -496112.437500\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -776752.625000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -660730.187500\n",
      "    epoch          : 161\n",
      "    loss           : -630509.33375\n",
      "    val_loss       : -630231.778125\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -781737.375000\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -760367.375000\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -498915.718750\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -649984.187500\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -526382.750000\n",
      "    epoch          : 162\n",
      "    loss           : -630555.69375\n",
      "    val_loss       : -630760.1484375\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -545223.062500\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -659362.937500\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -706684.875000\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -777130.375000\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -703226.250000\n",
      "    epoch          : 163\n",
      "    loss           : -630729.08625\n",
      "    val_loss       : -630162.05703125\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -701644.125000\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -499323.750000\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -524555.562500\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -556272.000000\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -704698.500000\n",
      "    epoch          : 164\n",
      "    loss           : -630634.5890625\n",
      "    val_loss       : -630427.1125\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -705483.062500\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -656465.312500\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -776368.125000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -524169.156250\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -556243.437500\n",
      "    epoch          : 165\n",
      "    loss           : -630760.1203125\n",
      "    val_loss       : -629695.11640625\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -497288.500000\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -776770.562500\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -705364.250000\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -558084.250000\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -651191.625000\n",
      "    epoch          : 166\n",
      "    loss           : -630754.338125\n",
      "    val_loss       : -629270.8859375\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -767170.500000\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -655370.375000\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -653680.875000\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -786066.375000\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -705848.625000\n",
      "    epoch          : 167\n",
      "    loss           : -630899.2653125\n",
      "    val_loss       : -629575.76328125\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -775729.062500\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -502677.593750\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -515421.156250\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -532148.500000\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -765792.937500\n",
      "    epoch          : 168\n",
      "    loss           : -630819.710625\n",
      "    val_loss       : -630673.25\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -552277.125000\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -499099.406250\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -779596.625000\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -771371.625000\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -490341.593750\n",
      "    epoch          : 169\n",
      "    loss           : -630778.8225\n",
      "    val_loss       : -629352.57890625\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -693421.250000\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -502116.406250\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -557935.187500\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -500763.156250\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -654668.000000\n",
      "    epoch          : 170\n",
      "    loss           : -630795.765625\n",
      "    val_loss       : -630026.66171875\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -779592.937500\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -662120.250000\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -655999.187500\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -651749.625000\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -493876.812500\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "    epoch          : 171\n",
      "    loss           : -630755.9284375\n",
      "    val_loss       : -630076.1859375\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -698616.187500\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -761326.937500\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -555666.937500\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -498797.937500\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -503447.437500\n",
      "    epoch          : 172\n",
      "    loss           : -630914.3696875\n",
      "    val_loss       : -630402.5734375\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -699065.750000\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -697749.312500\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -526464.375000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -498685.531250\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -502250.812500\n",
      "    epoch          : 173\n",
      "    loss           : -631061.5459375\n",
      "    val_loss       : -630080.93203125\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -558715.750000\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -780442.437500\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -500855.093750\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -556547.625000\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -499745.687500\n",
      "    epoch          : 174\n",
      "    loss           : -630986.20375\n",
      "    val_loss       : -629733.8703125\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -548616.625000\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -552139.437500\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -497080.875000\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -775391.125000\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -658874.750000\n",
      "    epoch          : 175\n",
      "    loss           : -631096.4896875\n",
      "    val_loss       : -630344.9046875\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -780594.375000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -556276.375000\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -495927.375000\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -763780.562500\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -766387.375000\n",
      "    epoch          : 176\n",
      "    loss           : -630911.1246875\n",
      "    val_loss       : -631171.07890625\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -551775.250000\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -519337.093750\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -547609.250000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -650643.250000\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -701433.375000\n",
      "    epoch          : 177\n",
      "    loss           : -631053.274375\n",
      "    val_loss       : -631000.578125\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -505022.187500\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -516290.875000\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -779388.875000\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -697875.000000\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -770126.687500\n",
      "    epoch          : 178\n",
      "    loss           : -631062.229375\n",
      "    val_loss       : -631110.6609375\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -769921.375000\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -499618.250000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -549627.000000\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -544831.750000\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -767304.812500\n",
      "    epoch          : 179\n",
      "    loss           : -630967.3284375\n",
      "    val_loss       : -629950.7625\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -557093.375000\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -779511.250000\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -771447.187500\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -526888.625000\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -704993.750000\n",
      "    epoch          : 180\n",
      "    loss           : -630968.9690625\n",
      "    val_loss       : -630162.67265625\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -767683.875000\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -778271.062500\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -555048.812500\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -496992.875000\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -704501.000000\n",
      "    epoch          : 181\n",
      "    loss           : -630930.5584375\n",
      "    val_loss       : -630985.1328125\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -501014.687500\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -498113.812500\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -696102.250000\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -524931.625000\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -649431.875000\n",
      "    epoch          : 182\n",
      "    loss           : -630907.684375\n",
      "    val_loss       : -630315.1765625\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -777930.375000\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -764589.875000\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -501107.031250\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -780406.375000\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -491517.750000\n",
      "    epoch          : 183\n",
      "    loss           : -630797.0890625\n",
      "    val_loss       : -630489.853125\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -553643.500000\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -551361.437500\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -529832.375000\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -779460.312500\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -502225.375000\n",
      "    epoch          : 184\n",
      "    loss           : -630885.3896875\n",
      "    val_loss       : -630039.86640625\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -655078.875000\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -521674.312500\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -650512.500000\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -654969.312500\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -506430.437500\n",
      "    epoch          : 185\n",
      "    loss           : -630825.9028125\n",
      "    val_loss       : -631087.86875\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -772009.937500\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -780373.750000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -493567.000000\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -554810.000000\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -500687.656250\n",
      "    epoch          : 186\n",
      "    loss           : -631222.2815625\n",
      "    val_loss       : -630875.07578125\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -778574.562500\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -500851.687500\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -498797.125000\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -704480.625000\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -552473.375000\n",
      "    epoch          : 187\n",
      "    loss           : -630800.621875\n",
      "    val_loss       : -630891.16875\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -778788.750000\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -552143.312500\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -514463.062500\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -553333.750000\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -495175.531250\n",
      "    epoch          : 188\n",
      "    loss           : -630885.085\n",
      "    val_loss       : -630908.096875\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -508811.750000\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -556780.250000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -549057.750000\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -505052.125000\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -763396.312500\n",
      "    epoch          : 189\n",
      "    loss           : -630995.904375\n",
      "    val_loss       : -630683.828125\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -557260.625000\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -654888.250000\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -500231.312500\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -517814.937500\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -647197.375000\n",
      "    epoch          : 190\n",
      "    loss           : -630981.1184375\n",
      "    val_loss       : -630178.67890625\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -493113.718750\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -522970.343750\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -551471.500000\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -779970.000000\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -654507.875000\n",
      "    epoch          : 191\n",
      "    loss           : -630940.7159375\n",
      "    val_loss       : -631927.18828125\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -765232.187500\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -779022.625000\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -506618.000000\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -653124.062500\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -521552.375000\n",
      "    epoch          : 192\n",
      "    loss           : -630993.3828125\n",
      "    val_loss       : -630741.21171875\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -550317.937500\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -770815.000000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -778083.375000\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -661694.937500\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -708191.312500\n",
      "    epoch          : 193\n",
      "    loss           : -630953.91125\n",
      "    val_loss       : -631210.26484375\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -775261.062500\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -703770.250000\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -558883.562500\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -492839.562500\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -706571.312500\n",
      "    epoch          : 194\n",
      "    loss           : -631009.0903125\n",
      "    val_loss       : -630148.68359375\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -523204.687500\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -518940.281250\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -760570.562500\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -695195.312500\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -765758.875000\n",
      "    epoch          : 195\n",
      "    loss           : -630974.88375\n",
      "    val_loss       : -629533.94453125\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -705446.875000\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -777545.500000\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -497402.156250\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -558468.750000\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -550816.250000\n",
      "    epoch          : 196\n",
      "    loss           : -630906.709375\n",
      "    val_loss       : -630765.671875\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -781499.375000\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -701885.500000\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -522659.906250\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -778065.375000\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -762281.187500\n",
      "    epoch          : 197\n",
      "    loss           : -630999.158125\n",
      "    val_loss       : -629933.86328125\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -551607.187500\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -522234.000000\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -561090.312500\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -532095.375000\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -502251.312500\n",
      "    epoch          : 198\n",
      "    loss           : -631072.8621875\n",
      "    val_loss       : -630749.63125\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -555820.187500\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -552115.812500\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -774024.187500\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -707031.375000\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -504621.625000\n",
      "    epoch          : 199\n",
      "    loss           : -630955.6709375\n",
      "    val_loss       : -630417.3921875\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -557298.500000\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -702717.875000\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -499985.031250\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -657852.625000\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -498001.125000\n",
      "    epoch          : 200\n",
      "    loss           : -630844.8165625\n",
      "    val_loss       : -629830.65390625\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0826_103239/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -709600.375000\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -771831.937500\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -642847.812500\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -561918.625000\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -500520.906250\n",
      "    epoch          : 201\n",
      "    loss           : -631105.714375\n",
      "    val_loss       : -630666.7234375\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -766812.062500\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -565751.875000\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -772396.562500\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -783355.312500\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -705018.875000\n",
      "    epoch          : 202\n",
      "    loss           : -630955.241875\n",
      "    val_loss       : -630397.98984375\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -764483.312500\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -646710.375000\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -498358.500000\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -500202.781250\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -782369.750000\n",
      "    epoch          : 203\n",
      "    loss           : -631116.550625\n",
      "    val_loss       : -630127.690625\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -775747.312500\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -550466.625000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -654619.937500\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -656659.437500\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -503359.750000\n",
      "    epoch          : 204\n",
      "    loss           : -631148.8121875\n",
      "    val_loss       : -630015.4078125\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -782666.062500\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -554692.625000\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -500009.687500\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -650466.750000\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -553015.625000\n",
      "    epoch          : 205\n",
      "    loss           : -630959.16375\n",
      "    val_loss       : -630417.2140625\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -779020.625000\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -779936.187500\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -767036.750000\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -778452.062500\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -489909.062500\n",
      "    epoch          : 206\n",
      "    loss           : -630937.1615625\n",
      "    val_loss       : -630986.2046875\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -500011.625000\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -551450.000000\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -778914.500000\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -778367.625000\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -652695.500000\n",
      "    epoch          : 207\n",
      "    loss           : -631012.7903125\n",
      "    val_loss       : -629850.521875\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -502459.593750\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -782892.062500\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -551185.062500\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -700593.875000\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -559807.000000\n",
      "    epoch          : 208\n",
      "    loss           : -630829.6765625\n",
      "    val_loss       : -631761.065625\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -500614.156250\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -780038.875000\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -498643.000000\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -656179.375000\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -702579.500000\n",
      "    epoch          : 209\n",
      "    loss           : -630943.095625\n",
      "    val_loss       : -630497.1453125\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -496720.687500\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -505090.125000\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -706057.375000\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -501540.375000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -506288.562500\n",
      "    epoch          : 210\n",
      "    loss           : -630852.36875\n",
      "    val_loss       : -630379.5078125\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -778119.375000\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -781046.750000\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -521909.437500\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -699646.250000\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -496272.187500\n",
      "    epoch          : 211\n",
      "    loss           : -630950.9475\n",
      "    val_loss       : -631081.60859375\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -547833.312500\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -546687.500000\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -552823.187500\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -707900.375000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -767030.625000\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch   212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "    epoch          : 212\n",
      "    loss           : -630812.3578125\n",
      "    val_loss       : -630410.63984375\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -552642.750000\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -521305.375000\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -550792.000000\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -503913.343750\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -496642.687500\n",
      "    epoch          : 213\n",
      "    loss           : -631054.31375\n",
      "    val_loss       : -630368.04453125\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -771707.875000\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -554545.375000\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -505743.781250\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -548955.562500\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -656995.312500\n",
      "    epoch          : 214\n",
      "    loss           : -631197.1184375\n",
      "    val_loss       : -630373.4359375\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -777275.750000\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -555588.000000\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -775381.562500\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -498981.187500\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -767254.687500\n",
      "    epoch          : 215\n",
      "    loss           : -631044.8234375\n",
      "    val_loss       : -631016.2859375\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -776818.125000\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -774178.250000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -547998.000000\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -783871.125000\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -504636.468750\n",
      "    epoch          : 216\n",
      "    loss           : -630918.944375\n",
      "    val_loss       : -630451.70078125\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -761619.750000\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -562173.875000\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -776576.500000\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -532149.000000\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -699045.750000\n",
      "    epoch          : 217\n",
      "    loss           : -630959.8625\n",
      "    val_loss       : -630010.14375\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -561359.187500\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -518448.843750\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -651338.937500\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -783289.750000\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -699623.562500\n",
      "    epoch          : 218\n",
      "    loss           : -630953.6434375\n",
      "    val_loss       : -630457.11484375\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -657372.875000\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -653150.062500\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -552662.562500\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -780159.875000\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -771398.625000\n",
      "    epoch          : 219\n",
      "    loss           : -631160.7034375\n",
      "    val_loss       : -630322.94140625\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -757604.000000\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -775453.937500\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -702598.812500\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -777377.500000\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -494443.000000\n",
      "    epoch          : 220\n",
      "    loss           : -630988.97375\n",
      "    val_loss       : -630575.93828125\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -508209.156250\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -704572.562500\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -522762.312500\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -516132.875000\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -504836.593750\n",
      "    epoch          : 221\n",
      "    loss           : -631118.8621875\n",
      "    val_loss       : -630807.9015625\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -757445.750000\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -707674.437500\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -500063.687500\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -549296.875000\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -689484.375000\n",
      "    epoch          : 222\n",
      "    loss           : -631005.9228125\n",
      "    val_loss       : -629937.96484375\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -555992.312500\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -502143.437500\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -771203.500000\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -526631.687500\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -508041.562500\n",
      "    epoch          : 223\n",
      "    loss           : -631004.44625\n",
      "    val_loss       : -630796.18671875\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -500363.750000\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -557615.500000\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -778036.750000\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -655118.250000\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -700957.750000\n",
      "    epoch          : 224\n",
      "    loss           : -631183.3921875\n",
      "    val_loss       : -629852.0546875\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -652153.375000\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -493958.781250\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -775464.687500\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -523326.468750\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -498566.812500\n",
      "    epoch          : 225\n",
      "    loss           : -631049.7775\n",
      "    val_loss       : -629956.2203125\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -772731.500000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -551994.500000\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -493816.562500\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -760678.875000\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -767545.812500\n",
      "    epoch          : 226\n",
      "    loss           : -631053.6946875\n",
      "    val_loss       : -630548.85390625\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -703014.812500\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -553131.875000\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -699636.750000\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -699994.250000\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -765647.000000\n",
      "    epoch          : 227\n",
      "    loss           : -630983.39625\n",
      "    val_loss       : -631119.253125\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -517961.843750\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -650456.750000\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -655927.812500\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -548887.250000\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -495794.406250\n",
      "    epoch          : 228\n",
      "    loss           : -630920.1471875\n",
      "    val_loss       : -630188.4140625\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -526383.875000\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -649812.375000\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -553583.500000\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -548029.125000\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -764585.500000\n",
      "    epoch          : 229\n",
      "    loss           : -631054.065\n",
      "    val_loss       : -630712.23828125\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -698061.937500\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -512905.812500\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -504359.593750\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -782338.375000\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -772230.250000\n",
      "    epoch          : 230\n",
      "    loss           : -631207.8809375\n",
      "    val_loss       : -630565.47421875\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -547837.062500\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -499175.343750\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -497120.812500\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -504871.250000\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -505485.750000\n",
      "    epoch          : 231\n",
      "    loss           : -630953.0171875\n",
      "    val_loss       : -630657.4984375\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -504299.468750\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -779309.812500\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -782431.125000\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -496137.156250\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -648201.375000\n",
      "    epoch          : 232\n",
      "    loss           : -630819.5428125\n",
      "    val_loss       : -631157.9828125\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -497787.750000\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -653114.000000\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -769612.437500\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -766841.000000\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -506522.093750\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "    epoch          : 233\n",
      "    loss           : -630889.7634375\n",
      "    val_loss       : -629494.99375\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -780072.250000\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -524961.375000\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -550191.125000\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -555132.062500\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -649320.750000\n",
      "    epoch          : 234\n",
      "    loss           : -631009.328125\n",
      "    val_loss       : -630270.321875\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -489732.187500\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -501750.000000\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -559573.375000\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -501731.875000\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -767754.000000\n",
      "    epoch          : 235\n",
      "    loss           : -631071.87875\n",
      "    val_loss       : -631304.4046875\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -495017.062500\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -503709.468750\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -550389.437500\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -778336.312500\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -503309.187500\n",
      "    epoch          : 236\n",
      "    loss           : -631120.735625\n",
      "    val_loss       : -629672.828125\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -777111.750000\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -767109.750000\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -496554.500000\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -648831.625000\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -765995.562500\n",
      "    epoch          : 237\n",
      "    loss           : -631011.2121875\n",
      "    val_loss       : -629768.03125\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -499731.937500\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -652238.000000\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -546803.375000\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -560809.000000\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -764527.750000\n",
      "    epoch          : 238\n",
      "    loss           : -631019.431875\n",
      "    val_loss       : -630322.1625\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -784131.500000\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -552320.875000\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -651966.250000\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -655716.625000\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -654517.875000\n",
      "    epoch          : 239\n",
      "    loss           : -631112.28375\n",
      "    val_loss       : -630734.571875\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -777119.437500\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -493605.562500\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -505444.125000\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -551755.125000\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -500429.125000\n",
      "    epoch          : 240\n",
      "    loss           : -631209.57\n",
      "    val_loss       : -630694.03515625\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -778001.062500\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -654194.437500\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -559128.562500\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -652995.250000\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -769419.812500\n",
      "    epoch          : 241\n",
      "    loss           : -631170.6715625\n",
      "    val_loss       : -629616.859375\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -502247.687500\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -768459.750000\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -516819.062500\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -525969.250000\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -551858.500000\n",
      "    epoch          : 242\n",
      "    loss           : -630934.77375\n",
      "    val_loss       : -629703.85234375\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -553757.500000\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -531341.437500\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -553734.125000\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -517929.562500\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -764643.250000\n",
      "    epoch          : 243\n",
      "    loss           : -630992.0140625\n",
      "    val_loss       : -630389.62265625\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -488468.750000\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -697921.000000\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -555099.000000\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -707271.750000\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -659117.500000\n",
      "    epoch          : 244\n",
      "    loss           : -631089.239375\n",
      "    val_loss       : -630202.88125\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -557264.062500\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -503249.906250\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -647772.125000\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -502920.187500\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -489250.000000\n",
      "    epoch          : 245\n",
      "    loss           : -631162.1396875\n",
      "    val_loss       : -629917.70078125\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -508167.156250\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -703516.437500\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -706896.000000\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -506419.062500\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -766847.250000\n",
      "    epoch          : 246\n",
      "    loss           : -631126.9721875\n",
      "    val_loss       : -630750.75078125\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -523863.437500\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -777689.125000\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -785907.250000\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -550567.250000\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -654674.250000\n",
      "    epoch          : 247\n",
      "    loss           : -631171.140625\n",
      "    val_loss       : -630351.425\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -650519.625000\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -655298.125000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -555320.375000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -556163.937500\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -654988.000000\n",
      "    epoch          : 248\n",
      "    loss           : -630961.2503125\n",
      "    val_loss       : -630789.06171875\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -697666.000000\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -777253.375000\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -765520.500000\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -496278.218750\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -556657.250000\n",
      "    epoch          : 249\n",
      "    loss           : -631091.9953125\n",
      "    val_loss       : -630011.2921875\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -552233.375000\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -558814.812500\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -765896.562500\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -777928.375000\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -502749.375000\n",
      "    epoch          : 250\n",
      "    loss           : -631015.149375\n",
      "    val_loss       : -630383.79765625\n",
      "Saving checkpoint: saved/models/FashionMnist_VlaeCategory/0826_103239/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -521508.312500\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -778322.812500\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -509833.062500\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -524432.750000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -700033.000000\n",
      "    epoch          : 251\n",
      "    loss           : -630861.8446875\n",
      "    val_loss       : -630445.965625\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -776412.250000\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -657948.125000\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -496755.468750\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -768007.125000\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -555337.312500\n",
      "    epoch          : 252\n",
      "    loss           : -630709.886875\n",
      "    val_loss       : -630286.52421875\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -520569.500000\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -551815.750000\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -771566.250000\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -659099.812500\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -704955.062500\n",
      "    epoch          : 253\n",
      "    loss           : -631126.5671875\n",
      "    val_loss       : -631022.07109375\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -780450.187500\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -779346.250000\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -494080.406250\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -704221.187500\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -503600.343750\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch   254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "    epoch          : 254\n",
      "    loss           : -631003.8475\n",
      "    val_loss       : -631336.55234375\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -486746.750000\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -522663.843750\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -768388.437500\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -501175.375000\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -498705.250000\n",
      "    epoch          : 255\n",
      "    loss           : -631040.47125\n",
      "    val_loss       : -630453.684375\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -553937.875000\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -652750.562500\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -499986.375000\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -777523.250000\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -758707.562500\n",
      "    epoch          : 256\n",
      "    loss           : -630956.4334375\n",
      "    val_loss       : -630357.446875\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -648680.312500\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -518850.031250\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -522130.343750\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -553381.500000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -654880.750000\n",
      "    epoch          : 257\n",
      "    loss           : -630949.745\n",
      "    val_loss       : -629606.434375\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -488056.500000\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -503315.656250\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -768332.875000\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -494783.312500\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -655280.500000\n",
      "    epoch          : 258\n",
      "    loss           : -631347.3425\n",
      "    val_loss       : -631322.9421875\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -498580.968750\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -558024.187500\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -781837.875000\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -502925.625000\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -662546.625000\n",
      "    epoch          : 259\n",
      "    loss           : -631025.40625\n",
      "    val_loss       : -630618.14609375\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -772332.625000\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -549919.250000\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -767208.625000\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -551044.562500\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -558827.312500\n",
      "    epoch          : 260\n",
      "    loss           : -631156.055\n",
      "    val_loss       : -630838.0578125\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -524490.750000\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -549190.187500\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -777255.500000\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -652725.625000\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -547283.000000\n",
      "    epoch          : 261\n",
      "    loss           : -630991.0084375\n",
      "    val_loss       : -630857.38515625\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -506165.875000\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -499038.625000\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -552457.937500\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -765318.000000\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -499168.281250\n",
      "    epoch          : 262\n",
      "    loss           : -630965.421875\n",
      "    val_loss       : -631128.97265625\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -653845.000000\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -656816.250000\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -551967.500000\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -497795.437500\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -704910.937500\n",
      "    epoch          : 263\n",
      "    loss           : -631130.6103125\n",
      "    val_loss       : -630003.85859375\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -498168.781250\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -556063.437500\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -769112.750000\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -525165.437500\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -772530.000000\n",
      "    epoch          : 264\n",
      "    loss           : -631109.5334375\n",
      "    val_loss       : -630700.5\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -497434.000000\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -659097.937500\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -502207.750000\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -651042.250000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -776854.000000\n",
      "    epoch          : 265\n",
      "    loss           : -631103.0978125\n",
      "    val_loss       : -631223.275\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -701163.250000\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -781090.312500\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -651401.250000\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -654603.312500\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -648414.750000\n",
      "    epoch          : 266\n",
      "    loss           : -630950.2740625\n",
      "    val_loss       : -629830.6203125\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -495802.343750\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -697518.500000\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -552476.437500\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -775771.750000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -761571.937500\n",
      "    epoch          : 267\n",
      "    loss           : -631213.8046875\n",
      "    val_loss       : -630475.23125\n",
      "Validation performance didn't improve for 75 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VlaeCategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): LadderDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_5_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (noise_linear): Linear(in_features=2744, out_features=4, bias=True)\n",
       "      (ladder_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (ladder_linear): Linear(in_features=2744, out_features=1024, bias=True)\n",
       "    )\n",
       "    (generator_6): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_6_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_7_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=512, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_8_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=32, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_9_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_10): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_10_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_11): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_11_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=16, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "    (global_element_6): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=50, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAT4UlEQVR4nO3dfZBddX3H8fdnlw1JNokk5IGYBAMIatQWNEYqTgv1oUBV9A+ttFXo2Imd0SozDNax0xFn+kBbFZ+diQWJgqAzSqFtaqFBSqFjyoIxBEMhpoGExDwS8kCedvfbP+5J57Ls+Z3Nfdh7k9/nNXNn772/e+753rv3s+fs/Z3f+SkiMLOTX0+nCzCz8eGwm2XCYTfLhMNulgmH3SwTDrtZJhz2k4yk6yXd2uCyr5L0M0n7JH2i1bW1mqQ/kHRPp+s4UTjsLSLprZL+S9LzknZLekjSmzpd13H6FHB/REyNiK90upgqEXFbRLyz03WcKBz2FpA0Dfhn4KvADGAe8DngcCfrasArgMfLGiX1jmMtSZJOaWJZScrus5/dC26T8wAi4vaIGIqIgxFxT0SsAZB0jqT7JO2StFPSbZJOO7awpI2SrpO0RtIBSTdJmiPpX4td6n+XNL147EJJIWmppC2Stkq6tqwwSRcWexx7JP1c0sUlj7sPuAT4mqT9ks6TdIukb0paIekAcImk10i6v3i+xyW9p+45bpH0jaLu/cXezRmSviTpOUlPSLogUWtI+oSkDcX79PfHQinp6uL5bpS0G7i+uO/BuuXfIunhYu/qYUlvqWu7X9JfSXoIeAE4O/H7PDlFhC9NXoBpwC5gOXAZMH1E+yuBdwCnArOAB4Av1bVvBH4KzKG2V7AdeBS4oFjmPuCzxWMXAgHcDvQDrwd2AG8v2q8Hbi2uzyvqupzaH/Z3FLdnlbyO+4E/rrt9C/A8cFGx/FRgPfAZYALw28A+4FV1j98JvBGYWNT9v8CHgV7gL4GfJN7HAH5Cbe/oTODJY/UAVwODwJ8CpwCTivseLNpnAM8BHyraryxun1732p4BXlu093X6czPeF2/ZWyAi9gJvpfZh/RawQ9LdkuYU7esj4t6IOBwRO4AvAr814mm+GhHbIuJZ4D+BVRHxs4g4DNxJLfj1PhcRByLiMeDb1D7cI/0hsCIiVkTEcETcCwxQC/9Y3RURD0XEMHA+MAW4ISKORMR91P59qV/3nRHxSEQcKuo+FBHfiYgh4PujvI6R/jYidkfEM8CXRjz3loj4akQMRsTBEcv9LvBURHy3aL8deAJ4d91jbomIx4v2o8fxHpwUHPYWiYh1EXF1RMwHXge8nNqHFUmzJd0h6VlJe4FbgZkjnmJb3fWDo9yeMuLxm+quP12sb6RXAO8vdrn3SNpD7Y/S3ON4afXreTmwqQh+/brn1d0+3teRWt/I17WJci8vHl9vZG2p5U96DnsbRMQT1HZpX1fc9TfUtvq/FhHTqG1x1eRqFtRdPxPYMspjNgHfjYjT6i79EXHDcaynfljkFmDBiC+3zgSePY7nq5J6Xakhmluo/XGrN7K2rId4OuwtIOnVkq6VNL+4vYDa7udPi4dMBfYDeyTNA65rwWr/QtJkSa8F/ojaLvJItwLvlvQ7knolTZR08bE6G7AKOAB8SlJf8WXfu4E7Gny+0VwnaXrxHn6S0V/XaFYA50n6fUmnSPo9YBG1fzMMh71V9gFvBlYV31r/FFgLHPuW/HPAG6h92fUvwI9asM7/oPZl2Urg8xHxkoNLImITcAW1L9R2UNvSX0eDv/eIOAK8h9qXkDuBbwAfLvZkWuUu4BFgNbX36qYx1rYLeBe193wXtWMG3hURO1tY2wlNxTeVdoKQtJDaN9x9ETHY4XJaSlIA50bE+k7XcjLylt0sEw67WSa8G2+WCW/ZzTLR8GCCRkzQqTGR/vFcpVlWDnGAI3F41GM4mgq7pEuBL1M77vkfqg7WmEg/b9bbmlmlmSWsipWlbQ3vxhfDHb9Orc91EXClpEWNPp+ZtVcz/7MvAdZHxIbiYIs7qB3AYWZdqJmwz+PFAws28+JBBwAU464HJA0cPeHO5WB28mgm7KN9CfCSfryIWBYRiyNicR+nNrE6M2tGM2HfzItHKM1n9JFXZtYFmgn7w8C5ks6SNAH4IHB3a8oys1ZruOstIgYlfRz4N2pdbzdHROnJCs2ss5rqZ4+IFdTGEZtZl/PhsmaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTTU3ZLGkjsA8YAgYjYnErijKz1msq7IVLImJnC57HzNrIu/FmmWg27AHcI+kRSUtHe4CkpZIGJA0c5XCTqzOzRjW7G39RRGyRNBu4V9ITEfFA/QMiYhmwDGCaZkST6zOzBjW1ZY+ILcXP7cCdwJJWFGVmrddw2CX1S5p67DrwTmBtqwozs9ZqZjd+DnCnpGPP872I+HFLqjLrtNrnulTveeck29/+w0eT7esPzi5t++WbDiWXbVTDYY+IDcCvt7AWM2sjd72ZZcJhN8uEw26WCYfdLBMOu1kmWjEQxqw9Krq/ek49Nb18T/m2LBadnVx0aFJfsv1Ifzo6337qwmT7hFMGS9tm8mRy2UZ5y26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcL97Na9lN4WaUp/evne3tKmns07kov2HEoPM+09mG4/c+3MZPuRhbOS7e3gLbtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgn3s9sJKyr6utVX/vEeXjgv/eRPbEg2V42lj737ku1DE88of+7kko3zlt0sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4T72a1j1Dch2d47c0ayffj005Ltgy+bWNp2aGZ63VOenpRe9wsvJNs1If38E7eU98MPJZdsXOWWXdLNkrZLWlt33wxJ90p6qvg5vU31mVmLjGU3/hbg0hH3fRpYGRHnAiuL22bWxSrDHhEPALtH3H0FsLy4vhx4b2vLMrNWa/QLujkRsRWg+Dm77IGSlkoakDRwlMMNrs7MmtX2b+MjYllELI6IxX1UTMRnZm3TaNi3SZoLUPzc3rqSzKwdGg373cBVxfWrgLtaU46ZtUtlP7uk24GLgZmSNgOfBW4AfiDpI8AzwPvbWaSduJQY9907K31u9cH5p6ef+2i6R/rg7PJ1b39Dejs3dc20ZHtPT3rueE5JR2vXG8tf22m/SD91oyrDHhFXljS9rcW1mFkb+XBZs0w47GaZcNjNMuGwm2XCYTfLhIe4Zq6nv2La4yoR6eefleg+U7r7qndv+lTRVSbsHSxtm/yr9NGcQ6dPTbb3PLcnvfKj5esGmPnA5tK29JKN85bdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uE+9kzl5rWGCAq+os1oS+9fKq9YlkdOJhsH96xK9ne98vy2s94KN3HH4Pp1z00nD6+gOGKE0LvS0/p3A7esptlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXA/e+aG9x9IP0Dp7YF6e5Ptuy6cU9q2Y8lwctn+Z9Knkp7/9Z3J9jhypLyt4nUR6dqqxvFXanb5BnjLbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwv3smasat12lojeaic+Vj+uefc7u5LIHNs1uoKKxqTo+QL0VY+1flp7SeXjP8+nlE+fM11kLkssOrXsq2V6mcssu6WZJ2yWtrbvveknPSlpdXC5vaO1mNm7Gsht/C3DpKPffGBHnF5cVrS3LzFqtMuwR8QCQ3t8ys67XzBd0H5e0ptjNn172IElLJQ1IGjjK4SZWZ2bNaDTs3wTOAc4HtgJfKHtgRCyLiMURsbiP9GR6ZtY+DYU9IrZFxFBEDAPfApa0tiwza7WGwi5pbt3N9wFryx5rZt2hsp9d0u3AxcBMSZuBzwIXSzofCGAj8NH2lWjdLI6WjxkHmPTjR0vbJm98ZXLZGZsfT7YPHUyfVz41ZjyGKs7r3pM+r3xPX7ofvmfSxGT78MHyuef3v3pGctnJ65LNpSrDHhFXjnL3TY2tzsw6xYfLmmXCYTfLhMNulgmH3SwTDrtZJjzEdax6EkMiq6bnTS3bClXrb6fEUE0ATZhQ2jY8uWK650ULk+29P1+fbCfRvaYp/clFNWlSsn3Pb8xPtk/7xZ5k++H5U0vblt14Y3LZa/7xovLGxBmqvWU3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhfvZCz+TJyXb1l/fLxr59yWUPXvL6ZPszl6X/5saUdD+6Dpb340/ZkO7jn7grPXXwtKfTpxLbc0767EOHZ5T3w3/s6ruSy/7Jac8m21cfTtf2xJEzStsm9hxNLvvXT16WbH/hcPp3ru+9LNk+7Z7ycaofveaa5LKT4r+T7WW8ZTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMqFInG631aZpRrxZbxu39R2XJsacqy99uILOPSvZHhMrDncYSv+Odiwpnz744Kz0ePP+LennnvXQjmS7DqdPJR2JUyYPL0hPyfzcovIx3wCHEn34AKmu9D2L03308/8p/TvZ88r052X+V8pPoQ0wfKS8uKrPUySOL1gVK9kbu0d9Y7xlN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0yMZYpmxcA3wHOAIaBZRHxZUkzgO8DC6lN2/yBiHiufaWmVY5Hn5w+D3icMSu9/I7dpW3Du8rbALRtV7q9Ylx21fTBs4fmlrYNTy4/bztA797yfnAAHUhPixyTK6YmnlU+rvvA/PTv7IU56dc9cWfFMQKryt/3uXfsTC47vHd/sn1qf/rzNFT1O01I9aM3Yyxb9kHg2oh4DXAh8DFJi4BPAysj4lxgZXHbzLpUZdgjYmtEPFpc3wesA+YBVwDLi4ctB97bphrNrAWO6392SQuBC4BVwJyI2Aq1PwhA+thHM+uoMYdd0hTgh8A1EbH3OJZbKmlA0sBR2vO/iJlVG1PYJfVRC/ptEfGj4u5tkuYW7XOB7aMtGxHLImJxRCzuI31yQjNrn8qwSxJwE7AuIr5Y13Q3cFVx/SogfapQM+uosZxK+iLgQ8BjklYX930GuAH4gaSPAM8A729LhWM0/MIL6QdUte/eU7GCxPS/p1bssQwOptsnpKcu1rT0UM/DM8tPc71vQbrrberm9LonTEh/RPafna5t96vKh4Iu+PHzyWUnbUuvu+dw+n3VvvLf+XBi6O2YqGI7WTV0PDWkOtozBXdl2CPiQaCsw7NLB6eb2Ug+gs4sEw67WSYcdrNMOOxmmXDYzTLhsJtlwlM2H5PoR69SNSSxmeGOAOxMD5E9ZcPG0rbpza2Z4Yr2yasr2hNtVScxTw9wrV5+UOXPoN6KU4dXtGtik0eDNvF5a5S37GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJtzPbievxJjyqDrHwFC6H3zwV9saqaijvGU3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhfnaz0VSd9/0E5C27WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpaJyrBLWiDpJ5LWSXpc0ieL+6+X9Kyk1cXl8vaXa2aNGstBNYPAtRHxqKSpwCOS7i3aboyIz7evPDNrlcqwR8RWYGtxfZ+kdcC8dhdmZq11XP+zS1oIXACsKu76uKQ1km6WNOpMQ5KWShqQNHCUJqdBMrOGjTnskqYAPwSuiYi9wDeBc4DzqW35vzDachGxLCIWR8TiPpqcH8vMGjamsEvqoxb02yLiRwARsS0ihiJiGPgWsKR9ZZpZs8bybbyAm4B1EfHFuvvn1j3sfcDa1pdnZq0ylm/jLwI+BDwmaXVx32eAKyWdT23m3I3AR9tQn5m1yFi+jX+Q0afKXtH6csysXXwEnVkmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEYhynppW0A3i67q6ZwM5xK+D4dGtt3VoXuLZGtbK2V0TErNEaxjXsL1m5NBARiztWQEK31tatdYFra9R41ebdeLNMOOxmmeh02Jd1eP0p3Vpbt9YFrq1R41JbR/9nN7Px0+ktu5mNE4fdLBMdCbukSyX9j6T1kj7diRrKSNoo6bFiGuqBDtdys6TtktbW3TdD0r2Snip+jjrHXodq64ppvBPTjHf0vev09Ofj/j+7pF7gSeAdwGbgYeDKiPjFuBZSQtJGYHFEdPwADEm/CewHvhMRryvu+ztgd0TcUPyhnB4Rf9YltV0P7O/0NN7FbEVz66cZB94LXE0H37tEXR9gHN63TmzZlwDrI2JDRBwB7gCu6EAdXS8iHgB2j7j7CmB5cX05tQ/LuCuprStExNaIeLS4vg84Ns14R9+7RF3johNhnwdsqru9me6a7z2AeyQ9Imlpp4sZxZyI2Aq1Dw8wu8P1jFQ5jfd4GjHNeNe8d41Mf96sToR9tKmkuqn/76KIeANwGfCxYnfVxmZM03iPl1GmGe8KjU5/3qxOhH0zsKDu9nxgSwfqGFVEbCl+bgfupPumot52bAbd4uf2Dtfz/7ppGu/RphmnC967Tk5/3omwPwycK+ksSROADwJ3d6COl5DUX3xxgqR+4J1031TUdwNXFdevAu7qYC0v0i3TeJdNM06H37uOT38eEeN+AS6n9o38L4E/70QNJXWdDfy8uDze6dqA26nt1h2ltkf0EeB0YCXwVPFzRhfV9l3gMWANtWDN7VBtb6X2r+EaYHVxubzT712irnF533y4rFkmfASdWSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpaJ/wPClCvq88za8QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVtUlEQVR4nO3de5CkVX3G8e8zszM7uzu77C6wF5blIjdBVNANGsAIIopEBau8QBIEy4h/aMQqgrGwUmKVqZBERWOilSUgIAQ0pRRUQgjIRUQjYUDkIggrruyNXfbGzl7n0r/80e+aYZj39Ox0z3TPnOdTNTU9/eu339M988zb3ec95ygiMLOpr63ZDTCzieGwm2XCYTfLhMNulgmH3SwTDrtZJhz2KUbSFZJuHOO2x0j6haReSZ9pdNsaTdKfSrqr2e2YLBz2BpF0qqSfSXpZ0mZJP5X0B81u1z76HHB/RMyOiH9sdmNqiYibIuJdzW7HZOGwN4CkOcB/AN8E5gNLgC8Be5rZrjE4FHiqrCipfQLbkiRpWh3bSlJ2f/vZPeBxcjRARNwcEYMRsSsi7oqIxwEkHSHpXkmbJG2UdJOkuXs3lrRS0mWSHpe0Q9I1khZK+q/iJfWPJM0rbnuYpJB0saS1ktZJurSsYZLeWrzi2Crpl5JOK7ndvcDpwD9J2i7paEnXSfq2pDsk7QBOl3SspPuL+3tK0vuH3Md1kr5VtHt78epmkaSvS9oi6RlJJybaGpI+I+n54nn6h72hlHRRcX9XSdoMXFFc9+CQ7U+W9HDx6uphSScPqd0v6W8k/RTYCbwm8fucmiLCX3V+AXOATcD1wHuAecPqRwJnAtOBA4EHgK8Pqa8Efg4spPqqYAPwKHBisc29wBeL2x4GBHAzMAt4PfAS8M6ifgVwY3F5SdGus6n+Yz+z+PnAksdxP/DnQ36+DngZOKXYfjawArgc6ATeAfQCxwy5/UbgzUBX0e7fAh8F2oEvA/clnscA7qP66ugQ4Nm97QEuAgaAvwCmATOK6x4s6vOBLcAFRf384uf9hzy2F4DXFfWOZv/dTPSXj+wNEBHbgFOp/rFeDbwk6XZJC4v6ioi4OyL2RMRLwNeAtw+7m29GxPqIWAP8BHgoIn4REXuAW6kGf6gvRcSOiHgC+A7VP+7h/gy4IyLuiIhKRNwN9FAN/2jdFhE/jYgKcALQDVwZEX0RcS/Vty9D931rRDwSEbuLdu+OiBsiYhD43giPY7i/i4jNEfEC8PVh9702Ir4ZEQMRsWvYdn8MPBcR3y3qNwPPAO8bcpvrIuKpot6/D8/BlOCwN0hEPB0RF0XEwcDxwEFU/1iRtEDSLZLWSNoG3AgcMOwu1g+5vGuEn7uH3X7VkMu/K/Y33KHAh4qX3FslbaX6T2nxPjy0ofs5CFhVBH/ovpcM+XlfH0dqf8Mf1yrKHVTcfqjhbUttP+U57OMgIp6h+pL2+OKqv6V61H9DRMyhesRVnbtZOuTyIcDaEW6zCvhuRMwd8jUrIq7ch/0MHRa5Flg67MOtQ4A1+3B/taQeV2qI5lqq/9yGGt62rId4OuwNIOm1ki6VdHDx81KqLz9/XtxkNrAd2CppCXBZA3b715JmSnod8DGqL5GHuxF4n6R3S2qX1CXptL3tHIOHgB3A5yR1FB/2vQ+4ZYz3N5LLJM0rnsNLGPlxjeQO4GhJfyJpmqSPAMdRfZthOOyN0gu8BXio+NT658CTwN5Pyb8EvInqh13/CfywAfv8MdUPy+4BvhIRrzq5JCJWAedQ/UDtJapH+ssY4+89IvqA91P9EHIj8C3go8UrmUa5DXgEeIzqc3XNKNu2CXgv1ed8E9VzBt4bERsb2LZJTcUnlTZJSDqM6ifcHREx0OTmNJSkAI6KiBXNbstU5CO7WSYcdrNM+GW8WSZ8ZDfLxJgHE4xFp6ZHF7MmcpfWwtTZkaxHX3YnudVtNzvoiz0jnsNRV9glnQV8g+p5z/9a62SNLmbxFp1Rzy5tslH5uUPTFi0prQEMrFmXvu/K4FhaNKU9FPeU1sb8Mr4Y7vjPVPtcjwPOl3TcWO/PzMZXPe/ZTwJWRMTzxckWt1A9gcPMWlA9YV/CKwcWrOaVgw4AKMZd90jq6Z90czmYTR31hH2kN2Ov6seLiOURsSwilnUwvY7dmVk96gn7al45QulgRh55ZWYtoJ6wPwwcJelwSZ3AecDtjWmWmTXamLveImJA0qeB/6ba9XZtRJROVmhj9+IlJyfru0/eXlo7ZtGG5LZ/ufTOZP31HTuT9f46hohPrzHn43lnfSxZr/zqufQO3DX3CnX1s0fEHVTHEZtZi/PpsmaZcNjNMuGwm2XCYTfLhMNulgmH3SwTEzqe3cZm0UPl/egAazvL113Y8e6tyW1nqW8sTfq9FwfTaz3urJSPWf/ccx9MbjvjyUZOWms+sptlwmE3y4TDbpYJh90sEw67WSYcdrNMuOttEmh/bnWyfmD34aW1lQeNtGz7/zuv55JkvfvYLcn67O/MSdbf8IVfltbWPL0wue2R/DZZt33jI7tZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgn3s08Cg5s2J+ud928rrR3TU2OJ7MrYp4IGqOxckaz/+MiTSmsL1lbq2rftGx/ZzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMuJ99CtC0xK/xoPSYcTZsSpZj5650fTC9LLIS3fibXq/ktumR8rav6gq7pJVALzAIDETEskY0yswarxFH9tMjYmMD7sfMxpHfs5tlot6wB3CXpEckXTzSDSRdLKlHUk8/e+rcnZmNVb0v40+JiLWSFgB3S3omIh4YeoOIWA4sB5ij+fWNujCzMavryB4Ra4vvG4BbgfIhTmbWVGMOu6RZkmbvvQy8C3iyUQ0zs8aq52X8QuBWSXvv598i4s6GtMpeQdOnp2/QUb4sMkr3ZaP0//tkH/4otCc+punaWKNt1lBj/k1GxPPAGxvYFjMbR+56M8uEw26WCYfdLBMOu1kmHHazTHiI62RQYxhp9PWV1voWdie31YHpqaY71/cm6+0vtifrAzPKazPX+4TKieQju1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCfezTwJRY1llJeZr7tsv/SvuPTjdT754S3oq6bau9PDbaTvLa12b0+cPWGP5yG6WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcL97JOA2tJTLquj/Ne49m3pbdv31OjDH6gk65WF85P17nXlfemVDk8lPZF8ZDfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuF+9kmgnmWTY3q6H73WiPKBOV3JetvugWS9a1P5nPYM1pg3vtZy0+F55/dFzSO7pGslbZD05JDr5ku6W9Jzxfd549tMM6vXaF7GXwecNey6zwP3RMRRwD3Fz2bWwmqGPSIeADYPu/oc4Pri8vXAuY1tlpk12lg/oFsYEesAiu8Lym4o6WJJPZJ6+tkzxt2ZWb3G/dP4iFgeEcsiYlkH6ckJzWz8jDXs6yUtBii+b2hck8xsPIw17LcDFxaXLwRua0xzzGy81OzAlXQzcBpwgKTVwBeBK4HvS/o48ALwofFsZO7UnV5DPdnf3N2f3HTmynQ/OjXG0g/O6khvnxgOrxr97JqWvu/oT/Th26vUDHtEnF9SOqPBbTGzceTTZc0y4bCbZcJhN8uEw26WCYfdLBMe4joFqKu8+0ybO5Pbdq9JTxU9beP2ZL0yO911t/PgmaW1thrTWM+s0eVY6U23PQbSw29z4yO7WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJ97NPBvPnJsuVGeV96dM3pf+fz318+PSCr6Td6anElNg3wNYjyv/Eujal+9lnLF2YrLetSpYZ3LIlfYPM+MhulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XC/eytoK09WY7OGlMqd5b/Gjt2pHc9ODu9Sk9MT7dtYE56+10Ly/vSp9foBu+fXz4WHmB675z0Hby8rbxWqbVY9dTjI7tZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgn3s7eAtlnp/uS23nRn+Zbjl5QX01OrU6nRj16rvmdu+hyAgdnl/dkb35oez75rQY056T+4KFk/9su7y9v14vrktlNRzSO7pGslbZD05JDrrpC0RtJjxdfZ49tMM6vXaF7GXwecNcL1V0XECcXXHY1tlpk1Ws2wR8QDQHruIjNrefV8QPdpSY8XL/Pnld1I0sWSeiT19JOez8zMxs9Yw/5t4AjgBGAd8NWyG0bE8ohYFhHLOkgPmjCz8TOmsEfE+ogYjIgKcDVwUmObZWaNNqawS1o85McPAE+W3dbMWkPNfnZJNwOnAQdIWg18EThN0glAACuBT45fE6c+zZyRvsGevmS5rb+8v7pjR7ovu31nf7Ku/nRHvaLG/W8v7yvvfkHJbbvXpcec989OH6t2nnBIaa3zzvz62WuGPSLOH+Hqa8ahLWY2jny6rFkmHHazTDjsZplw2M0y4bCbZcJDXCfAtIMTQ1CB2K87Wa+0p7uo+meV/8/ecVB62wUP7EzWa01jPW1r+TBSgEpn+VmTL79hILnt4PT0vjlye7K8qqt86PARd6bveirykd0sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4T72SfAqg8fmqx3vpweJhrp2ZzZ/Oby/uqu+el+8L4H9kvWB7vSO2/fnR6GOm3BrvL7Hkwfa6I93c/etyU91bT2Tw/fzY2P7GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJtzP3gBts2cn67sWpvvRO19O33/HzlrTNZf3he/u7Exuu+XodL1vbno8/MwX039Cpx/xWGnt0K70EoI3zViWrB8zb2uy/tyaBcl6bnxkN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0yMZolm5cCNwCLgAqwPCK+IWk+8D3gMKrLNn84IraMX1NbV60ll2euS/dVD6aHZVOpMX16ZVp5P7y2pTfu2pruw690pNu+Z790fUZ7+ZjyAzp6k9su2i9dP3rOhmT91ysOKi8q3W5qLEU9GY3myD4AXBoRxwJvBT4l6Tjg88A9EXEUcE/xs5m1qJphj4h1EfFocbkXeBpYApwDXF/c7Hrg3HFqo5k1wD69Z5d0GHAi8BCwMCLWQfUfAuBzE81a2KjDLqkb+AHw2YjYtg/bXSypR1JPP3vG0kYza4BRhV1SB9Wg3xQRPyyuXi9pcVFfDIz4aUlELI+IZRGxrIPyRf7MbHzVDLskAdcAT0fE14aUbgcuLC5fCNzW+OaZWaOMZojrKcAFwBOSHiuuuxy4Evi+pI8DLwAfGpcWTgLrzzkiWe99TXq65bY96f+5lZmVZD2ml9//tJnpZZF3HlC+rDFAx450F9R+v0lPVb2i98DS2ttmP5vc9oR5q5P1n7yYft61KzENtmoc5yL9O5uMaoY9Ih4Eyjolz2hsc8xsvPgMOrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJTyXdAJtPSi8NPGt++bLFADu3pce4nvW6p5L1udN2ltZmtvclt73hhdOT9f4D04+tfU/6rMhzFzxZWhss7dGt2r9jR7LephpLXXeXn2PQ1pVud2VX+nc2GYfA+shulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XC/ewN8ImTfpKsX3tXui/7gjPT23907kPJel+U/8+u1OjLnvnedD98b415rv/92dOS9dlt5f3Vc9vKzw8A6I/EeHRg45b0Utldc8qnQdP0dD+7+tLPSwyk5wloRT6ym2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcD97A5za/etk/erutyfrZ8xOj1c/oD3d39ye6Evvj/Sc82+c8btkvUPp+dNvfONJyfrbZqxM1lMW7fdosn7fgUcn6+1ticfeXuM4V2te+Ulo6j0iMxuRw26WCYfdLBMOu1kmHHazTDjsZplw2M0yUbOfXdJS4AZgEVABlkfENyRdAXwCeKm46eURccd4NbTZ2ufMKa29ZXp6bvV/eed3kvWXBsrvG6C/xvzp6b2nHdTem6zvjPSfSER6vPzvEo/tyI5tyW0rlfR9f/bwHyXrXSp/Zq468iPJbfnf9LkPk9FoTqoZAC6NiEclzQYekXR3UbsqIr4yfs0zs0apGfaIWAesKy73SnoaWDLeDTOzxtqn9+ySDgNOBPbOk/RpSY9LulbSvJJtLpbUI6mnn/JpgsxsfI067JK6gR8An42IbcC3gSOAE6ge+b860nYRsTwilkXEsg7S836Z2fgZVdgldVAN+k0R8UOAiFgfEYMRUQGuBtIjIsysqWqGXZKAa4CnI+JrQ65fPORmHwDKl+s0s6YbzafxpwAXAE9Ieqy47nLgfEknAAGsBD45Du1rGYO95V1UL1fS0w53Kv325R0zXkzW57XPStbrsV9beghrf6Tr1//hNcn68Z3ln9N01XheqPEZz3WbX5us339d+YvNhQ/9T3rXk3BJ5lpG82n8gzDigOkp26duNhX5DDqzTDjsZplw2M0y4bCbZcJhN8uEw26WCU8lPVqJftcLlp4yrrtu339+sh4HLyytaXd6AGzl+RfSO68xFXXNpYuVGKZad192et8L+Vmd9z+1+MhulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2VCMYHjdiW9BAxdI/gAYOOENWDftGrbWrVd4LaNVSPbdmhEHDhSYULD/qqdSz0RsaxpDUho1ba1arvAbRuriWqbX8abZcJhN8tEs8O+vMn7T2nVtrVqu8BtG6sJaVtT37Ob2cRp9pHdzCaIw26WiaaEXdJZkn4taYWkzzejDWUkrZT0hKTHJPU0uS3XStog6ckh182XdLek54rvI66x16S2XSFpTfHcPSbp7Ca1bamk+yQ9LekpSZcU1zf1uUu0a0Ketwl/zy6pHXgWOBNYDTwMnB8Rv5rQhpSQtBJYFhFNPwFD0h8B24EbIuL44rq/BzZHxJXFP8p5EfFXLdK2K4DtzV7Gu1itaPHQZcaBc4GLaOJzl2jXh5mA560ZR/aTgBUR8XxE9AG3AOc0oR0tLyIeADYPu/oc4Pri8vVU/1gmXEnbWkJErIuIR4vLvcDeZcab+twl2jUhmhH2JcCqIT+vprXWew/gLkmPSLq42Y0ZwcKIWAfVPx5gQZPbM1zNZbwn0rBlxlvmuRvL8uf1akbYR5qUrJX6/06JiDcB7wE+VbxctdEZ1TLeE2WEZcZbwliXP69XM8K+Glg65OeDgbVNaMeIImJt8X0DcCuttxT1+r0r6BbfNzS5Pb/XSst4j7TMOC3w3DVz+fNmhP1h4ChJh0vqBM4Dbm9CO15F0qzigxMkzQLeRestRX07cGFx+ULgtia25RVaZRnvsmXGafJz1/TlzyNiwr+As6l+Iv8b4AvNaENJu14D/LL4eqrZbQNupvqyrp/qK6KPA/sD9wDPFd/nt1Dbvgs8ATxONViLm9S2U6m+NXwceKz4OrvZz12iXRPyvPl0WbNM+Aw6s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwT/wcGk5a6rN48QgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZ90lEQVR4nO3de3Bc1X0H8O9X0kqyZVmy5Se2LPOyMSG8Yh4JtIEQwJAQyExeTkOgTeu0kzRJh5IypJmQaTuhaRKgaULHFGpegaQNBJI4AWKgBAjGsnH8wA422PglZFu2rJf12v31j71uhdD9rbza1a51vp8ZjVb727P37N396e7u755zaGYQkbGvpNAdEJHRoWQXCYSSXSQQSnaRQCjZRQKhZBcJhJJ9jCF5C8kHsmw7n+QrJNtJfinXfcs1kn9C8slC9+NYoWTPEZIXknyR5CGSB0i+QPKcQvfrKH0VwLNmVm1m/1rozmRiZg+a2WWF7sexQsmeAyQnAvgFgO8DmAxgFoBvAugpZL+y0ABgY1yQZOko9sVFsmwEbUkyuNd+cA84T+YBgJk9ZGZJMztsZk+a2ToAIHkiyadJtpDcT/JBkrVHGpPcTvJGkutIdpK8m+R0kr+K3lL/huSk6LZzSRrJJST3kGwieUNcx0ieH73jaCX5e5IXxdzuaQAXA/g3kh0k55FcRvJOkstJdgK4mOQCks9G97eR5EcG3Mcykj+M+t0RvbuZQfJ2kgdJbiZ5ltNXI/klkm9E++lfjiQlyeuj+7uN5AEAt0TXPT+g/ftIroreXa0i+b4BsWdJ/hPJFwB0ATjBeT7HJjPTzwh/AEwE0ALgXgBXAJg0KH4SgEsBVACYCuA5ALcPiG8H8BKA6Ui/K9gLYA2As6I2TwP4RnTbuQAMwEMAqgC8G8A+AB+M4rcAeCC6PCvq15VI/2O/NPp7aszjeBbAnw/4exmAQwAuiNpXA9gK4GYA5QA+AKAdwPwBt98P4D0AKqN+bwPwWQClAP4RwDPOfjQAzyD97mgOgNeO9AfA9QD6Afw1gDIA46Lrno/ikwEcBHBtFF8c/V034LHtAPCuKJ4o9OtmtH90ZM8BM2sDcCHSL9a7AOwj+TjJ6VF8q5k9ZWY9ZrYPwPcAvH/Q3XzfzJrNbDeA3wJYaWavmFkPgEeRTvyBvmlmnWa2HsB/Iv3iHuwzAJab2XIzS5nZUwAakU7+4XrMzF4wsxSAMwFMAHCrmfWa2dNIf3wZuO1HzWy1mXVH/e42s/vMLAngx0M8jsH+2cwOmNkOALcPuu89ZvZ9M+s3s8OD2n0IwBYzuz+KPwRgM4CrBtxmmZltjOJ9R7EPxgQle46Y2SYzu97MZgM4DcBxSL9YQXIayYdJ7ibZBuABAFMG3UXzgMuHh/h7wqDb7xxw+c1oe4M1APh49Ja7lWQr0v+UZh7FQxu4neMA7IwSf+C2Zw34+2gfh7e9wY9rJ+IdF91+oMF989qPeUr2PDCzzUi/pT0tuupbSB/1TzeziUgfcTnCzdQPuDwHwJ4hbrMTwP1mVjvgp8rMbj2K7QwcFrkHQP2gL7fmANh9FPeXife4vCGae5D+5zbQ4L4FPcRTyZ4DJE8heQPJ2dHf9Ui//Xwpukk1gA4ArSRnAbgxB5v9OsnxJN8F4E+Rfos82AMAriJ5OclSkpUkLzrSzyysBNAJ4KskE9GXfVcBeDjL+xvKjSQnRfvwyxj6cQ1lOYB5JD9NsozkJwGcivTHDIGSPVfaAZwHYGX0rfVLADYAOPIt+TcBnI30l12/BPBIDrb5P0h/WbYCwHfM7B0nl5jZTgBXI/2F2j6kj/Q3Isvn3cx6AXwE6S8h9wP4IYDPRu9kcuUxAKsBrEV6X909zL61APgw0vu8BelzBj5sZvtz2LdjGqNvKuUYQXIu0t9wJ8ysv8DdySmSBuBkM9ta6L6MRTqyiwRCyS4SCL2NFwmEjuwigch6MEE2yllhlagazU2KBKUbnei1niHP4RhRspNcBOAOpM97/o9MJ2tUogrn8ZKRbFJEHCttRWws67fx0XDHHyBdcz0VwGKSp2Z7fyKSXyP5zH4ugK1m9kZ0ssXDSJ/AISJFaCTJPgtvH1iwC28fdAAAiMZdN5Js7Dvm5nIQGTtGkuxDfQnwjjqemS01s4VmtjCBihFsTkRGYiTJvgtvH6E0G0OPvBKRIjCSZF8F4GSSx5MsB/ApAI/nplsikmtZl97MrJ/kFwE8gXTp7R4zi52sULL32lJ/ktonLr896/te9LPY6esAAJM3+MPupyze4cZ/cGL8CNVLf+Fve96XV7tx6x9T44DybkR1djNbjvQ4YhEpcjpdViQQSnaRQCjZRQKhZBcJhJJdJBBKdpFAjOpMNRM52UIc4tr5sfPc+M9vu82NTyod78Z7nMVNkhme344MC6O8lfTXcjypzD9eVDjrL/Yj6bZtTvpjKS576a/ceMOnXo0PpvxtH6tW2gq02YEhT47QkV0kEEp2kUAo2UUCoWQXCYSSXSQQSnaRQIzqVNJj1Wt3+UNQH7jkTje+J+kPI13Vk3DjM8q642Olfompgv7/+6kl/jDS/Sk3jM5UfPs1PfWxMQCoLe104w+e46/5eMOvPxEb61k2w2078eFVbvxYLN3pyC4SCCW7SCCU7CKBULKLBELJLhIIJbtIIJTsIoFQnX2Y2hafHxurn9Pstr1v/wVufEKpP5Rz1f4GN/5nDS/Exs6r3O62bSjzh7AmMwxDPZD0zwHYnayJjf20+Wy37c62SW789Cn+miRNL8+MjfWf6w/9bZ3nD0tu+MeX3XgxTnOtI7tIIJTsIoFQsosEQskuEgglu0gglOwigVCyiwRCdfZI6YKT3fjBBfFjzi+r2+W2Xb3fH7ddnmHMeQn9mvDv2k6Mjb122B+3vahmnRtvTda68fveep8bX73xhNhYeYtf4++b7Z9/8Gyr/5xVdMc/Z301/kD8v/nkz9z4HT3XuPHZ33rRjRfCiJKd5HYA7QCSAPrNbGEuOiUiuZeLI/vFZrY/B/cjInmkz+wigRhpshuAJ0muJrlkqBuQXEKykWRjH/zPYCKSPyN9G3+Bme0hOQ3AUyQ3m9lzA29gZksBLAXSa72NcHsikqURHdnNbE/0ey+ARwGcm4tOiUjuZZ3sJKtIVh+5DOAyABty1TERya2RvI2fDuBRkkfu50dm9uuc9CoPymZMd+PJynI3XtESX7P9+boz/LYT/O8qUm9McON9df7Y6N2vxI/bZoZ53X/ZfKEbL+3xP3nVbYifsx4ATjnUFhuzRIax9OP8sfIH549z4+Nb4vdbS9J/6R/sr3Lj86/Y4sYP3+Evs53q6nLj+ZB1spvZGwD8V7mIFA2V3kQCoWQXCYSSXSQQSnaRQCjZRQIxZoa4lk6d6sd/7C+LvHl1tRuvm78vNlbe4ZeA6G8a/dV+fayixi9vTZodX8Zp66p027ZN8/teUuEPv+2u89tbSXx8+uo+t23XVP/lmfKrpXjr/PjS3pkX/sFtO7+yyY0vmLnbjd9483VufO7f/86N54OO7CKBULKLBELJLhIIJbtIIJTsIoFQsosEQskuEogxU2d/7ab46ZQB4Nra59z4nD866MbfO3FrbOzdFX7NNWV+of2xtrPc+OSyTjde4oxj7Uj6dXav7XC8UH+SGz/QHT/Uc8cJk922DbP8JZnPmOzv97mV8fOgXl71qtu2tmRk++VrH/svN/7wbafHxpItB0a07Tg6sosEQskuEgglu0gglOwigVCyiwRCyS4SCCW7SCDGTJ194ha/lt3wIX/tybZ+vx49tTR+SuQZGZZc3pf0/6cuGOfXi1uT/rTG27unxMYqSvxpqJt7J7rxEvhTSb9x0K+VT50Qf47A8bPj5wgAgE/PetmN15V1uPEE4x97eYbzC7oyrF00PsMcBe+p3OnGf3TCFbExdvjnVVhPdsuo6cguEgglu0gglOwigVCyiwRCyS4SCCW7SCCU7CKBGDN19prtfj25NekvoTt//FtZbzsBv+haU+LX4etK/XpxVYlfV61kb2xsW880t+1J4/a68UNJf174eXV+rbyyNP55OdDjPyczEq1uvDNV4cZ39tbFxt5d7p930ZryU6PCqeEDwJY+fx0Dro9f8jnVG/98jkTGIzvJe0juJblhwHWTST5Fckv0e1JeeiciOTOct/HLACwadN1NAFaY2ckAVkR/i0gRy5jsZvYcgMHz5FwN4N7o8r0Arsltt0Qk17L9gm66mTUBQPQ79oMhySUkG0k29iG7c3pFZOTy/m28mS01s4VmtjAB/wsVEcmfbJO9meRMAIh++1/pikjBZZvsjwM4sibtdQAey013RCRfMtbZST4E4CIAU0juAvANALcC+AnJzwHYAeDj+ezkcIzf3OzGP1a9wY0fSsWv5Q0A7c5i4BNL/LHw3cn49dMBoK7Ej+9NTnDjjR3Hx8Y6+/2PTnPG+XOUjy/xa74fnfaKG9/aPT02Vlntr89em2G/zC3z5/r3tGd4vivpnxtRSf/cir99/DNu/MTul9x4PmRMdjNbHBO6JMd9EZE80umyIoFQsosEQskuEgglu0gglOwigRgzQ1xTb/nn9TzWscCNZxpm2mvxpZrTy/2lhdf0xk/1DAAbDte78S6n7AcAa1tmx8b2tfllu911NW58Ynm3G19fMsuNb9w3IzbW2++Xv96cEz9EFQAW1a5z4/fsujA29unjVrptZyX8st7U8nY3Pvs3fumuEHRkFwmEkl0kEEp2kUAo2UUCoWQXCYSSXSQQSnaRQIyZOrsl/SV46xMtbvzsCr9Ov7UvfmnjBP168Zae+FozkLmOvqXDnw66hPHrC5eV+fXevgxDPbv6/b6lSv1hqocOxU8XPWeGP7y2tizD0OAM50acPSl+2eSqDEN3Z2S47wT942Rbg59a/kTT+aEju0gglOwigVCyiwRCyS4SCCW7SCCU7CKBULKLBGLM1Nlxxjw3fH7lM248keH/XlvKmy7arzX3pBJuvATxdXIAqCrza8LHT4w/h+DQYX+a6ze2xU/1PBxM+Oc3VL4eP5V1xSx/2eOKEj9+XKlfh5+U6IyNnVLuTz2eoP+4Kujv186L47cNAFP/3Q3nhY7sIoFQsosEQskuEgglu0gglOwigVCyiwRCyS4SiDFTZ7fVG934nQfOceOLa1e58Td740cgd43zx8r3OXPOA8BJlX7Nd3xpjxtPOMsLl2eoVb9ZPdmNN7XFj+Mfjl7G19nn12Sqdftj8f1KOFCfiB8vPz7DfbebnxqHUv58+lXPVrnxQsh4ZCd5D8m9JDcMuO4WkrtJro1+rsxvN0VkpIbzNn4ZgEVDXH+bmZ0Z/SzPbbdEJNcyJruZPQfAnz9IRIreSL6g+yLJddHb/ElxNyK5hGQjycY++J89RSR/sk32OwGcCOBMAE0Avht3QzNbamYLzWxhAvFf1ohIfmWV7GbWbGZJM0sBuAvAubntlojkWlbJTnLmgD8/CmBD3G1FpDhkrLOTfAjARQCmkNwF4BsALiJ5JgADsB3A5/PXxWEyf0z48u++341Xf9Wvm+7v89c595w+bocbzzT/+UjWjs80lv60Kn9t+f11/uPecdiv0z9bH9/+AzWb3LaZVNCPv9h+UmysuvSw2/aFdn9+hExj7Wc8vNmNF2L19ozJbmaLh7j67jz0RUTySKfLigRCyS4SCCW7SCCU7CKBULKLBGLMDHHNZNKr7W582T1DjfX5f1WXxg/H/Mu6F922M8oOufF2d5pqYHdf7NnIAIDmvprY2LbDU9y2sysPuvGupL9kc1WpP831e07ZFhurLfGngvZKigDQnvKPVWv218fGrqt7wW2baRntCRmGHRcjHdlFAqFkFwmEkl0kEEp2kUAo2UUCoWQXCYSSXSQQwdTZS3ftc+M12/ypf5sOVsfGkv7oWnSbP8w005TJXh0d8GvCPSm/Vl1T5te6D/aPd+MlGZY2rk7E16OnZxi6eyjlz2xUXeJvuzcZ/9hbU+Pctjs6/XMbnmhe4MYbul5z44WgI7tIIJTsIoFQsosEQskuEgglu0gglOwigVCyiwQimDp7/1v+8sATnvDHu9dOOSM+eIG/7RmlnW58T398DR8AWvr8cwBKGF/onzPOH6/e1Fvrxrd31LnxM2p3ufG6RPxj35fya/ibe45z4w1l2dey/+H1q9x46a3+4z5+zVY3nuz1x/kXgo7sIoFQsosEQskuEgglu0gglOwigVCyiwRCyS4SiOEs2VwP4D4AMwCkACw1sztITgbwYwBzkV62+RNm5hd1i1iqyx/XPf1X8csut3/N343dGeY/r2SfG59W7p8DUFMa3/fqUn8p6kxj5V/ubXDj+3r9cwQO9MbX0t9btcVtm+kcAPinH6DlQPxy0b2r/PHsM19a68aTGV4vxWg4R/Z+ADeY2QIA5wP4AslTAdwEYIWZnQxgRfS3iBSpjMluZk1mtia63A5gE4BZAK4GcG90s3sBXJOnPopIDhzVZ3aScwGcBWAlgOlm1gSk/yEAmJbz3olIzgw72UlOAPBTAF8xs7ajaLeEZCPJxj4ce+tjiYwVw0p2kgmkE/1BM3skurqZ5MwoPhPA3qHamtlSM1toZgsT8CcQFJH8yZjsJAngbgCbzOx7A0KPA7guunwdgMdy3z0RyZXhDHG9AMC1ANaTXBtddzOAWwH8hOTnAOwA8PG89LBI9M6dGhtLGd2267vjlw4GgLoyf0rlrV3+1yHvmrA7NlZf2uK2rU/48fXVs9x4bYapqL3S26s9/n1nGtq7pT++tAYAfCt+Kezj7t/ktk0ePuzGj0UZk93MngcQ92q+JLfdEZF80Rl0IoFQsosEQskuEgglu0gglOwigVCyiwQimKmkR8oS8f8XWzJMifzMwVPc+Hk129x40+GJbryzP37J5o7q+FozAMyrbHLjuzpr3Xh5Sb8bf/n1ubGx6vn+6dPt/f4Zl+Xwl7quftM5/8H85Z5LJvg1/FS7P+y4GOnILhIIJbtIIJTsIoFQsosEQskuEgglu0gglOwigVCdfZj6x8dPB93YdYLbtrXXn7Z4fIlfb/7gVH/s9SO7zoqNzaw85LbtLk+48f6Ufzw46IxXB4Cy8vg6/PM7/P22YPpbbnx3/yQ3nmGWa1f75ae68ar/Xpn9nReIjuwigVCyiwRCyS4SCCW7SCCU7CKBULKLBELJLhII1dmHKXEoflnl+7ec67a1Vf6yyL+60l+yuTvp18J3NU3OKgYAzfP8sfLbt/lz1u+a4Ne6y9fH1+H7x5vbdu1hf779RVM3uvGpa+P3a6rTnxe+ssV/To5FOrKLBELJLhIIJbtIIJTsIoFQsosEQskuEgglu0ggMtbZSdYDuA/ADAApAEvN7A6StwD4CwD7opvebGbL89XRQivpj59nvGunP3B6jlPvBYB1NSe78b5J/tzsZa3xT+OEnf7a8Zt/u8CNNzT52z50vD+evaI1fr+Nb/b3S3+V//L8dunlbvyk3/w+NmZJf8758uYON54siZ/fAACQ8u+/EIZzUk0/gBvMbA3JagCrST4VxW4zs+/kr3sikisZk93MmgA0RZfbSW4CMCvfHROR3Dqqz+wk5wI4C8CROXm+SHIdyXtIDnneJMklJBtJNvbBn35JRPJn2MlOcgKAnwL4ipm1AbgTwIkAzkT6yP/dodqZ2VIzW2hmCxPw1+4SkfwZVrKTTCCd6A+a2SMAYGbNZpY0sxSAuwD4o0FEpKAyJjtJArgbwCYz+96A62cOuNlHAWzIffdEJFeG8238BQCuBbCe5NroupsBLCZ5JgADsB3A5/PQv6Kx76yq2FjJlG63beU+v8TUcI4/3fNl01914/WJA7Gxr6++2m2bSvqlOT7tL/ncfZG/dHHr/vhptJn0X341m/3yVkVlpxu3/vj9zlL/vjvm1brxqu3+R9JUV5cbL4ThfBv/PIChXhFjtqYuMhbpDDqRQCjZRQKhZBcJhJJdJBBKdpFAKNlFAkEzfzrfXJrIyXYeLxm17Y0a+rVqZNrHGYZLltb500F3LZwbGxu/Zb+/7YpyN8wOv16cqok//wAA2O1M57xtp9vWq5OnbzB6r91jxUpbgTY7MOQLUkd2kUAo2UUCoWQXCYSSXSQQSnaRQCjZRQKhZBcJxKjW2UnuA/DmgKumAMhQCC6YYu1bsfYLUN+ylcu+NZjZ1KECo5rs79g42WhmCwvWAUex9q1Y+wWob9karb7pbbxIIJTsIoEodLIvLfD2PcXat2LtF6C+ZWtU+lbQz+wiMnoKfWQXkVGiZBcJREGSneQikn8guZXkTYXoQxyS20muJ7mWZGOB+3IPyb0kNwy4bjLJp0huiX4PucZegfp2C8nd0b5bS/LKAvWtnuQzJDeR3Ejyy9H1Bd13Tr9GZb+N+md2kqUAXgNwKYBdAFYBWGxm/koIo4TkdgALzazgJ2CQ/GMAHQDuM7PTouu+DeCAmd0a/aOcZGZ/VyR9uwVAR6GX8Y5WK5o5cJlxANcAuB4F3HdOvz6BUdhvhTiynwtgq5m9YWa9AB4G4C9bEigzew7A4OVergZwb3T5XqRfLKMupm9FwcyazGxNdLkdwJFlxgu675x+jYpCJPssAAPnI9qF4lrv3QA8SXI1ySWF7swQpptZE5B+8QCYVuD+DJZxGe/RNGiZ8aLZd9ksfz5ShUj2oebHKqb63wVmdjaAKwB8IXq7KsMzrGW8R8sQy4wXhWyXPx+pQiT7LgD1A/6eDWBPAfoxJDPbE/3eC+BRFN9S1M1HVtCNfu8tcH/+TzEt4z3UMuMogn1XyOXPC5HsqwCcTPJ4kuUAPgXg8QL04x1IVkVfnIBkFYDLUHxLUT8O4Lro8nUAHitgX96mWJbxjltmHAXedwVf/tzMRv0HwJVIfyP/OoCvFaIPMf06AcDvo5+Nhe4bgIeQflvXh/Q7os8BqAOwAsCW6PfkIurb/QDWA1iHdGLNLFDfLkT6o+E6AGujnysLve+cfo3KftPpsiKB0Bl0IoFQsosEQskuEgglu0gglOwigVCyiwRCyS4SiP8F0e8DCmXSjNsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
